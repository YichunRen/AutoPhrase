0.9678856944	artificial intelligence
0.9673946549	hearing aid
0.9654248009	hearing aids
0.9649522348	impulse response
0.9617466968	parkinson's disease
0.9616047902	question answering
0.9585631425	lip reading
0.9583339734	cover song
0.9569955853	alzheimer's disease
0.9556177337	cocktail party
0.9533435220	deep learning
0.9526306294	gradient descent
0.9525997295	neural networks
0.9515476848	receptive field
0.9514906985	beam search
0.9507760130	vocal tract
0.9501078781	false alarm
0.9476893213	wall street journal
0.9467626430	support vector machines
0.9462374009	sentiment analysis
0.9447874688	fourier transform
0.9443548873	hidden markov model
0.9429773777	mental health
0.9420270832	random forest
0.9414325855	hidden markov models
0.9413579313	predictive coding
0.9404936335	maximum likelihood
0.9386901124	nist sre
0.9378830210	latent variable
0.9378520025	reinforcement learning
0.9377649900	fundamental frequency
0.9372142554	monte carlo
0.9366642098	correlation coefficient
0.9362159709	kalman filter
0.9357458305	distortionless response
0.9350960757	weakly labelled
0.9349776510	machine translation
0.9347316962	support vector machine
0.9335308284	weakly labeled
0.9324195749	packet loss
0.9313469188	microphone arrays
0.9311788031	impulse responses
0.9304047264	mandarin chinese
0.9298688468	anomaly detection
0.9298451494	neural network
0.9286304636	logistic regression
0.9285657515	mutual information
0.9278553992	loss function
0.9277705880	ultrasound tongue
0.9269429728	feature extraction
0.9269295789	complex valued
0.9267762723	dimensionality reduction
0.9267359022	covariance matrix
0.9264733825	lombard effect
0.9263898664	speech recognition
0.9258501216	wavelet transform
0.9253225030	kalman filtering
0.9247910844	automatic speech recognition
0.9244994841	visually grounded
0.9241180597	high fidelity
0.9240589708	knowledge distillation
0.9238174084	speech synthesis
0.9232718775	vector quantization
0.9213452079	musical instrument
0.9197858265	code switching
0.9195018191	sheet music
0.9191900943	polyphonic music
0.9189430023	doa estimation
0.9188987724	black box
0.9187785866	machine learning
0.9183863289	logical access
0.9176044564	latent spaces
0.9175909178	ad hoc
0.9173230090	gated recurrent unit
0.9168351546	alzheimer's dementia
0.9167334938	filter bank
0.9163038901	weakly supervised
0.9162392298	nonnegative matrix factorization
0.9155473217	steady state
0.9148082231	voice cloning
0.9147761130	speaker recognition
0.9145950330	variational autoencoders
0.9142427945	room impulse response
0.9135172573	inductive bias
0.9133317566	open sourced
0.9130701771	recurrent neural network
0.9118760819	small footprint
0.9114997316	natural language processing
0.9114044136	adversarial examples
0.9110482637	principal component analysis
0.9100000176	deep neural networks
0.9096486589	transfer function
0.9084638916	receptive fields
0.9083107273	spoofing countermeasures
0.9075184144	linear prediction
0.9071926335	speech enhancement
0.9071756033	cross entropy
0.9070576712	cross correlation
0.9070171168	wiener filter
0.9064957747	open source
0.9061646157	signal processing
0.9054621262	multitask learning
0.9053818112	variational autoencoder
0.9052013416	glottal closure instants
0.9050926775	fearless steps
0.9044229091	speaker verification
0.9040016674	equal error rate
0.9036244121	unit discovery
0.9032710329	connectionist temporal
0.9026520166	dilated convolutions
0.9023650227	pattern recognition
0.9019984501	hyper parameter
0.9014080730	feed forward
0.9013244161	chord progression
0.9012326677	slot filling
0.9007464594	emotion recognition
0.9001960614	bit rate
0.8998860384	noise pollution
0.8996156358	keyword spotting
0.8994609414	social media
0.8994263712	recurrent neural networks
0.8994183199	tonal tension
0.8992773249	auto encoders
0.8990393328	gaussian process
0.8989244191	dinner party
0.8988990158	spectral subtraction
0.8986222144	linear discriminant analysis
0.8982093629	nearest neighbor
0.8978422783	latent space
0.8978094780	microphone array
0.8973514743	parallel wavegan
0.8972469121	classical music
0.8972255750	voice conversion
0.8971967392	character error rate
0.8971253491	shallow fusion
0.8967842376	adversarial attacks
0.8966693833	knowledge transfer
0.8965872904	replay attack
0.8958280423	room impulse responses
0.8955886710	upper bound
0.8951728645	raw waveform
0.8946949057	blind source separation
0.8945816986	permutation invariant training
0.8940429077	symbolic music
0.8940228145	probabilistic linear discriminant analysis
0.8930554926	transfer learning
0.8928789416	connectionist temporal classification
0.8928371597	depthwise separable
0.8922703231	hand crafted
0.8920832598	wake word
0.8917332828	physical access
0.8909830412	indian languages
0.8906276850	vector quantized
0.8904614225	deep attractor
0.8897552000	convolutional neural networks
0.8896861334	wavenet vocoder
0.8894032114	supervised learning
0.8891853276	bi directional
0.8889597736	acoustic scenes
0.8888775752	bandwidth extension
0.8884690315	active learning
0.8884476564	music genre
0.8871767988	subword modeling
0.8871213726	urban sound tagging
0.8868690547	subword units
0.8866717583	compares favorably
0.8865089610	hearing impaired
0.8864490099	percentage points
0.8860140572	short time fourier transform
0.8860115170	respiratory diseases
0.8859366678	binary mask
0.8857331565	von mises
0.8854567129	embedded devices
0.8845068807	data augmentation
0.8844948559	convolutional neural network
0.8844655384	sound events
0.8841589547	white box
0.8841251787	artificial neural networks
0.8840130289	gaussian distribution
0.8838662889	pop music
0.8834919402	griffin lim
0.8831169673	dialect identification
0.8830665711	dilated convolution
0.8828512144	noise reduction
0.8824201441	listening tests
0.8818173293	drum transcription
0.8817515632	computationally efficient
0.8812704559	optimal transport
0.8801584168	cycle consistency
0.8801266547	late fusion
0.8796114271	matrix factorization
0.8795367654	spoken term
0.8793141431	vocal fold
0.8789200743	contrastive learning
0.8785372461	auto encoder
0.8782826321	source separation
0.8782002685	speaking style
0.8781747816	named entities
0.8780846600	anti spoofing
0.8779001816	code switched
0.8774654504	triplet loss
0.8773977748	style transfer
0.8773552276	word error rates
0.8771026640	mel frequency cepstral coefficients
0.8770693629	information retrieval
0.8769688766	real valued
0.8768807176	presentation attack detection
0.8768666652	frequency domain
0.8764993835	fine tune
0.8759037227	spoken language understanding
0.8755256439	low latency
0.8750611965	closed set
0.8750118573	generative adversarial
0.8748627193	false trigger
0.8748618054	cycle consistent
0.8747445950	rnn transducer
0.8743340180	deep neural network
0.8741081958	unsupervised learning
0.8739486822	angular margin
0.8736178836	error rate
0.8732613284	generative adversarial networks
0.8730678564	raw waveforms
0.8729438400	mel spectrograms
0.8728563640	residual connections
0.8724750024	sound event detection
0.8719573010	prosody transfer
0.8715737548	ground truth
0.8715629377	facial animation
0.8711449128	gaussian mixture
0.8699385636	attention mechanism
0.8696947421	mel frequency cepstral
0.8696371643	replay attacks
0.8694315192	squeeze excitation
0.8693526634	computational complexity
0.8693479654	adversarial perturbations
0.8688248742	feature selection
0.8685980139	post processing
0.8681959633	high quality
0.8681041498	music theory
0.8679174199	image processing
0.8672826850	ablation studies
0.8671715580	tcd timit
0.8671226833	error rates
0.8670100389	privacy preserving
0.8668372943	variable length
0.8665093248	student teacher
0.8661760563	cosine distance
0.8653321788	f1 score
0.8651397752	close talk
0.8649243022	mel spectrogram
0.8646538699	human perception
0.8646023150	bottleneck features
0.8633052768	large vocabulary
0.8630786587	wind noise
0.8630307987	narrow band
0.8629044970	speech coding
0.8626941241	expectation maximization
0.8623760486	music composition
0.8622331245	open set
0.8622313496	music information retrieval
0.8621055050	singing voice separation
0.8619476371	design choices
0.8619384776	latent variables
0.8615682379	conv tasnet
0.8614508314	closed form
0.8614454775	music tagging
0.8614330085	speaker diarisation
0.8611972146	midi files
0.8611929736	heart sound
0.8611384752	glottal flow
0.8611103683	wall street
0.8608383393	sound source localization
0.8605200142	lexical stress
0.8604909416	fine tuned
0.8601263982	discriminant analysis
0.8600716595	mel frequency
0.8598575309	adversarial attack
0.8598364628	event detection
0.8598205576	short term memory
0.8585579770	valence arousal
0.8584202945	frame rate
0.8582459462	log mel
0.8580988860	quasi periodic
0.8573435516	cepstral coefficients
0.8571408820	linear regression
0.8569577620	short term
0.8568281390	reverberant environments
0.8565831010	word error rate
0.8564363190	anomalous sound
0.8555317268	long short term memory
0.8552440159	group delay
0.8551833297	language identification
0.8549722402	auto regressive
0.8549677678	binary classification
0.8547470916	fine grained
0.8543240773	feature extractors
0.8542504530	metric learning
0.8541415939	recurrent neural
0.8539904117	multi talker
0.8539466705	activity detection
0.8526038942	principal component
0.8521594839	cross lingual
0.8521090222	teacher student
0.8520994220	spectral envelope
0.8520180892	speaker diarization
0.8514468935	meta learning
0.8512266903	mini batch
0.8510149802	spoken language
0.8507874012	average pooling
0.8502604973	natural sounding
0.8502343655	quality assessment
0.8501164048	sound event
0.8499995636	bidirectional long short term memory
0.8493688244	spectral clustering
0.8493093761	low resource
0.8492181381	general purpose
0.8490092467	disentangled representations
0.8489707748	minimum variance
0.8489590505	relevance weighting
0.8486766680	popular music
0.8481393998	memory footprint
0.8478918576	giantmidi piano
0.8477889087	denoising autoencoder
0.8476378211	reconstruction loss
0.8475153757	wsj0 2mix
0.8469661694	recurrent neural network transducer
0.8466730275	noise suppression
0.8461965562	spoofing detection
0.8457991067	attention mechanisms
0.8457417376	singing voice
0.8454461013	lf mmi
0.8450601873	single channel
0.8447827988	speaker clustering
0.8445510469	spatio temporal
0.8443141356	mobile devices
0.8428976207	instance normalization
0.8426386077	transformer transducer
0.8426288841	monaural speech enhancement
0.8425436424	average precision
0.8422478590	speech separation
0.8422074436	temporal resolution
0.8420695170	long term
0.8418319485	frequency bands
0.8414368989	attention weights
0.8413653688	batch normalization
0.8413096708	voice trigger detection
0.8411338981	speaker identification
0.8402991652	lip movements
0.8398510328	youtube videos
0.8393565386	vq vae
0.8392986612	semi supervised
0.8392945277	replay spoofing
0.8391021580	modeling units
0.8390520052	long range
0.8390490441	fine tuning
0.8389349313	locata challenge
0.8388431845	perceptual quality
0.8385293963	pattern discovery
0.8381985630	data set
0.8380083798	voice activity detection
0.8378761994	mask estimation
0.8377375684	inverse filtering
0.8376140172	spectro temporal
0.8375939452	kullback leibler
0.8372536520	scene analysis
0.8368245746	domain adaptation
0.8367919904	multi view
0.8364682975	dilated convolutional
0.8361695148	dihard ii
0.8361304944	meeting transcription
0.8361227228	siamese networks
0.8360891379	max pooling
0.8359710398	main contribution
0.8355268363	mispronunciation detection
0.8355019653	cosine similarity
0.8352518331	neural architecture search
0.8350210175	scene classification
0.8346889897	encoding layer
0.8345337430	glottal source
0.8337465846	visual cues
0.8333140820	spoken word
0.8331588701	punctuation prediction
0.8331461829	neural vocoders
0.8328135219	cepstral coefficient
0.8327819230	natural language
0.8326424133	weighted prediction error
0.8326229489	voice leading
0.8325593523	perceived quality
0.8324007402	deep clustering
0.8322621594	variance distortionless
0.8320754907	source localization
0.8320631984	acoustic scene classification
0.8311859535	srp phat
0.8308248609	glottal closure
0.8307744902	voice command
0.8304109227	texture synthesis
0.8300187624	network architecture
0.8297054498	representation learning
0.8295519375	test set
0.8290786988	frame level
0.8288458108	maximum mutual information
0.8288447685	large margin
0.8283126372	pitch dependent dilated convolution
0.8282336485	source position
0.8276336629	bi modal
0.8274640723	unsupervised domain adaptation
0.8271271213	music generation
0.8270593761	equal error
0.8268783353	transfer functions
0.8263876509	environmental sound
0.8258539822	low resourced
0.8256178839	event classification
0.8249462664	environmental sounds
0.8243188041	length normalization
0.8242803216	ami meeting
0.8239755306	user experience
0.8239336736	transformer xl
0.8236153771	super resolution
0.8235846879	edge devices
0.8231333188	labelled data
0.8231179103	singing voice synthesis
0.8230939107	fully convolutional
0.8227979472	normal hearing
0.8227838033	statistical parametric speech synthesis
0.8226588504	spectral mapping
0.8226094724	small footprint keyword spotting
0.8222476537	million song
0.8220640940	machine listening
0.8217035425	dual path
0.8211499352	feature map
0.8207697385	contextual information
0.8206766416	fully connected
0.8206381842	signal to noise ratio
0.8205389496	feature sets
0.8191998276	cycle consistent generative adversarial
0.8189418546	environmental sound classification
0.8188416584	large scale
0.8183281449	singing synthesis
0.8183182396	instantaneous frequency
0.8180700545	music genres
0.8177362804	si sdr
0.8177077884	spatial covariance
0.8175779215	sensor networks
0.8174137546	speaker embeddings
0.8173857212	domain mismatch
0.8170195016	training criteria
0.8169793636	late reverberation
0.8168533324	intent detection
0.8157092932	crowd sourced
0.8155918631	vaw gan
0.8154544422	acoustic events
0.8153164409	cross modal
0.8152547947	skip connections
0.8151205717	cocktail party problem
0.8150548657	background noise
0.8148596023	gated recurrent
0.8141301809	low rank
0.8140605753	convolutional beamformer
0.8137870554	feature vectors
0.8137076780	convolutional recurrent neural network
0.8135927705	e2e asr
0.8135304114	chord recognition
0.8133873787	softmax loss
0.8131349715	language pairs
0.8131114463	singular value decomposition
0.8130973438	adversarial training
0.8129933890	closely related
0.8128925841	convolutional neural
0.8128153932	generative adversarial network
0.8127706083	neural vocoder
0.8127276676	source code
0.8120018720	melody generation
0.8118443351	speech denoising
0.8118258968	multi resolution
0.8115930255	power consumption
0.8110353560	test sets
0.8108476305	noisy labels
0.8107037468	street journal
0.8105899465	waveform generation
0.8105450818	amr wb
0.8104891370	objective measure
0.8104201489	gaussian mixture model
0.8102002947	acoustic echo cancellation
0.8099157662	recurrent unit
0.8092615355	distant speech recognition
0.8091149030	independent vector analysis
0.8089036507	multi modal
0.8086356557	pitch controllability
0.8083420357	empirical study
0.8079395622	power spectral density
0.8074023079	overlap add
0.8072026641	convolutional networks
0.8067946142	speaker extraction
0.8067645439	multiple instance learning
0.8060152774	onset detection
0.8059499264	si snr
0.8054541811	minimum word error rate
0.8054132019	real life
0.8052471561	encoder decoder
0.8051353631	pitch synchronous
0.8050457800	white noise
0.8047185588	streaming asr
0.8046458834	voice search
0.8039749815	reconstruction error
0.8039482570	sound classes
0.8038245616	speaking styles
0.8038160171	context aware
0.8037805614	generative models
0.8036979272	low dimensional
0.8030563964	forced alignment
0.8025242643	complex cepstrum
0.8024497091	cycle gan
0.8021544322	variational auto encoder
0.8017942630	future context
0.8014881153	cough detection
0.8013269482	sound localization
0.8010771273	automatic speaker verification
0.8009502068	jointly trained
0.8006466818	target language
0.8002168009	real world
0.7999729408	root mean square
0.7997646386	diffuse noise
0.7997456847	future research
0.7996394838	speaker embedding
0.7994755686	acoustic modeling
0.7994154947	information bottleneck
0.7992845178	previous works
0.7989728878	scene aware
0.7988587465	voice assistants
0.7978292674	low snr
0.7974233675	phase reconstruction
0.7965498007	long sequences
0.7965191330	subjective evaluation
0.7961510506	dysarthric speech
0.7961065650	aligned lyrics
0.7959023831	margin softmax
0.7958621374	dnn hmm
0.7955355474	lattice free
0.7954073750	linguistic features
0.7953870478	high resolution
0.7951214391	overlapped speech
0.7950371523	piano transcription
0.7950001213	speaker adaptation
0.7949911931	sound synthesis
0.7948043857	deep generative
0.7947883459	recording conditions
0.7946728179	drum sounds
0.7944548809	adversarial learning
0.7944283875	hybrid ctc attention
0.7939417971	generative modeling
0.7939398269	spoofing attacks
0.7936795136	phase aware
0.7936753586	generative model
0.7934385666	universal sound separation
0.7933951693	machine speech chain
0.7932396607	low rank matrix analysis
0.7927866641	noise type
0.7927494672	diarization error rate
0.7924137551	generalization ability
0.7921691981	singing voice conversion
0.7921371459	speaker's identity
0.7919749030	cost function
0.7915409582	post filter
0.7914821651	musical score
0.7914412364	sound propagation
0.7914219811	high level
0.7911470842	contrastive loss
0.7910619765	synthesized speech
0.7908597782	synthetic data
0.7907944602	speech processing
0.7907050319	voxceleb speaker recognition challenge
0.7904634140	noisy environments
0.7904161502	acoustic scene
0.7903284898	speech commands
0.7902328576	data scarcity
0.7898034403	square error
0.7897154665	imagined speech
0.7897128486	autoregressive transformer
0.7896301821	pitch contour
0.7895399080	text independent speaker verification
0.7894968776	adversarial networks
0.7892886833	dutch
0.7887989562	short utterance
0.7887429502	hidden markov
0.7886214911	sentence level
0.7883715891	playing techniques
0.7880788226	clean speech
0.7879939975	support vector
0.7879008611	context dependent
0.7876922345	piano music
0.7876525939	dynamic time warping
0.7875828218	mel filterbank
0.7873675759	objective functions
0.7870551098	itu t
0.7870266785	polyphonic sound event detection
0.7868860217	sound sources
0.7863273414	monaural speech separation
0.7858253681	bidirectional lstm
0.7856709402	sound source
0.7852071416	style tokens
0.7851400709	latent representation
0.7849074986	speaker attributed
0.7845054617	computationally expensive
0.7844246052	multi scale
0.7842549807	long form
0.7842350761	parameter sharing
0.7836602035	scattering transform
0.7835979940	environmental noise
0.7833368760	joint optimization
0.7826496358	emotional content
0.7823130091	performance improvement
0.7819733257	domain invariant
0.7814178581	spoken words
0.7811930378	cloud based
0.7803129257	bengali
0.7799670612	speech translation
0.7792033541	multi genre
0.7790535873	feature extractor
0.7787309119	speaker counting
0.7783912127	microsoft
0.7775502684	pre trained
0.7774843024	close talking
0.7773120032	native language
0.7771392930	domain specific
0.7766375821	target domain
0.7765438527	spectral features
0.7763127589	multi channel
0.7761641998	spectral density
0.7755504247	distributed microphones
0.7755493204	pitch tracking
0.7754041634	bi lstm
0.7749555318	short time objective intelligibility
0.7747613166	log likelihood
0.7745957587	recurrent units
0.7743774996	objective intelligibility
0.7742962938	low complexity
0.7741101804	noise robust
0.7740148820	sequence modeling
0.7738970039	f0 contour
0.7736173944	echo cancellation
0.7735273531	sound field
0.7726268746	fully supervised
0.7726238368	detection cost function
0.7723885015	loss functions
0.7723770318	depression detection
0.7722748646	spontaneous speech
0.7721573316	hyper parameters
0.7721253546	cross corpus
0.7719553434	wave u net
0.7716776428	active speaker detection
0.7714066304	embedding spaces
0.7712072395	noise robustness
0.7707795653	synthetic speech
0.7703185165	multi head
0.7701888889	persian
0.7701582252	feature representation
0.7701161160	short duration
0.7700905264	soft labels
0.7699226789	emotional voice conversion
0.7694970284	neural machine translation
0.7693429919	minimum phase
0.7692474259	music genre classification
0.7687530656	higher order
0.7686916332	direct path
0.7681604545	accented speech
0.7679851490	acoustic environments
0.7679545935	deep noise suppression
0.7676741620	multi band
0.7675960956	signal to distortion ratio
0.7674562734	evaluation metrics
0.7669534445	word embeddings
0.7666722908	latent representations
0.7664889991	mask based
0.7662691565	target speaker
0.7659815306	ctc attention
0.7658480676	reverb challenge
0.7656787523	label noise
0.7652831003	danish
0.7647970008	multi head attention
0.7647802477	labeled data
0.7646158604	modeling ability
0.7642151170	bayes
0.7641861906	frequency bin
0.7639230948	multi stream
0.7635391888	visual features
0.7634744295	experimental results
0.7634735192	ted
0.7634246947	hindi
0.7633782498	data sets
0.7628160061	` `
0.7626621468	speech recognizers
0.7625492738	opinion score
0.7615812980	comparative study
0.7615740483	cross validation
0.7613791815	pre training
0.7613202446	single microphone
0.7612567893	visual modality
0.7609443786	viterbi
0.7606933392	speech intelligibility
0.7606247370	amazon
0.7604326094	gaussian mixture models
0.7601571477	convolution neural network
0.7600455600	phonetic structures
0.7600335503	computational resources
0.7599131079	arabic
0.7597072265	speech command
0.7592762484	nonnegative matrix
0.7592580007	untranscribed speech
0.7588633050	voice quality
0.7584686560	multiple speakers
0.7584594319	text dependent speaker verification
0.7583410344	subjective tests
0.7573784983	children's speech
0.7566826645	training strategy
0.7560450377	music transcription
0.7557641377	interfering sources
0.7555622934	decision making
0.7555122628	language modeling
0.7550208985	feature maps
0.7547084231	cycle consistent adversarial
0.7544320281	cross domain
0.7543660738	intent classification
0.7542467351	melody extraction
0.7530489790	teacher student learning
0.7528706486	cough sounds
0.7528341038	librispeech test clean
0.7528212050	convolutional recurrent neural networks
0.7526007421	room impulse
0.7525421109	singing voice detection
0.7524587943	cnn blstm
0.7521889113	frequency band
0.7520997285	dj
0.7520156494	speaker dependent
0.7519423534	pre processing
0.7518588572	euclidean
0.7517825867	weak labels
0.7514973121	target speakers
0.7514488863	pitch contours
0.7512483955	spatial filtering
0.7510931053	speech production
0.7510536160	acoustic event detection
0.7510050786	utterance level
0.7509430966	speaker identity
0.7508073641	long term dependencies
0.7500177079	language model
0.7489691796	visual information
0.7488844735	training data
0.7485553407	speaker identities
0.7482791668	early detection
0.7479591429	news
0.7478804873	acoustic cues
0.7478567721	multi lingual
0.7478348357	eeg signals
0.7477420330	unseen speakers
0.7476307436	feature space
0.7471529661	continuous speech recognition
0.7467221857	takes advantage
0.7465351247	mask ctc
0.7463812315	subjective evaluations
0.7463049618	residual noise
0.7459978516	external lm
0.7458575378	kazakh
0.7452330567	speaker independent
0.7451673897	bach
0.7447257823	fixed length
0.7446911792	north
0.7437260811	nvidia
0.7436414987	phoneme level
0.7428800933	rt
0.7424396250	russian
0.7423306556	objective evaluation
0.7422433856	magnitude spectrogram
0.7421347502	acceleration
0.7417343343	permutation problem
0.7416193952	error reduction
0.7415812504	f1 scores
0.7413830525	secondary task
0.7413017723	multi speaker
0.7412944809	masked conditional
0.7412407850	auxiliary tasks
0.7405198826	recurrent networks
0.7398967293	primary task
0.7393364090	segment level
0.7393338053	jazz
0.7393323757	magnitude spectrum
0.7390834029	dc
0.7390017526	probabilistic linear discriminant
0.7389526074	non negative matrix factorization
0.7387276741	speaker similarity
0.7387089087	low power
0.7380026898	portuguese
0.7379575616	clip level
0.7379350101	french
0.7378453756	deep learning based
0.7374982493	fixed dimensional
0.7368591143	acoustic conditions
0.7366617773	griffin lim algorithm
0.7365710007	ablation study
0.7361511015	low resource languages
0.7360031834	downstream tasks
0.7356306201	human voice
0.7354228972	voice controlled
0.7353540575	singer identification
0.7352369462	attention heads
0.7351573154	residual network
0.7346411681	neural tts
0.7344809673	vector space
0.7341959873	embodied
0.7340152475	ctc loss
0.7339347897	pseudo labels
0.7339023945	dnn based
0.7337188772	temporal structure
0.7334642080	acoustic models
0.7332405956	previously proposed
0.7316856034	permutation invariant
0.7315044779	sleep
0.7314817934	mfcc features
0.7313955614	librispeech corpus
0.7313455801	multichannel speech enhancement
0.7311318067	american
0.7308701507	building blocks
0.7304282456	short segments
0.7302295658	results suggest
0.7296990521	mel spectrum
0.7295951430	class label
0.7295702427	phase estimation
0.7295356153	significant progress
0.7294730337	hmm based
0.7294697947	multilingual bottleneck
0.7294471198	embedding space
0.7289365031	pressure
0.7282484691	research area
0.7282207771	hybrid dnn hmm
0.7278421933	acoustic features
0.7273256580	auxiliary task
0.7271676182	speech signals
0.7271110549	mixture model
0.7268051650	valence and arousal
0.7267806955	graphics
0.7265870803	prosodic features
0.7265328056	training procedure
0.7264517349	competitive results
0.7260523543	score level fusion
0.7259444030	vocal melody
0.7257997937	listening test
0.7255204928	speech quality
0.7254986892	language models
0.7253654934	surveillance
0.7251423053	low bit
0.7248249343	competitive performance
0.7247870372	deep reinforcement learning
0.7244976116	normal speech
0.7244847548	violin
0.7240070936	high degree
0.7239073169	carnatic
0.7238915129	training set
0.7234445096	inference speed
0.7232460285	multi task learning
0.7230921913	pooling layer
0.7230347048	international
0.7229170969	word embedding
0.7228832531	speech perception
0.7227205015	speech emotion recognition
0.7226851540	comparable performance
0.7225691224	human robot
0.7225309375	infant
0.7222477007	unlabeled data
0.7219359300	multiple instance
0.7217748332	perceptron
0.7217602631	native english
0.7213919342	teacher model
0.7212769138	python
0.7211275480	lombard
0.7210442704	recent advances
0.7207218165	transformer based
0.7205410803	radio
0.7204938532	mean squared error
0.7203283596	forensics
0.7200027308	phonetic information
0.7199676078	national
0.7196970680	dihard
0.7196200335	phone recognition
0.7185383337	success rate
0.7181784321	phoneme recognition
0.7172140981	acoustic event
0.7171925868	human listeners
0.7171274968	double
0.7169568069	low quality
0.7169490795	previous approaches
0.7168886549	attention based
0.7164308364	university
0.7163729007	eeg features
0.7163206880	conference
0.7162543733	film
0.7160875011	tree
0.7158013533	iemocap dataset
0.7156196345	frequency resolution
0.7153515408	mandarin english
0.7152693205	significantly outperforms
0.7152625117	multi frame
0.7152239793	wavegan
0.7151022631	validation set
0.7144868042	classification accuracy
0.7144239026	bass
0.7141789367	additive noise
0.7141331215	phat
0.7141132846	alzheimer's
0.7138191966	relative wer reduction
0.7136977333	unmanned
0.7134826047	acoustic model
0.7134626530	emotion classification
0.7131425122	n gram
0.7130309575	generalization capabilities
0.7129502460	human machine
0.7128474312	multi instrument
0.7127967864	noise types
0.7125311882	environmental sound synthesis
0.7124987778	extensive experiments
0.7122922819	manually labeled
0.7122894948	speaker embedding extraction
0.7122730997	indian
0.7122544958	san
0.7121602817	singing voices
0.7120133482	model agnostic
0.7119550705	cultural
0.7119550705	studio
0.7119550705	management
0.7116256355	temporal convolutional networks
0.7113814362	velocity
0.7112969751	tut
0.7110397181	generated music
0.7110267901	frequency cepstral coefficients
0.7105495881	controllers
0.7103340952	mean opinion score
0.7100842229	optical
0.7100571280	emotional speech
0.7100040092	hidden layers
0.7096196942	pre defined
0.7095278074	fully convolutional network
0.7091589048	forensic
0.7091370791	fewer parameters
0.7089707609	electronic
0.7087861796	gaussian noise
0.7085664499	input features
0.7085385887	multi stage
0.7082193638	flow based
0.7081707946	spiking
0.7080203537	voxceleb1
0.7077808951	music source separation
0.7071737829	scale invariant
0.7069494773	convolution layers
0.7067024230	editing
0.7067024230	acoustical
0.7058721995	carefully designed
0.7057545417	western
0.7057466170	traffic
0.7057299315	voice activity
0.7055486382	related tasks
0.7055317092	computational cost
0.7054834065	trends
0.7054618090	acoustic modelling
0.7053283182	gan based
0.7051745840	mechanical
0.7050121108	aurora
0.7049487250	aided
0.7045775753	harmonics
0.7045437694	distress
0.7044627467	speech recognizer
0.7043222661	providers
0.7043222661	linguistics
0.7043222661	exponential
0.7042640134	curve
0.7042523054	resource constrained
0.7041965172	adaptation techniques
0.7040260617	korean
0.7040038185	amr
0.7039240927	percussion
0.7038047829	mood
0.7038047829	computers
0.7034283473	absolute improvement
0.7034003570	feature engineering
0.7033754852	constant q transform
0.7033260062	evolutionary
0.7032080219	machine lipreading
0.7028054376	speaker variability
0.7026889090	performance degradation
0.7026725379	base model
0.7025110022	deep speaker embedding
0.7024905233	wiener
0.7022551876	input sequence
0.7021949837	recurrent layers
0.7021672010	interspeech
0.7017623573	sim
0.7017249184	asvspoof
0.7016392452	feature mapping
0.7016207911	esc 50
0.7015850699	pi
0.7012965611	objective measures
0.7012902331	satisfaction
0.7012902331	examination
0.7007071401	data collection
0.7006239904	acoustic word embeddings
0.7003365808	care
0.7003108810	squared error
0.7002922501	converted speech
0.7001091215	strong labels
0.6999155958	kalman
0.6999026412	semi supervised learning
0.6995755846	source filter
0.6994509409	emotion labels
0.6990855209	wireless
0.6990509066	convolutional layers
0.6987920845	backward
0.6984229904	modulated
0.6982601869	interaural
0.6982081549	multi task
0.6981901194	fourier
0.6980431518	video frames
0.6979654948	speaker's voice
0.6978375199	hat
0.6976889342	low level
0.6975394068	separable
0.6974422899	frame wise
0.6971526859	product
0.6970353812	speech dereverberation
0.6967345903	dcf
0.6967242858	multi accent
0.6966673274	recent years
0.6961593410	frequency bins
0.6954566509	improvisation
0.6954566509	personality
0.6954566509	electric
0.6954009482	monte
0.6954009482	carlo
0.6950380971	inter channel
0.6948809027	embedding vectors
0.6948523965	hidden layer
0.6948341324	phoneme sequence
0.6946924039	pop
0.6946092132	grammar
0.6945440850	mini
0.6944271521	high accuracy
0.6942992795	resonance
0.6941055748	automation
0.6940499782	game
0.6940056833	bayesian
0.6938439282	implant
0.6936702698	emotion detection
0.6935590429	multi layer
0.6930136551	joint training
0.6929760285	assisted
0.6927583866	mises
0.6926667136	ambient
0.6926034479	attend
0.6921807424	illness
0.6921807424	unitary
0.6920178784	recently proposed
0.6919691701	text independent
0.6919621390	science
0.6919473699	conversational context
0.6918819973	consistently outperforms
0.6914136711	markov
0.6911404198	packet
0.6910984390	challenging task
0.6907809231	secret
0.6907809231	differential
0.6907809231	union
0.6906385161	times faster
0.6905807260	supported
0.6903955148	data driven
0.6902090478	deep generative models
0.6901462934	manually annotated
0.6899977232	objective function
0.6897938799	opera
0.6897662847	gesture
0.6894375416	parkinson's
0.6893281185	significance
0.6889965642	wer reduction
0.6889239540	short utterances
0.6888466628	multi target
0.6888287049	mmi
0.6886748881	deep neural
0.6886324723	beta
0.6881743678	particle
0.6880182340	existing works
0.6877038507	automatic speech
0.6874871172	promising results
0.6874270465	multilingual asr
0.6873755493	significantly reduces
0.6873494158	existing methods
0.6869830914	max
0.6869783667	remains challenging
0.6869306783	equipped
0.6868831771	voice separation
0.6868544769	multi label
0.6867687895	signal to noise ratios
0.6866448014	humanoid
0.6865085655	multipath
0.6864163262	subsequent
0.6863851061	language understanding
0.6851735606	answering
0.6851710597	output layer
0.6851602025	phase spectrum
0.6851447667	coupled
0.6845766577	direction of arrival
0.6844308466	wsj0
0.6843354109	convolutional recurrent
0.6842877706	genre classification
0.6842529579	asr systems
0.6842056152	big
0.6841688029	speech activity detection
0.6838998200	instrument recognition
0.6836156493	tracing
0.6836052509	multi track
0.6835739166	speech segments
0.6835498771	perceptual loss
0.6832876196	cross entropy loss
0.6826134944	coded speech
0.6825302008	locality
0.6825302008	reflection
0.6823287145	block online
0.6821286224	intelligence
0.6819931416	decay
0.6819931416	communications
0.6819237038	internet
0.6816530750	cocktail
0.6815780353	emphasis
0.6813041688	attention pooling
0.6812658498	multilayer
0.6803986145	reverb
0.6803476244	reference signal
0.6803371220	classification tasks
0.6802907829	tension
0.6801316785	guitar
0.6798450308	training examples
0.6798087237	collaborative
0.6796334240	paired data
0.6796240625	machine learning techniques
0.6795682441	acoustic echo
0.6788606959	interspeech 2020
0.6788169672	fully connected layers
0.6782593858	classification task
0.6777504346	significantly improve
0.6772463510	human computer interaction
0.6771319339	multi microphone
0.6766518156	f1
0.6766298842	affine
0.6765331193	pm
0.6764494071	bank
0.6763937759	low cost
0.6761589438	formal
0.6761589438	mid
0.6761559780	residual signal
0.6757978150	evolution
0.6753624870	receiver
0.6753512198	chime
0.6753414556	sampling rate
0.6753010698	nearest
0.6749695623	home
0.6749546834	transport
0.6749243111	conversational speech
0.6746436686	domain adversarial training
0.6746306077	ami
0.6743990969	sine
0.6743990969	plane
0.6742485016	noisy speech
0.6741446751	experimental evaluations
0.6741442642	dinner
0.6740778835	bitrate
0.6740778835	record
0.6740429001	dance
0.6739271500	instants
0.6738537697	english language
0.6738115310	results demonstrate
0.6736075927	singular
0.6734938904	vehicle
0.6731638021	youtube
0.6730684183	musical instruments
0.6727060417	temporal convolutional
0.6726395014	multidimensional
0.6725647912	encoder decoder architecture
0.6724049582	aishell
0.6723397586	high dimensional
0.6722531843	introduction
0.6722207200	align
0.6722094923	failure
0.6719579150	speaker characteristics
0.6718558460	oral
0.6718400094	isolation
0.6717637869	relative wer
0.6717304832	speaker adaptive
0.6717268135	maximization
0.6716415184	parallel data
0.6714850514	depthwise
0.6714304254	authentication
0.6713391023	forest
0.6712237878	low frequency
0.6711524797	residual networks
0.6710238706	td
0.6710238706	slt
0.6709348941	srp
0.6709269426	locata
0.6708643341	brain
0.6708044984	user defined
0.6707954193	polyphonic sound event
0.6705967304	sync
0.6704594408	recently introduced
0.6703826246	superior performance
0.6700643927	live
0.6699123177	hiding
0.6696649495	neighbor
0.6696649495	acts
0.6695103378	optimization problem
0.6693876400	solo
0.6691585404	iemocap
0.6689776743	higher accuracy
0.6686836170	ablation
0.6683109417	quantum
0.6681925926	cope
0.6677405559	string
0.6677405559	folk
0.6676855431	weakly labeled data
0.6675982259	relative improvement
0.6674937682	visualization
0.6674503545	power spectral
0.6673974339	benchmark datasets
0.6673543243	gram
0.6672980906	lf
0.6672942740	air
0.6671309029	voiced sounds
0.6670247330	wb
0.6670097321	acquisition
0.6669185261	recognition accuracy
0.6669133925	rule
0.6666920429	programming
0.6662541469	universal adversarial
0.6657963710	composer
0.6657301006	recent works
0.6656219246	instrument classification
0.6655223006	results confirm
0.6653977444	word level
0.6653965819	intelligent
0.6649577258	esc
0.6646878982	deep embedding
0.6644018676	chinese
0.6642233869	topology
0.6641886988	target source
0.6640552107	cell
0.6640050038	table
0.6637584814	sequence generation
0.6637529027	attention based encoder decoder
0.6637302637	midi
0.6636038975	real world scenarios
0.6633659995	xl
0.6633535298	mandarin speech
0.6631310589	evaluation set
0.6626944387	research topic
0.6625983039	industry
0.6625957454	proxy
0.6625591234	siamese
0.6619042951	pollution
0.6617187594	osqa
0.6617187594	mae
0.6613460900	statistical parametric speech
0.6613367140	pwg
0.6612849530	sign
0.6612793894	google
0.6612738706	gcc
0.6610468184	experimental evaluation
0.6610103979	cross modal retrieval
0.6609567203	codec
0.6609263997	student model
0.6605339681	affective
0.6605034650	dgp
0.6605034650	fs
0.6605034650	gla
0.6605034650	tpib
0.6605034650	tt
0.6605034650	elm
0.6605034650	ubm
0.6604388810	dprnn
0.6604050566	musical structure
0.6603465476	real world applications
0.6601248231	cloning
0.6600989883	large scale datasets
0.6600069294	bmvdr
0.6599659040	multi source
0.6599253066	chain
0.6599169726	masked conditional neural
0.6598297716	dementia
0.6597084080	web
0.6595884706	action
0.6595601333	starting
0.6594278167	dominated
0.6593791190	stochastic
0.6593053193	neural network architectures
0.6592341998	diarisation
0.6591219062	snn
0.6590334045	gold
0.6588260469	noisy conditions
0.6588187985	significantly improves
0.6586752359	linguistic content
0.6586201710	dialects
0.6585965672	intensity
0.6583917305	conv
0.6582841567	sti
0.6580476213	hot
0.6571838446	temporal attention
0.6571462771	significantly improved
0.6571236771	games
0.6571226167	dsp
0.6570060451	attentive
0.6566940849	text to speech synthesis
0.6566774003	wn
0.6566199039	command
0.6565054054	spade
0.6564593523	temporal dependencies
0.6564022920	widely studied
0.6563264533	covid 19
0.6563104532	speaker verification systems
0.6562819794	wall
0.6561977339	f0
0.6561310008	pca
0.6560752449	mining
0.6560378984	wavenet
0.6560036225	ww
0.6559462518	body
0.6557863969	central
0.6557497783	overlapping speech
0.6555490962	hyper
0.6554647877	dp
0.6553879516	measuring
0.6553488266	bc
0.6552560129	gci
0.6550373563	law
0.6549395365	nsf
0.6547850004	roc
0.6547676347	nar
0.6547359671	transformer
0.6545668477	lightweight
0.6545663642	lm
0.6542726212	parametric speech synthesis
0.6542540715	media
0.6542193921	results showed
0.6539007602	proof of concept
0.6538996018	english
0.6538685240	internet of things
0.6538595117	ear
0.6537761769	timit
0.6536293043	lcm
0.6535519493	vad
0.6535410592	captioning
0.6533091217	meta
0.6532679177	auc
0.6530895876	mild
0.6530196326	structural
0.6529648213	gop
0.6529648213	mnmf
0.6527807453	dct
0.6527807453	segan
0.6525890783	covid
0.6523497419	wfst
0.6522329262	neural network architecture
0.6522076958	correction
0.6521481334	graphical
0.6516524810	significant improvement
0.6516427581	model achieves
0.6516402972	enf
0.6516056878	cold
0.6516056878	marginal
0.6516056878	behavioral
0.6514948334	squeeze
0.6514684594	multi level
0.6514496713	loop
0.6513072209	excitation source
0.6511769445	distortionless
0.6511255751	learning paradigm
0.6511208495	principal
0.6510501421	cmu
0.6509407715	convex
0.6509146210	directed
0.6508750859	relative reduction
0.6507930083	avse
0.6507930083	sru
0.6507930083	mvf
0.6507930083	itd
0.6507868397	aec
0.6506082100	comparative
0.6504779893	modulation
0.6502162769	source signal
0.6500715943	zero shot
0.6500256061	analyzing
0.6500013103	pase
0.6499935067	boundary
0.6499032047	speech commands dataset
0.6497201514	sc
0.6497079023	text independent speaker recognition
0.6497035064	mclnn
0.6495230366	high frequency
0.6493779291	timbre
0.6493278211	ppg
0.6493064625	matlab
0.6492516991	objective metrics
0.6492395423	anomaly
0.6491852544	significantly outperform
0.6491406853	blstm
0.6489803230	dns
0.6489803230	kl
0.6489279697	anechoic
0.6486846577	anti
0.6486796559	eend
0.6485920631	conditioned
0.6485081007	amt
0.6483276670	gaussian
0.6482947335	lc
0.6482414714	urban
0.6481217074	ic
0.6481096519	descent
0.6480927629	dominant
0.6479668281	va
0.6479215619	glu
0.6477860160	blcmv
0.6477126798	listen
0.6475631506	subjective listening tests
0.6475485270	target speaker's voice
0.6475237843	experimental results confirm
0.6474887008	vq
0.6474805643	taking into account
0.6473507395	deep convolutional neural network
0.6472867408	api
0.6472664275	therapy
0.6471188164	bi
0.6470211886	finite
0.6469984947	tasnet
0.6469023648	grapheme based
0.6468443751	trivial
0.6468248533	consistently improves
0.6467444673	connectionist
0.6466302317	ace
0.6465267889	mvdr
0.6464525871	convolutional network
0.6463937262	led
0.6463663466	nes
0.6463037438	overview
0.6460292759	weakly labelled data
0.6460129431	sre
0.6459858303	ilrma
0.6458003139	iwslt
0.6455267889	rnnlm
0.6455065781	vehicles
0.6454742502	psd
0.6454616456	asp
0.6453721850	ad
0.6453631665	lvcsr
0.6453411588	auto
0.6452570884	expressive speech
0.6451940294	domestic
0.6451822278	wild
0.6451403723	uniform
0.6451215739	similarly
0.6450850805	sensing
0.6450745398	nat
0.6450745398	sonyc
0.6450745398	hd
0.6448607501	multiple domains
0.6446965999	nas
0.6446755775	dwt
0.6446709785	lium
0.6444763278	journal
0.6443737812	sound separation
0.6443244032	prior knowledge
0.6442202938	apc
0.6441679557	rapid
0.6441349648	gpus
0.6439967797	mean square error
0.6439438419	nist
0.6439369342	mir
0.6436965999	fcn
0.6432960669	vc
0.6431657704	rtf
0.6431275005	element
0.6431247192	dtw
0.6428993569	rl
0.6427528414	articulation
0.6421842209	abx
0.6421154841	lyrics
0.6420024216	orthogonal
0.6419210815	nlu
0.6419210815	sid
0.6418956213	ss
0.6418476902	feedback
0.6418451443	dae
0.6418374349	inductive
0.6414491836	gru
0.6413676592	developments
0.6413448304	mix
0.6412846565	spanish
0.6411828690	tacotron
0.6410582465	multi condition
0.6409562899	darts
0.6408873585	svd
0.6408188541	vowel
0.6407704358	recent studies
0.6407334081	ml
0.6406715166	dfsmn
0.6406715166	mc
0.6406715166	pa
0.6406032315	tcn
0.6405856868	nlp
0.6405698888	deviation
0.6404614283	customer
0.6404614283	records
0.6404614283	aliasing
0.6403817890	disentangled
0.6402613383	transition
0.6401752429	language processing
0.6401392776	interfaces
0.6401270684	dialect
0.6400783825	soft attention
0.6399936158	da
0.6399525036	mimo
0.6399525036	sce
0.6399160831	mbr
0.6399160831	mp
0.6399160831	mb
0.6398470843	experimental
0.6397789082	librispeech
0.6396938254	covariance
0.6396281708	der
0.6396234272	musdb
0.6395831281	presentation
0.6395755309	multi class
0.6394988110	bn
0.6393437911	doas
0.6392761868	description
0.6391833175	dbs
0.6390841879	heart
0.6390396176	sdr
0.6390246565	melgan
0.6388325703	sitw
0.6388283374	fr
0.6387409522	mt
0.6387255810	cnn architecture
0.6386006571	wham
0.6385990142	strength
0.6385286251	wpd
0.6384755262	relative word error rate
0.6384209817	lp
0.6384111231	ast
0.6383619618	fca
0.6381775917	muse
0.6378587835	italian
0.6378587835	freesound
0.6378383204	relative improvements
0.6377204734	rbm
0.6375571149	asv
0.6375379786	ci
0.6374751934	mandarin
0.6374325266	entropy
0.6370586450	ease
0.6369767930	kd
0.6369622381	speaker separation
0.6369275048	svs
0.6369155832	iva
0.6368599481	alexa
0.6368447579	paper proposes
0.6368308056	neural network based
0.6367498973	dcase
0.6367029660	harmonic
0.6366426437	tsm
0.6366426437	lpc
0.6365663642	gmm
0.6365441797	ieee
0.6365258809	ipa
0.6364084907	tts
0.6363445362	snr
0.6362884859	signal to interference ratio
0.6362372053	significant improvements
0.6362097122	sequence to sequence
0.6360727901	dblstm
0.6358765932	multi modality
0.6356989494	pre train
0.6356730015	pearson
0.6356655859	receptive
0.6356604577	cnns
0.6354332626	availability
0.6351990766	rnn
0.6351412172	multimodal
0.6351121526	visual modalities
0.6350724084	simultaneous
0.6350429555	wsj
0.6350284672	status
0.6349935169	unpaired
0.6349571034	background music
0.6349502236	kws
0.6349106819	discriminant
0.6348701845	eeg
0.6346587177	sentiment
0.6346156989	linguistic information
0.6346113130	lrs2
0.6346113130	densenet
0.6345242846	smooth
0.6345051790	ca
0.6344262700	voxceleb
0.6342821620	speak
0.6341900784	speaker discriminative
0.6341539299	gan
0.6341154527	rir
0.6340346025	previous studies
0.6338349942	whispered
0.6337409522	sd
0.6336736857	low resource language
0.6336021689	regression model
0.6335993869	hierarchical
0.6334900036	tdnn
0.6333589152	texture
0.6333433974	adaptive
0.6331909974	aware training
0.6331256829	depression
0.6330758551	punctuation
0.6328557130	psds
0.6328449919	urban sound
0.6327234211	vector
0.6324970535	early
0.6324346205	aurora 4
0.6323970474	long short term
0.6323161613	synchronous
0.6322542432	cpc
0.6322542432	avsr
0.6322542432	bnf
0.6322264761	regression task
0.6320331468	opinion
0.6319096213	sheet
0.6318786215	vcc
0.6318786215	spss
0.6318733323	seld
0.6318083729	group
0.6317223781	mfcc
0.6316258365	multi objective
0.6315998032	sequence length
0.6315071476	specifically
0.6313973963	deep
0.6313777424	atc
0.6313308908	hrtf
0.6313308908	hkust
0.6313308908	crf
0.6313308908	vctk
0.6312559574	continuous speech
0.6312027350	sound event localization
0.6311956030	sir
0.6311871982	durian
0.6311563345	majority
0.6311351155	center
0.6310523387	bpe
0.6310523387	ipd
0.6309614944	hrtfs
0.6308617433	dual
0.6307758208	objective and subjective evaluations
0.6307696795	ratio mask
0.6307126840	urbansound8k
0.6307052760	localization and tracking
0.6306053800	wavelet
0.6305821490	population
0.6305754839	se
0.6304626627	wpe
0.6304224024	ann
0.6304224024	ce
0.6303874211	dl
0.6303396188	speech
0.6302846689	pd
0.6301639663	estimator
0.6301370733	rf
0.6300806662	review
0.6300389391	mtl
0.6300046620	monaural
0.6299738590	simulation
0.6299431213	vae
0.6299344493	ccc
0.6298140557	speaker recognition evaluation
0.6297711016	dnn
0.6297436523	transducer
0.6295197703	subjective test
0.6294988156	format
0.6294644117	language
0.6292289891	separating
0.6291890690	nmt
0.6291890690	cqt
0.6291884545	mushra
0.6291641170	cycle
0.6291107860	ctf
0.6291107860	dpcl
0.6290917492	service
0.6290814425	external language model
0.6290523387	dat
0.6289892626	contours
0.6289594591	naive
0.6289197886	frequency
0.6288586830	speech signal
0.6288081022	net
0.6287130372	ctc based
0.6285867869	passing
0.6284867570	phonetic content
0.6283072836	preliminary results
0.6282945060	perceptual evaluation of speech quality
0.6282552370	gradient
0.6282400763	si
0.6282193636	comprehension
0.6282193636	middle
0.6282193636	oriented
0.6281994299	coefficient
0.6281637128	battery
0.6280675186	party
0.6280293915	animation
0.6280293915	declipping
0.6279874263	data augmentation techniques
0.6277771824	recently
0.6276935674	ctc
0.6275175676	wsj0 2mix dataset
0.6273129593	detecting
0.6272197492	bnfs
0.6271950202	doa
0.6271857182	mp3
0.6271857182	hinet
0.6271691404	bird
0.6271436371	ae
0.6270075341	cepstrum
0.6269877455	creaky
0.6269833406	tensorflow
0.6269299274	tacotron2
0.6267165990	ultrasound
0.6267133573	stacked
0.6261231309	energy
0.6261012166	channel wise
0.6260596459	mil
0.6259369148	egg
0.6258460349	channel
0.6256941310	emotion
0.6256890679	region
0.6256096735	piano
0.6255901828	social
0.6254497972	multi domain
0.6254178516	universal
0.6253926218	mimic
0.6253185818	iot
0.6252964831	city
0.6250918614	nn
0.6250914531	envelope
0.6248743998	acoustics
0.6248013247	creative
0.6247642346	writing
0.6247642346	session
0.6247642346	plain
0.6247642346	compatible
0.6247339321	volte
0.6247339321	qbe
0.6246390244	virtual
0.6245354726	training scheme
0.6243571530	higher level
0.6242828095	verbal
0.6242474769	asr
0.6241011560	noise ratio
0.6240620513	transfer
0.6240160086	voiced
0.6239570128	music style transfer
0.6238036141	native speakers
0.6237839372	cs
0.6237578815	slu
0.6237485824	learning
0.6235663314	adaptation
0.6235593215	fftnet
0.6234683054	hand crafted features
0.6234333178	free
0.6234305478	cnn
0.6233584907	coefficients
0.6233065671	disorder
0.6232216793	modelling
0.6231641689	graph based
0.6230808386	permutation
0.6230759041	gap
0.6230497227	improved performance
0.6229614285	angular
0.6229154374	playing
0.6228698522	advancements
0.6227894827	cost effective
0.6227447865	spread
0.6227187308	embedding
0.6226748908	ahc
0.6226748908	sr
0.6224121255	masked
0.6223368733	transmission
0.6223268315	lid
0.6223100748	aes
0.6220224200	few shot
0.6220223736	music
0.6219628883	messages
0.6219628883	inferior
0.6219628883	powered
0.6218641076	hash
0.6217996088	parallel
0.6216725017	spell
0.6215164830	spex
0.6213809113	bss
0.6211454638	desired signal
0.6211399218	white
0.6208431552	lstm
0.6202640563	fhvae
0.6202618595	lipreading
0.6201366198	mdd
0.6200706443	conditional
0.6200387478	image source
0.6200208140	load
0.6200208140	ongoing
0.6199446310	wasserstein
0.6198898352	cae
0.6198246637	synthesis
0.6197825063	transcription
0.6197320313	species
0.6196264905	survey
0.6196177544	lda
0.6196177544	mlp
0.6195054343	mozilla
0.6195054343	laplacian
0.6195054343	google's
0.6195054343	libri
0.6195054343	gabor
0.6195054343	bachprop
0.6194631456	deep residual
0.6194319231	grapheme
0.6194241355	nu
0.6193050321	voip
0.6191808404	stoi
0.6191341731	u net
0.6191279839	experimental results demonstrate
0.6189780712	musical styles
0.6189738982	interface
0.6189588689	feature representations
0.6189104832	machine generated
0.6188990567	eer
0.6188508592	ssl
0.6188229337	opus
0.6187897063	reality
0.6187622970	target speaker's
0.6184897695	instantaneous
0.6184338505	minor
0.6183500275	imagenet
0.6182354025	sv
0.6182157772	challenging problem
0.6182148216	wer
0.6181922030	report describes
0.6181454652	attention fusion
0.6181415574	oov
0.6181023385	recognition systems
0.6180857255	pit
0.6180602506	inspired
0.6180494098	maestro
0.6180035153	contrast
0.6179429943	extensive
0.6179329978	objective quality
0.6178849883	target speech separation
0.6178612291	short time fourier
0.6178113193	multitask
0.6178079274	lombard speech
0.6178074859	evaluation sets
0.6176574731	improving
0.6176032864	mwer
0.6176032864	clnn
0.6175438963	reverberant speech
0.6174517998	ts
0.6174463447	gpu
0.6174206439	weighted
0.6172480607	las
0.6172179053	ge2e
0.6171347585	integrating
0.6170041970	hmm
0.6169996570	sa
0.6167433216	leveraging
0.6167186342	phoneme based
0.6166263066	interactive
0.6165303022	av
0.6162123828	multilingual
0.6161134266	l1
0.6159068891	cms
0.6158499286	unlabelled data
0.6157814741	language independent
0.6156599040	pr
0.6155985379	sequential data
0.6155688112	filtering
0.6154880900	estoi
0.6153294141	mutual
0.6153233619	cpu
0.6152919970	combining
0.6152315092	discriminate
0.6149830288	building
0.6149800834	cer
0.6149554117	fast
0.6149317471	attend and spell
0.6149102080	moving
0.6148340076	occurrence
0.6146965193	synthetic
0.6146664184	late
0.6146007340	sound quality
0.6145138506	arctic
0.6143590484	conversion
0.6142506169	perceptual evaluation
0.6141573589	developing
0.6141475431	distance
0.6141164676	robot
0.6140425603	cat
0.6139960987	visual
0.6139691934	deep learning approaches
0.6139192313	frame
0.6138949760	official
0.6138235042	online
0.6138142714	mental
0.6138054425	neural
0.6137984557	exploiting
0.6137771842	normalization
0.6137159300	xlnet
0.6136822718	bilstm
0.6136729803	operator
0.6136705980	generalized
0.6136137390	modern
0.6134581677	graph
0.6134062508	classification
0.6132774457	ams
0.6132731629	fasnet
0.6132731629	g2p
0.6132507526	raw speech
0.6132305131	natural speech
0.6131699607	masking
0.6130865356	voices
0.6129178561	license
0.6126515077	callhome
0.6125906175	libritts
0.6124276367	interestingly
0.6121248654	constant q
0.6120655638	risk
0.6120049225	adopting
0.6119876723	ar
0.6119052944	line
0.6118833253	hidden representations
0.6118776121	spontaneous
0.6117198229	variational
0.6116706477	closed
0.6115908342	applying
0.6115568906	fbank
0.6113932755	sota
0.6112161005	rsr2015
0.6109671337	cochlear
0.6107951062	paralinguistics
0.6106310347	imagined
0.6106310347	proof
0.6106310347	treated
0.6105325132	compression
0.6104873556	embeddings
0.6104600531	technology
0.6104224716	weight
0.6103267240	robust
0.6103257592	identification
0.6102933998	singing
0.6102339783	onsets
0.6102210456	forward
0.6101047511	single channel speech enhancement
0.6100052141	svm
0.6099995121	direct
0.6099089494	talk
0.6098567128	optimizing
0.6097915896	sequential
0.6097531359	dynamic
0.6097410425	anomalous
0.6096737043	binaural
0.6096349437	cepstral
0.6096193064	convolutional
0.6095849389	contribute
0.6095682279	mel
0.6095544109	model size
0.6094340989	mobile
0.6092601929	services
0.6091840480	emotion related
0.6091798023	speaker
0.6090499005	close
0.6090409638	rmse
0.6090359268	portion
0.6090083829	recent
0.6089063893	selection
0.6087712842	family
0.6087527284	e2e
0.6085825448	babel
0.6084705178	attention
0.6084255494	discrete
0.6084001491	random
0.6083942285	inpainting
0.6083927190	semantic
0.6083900575	vc2
0.6083717100	tracking
0.6083565782	shake
0.6083492810	prosody
0.6083400079	lstms
0.6083134062	feature set
0.6082677203	hmms
0.6082307235	spectral
0.6082144965	post
0.6081492218	text to speech
0.6081406696	cnn architectures
0.6080176992	independent vector
0.6080032182	mocha
0.6080006708	accent
0.6079673073	active
0.6078371856	demand
0.6078295465	waveglow
0.6077089668	augmented
0.6075446283	regression
0.6073462551	rirs
0.6072671519	sincnet
0.6071721379	dictionary
0.6071674720	timit corpus
0.6070054837	period
0.6069494066	github
0.6069184926	squeeze and excitation
0.6068671166	trainable parameters
0.6067824694	multichannel
0.6067753605	rnn t
0.6066601159	respiratory
0.6065676044	proposed method
0.6065652782	ser
0.6063805966	arrays
0.6063373213	estimation
0.6062897156	zerospeech
0.6062891267	nonetheless
0.6062715005	finding
0.6062574281	perceptual
0.6062395958	relied
0.6061674798	german
0.6061299985	convolution layer
0.6061223923	l2
0.6061080550	lastly
0.6061021660	state
0.6061003781	maximum
0.6058844769	root
0.6058532196	acoustic signals
0.6058335918	st
0.6058192237	sum
0.6057842319	minimizing
0.6057684443	eeg based
0.6057108934	emergence
0.6057108934	sacrificing
0.6057108934	belonging
0.6057108934	suffering
0.6057108934	attractor
0.6055680281	contextual
0.6054601398	terms
0.6053701268	predictive
0.6052591185	fraction
0.6052303036	notation
0.6052239728	scattering
0.6050480929	cm
0.6050351207	melody
0.6048311146	single channel speech separation
0.6048155324	probabilistic
0.6047917133	acoustic units
0.6046130931	samplernn
0.6045747095	countermeasures
0.6045688577	interact
0.6045341996	mse
0.6045259074	artificial
0.6043704053	asv systems
0.6043282795	prediction
0.6040548829	disorders
0.6039428870	acoustic
0.6039205334	agnostic
0.6039036102	distillation
0.6038455876	sound event localization and detection
0.6038077850	simplified
0.6037737413	t dcf
0.6037339970	auditory
0.6036137325	unsupervised
0.6035632650	camera
0.6035515077	generating
0.6033905092	generative
0.6032905398	aiming
0.6032767899	sound event classification
0.6032277894	musical piece
0.6030765505	impulse
0.6030230034	impaired
0.6028647687	er
0.6028405297	recurrent
0.6027011008	hybrid
0.6026394333	encoder
0.6025013864	autoregressive models
0.6024779867	connection
0.6023694564	single stage
0.6022994972	attention module
0.6022830160	separation performance
0.6022606989	great success
0.6022475128	keyword
0.6022118685	higher quality
0.6022095914	beat
0.6021470473	model parameters
0.6020379950	text embeddings
0.6020377000	independent
0.6019372924	decoder
0.6019138015	speech waveforms
0.6018983268	fastspeech
0.6018923159	en
0.6018557195	vgg
0.6017064154	restricted
0.6015341120	adress
0.6015341120	voxceleb2
0.6015320403	gcis
0.6014357843	convolution
0.6012892808	lms
0.6012602466	automatic
0.6011286399	lite
0.6011286399	consortium
0.6011259920	statistical
0.6011257987	score
0.6011200284	introducing
0.6009743793	sound
0.6009707743	feature
0.6009703092	ensemble
0.6008322237	separation quality
0.6007718464	generation
0.6006579430	significantly higher
0.6005430306	spoken
0.6004801270	skip
0.6004051792	cognitive
0.6003849536	speech waveform
0.6003192862	folds
0.6003144514	counting
0.6002182697	aed
0.6001902423	previous
0.6000855148	teacher
0.6000806907	quantitative
0.5999926902	phone
0.5999805866	improved
0.5999722131	bleu
0.5998813640	pitch
0.5998387835	lpcnet
0.5997348929	convolutions
0.5997288452	wers
0.5997202455	open
0.5997025220	long
0.5996470028	expression
0.5995968606	enhanced
0.5995805551	speaker representations
0.5995768014	inverse
0.5994765712	grained
0.5991517476	fusion strategy
0.5990400753	heavy
0.5990400753	reports
0.5990070705	encoding
0.5989553937	spectrogram
0.5989473776	aware
0.5987926143	low
0.5985209721	presence
0.5983347391	digital
0.5982828276	processing
0.5982772490	preservation
0.5982726614	vaes
0.5982547960	motivated
0.5981663567	sound classification
0.5981202222	language specific
0.5980371276	local
0.5979824919	listening
0.5979150737	objective
0.5978399673	stargan
0.5978256422	taking
0.5976187316	feature learning
0.5975472836	chime 6
0.5973980282	res2net
0.5973980282	emirati
0.5973971113	song
0.5972053448	voxsrc
0.5971545765	transformation
0.5971434517	switching
0.5971004291	reconstruction
0.5970538220	crnn
0.5970292427	consequence
0.5969374620	evaluation
0.5969324495	wave
0.5969131727	prosodic
0.5968734624	ii
0.5968019074	storage
0.5965637502	event based
0.5965540720	limited data
0.5964508591	corpus
0.5964083371	focusing
0.5960589198	efficient
0.5960238804	iterative
0.5959464999	idea
0.5959428793	unified
0.5958597143	ads
0.5958378187	v2
0.5958263057	guide
0.5957712989	statistical model
0.5957689853	spectrum
0.5957187695	phones
0.5956740725	evaluations
0.5955862790	event
0.5955446241	cnn based
0.5954946419	evaluating
0.5954855945	lattice based
0.5952482987	units
0.5951621547	onset and offset
0.5951275530	specaugment
0.5951071315	maps
0.5950896808	conventional approaches
0.5950555244	articulatory
0.5948506489	existing
0.5948090752	enhanced speech
0.5947045279	maximizing
0.5946750710	cross lingual voice conversion
0.5946691319	musical
0.5946334762	modeling
0.5945964503	read
0.5945705841	comparing
0.5945370901	bert
0.5944635935	protocol
0.5944182825	fool
0.5941891483	conventional
0.5941806658	network
0.5940552227	room reverberation
0.5938337082	children
0.5937394093	nmf
0.5937031387	subword
0.5935504819	mean opinion scores
0.5935378235	spatial
0.5933815290	extraction
0.5931189497	perception
0.5930650241	chord
0.5930478686	history
0.5930143483	activity
0.5930061653	student
0.5929341670	la
0.5928956708	complex
0.5927531012	consistent
0.5926742380	variable
0.5926543331	experimental results showed
0.5926348226	hidden
0.5926285595	duration
0.5925287426	difference
0.5923828527	spoofing
0.5923669708	role
0.5922831830	blind
0.5922243803	attempted
0.5921932770	subjective assessment
0.5921119498	tf
0.5920622164	autoencoders
0.5917834343	map
0.5917320731	auxiliary
0.5917242554	conformer
0.5915603609	talking
0.5915578825	controllability
0.5914617100	autoencoder
0.5913923402	sounds
0.5913865625	millions
0.5913660333	bidirectional
0.5913349969	phonetic
0.5912230255	experiments
0.5910929196	class
0.5910464711	deep speaker embeddings
0.5910286578	mathematical
0.5906246923	contained
0.5906246923	divided
0.5906075878	lip
0.5905706241	coding
0.5905465462	noise
0.5904108771	matrix
0.5902592025	driven
0.5902118234	source
0.5901795153	markov model
0.5901546200	pooling function
0.5900981320	encoder decoder framework
0.5900746122	pooling
0.5900725571	employing
0.5900361281	denoising
0.5899288359	deepspeech
0.5895955913	hearing
0.5895919437	fidelity
0.5895487549	ray
0.5894752187	multi label classification
0.5893812624	source domain
0.5893054914	platform
0.5893034702	handcrafted features
0.5892702363	sed
0.5891679151	snrs
0.5891120747	model
0.5891018055	similarity matrix
0.5889025920	accompaniment
0.5886384853	version
0.5884524964	diseases
0.5883127069	area
0.5880988229	alignment
0.5879086731	multilingual speech recognition
0.5878953490	single
0.5878904827	fixed
0.5878660683	cluster
0.5878306228	wavernn
0.5877588767	assistants
0.5877268379	sing
0.5876871623	distributed
0.5876839822	trainable
0.5876208056	collections
0.5875686735	features
0.5874424921	speaker information
0.5874232299	continuous
0.5872711331	reinforcement
0.5872643927	s2s
0.5872462888	incorporating
0.5870953264	generation process
0.5870497144	lstm based
0.5870322523	contrastive
0.5870123315	polynomial
0.5869974083	linear
0.5869636137	trade
0.5869082361	logical
0.5869076343	extracting
0.5868964063	binary
0.5867468712	dereverberation
0.5867226480	predicting
0.5866629804	multimodal learning
0.5866568857	study
0.5865921654	fisher
0.5865098308	optimization
0.5865001479	challenge
0.5864779668	context
0.5864574524	estimating
0.5863785443	sampling
0.5863369301	adversarial
0.5863009981	similarity
0.5862622875	ai
0.5862442895	world
0.5862280485	raga
0.5861036251	dataset
0.5859301638	localization
0.5859080050	intrusive
0.5858570852	track
0.5858528148	alarm
0.5858528148	thesis
0.5858504900	tagging
0.5858351364	parallel training data
0.5858339124	pure
0.5857429015	connected
0.5857313656	temporal
0.5855188062	joint
0.5854173605	augmentation
0.5853647196	comprised
0.5853322383	word
0.5852470634	spatio
0.5852470634	priori
0.5851894611	filterbank
0.5851820954	coded
0.5850777979	tune
0.5848381334	recognition
0.5847800529	current
0.5847559153	advances
0.5845972837	forced
0.5845182248	audiovisual
0.5844482052	machine
0.5843382231	deaf
0.5843382231	elderly
0.5843382231	contextualized
0.5841627026	discovery
0.5840118389	emphasize
0.5840118389	watermarking
0.5840094774	autoregressive
0.5840089484	event localization
0.5839674941	background model
0.5839602579	reliability
0.5839409019	series
0.5839377826	detection
0.5839320942	enhancement
0.5839210944	ground
0.5838351144	discriminative feature
0.5838068800	transformer architecture
0.5838037131	face
0.5837714723	submission
0.5837412010	generalization
0.5837354439	semi supervised training
0.5836655512	untranscribed
0.5836087086	invariant
0.5836050375	crowdsourced
0.5836050375	workshop
0.5836050375	pathology
0.5834962677	steganography
0.5834962677	conventionally
0.5834781559	linguistic representations
0.5834601248	expressive
0.5834289042	front ends
0.5834119080	quantization
0.5834046476	open domain
0.5833678122	loss
0.5831957881	resulted
0.5831957881	shelf
0.5831390132	construction
0.5831196124	conditioning
0.5829890416	systematic
0.5829442707	tdnn f
0.5829383821	cough
0.5829175141	separation
0.5829130556	invasive
0.5829130556	cite
0.5829130556	von
0.5828398578	musdb18
0.5828205432	domain adversarial
0.5827764840	speech driven
0.5827626691	computational
0.5826811001	quality
0.5826439255	echo
0.5826405033	sequence
0.5825857133	adversarial loss
0.5825794117	belong
0.5823702457	evidence
0.5823430808	drum
0.5823342772	beamformer
0.5823140178	resolve
0.5821821353	autoregressive model
0.5821682569	super
0.5821258202	architecture search
0.5820502802	heads
0.5819529317	positive
0.5819229772	task
0.5818937012	vocal
0.5818351173	long utterances
0.5818173870	humans
0.5817955646	adversarially
0.5817955646	prototypical
0.5817185564	vocoder
0.5816570027	sentence
0.5815080803	supervision
0.5814947780	fusion
0.5814850652	competitive baselines
0.5813980863	sensitivity
0.5813798716	search
0.5813693729	compress
0.5813693729	favorably
0.5813474484	anonymization
0.5813474484	quantifying
0.5813276945	mr
0.5812684458	car
0.5812629820	human
0.5811085457	mir tasks
0.5810725665	streaming
0.5810707817	algorithm
0.5810485595	stft
0.5810163135	relation
0.5809177553	diarization
0.5808878586	identities
0.5808814283	clinical
0.5808474586	movies
0.5807975479	existing approaches
0.5807795057	mos
0.5807108919	instrument
0.5807089050	dilated
0.5806947061	constrained
0.5805337472	long duration
0.5805108259	asc
0.5804141259	impairment
0.5803869453	orders of magnitude
0.5803613981	noisy
0.5803570468	ambisonics
0.5802106433	determined
0.5802007306	effective
0.5801932523	greatly improved
0.5801621365	time delay neural network
0.5801550647	spotting
0.5800898690	fluent
0.5799694343	personalization
0.5799694343	discovering
0.5799694343	mitigation
0.5799694343	drawing
0.5799694343	densely
0.5799694343	octave
0.5799694343	broadband
0.5799672025	preliminary
0.5799231830	speaker specific
0.5799056322	japanese
0.5797854907	environments
0.5797759383	quantized
0.5797675075	filter
0.5796379497	video
0.5794072968	greater
0.5793519698	device
0.5791734272	attackers
0.5791734272	templates
0.5791734272	spatiotemporal
0.5791734272	specially
0.5790780693	flow
0.5790436083	pesq
0.5790359895	probing
0.5790359895	babble
0.5790359895	ultimately
0.5790359895	characterizing
0.5789652012	gated
0.5789420337	existence
0.5789300992	approximate
0.5788599472	static
0.5786441660	analysis
0.5785439372	tone
0.5784509687	trade off
0.5783953504	rank
0.5783505537	asvspoof 2017
0.5782871097	videos
0.5782838820	box
0.5782223729	transform
0.5781024901	mispronunciation
0.5781024901	contributes
0.5781024901	narrow
0.5780848661	adoption
0.5780709716	residual
0.5777402991	handcrafted
0.5777224282	large
0.5776505682	room
0.5775354845	coherence
0.5773960567	transformer network
0.5773700485	self attentive
0.5772745977	data selection
0.5771907282	translation
0.5771658875	transformer tts
0.5771270945	quantitative and qualitative
0.5771212527	preserving
0.5770829222	correlation
0.5770672726	text
0.5769528605	architectural
0.5769528605	intensive
0.5769180612	development
0.5768927779	end to end
0.5768898126	environment
0.5768799405	parametric
0.5768447777	phase
0.5767477253	profiling
0.5765138539	progression
0.5764847582	pytorch
0.5762975162	smart
0.5762946444	grounded
0.5761944129	interaction
0.5761760926	explicit
0.5761760926	controlled
0.5761393917	character
0.5761224593	custom
0.5758754538	content based
0.5758706176	networks
0.5757794486	accented
0.5756739256	replay
0.5755559474	modular
0.5754955198	ensembles
0.5754955198	autonomous
0.5754955198	timbral
0.5754955198	layered
0.5754866952	secondary
0.5754866952	reproduce
0.5754830909	mask
0.5754278394	regressive
0.5753973776	translate
0.5752686229	real recordings
0.5752588096	audioset
0.5752412517	clustering
0.5752283651	control
0.5751729745	conversational
0.5751063391	view
0.5750494581	global style
0.5750335666	native
0.5750242432	espnet
0.5750185185	toolkit
0.5749500601	detailed
0.5748239777	proper
0.5748133952	results
0.5747325038	silent
0.5745826931	viseme
0.5745596964	domain
0.5745402589	alongside
0.5745402589	classroom
0.5745402589	bioacoustic
0.5744681583	memory
0.5744187415	visually
0.5743145846	clarinet
0.5742923485	monitoring
0.5741341636	accurate
0.5740534786	theoretical
0.5740291235	mfccs
0.5739179737	classical
0.5738587104	encountered
0.5737111644	steady
0.5737111644	rise
0.5737103439	plda
0.5736261063	music streaming
0.5734613093	sound source separation
0.5732916063	closure
0.5731726774	nets
0.5731518831	verification
0.5728982456	transformer encoder
0.5728615170	object
0.5728521259	realistic
0.5728378996	singular value
0.5727295498	practice
0.5727086390	method outperforms
0.5723836666	code
0.5723682269	cloud
0.5723306527	surprisingly
0.5722566038	seq2seq
0.5721778732	stress
0.5721321679	music classification
0.5721075385	microphone signals
0.5720577025	medical
0.5720016984	set
0.5719081685	vision
0.5717972893	million
0.5717484369	reducing
0.5717045369	cosine
0.5716956425	multiple languages
0.5716574985	backpropagation
0.5716442624	pseudo
0.5715718562	communication
0.5715159799	nonparallel
0.5715159799	visualizing
0.5715159799	wearable
0.5715075295	boundaries
0.5715033199	short
0.5714687482	relationship
0.5714455062	autoencoding
0.5714325878	distant
0.5714273674	mixtures
0.5712300212	sounding
0.5712259985	rnn based
0.5711503897	wide
0.5711502700	edge
0.5710555340	straight
0.5709622973	discriminative features
0.5709558382	crowd
0.5709558382	logistic
0.5708899944	replacing
0.5707440527	comprehensive
0.5704344387	regarded
0.5703425426	based
0.5703038510	supervised
0.5703036022	switchboard
0.5702221803	data
0.5701799558	uncertainty
0.5701103582	major challenge
0.5700806534	lexical
0.5700540087	convolutional layer
0.5700252920	output sequence
0.5699025362	delay
0.5698775129	diffuse
0.5698637259	causal
0.5698630512	monolingual
0.5696132675	interested
0.5695833129	iv
0.5694754190	head
0.5692486732	discriminative
0.5692391149	learned
0.5691756856	interpreted
0.5691155160	relevance
0.5690940280	empirical
0.5690913093	representations
0.5690295595	stack
0.5690295595	viewed
0.5690295595	utilization
0.5690278971	order
0.5690250694	global
0.5689919281	creativity
0.5689500643	contour
0.5688716797	human emotion
0.5687564995	stereo
0.5687170864	message
0.5687069572	bit
0.5686918802	disguise
0.5686314220	faster
0.5686258776	multi
0.5685633978	microphone
0.5685498588	submitted systems
0.5684787300	audio signal processing
0.5684610775	type
0.5683473297	specifically designed
0.5682908070	mixing
0.5681458372	cloned
0.5681310011	anchor
0.5681142274	proposed approach
0.5679910511	conventional hybrid
0.5679836954	soundscapes
0.5679836954	discrepancy
0.5679836954	teachers
0.5679705427	suppression
0.5679472858	whisper
0.5679124785	stand
0.5678406176	talker
0.5678232192	creating
0.5678204038	simple
0.5677918585	lead
0.5676940056	note
0.5676116185	phonemic
0.5675469462	languages
0.5674706420	target domains
0.5674350137	statistics
0.5673774612	compressed
0.5672802473	routing
0.5672268018	tuning
0.5670804703	voice
0.5669691159	facial
0.5669609225	replaced
0.5669609225	proposing
0.5669243833	lies
0.5669187959	usefulness
0.5669023680	segmentation
0.5668199952	entities
0.5667029319	music pieces
0.5666924849	genre
0.5666498324	alpha
0.5666113199	concept
0.5664552246	reverberation
0.5664230448	total
0.5662680095	spectrograms
0.5662132492	bin
0.5661489657	clustering algorithm
0.5661284760	aforementioned
0.5660520614	weak
0.5659825025	k means
0.5659084195	circular
0.5658539134	broad
0.5657157196	training
0.5656332350	lattice
0.5655476585	reconstruct
0.5655214402	composition
0.5654912133	minute
0.5654912133	remixing
0.5654771717	art
0.5654116676	recordings
0.5653273526	mindcf
0.5653273526	lab
0.5651912594	pretrained
0.5651532662	segment
0.5651208459	zero resource
0.5651163422	utilizing
0.5650537355	information
0.5649704851	indistinguishable
0.5649704851	filling
0.5649499371	mixture
0.5648538033	crime
0.5648500806	generic
0.5647621334	reverberant conditions
0.5646807986	conditional neural network
0.5646526050	digit
0.5645861293	traditional
0.5645287027	hi
0.5644595503	sadness
0.5644595503	lessons
0.5644595503	conclusion
0.5644595503	overcoming
0.5644544847	t f
0.5642282627	cyclegan
0.5642020096	smoothing
0.5640381043	analysis synthesis
0.5639171446	full rank spatial
0.5639004713	ends
0.5636775790	devices
0.5636688796	computing
0.5636431795	embedded
0.5636354332	secure
0.5636065045	basis
0.5636033479	recording devices
0.5635938900	extractors
0.5635382622	pyramid
0.5635325429	power
0.5634415472	stage
0.5633957646	sinc
0.5633907769	scarcity
0.5633757081	phase information
0.5632892819	overlap
0.5632596082	test clean
0.5632020245	percentage
0.5631775729	interpolation
0.5629902214	measurement
0.5629400048	graphemic
0.5629299406	label
0.5626901213	contrary
0.5626374290	file
0.5626118808	shape
0.5625612922	output sequences
0.5625505532	equal
0.5623842380	sensor
0.5623733204	suprasegmental
0.5622792651	professional
0.5621406771	deploy
0.5621204332	decoding
0.5621076712	scaling
0.5620755644	unknown number of speakers
0.5620235056	progressive
0.5620108026	resnet
0.5619794715	mixup
0.5619499225	nowadays
0.5618830665	fact
0.5617920596	approach outperforms
0.5617487722	tongue
0.5617272786	consistency
0.5617084082	jointly optimized
0.5616631290	depth
0.5615818357	binaural cues
0.5614662178	bag
0.5613522385	sad
0.5612873360	machines
0.5612775682	music tracks
0.5612252848	word2vec
0.5612191418	wav2vec
0.5612191418	arrangement
0.5612191418	restoration
0.5612191418	quantity
0.5612191418	phonation
0.5612191418	converter
0.5612191418	laboratory
0.5612191418	quick
0.5612191418	aggregated
0.5612191418	neuron
0.5612191418	landmark
0.5612191418	rendering
0.5612126499	minimum
0.5611636765	development set
0.5610447513	revealed
0.5609314167	capturing
0.5608954463	speech recognition systems
0.5608907349	gammatone
0.5608854904	level
0.5605081999	ambisonic
0.5604398330	unifying
0.5604398330	discriminatively
0.5604391943	degradation
0.5603520725	normalizing
0.5603434079	interviews
0.5602645946	phoneme
0.5599437553	transcribed speech
0.5598354039	multi talker speech separation
0.5598010127	subjective quality
0.5594652223	sdr improvement
0.5594367160	parameter
0.5593945175	result shows
0.5593821229	amplitude
0.5593290581	valence
0.5592253993	extended
0.5591897721	range
0.5590825454	system's
0.5590772327	labels
0.5589847301	owing
0.5589847301	corresponds
0.5589731235	varying
0.5589309980	recurrent network
0.5587392511	health
0.5587310549	prior
0.5585953193	forgetting
0.5585797209	median
0.5585655555	utterance
0.5585472375	test
0.5585427682	output
0.5585045589	means
0.5584716911	typical
0.5584138766	sets
0.5583509040	trajectory
0.5583509040	fake
0.5583509040	curriculum
0.5583428181	future
0.5583063628	phoneme to viseme
0.5582744306	environmental
0.5581487001	cyclic
0.5581039023	attack
0.5580558645	optimized
0.5580193014	polarity
0.5580193014	ontology
0.5580193014	karaoke
0.5580193014	archive
0.5579684723	assumed
0.5579684723	emerged
0.5579422443	kaldi
0.5579088842	minimize
0.5579052233	motion
0.5578431438	stand alone
0.5578132492	principles
0.5577960378	image
0.5576980807	representation
0.5575800872	leading
0.5575045935	voiceprint
0.5573465159	speaker recognition systems
0.5573400341	recover
0.5573400341	bound
0.5573400341	interpretability
0.5573224633	flows
0.5573224633	bootstrapping
0.5573224633	advancing
0.5573193484	consist
0.5571999699	benchmark dataset
0.5571942201	mixed
0.5571429261	imaging
0.5570510078	hoc
0.5570510078	depend
0.5570254074	waveform
0.5568911586	flexible
0.5568605912	modification
0.5568449758	band
0.5568421702	biometric
0.5568003357	geometry
0.5567959813	performances
0.5567875067	pulse
0.5566590030	knowledge
0.5565522928	attributed
0.5564820457	deep feature
0.5563112829	bandwidth
0.5562747066	model complexity
0.5562411829	dev
0.5562162116	feasibility
0.5561618330	signals
0.5561332199	arrival
0.5561142041	scene
0.5560829436	attacks
0.5560214319	music production
0.5559946279	unlabeled dataset
0.5559405592	intelligibility
0.5559293898	aids
0.5558344350	scenes
0.5558303772	distortion
0.5557935119	density
0.5557900856	eval
0.5557797818	assuming
0.5557383062	tensor
0.5556263415	emotional
0.5556019029	kernel
0.5555613934	objective evaluations
0.5555343333	factorization
0.5555130465	classification problem
0.5554240466	high
0.5554226215	variables
0.5553613327	dimensional
0.5553347039	capsule
0.5553193484	thousands
0.5553141098	multiple
0.5552649043	frequency components
0.5552293412	additive
0.5552237637	relative
0.5551657475	models
0.5550494764	subsequently
0.5550283571	traditionally
0.5550243172	overcome
0.5549998805	unique
0.5549553002	daily
0.5548942107	invariant training
0.5547203214	database
0.5547145735	log
0.5546946640	notably
0.5546754156	fair
0.5546586994	feature level
0.5546407130	retaining
0.5546407130	popularity
0.5545178383	style
0.5544673542	songs
0.5544648041	rich
0.5544264625	gender
0.5544245965	theory
0.5544236067	feature embedding
0.5544116273	tracks
0.5543823276	general
0.5541918004	commands
0.5540092686	relationships
0.5539984265	reading
0.5539460669	position
0.5539438826	possibility
0.5538870687	glottal
0.5538748943	field
0.5538723517	characterized
0.5538064105	rakugo
0.5538064105	byte
0.5537536331	propagation
0.5537257552	detectors
0.5536659976	support
0.5536483741	compact
0.5536471847	interval
0.5535987555	sourced
0.5535522928	correspond
0.5534185336	dequantization
0.5534185336	remarkably
0.5534185336	separator
0.5534185336	divide
0.5534185336	analogy
0.5534185336	surgical
0.5533497824	distribution
0.5533410245	microphones
0.5532656349	acoustic to articulatory
0.5531061785	fitting
0.5530734282	text dependent
0.5530454354	transparent
0.5530454354	fluctuations
0.5530454354	laughter
0.5530454354	deriving
0.5530454354	simulator
0.5528395799	tuned
0.5528347932	functional
0.5525930242	negative
0.5524707172	experimented
0.5524707172	contact
0.5524707172	clarity
0.5524707172	hands
0.5524707172	assistive
0.5524707172	noticeable
0.5524043671	deliberation
0.5524043671	prosthetic
0.5524043671	choir
0.5524043671	merging
0.5524043671	anger
0.5524043671	federated
0.5524043671	awareness
0.5524043671	linking
0.5524043671	opening
0.5524043671	regularizer
0.5524043671	relational
0.5524043671	sustain
0.5524043671	scalability
0.5524043671	steered
0.5524043671	versatile
0.5524043671	reflecting
0.5524043671	matter
0.5524043671	observing
0.5524043671	motifs
0.5524043671	bimodal
0.5524043671	recommendations
0.5524043671	inspection
0.5524043671	ensuring
0.5524043671	collapsed
0.5524043671	multitrack
0.5524043671	examining
0.5523739512	diversity
0.5523487282	algorithmic
0.5522950077	voice recordings
0.5522924089	mapped
0.5522848847	techniques
0.5522460581	playlist
0.5521980981	focused
0.5520041806	unseen noise
0.5519375877	abnormal
0.5519210657	subtraction
0.5519003155	follow
0.5517375397	encoders
0.5517212651	softmax
0.5516863790	compensate
0.5514910142	triplet
0.5513585641	db
0.5513398580	bridging
0.5513059095	esc 10
0.5512895891	experiences
0.5512895891	induced
0.5512895891	sequentially
0.5512895891	epoch
0.5512895891	manifold
0.5512895891	performers
0.5512895891	teaching
0.5511825436	waveform synthesis
0.5510891952	ladder
0.5510841957	query
0.5509606071	unison
0.5509531114	stuttering
0.5507986202	impulsive
0.5507986202	interleaved
0.5507446782	sensitive
0.5507210581	careful
0.5507001883	principle
0.5505637632	understanding
0.5504795452	working
0.5503960922	pair
0.5502770708	directional
0.5502759736	algorithms
0.5501373626	standard cnns
0.5500875018	x vectors
0.5498654264	insight
0.5498448541	contributions
0.5497231317	vocabulary continuous
0.5496778223	broadcast
0.5496546540	child
0.5496382189	rnns
0.5496077049	spelling
0.5496077049	simulating
0.5496077049	entrainment
0.5495958146	retrieval
0.5495792702	shift
0.5495667240	count
0.5495495870	sub band
0.5495230863	dysarthric
0.5494825967	acoustic properties
0.5494057408	deep learning approach
0.5493217172	importantly
0.5493217172	percussive
0.5493217172	competition
0.5493217172	multispeaker
0.5493170701	text data
0.5492018367	benchmark
0.5491786878	assessment
0.5491326570	orders
0.5491162781	ultra
0.5491162781	ultrasonic
0.5491162781	distilling
0.5491162781	interpreting
0.5490073853	rescore
0.5490073853	theme
0.5490073853	chapter
0.5490073853	generalizable
0.5490073853	mitigating
0.5490073853	bio
0.5490073853	stemming
0.5490073853	boosted
0.5487579197	privacy
0.5486506284	refers
0.5486193968	deep learning based speech enhancement
0.5485932489	generalizability
0.5485554243	arousal
0.5485461792	expectation
0.5485241674	caption
0.5485241674	translating
0.5485241674	landmarks
0.5485241674	completion
0.5485241674	knowing
0.5483463422	words
0.5482178074	recognizers
0.5481463837	utility
0.5481158198	shared
0.5478924048	vocoding
0.5478555065	things
0.5477410072	events
0.5475748577	refined
0.5475748577	explanations
0.5475362866	disfluency
0.5475362866	pursuit
0.5474762109	reverberant
0.5473893244	subjective
0.5473612432	modality
0.5473386480	hypernasality
0.5473386480	clipping
0.5473386480	supervector
0.5473386480	inaudible
0.5473386480	dubbing
0.5473090689	attentional
0.5470347261	soft
0.5469231211	boosting
0.5468945214	investigation
0.5468655845	spectra
0.5468655845	initial
0.5468564155	architecture
0.5467654629	data processing
0.5467584279	gans
0.5467273633	gating
0.5466064749	absolute
0.5465587251	insights
0.5465209516	characterization
0.5465209516	hear
0.5464551411	recursive
0.5463417960	bias
0.5463229358	tutorial
0.5463069888	applications
0.5462704158	noise conditions
0.5462312942	warping
0.5460314513	experiment
0.5459695267	quasi
0.5459695267	necessarily
0.5458940695	chime 4
0.5458259601	wind
0.5458152777	index
0.5457712244	grid
0.5457126698	mechanisms
0.5456901079	cross
0.5456436478	refining
0.5456436478	animal
0.5456436478	novelty
0.5456436478	foundation
0.5456436478	essentially
0.5456436478	pilot
0.5456436478	updates
0.5456436478	symmetric
0.5456436478	periodicity
0.5456436478	programs
0.5456436478	spatialized
0.5456436478	consonants
0.5456436478	choosing
0.5456436478	annotators
0.5456436478	template
0.5456436478	surpass
0.5456436478	surrogate
0.5456436478	narrowband
0.5454703963	drawn
0.5454703963	assist
0.5453691636	sound scene
0.5453376102	error
0.5453366473	tied
0.5452708874	integrate
0.5452022235	significantly reduce
0.5451604559	psychological
0.5451604559	organization
0.5451600320	project
0.5451264860	singer
0.5449725150	transcribe
0.5449442900	block
0.5447241256	diagnosis
0.5446921997	major
0.5446751669	input signal
0.5445797648	numbers
0.5445248462	bring
0.5443034739	dedicated
0.5442743212	benefit
0.5441643509	deep learning methods
0.5441558466	influence
0.5441173720	mapping
0.5439849166	chime 5
0.5439107552	disentangling
0.5439107552	assessing
0.5439107552	refinement
0.5438843493	baseline
0.5438781505	numerous
0.5438694323	point
0.5438253456	dictionaries
0.5437387026	et al
0.5435619301	alternative
0.5433575549	image classification
0.5432952331	affected
0.5432251972	target
0.5432133884	audio signal
0.5432028075	batch
0.5429413826	sources
0.5428781035	false
0.5428704270	adapt
0.5428035075	guided
0.5427953243	augmenting
0.5427556401	freely
0.5426774930	breathing
0.5426497576	instance
0.5425265792	serve
0.5424787237	approach
0.5424774980	rhythm
0.5423911198	arbitrary
0.5423874724	question
0.5422681089	factorized
0.5422614597	retrieve
0.5422511853	weakly
0.5422281412	polyphonic
0.5422192135	librispeech dataset
0.5422190660	high level features
0.5422054568	dnns
0.5421810431	metric
0.5421604974	commonly
0.5421525760	robotic
0.5421506352	optimal
0.5421424121	serves
0.5419730744	technical
0.5419228008	paper describes
0.5419097024	informed
0.5418493638	firstly
0.5418440720	decomposition
0.5418420955	intent
0.5418350855	jointly
0.5418136663	far field
0.5417804900	purpose
0.5417107743	maintain
0.5416826545	parts
0.5415097969	telephone
0.5414961427	asynchronous
0.5414705378	qualitative
0.5414028208	probabilistic linear
0.5412685553	naturalistic
0.5412420220	kinds
0.5412256027	single source
0.5411282083	classify
0.5409870052	distributions
0.5409811903	testing
0.5409678706	common
0.5408648166	unable
0.5408616145	inferring
0.5408616145	analog
0.5408616145	initially
0.5408616145	expansion
0.5408074118	perturbations
0.5407468903	residual learning
0.5405523859	validity
0.5405513247	nonnegative
0.5405511442	fully
0.5405121368	investigating
0.5404428061	convergence
0.5404370234	location
0.5403453869	end
0.5402745236	adult
0.5402546317	deterministic
0.5402417817	base
0.5402291882	learn
0.5402275138	controllable
0.5401204219	model outperforms
0.5400296553	turn
0.5398743212	disease
0.5398665159	neuromorphic
0.5398665159	pouring
0.5398665159	diagnose
0.5398665159	audition
0.5398665159	documentation
0.5398665159	vibration
0.5398665159	indicators
0.5398665159	eventually
0.5398665159	anomalies
0.5398665159	experimentation
0.5398665159	spoof
0.5398665159	interacting
0.5398665159	cascading
0.5398665159	practically
0.5398665159	repetitive
0.5398375115	definition
0.5397733644	prone
0.5397667493	vectors
0.5397623692	pattern
0.5397115679	bands
0.5395525307	semi
0.5393150311	date
0.5393068424	input
0.5390935403	refer
0.5390884444	transformers
0.5390714894	integrated
0.5390429253	music score
0.5389589725	included
0.5388319583	computer vision
0.5386431414	automatic music generation
0.5386139399	list
0.5385797867	past
0.5385111149	dependent
0.5384424886	sound recognition
0.5384342610	operating
0.5384232575	direction
0.5384034537	dysarthria
0.5382689967	spatial information
0.5382474763	suited
0.5381831977	kind
0.5381831977	scratch
0.5379774587	previously
0.5379432737	rate
0.5378403435	perplexity
0.5378214428	unit
0.5378150912	sequence level
0.5377523082	direction of arrival estimation
0.5377411726	segmental
0.5377012276	improve
0.5376206577	switched
0.5375652982	processing tasks
0.5375602538	recommendation
0.5375404428	reflections
0.5374468146	integration
0.5374137993	asvspoof 2019
0.5372640239	beats
0.5371842973	enhancing
0.5371233709	street
0.5369706492	background
0.5369359961	excitation
0.5369092163	hardware
0.5368070199	vocals
0.5367495236	positional
0.5367495236	transcribing
0.5367495236	echoes
0.5367495236	feedforward
0.5367495236	guidance
0.5367001432	sharing
0.5366430508	sparsity
0.5366021170	listeners
0.5365706236	established
0.5364737339	formant
0.5364546947	vocoders
0.5364251659	clustering based
0.5363891546	validated
0.5363279426	cancellation
0.5362489845	parameters
0.5362237176	subset
0.5360908457	screen
0.5360862942	sonification
0.5360814408	incremental
0.5359932713	difficulty
0.5359631539	processes
0.5358995192	user
0.5358836127	vast
0.5358657429	removal
0.5356769481	dcase 2017
0.5354663303	fields
0.5354148348	tonal
0.5353850183	characters
0.5353542113	quaternion
0.5353214490	computer assisted
0.5352640651	speaker encoder
0.5352605374	term
0.5352585914	drone
0.5352093827	subspace
0.5351613683	transferable
0.5350922512	stimulus
0.5350132228	practical
0.5349893269	superiority
0.5348253626	artist
0.5348094509	compositional
0.5347280391	actions
0.5347237339	localisation
0.5345434702	steps
0.5344841002	functions
0.5344803261	distances
0.5343114478	sufficient
0.5343100313	shallow
0.5342871923	upsampling
0.5342792492	fuse
0.5342003553	2mix
0.5342003553	depends
0.5341771944	window
0.5341735247	spaces
0.5341380282	cross task
0.5341376078	pathological
0.5341376078	transforming
0.5341376078	waves
0.5341376078	implicit
0.5341376078	navigation
0.5341376078	spatially
0.5341325698	heterogeneous
0.5340649354	latent
0.5339711793	dependency
0.5338903498	slot
0.5338126586	substantial
0.5337989785	physical
0.5336357641	image domain
0.5333719883	combination
0.5332394507	tandem
0.5332394507	protection
0.5331516473	bridge
0.5331447368	vulnerable
0.5331210406	image to image
0.5331002409	designing
0.5328202687	suffers
0.5327229508	train
0.5326936899	additional
0.5326683909	length
0.5326565966	similar
0.5325900839	baselines
0.5324105401	assistant
0.5324105401	voicing
0.5321658862	signal
0.5321563025	significant
0.5319941030	light
0.5318609056	decision
0.5317224266	stop
0.5317224266	toolkits
0.5317224266	instrumentation
0.5317224266	policy
0.5316519698	augment
0.5315944920	requirement
0.5315454068	skipping
0.5315454068	summary
0.5315454068	adversary
0.5315454068	transferability
0.5315454068	pronounced
0.5315454068	extensions
0.5315454068	reverberated
0.5315454068	centered
0.5315454068	mimicking
0.5315454068	regularize
0.5315454068	paradigms
0.5315454068	constructing
0.5315454068	engineered
0.5315454068	motions
0.5315454068	unvoiced
0.5315454068	smartphones
0.5315454068	coverage
0.5315285431	learnt
0.5315285431	deployed
0.5314651962	noise signals
0.5313903002	auscultation
0.5313903002	rating
0.5313903002	acted
0.5313903002	differently
0.5313903002	sliding
0.5313903002	shaping
0.5313903002	elementary
0.5313903002	prominence
0.5313903002	dominance
0.5313903002	quadratic
0.5313903002	shouted
0.5313903002	diagonal
0.5313903002	highlights
0.5313903002	transposition
0.5313903002	bootstrap
0.5313903002	references
0.5313903002	nonstationary
0.5313903002	summarization
0.5313903002	morphing
0.5313566248	method
0.5313427108	hard
0.5313051585	stream
0.5312845678	source language
0.5312820549	actual
0.5312347214	attempts
0.5311877899	loops
0.5311877899	aural
0.5311877899	searching
0.5311877899	absorption
0.5311877899	occurrences
0.5311877899	parameterized
0.5311877899	graphs
0.5311877899	comparatively
0.5311877899	grounding
0.5311877899	height
0.5311877899	preserved
0.5311877899	standing
0.5311877899	analogous
0.5311877899	perspectives
0.5311877899	deploying
0.5311877899	selective
0.5311877899	cities
0.5311877899	coordinate
0.5311877899	balancing
0.5311877899	bases
0.5311877899	correlates
0.5311503506	strategies
0.5311032833	repeated
0.5311032833	concatenation
0.5311032833	repetition
0.5311032833	eigenvalue
0.5311032833	package
0.5311032833	letter
0.5311032833	eliminating
0.5310299658	sense
0.5310011104	individual
0.5310002861	adapted
0.5309699079	detect
0.5309683173	left
0.5308863115	tens
0.5308863115	fails
0.5307476484	space
0.5307169321	efficiency
0.5306803749	cues
0.5306021652	consonance
0.5306021652	shortcomings
0.5306021652	decade
0.5306021652	practitioners
0.5306021652	aligning
0.5306021652	whilst
0.5306021652	feeding
0.5305146724	wise
0.5304565812	inference
0.5302795174	ranging
0.5302396420	increasing attention
0.5301761035	instruments
0.5300356843	creation
0.5300253403	graphemes
0.5300253403	lips
0.5300253403	volume
0.5300253403	forcing
0.5299543992	person
0.5299543992	solely
0.5299292983	mismatch
0.5298289544	half
0.5296447156	pass
0.5296078210	linguistic
0.5294752328	bottleneck
0.5294470308	interactions
0.5293180362	classic
0.5293180362	refine
0.5292117088	input feature
0.5290393381	examples
0.5290073983	data sparsity
0.5289354375	wideband
0.5289234873	array
0.5288877325	minimal
0.5288485597	complementary
0.5287595563	syntactic
0.5287595563	edition
0.5287595563	mouth
0.5286921622	generated
0.5286612553	shaped
0.5286612553	paralinguistic
0.5286358594	mixed speech
0.5284899612	aishell 1
0.5283164961	iii
0.5283106013	machine learning models
0.5282986716	dense
0.5282975787	denoising and dereverberation
0.5281206790	subband
0.5281206790	regularized
0.5280572865	security
0.5280043887	modal
0.5279931027	measures
0.5279161714	document
0.5279161714	pixel
0.5279161714	outcomes
0.5278834241	controlling
0.5278834241	pairwise
0.5278107809	maximize
0.5277634800	validation
0.5276940741	dialogue
0.5276255385	samples
0.5272817586	evaluation dataset
0.5272662091	speed
0.5271923806	multiple sources
0.5271137744	structure
0.5268905742	ensure
0.5268037872	files
0.5266265495	architectures
0.5265776038	silent speech
0.5265053643	weighting
0.5264982692	variability
0.5264561207	software
0.5264310004	hundreds
0.5263439608	x vector
0.5263057719	classifier
0.5262749856	suppress
0.5262514205	problem
0.5259819635	select
0.5258943033	recognizing
0.5258230729	loudspeakers
0.5258169885	deconvolution
0.5258169885	bad
0.5256707517	maximum mutual
0.5256471715	statistical parametric
0.5255855133	analytic
0.5255855133	spontaneity
0.5255855133	mixes
0.5255855133	wearing
0.5255855133	biasing
0.5255196309	establish
0.5254422797	speaking
0.5254260082	par
0.5253918769	ratio
0.5253811311	errors
0.5253806690	mitigate
0.5253471334	content
0.5253011319	formants
0.5252571285	straightforward
0.5251293871	styles
0.5251100988	life
0.5250875239	differentiable
0.5249834501	performance
0.5249103520	function
0.5247238714	bins
0.5245718461	utterances
0.5244573060	speakers
0.5242205105	corrupted
0.5241755754	contribution
0.5240601382	transcribed
0.5240518071	musical scores
0.5240295682	small
0.5237877271	high performance
0.5237813006	target sequence
0.5237563697	robust speaker recognition
0.5237532462	data efficient
0.5236766266	noisy reverberant
0.5236387648	team
0.5236387648	convolutive
0.5236288276	relevant
0.5236158033	comparison
0.5235690246	step
0.5235020451	spike
0.5233187209	exploring
0.5231584175	degrees
0.5231190591	reliable
0.5229880533	property
0.5229707065	condition
0.5229588560	clean
0.5229337740	change
0.5227657629	converted
0.5227532421	tacotron based
0.5225417461	entire input
0.5224811930	taxonomy
0.5224811930	min
0.5224333427	number
0.5221604357	research
0.5220788558	consisting
0.5220443952	deep learning models
0.5219888390	previous methods
0.5219886342	keeping
0.5218402357	periodic
0.5217880792	matching
0.5217662849	user's
0.5216966341	resourced
0.5215764020	age
0.5215761185	represented
0.5215676555	prevent
0.5214700957	finally
0.5213633883	calibration
0.5213447819	supervised manner
0.5213141887	application
0.5212986119	usability
0.5212558827	development dataset
0.5211777961	achieving
0.5211506736	genres
0.5209681217	recognize
0.5208997896	calculate
0.5208997896	heavily
0.5208997896	upper
0.5208997896	answer
0.5207615975	crafted
0.5207151874	footprint
0.5206600668	public datasets
0.5204898765	systems
0.5204384014	severity
0.5203808314	high level feature
0.5202660775	methods
0.5202535388	acoustic to word
0.5201141163	margin
0.5201116108	measured
0.5199412411	additionally
0.5199316202	differences
0.5198844882	synthesized
0.5198442273	abstract
0.5197844311	popular
0.5197740336	compared
0.5196803749	linear predictive
0.5193586019	public
0.5193579974	tokens
0.5193422499	unlabelled
0.5192822622	collection
0.5190402087	predictor
0.5189267488	hybrid asr
0.5188369323	shapes
0.5188006210	network architectures
0.5187962063	probabilities
0.5184757016	components
0.5183750780	conditions
0.5183677205	overlapped
0.5183379768	layer
0.5182674488	proposed
0.5181292157	topic
0.5181206868	named
0.5180651303	speech to text translation
0.5180338450	text independent speaker
0.5178615521	corpora
0.5177667207	exploration
0.5177551305	report
0.5177460541	specific
0.5174966958	experience
0.5173951995	transcribed data
0.5172279975	effect
0.5171905189	aspect
0.5171141894	flexibility
0.5168314033	interfering
0.5168010193	external
0.5167261752	inter
0.5164365668	audio source separation
0.5163313604	trigger
0.5162388003	unlike
0.5161416320	voxceleb1 dataset
0.5159785292	efficacy
0.5158166343	partial
0.5157991417	end to end speech recognition
0.5156521585	extractor
0.5155604499	gradients
0.5155604499	imbalance
0.5155604499	rest
0.5155604499	split
0.5155604499	reproducible
0.5155132173	meetings
0.5155132173	personalized
0.5155132173	extending
0.5154901681	partially
0.5154863563	attention model
0.5154830488	beam
0.5154812294	relying
0.5154685126	projected
0.5154204747	criteria
0.5154144663	r
0.5153830941	reference
0.5153513129	acoustic signal
0.5151206566	setting
0.5150420569	articulatory to acoustic
0.5150266716	basic
0.5148914888	black
0.5147738523	offset
0.5145490929	aid
0.5145181391	reference speech
0.5144733423	invertible
0.5144733423	tag
0.5144478449	resolution
0.5144263423	weights
0.5144198458	telephone speech
0.5143799752	choice
0.5142339031	framework
0.5141617055	carrier
0.5141617055	coughs
0.5140819382	candidates
0.5140819382	passive
0.5140819382	profiles
0.5140763248	localize
0.5139859891	dialog
0.5137598779	pre
0.5136963194	relies
0.5136930635	domain translation
0.5135510434	confusion
0.5135510434	priors
0.5135112793	crowdsourcing
0.5135112793	foreign
0.5135112793	ranking
0.5134843085	unseen
0.5134304426	deep neural network based
0.5134013776	pieces
0.5132794035	profile
0.5132794035	scientific
0.5132794035	sections
0.5132794035	smartphone
0.5132794035	irrelevant
0.5132794035	iteration
0.5132794035	distinguishing
0.5132794035	locally
0.5132794035	prevalent
0.5132794035	engine
0.5132794035	shifting
0.5132794035	transitions
0.5132761655	generalize
0.5131224470	infer
0.5130851898	rely
0.5130731358	experts
0.5130731358	tones
0.5130696622	addition
0.5130616613	patient
0.5129838645	formulated
0.5128354302	applicability
0.5128154068	psychoacoustic
0.5128154068	phonological
0.5128154068	robots
0.5126792271	approximation
0.5125672293	onset
0.5125516979	smoothed
0.5125045629	speech representations
0.5123651221	infection
0.5123651221	fluency
0.5123651221	multiplicative
0.5123651221	unmixing
0.5123651221	delayed
0.5123651221	apparent
0.5123651221	interpolate
0.5123651221	fragments
0.5123651221	briefly
0.5123651221	adjusted
0.5123651221	engagement
0.5123651221	connect
0.5123651221	unconditional
0.5123651221	estimations
0.5123651221	alternatives
0.5123651221	ordering
0.5123651221	skills
0.5123651221	frontend
0.5123651221	women
0.5123651221	accessibility
0.5123651221	decomposing
0.5123651221	stops
0.5123651221	communicate
0.5123651221	vocalization
0.5123651221	memorability
0.5123651221	closing
0.5123651221	lookahead
0.5123651221	segmenting
0.5123651221	phenomena
0.5123651221	adjusting
0.5123651221	explanation
0.5123651221	operated
0.5123651221	artistic
0.5123651221	incorporation
0.5123651221	tail
0.5123651221	enforce
0.5123651221	multiscale
0.5123519944	compare
0.5122694218	final
0.5121958932	elements
0.5121432080	paradigm
0.5119852628	absence
0.5117348052	referred
0.5116786291	mechanism
0.5116759110	synthesis quality
0.5116026962	recently developed
0.5115671427	industrial
0.5115671427	purely
0.5115401230	computationally
0.5114397193	tasks
0.5113031386	problems
0.5113011778	modeled
0.5112946746	article
0.5112876468	limitations
0.5111921978	ratios
0.5109618064	aggregation
0.5107971444	caused
0.5107424451	task specific
0.5106938501	cascade
0.5106239944	factors
0.5105933396	accuracy
0.5105822173	meeting
0.5105019423	symbolic
0.5103154461	pass filter
0.5099782626	avoid
0.5099276776	captured
0.5098220946	carried
0.5096269360	sequences
0.5096249722	characterize
0.5096172182	neutral
0.5095923338	feed
0.5095318953	complete
0.5095264679	vowels
0.5094265042	minutes
0.5094221526	target voice
0.5093913238	submitted
0.5093802765	chime 3
0.5093437697	tract
0.5091587111	multimedia
0.5091390078	depending
0.5091047299	node
0.5090990270	constant
0.5090881452	related
0.5090683883	wake
0.5090633933	q
0.5090027490	amounts
0.5086894367	crucial
0.5086301847	speech activity
0.5086217664	suffer
0.5085701543	opinion scores
0.5085094917	codebook
0.5085094917	widespread
0.5084980813	consumption
0.5083479849	ratings
0.5083479849	semantics
0.5083139378	backend
0.5082910898	phonemes
0.5081855568	speech data
0.5080494619	unknown
0.5079493767	technique
0.5079268381	library
0.5078788681	helpful
0.5078689213	interpretable
0.5078586300	proved
0.5078141980	layers
0.5077991841	consuming
0.5076390078	al
0.5076318264	occur
0.5075935444	acoustic embeddings
0.5075276214	dynamics
0.5074738875	children's
0.5074325512	tests
0.5074045388	explicitly
0.5073948405	primary
0.5073680559	deal
0.5073369043	spoofed
0.5073369043	overlaps
0.5073369043	vulnerability
0.5073369043	gaining
0.5073369043	impairments
0.5073369043	hierarchically
0.5073369043	successive
0.5073369043	obstacle
0.5073369043	asymmetric
0.5073369043	surprising
0.5073369043	assessments
0.5073369043	symptom
0.5073369043	accepted
0.5073369043	students
0.5073369043	faced
0.5073369043	innovative
0.5073369043	expanding
0.5073369043	endpoints
0.5073369043	gate
0.5073369043	accompanying
0.5073369043	intuitively
0.5073369043	analogue
0.5073369043	influences
0.5073369043	exposure
0.5073369043	chroma
0.5073369043	triggered
0.5073369043	dropping
0.5073369043	demixing
0.5073369043	subtask
0.5073008103	main
0.5072661995	acoustic representations
0.5072345145	infants
0.5072054042	non parallel voice conversion
0.5071903400	ms
0.5071644471	empirically
0.5071558992	paper
0.5069888204	code switched speech
0.5069881392	captions
0.5068582047	fed
0.5068273073	monotonic
0.5066520625	proposed model outperforms
0.5065902603	active speaker
0.5065509890	making
0.5065407208	mixture signal
0.5064642390	bootleg
0.5064642390	worth
0.5064642390	damage
0.5064642390	nuisance
0.5064642390	timbres
0.5064642390	trials
0.5064642390	gemination
0.5064642390	instability
0.5064642390	transient
0.5064642390	expanded
0.5064642390	unify
0.5064642390	downsampling
0.5064642390	exclusively
0.5064642390	approximated
0.5064642390	cameras
0.5064642390	relate
0.5064642390	grade
0.5064642390	copy
0.5064642390	treating
0.5064642390	substitution
0.5064642390	finetuning
0.5064642390	compose
0.5064642390	symbol
0.5064642390	physics
0.5064642390	fingerprint
0.5064642390	updating
0.5064642390	keyboard
0.5064642390	trace
0.5064642390	vanishing
0.5064642390	beginning
0.5063437697	square
0.5062503836	target signal
0.5062059322	squared
0.5061902447	pre trained models
0.5061219468	entire
0.5060272618	successfully applied
0.5059496682	increasing
0.5059322759	significantly
0.5058991681	classification of acoustic scenes and events
0.5058957004	dimensionality
0.5058218007	likelihood
0.5057747472	built
0.5057623390	solutions
0.5056243434	determine
0.5055037772	connections
0.5053279010	generalizes
0.5053279010	fold
0.5053167868	transferring
0.5053167868	geometric
0.5053167868	ambiguity
0.5050095730	this technical report
0.5049867971	naturalness and similarity
0.5047140921	real
0.5047087996	angle
0.5047087996	massive
0.5045371868	source and target
0.5044382531	focuses
0.5044197690	speaker aware
0.5043478367	shot
0.5043420101	solve
0.5043144325	composed
0.5039036617	thresholding
0.5039036617	recovery
0.5038984944	stimuli
0.5037845856	shown promising results
0.5037215811	illustrate
0.5036354732	paper presents
0.5036171148	stages
0.5035707286	canonical
0.5035560904	disentanglement
0.5034981006	pipeline
0.5033318718	frames
0.5032892118	key
0.5031710830	perceived
0.5024868686	adding
0.5024616777	types
0.5023190797	design
0.5021766248	recording
0.5021501903	amplitude and phase
0.5020814470	child speech
0.5020412575	reduced
0.5019748196	alleviate
0.5019534560	magnitude
0.5019425117	reconstructing
0.5019039220	valued
0.5018614043	simulate
0.5018593244	motor
0.5018440270	compares
0.5017394777	aspects
0.5015606867	advantage
0.5015312727	scalable
0.5014820690	scores
0.5013444629	performance gain
0.5012387078	frisson
0.5011123937	clear
0.5010671343	identity
0.5009753115	hours
0.5009463993	approaches
0.5009403625	points
0.5008509402	capacity
0.5008012121	capable
0.5008008736	extension
0.5007986438	clean and noisy
0.5007030790	scheme
0.5006111174	remote
0.5006111174	opportunities
0.5006111174	redundancy
0.5006111174	accessible
0.5006111174	benchmarking
0.5006111174	linearly
0.5006111174	indoor
0.5006111174	recipes
0.5004597614	span
0.5004597614	kernels
0.5004388184	nature
0.5003643218	subjective listening
0.5000937801	noisy and reverberant
0.5000616564	interpretation
0.4999892340	m
0.4998968894	adversarial network
0.4997837827	limited
0.4997113859	success
0.4996013874	respect
0.4995056250	movements
0.4994856726	waveforms
0.4994223164	limitation
0.4991940846	reduction
0.4991301599	did
0.4990998595	whispered speech
0.4990897882	ears
0.4990897882	envelopes
0.4990897882	discriminating
0.4990662809	separated
0.4989654521	performance gains
0.4989111435	performing
0.4988689432	time frequency scattering
0.4987986438	objective and subjective
0.4987965518	temporally
0.4986566588	capability
0.4985848494	competitive
0.4985322339	network learns
0.4985249688	fewer
0.4984859369	magnitude and phase
0.4984410881	sparse
0.4984245261	diarization systems
0.4983554387	core
0.4983359393	manually
0.4982188945	process
0.4980039140	factor
0.4979091403	responses
0.4978815224	results reveal
0.4978092000	fusing
0.4978092000	collecting
0.4978092000	diagnostic
0.4978092000	curated
0.4977749408	combined
0.4977505263	data augmentation method
0.4977369729	maintaining
0.4977193416	periods
0.4976119357	lower
0.4976016275	encoder decoder models
0.4974983858	aligned
0.4973205234	tool
0.4972968582	reduces
0.4972400173	robustness
0.4972392597	audio adversarial examples
0.4971107459	spherical
0.4970652142	learnable
0.4970200080	replace
0.4969520640	topics
0.4968650033	variance
0.4967830430	extraction network
0.4965606864	consonant
0.4964842693	unpaired data
0.4964156156	access
0.4963442545	u
0.4962714916	capabilities
0.4961498276	bilingual
0.4961224601	scale
0.4960269099	normal
0.4958617114	lack
0.4958275695	smaller
0.4958074184	ubiquitous
0.4958074184	posteriors
0.4958074184	identical
0.4958074184	perfect
0.4958074184	properly
0.4958074184	biased
0.4958074184	strengths
0.4958074184	equipment
0.4958074184	act
0.4958063720	targeted
0.4957922264	pre trained model
0.4957142152	superior
0.4956193322	synthesizer
0.4954832933	relative word error
0.4953331378	goal
0.4952957811	inter speaker
0.4951690744	challenging
0.4950434961	relations
0.4950434961	engineering
0.4950249571	source target
0.4949575067	encoded
0.4947986438	subjective and objective
0.4947980735	acoustic unit
0.4947748343	noise and reverberation
0.4947737501	desired
0.4947612134	operate
0.4947612134	involved
0.4947528519	modified
0.4947186657	variant
0.4946933295	outperforms
0.4946705255	reasoning
0.4946705255	discussion
0.4946705255	rooms
0.4945593811	timbre and pitch
0.4945000174	invariance
0.4945000174	frequently
0.4945000174	exact
0.4944592644	equivalent
0.4944007731	run
0.4943427818	large scale weakly
0.4942898585	reverberant and noisy
0.4942489759	offsets
0.4942429720	datasets
0.4942393677	versions
0.4941953701	spatial sound
0.4941195077	co occurrence
0.4940908562	remaining
0.4940908562	pruning
0.4940908562	converting
0.4940331630	divergence
0.4939952170	times
0.4939389727	multivariate
0.4939389727	independence
0.4939389727	windows
0.4939248826	generate
0.4939004942	extracted features
0.4938836581	scenarios
0.4937401151	achieve
0.4936573013	textures
0.4936362900	degree
0.4935242513	speech chain
0.4934537604	posterior
0.4934140248	importance
0.4933710492	regularization
0.4932933793	small footprint keyword
0.4932908313	highlight
0.4932676104	latency
0.4931877615	characteristic
0.4931193859	timit dataset
0.4930986503	seconds
0.4929863249	present
0.4929406273	gradient based
0.4928122878	under noisy conditions
0.4928007644	standard
0.4926924031	advanced
0.4925213003	predicted
0.4924054559	predict
0.4921367137	rescoring
0.4918531850	cover
0.4918473475	adverse
0.4917801185	tackle
0.4917244374	audio event detection
0.4916663423	complementary information
0.4916585267	implementation
0.4916535849	asr and tts
0.4916274874	carried out
0.4915676363	dropout
0.4915432229	correlated
0.4915422365	achieves
0.4913958245	semantic information
0.4912855372	enhance
0.4912407168	variety
0.4912311549	constraints
0.4911174260	stability
0.4911174260	cue
0.4910357746	scenario
0.4910293056	defined
0.4909334659	applicable
0.4909181658	properties
0.4907444098	identify
0.4906452063	original
0.4905147127	multiple modalities
0.4905003680	carry
0.4905003680	activities
0.4904670363	phrase
0.4904204897	methodology
0.4904164828	evaluation shows
0.4903192934	handle
0.4902795737	multi speaker text to speech
0.4901852805	match
0.4900979237	great
0.4900373954	assess
0.4899822623	deep learning techniques
0.4899419139	automated
0.4898952900	bidirectional long short term
0.4897748001	metadata
0.4897748001	comparisons
0.4897482632	module
0.4895840565	stationary
0.4893474828	non negative matrix
0.4891966113	resource
0.4890512399	complexity
0.4888935693	gain
0.4888699721	precision
0.4888101323	improves
0.4887248673	response
0.4886881407	comparable
0.4885792823	derived
0.4885706391	specialized
0.4885706391	musically
0.4885706391	semantically
0.4885706391	communities
0.4885112134	disentangle
0.4884663122	dcase 2018 challenge
0.4884303286	encoder and decoder
0.4883517786	area under
0.4883135412	combinations
0.4882496242	quality and intelligibility
0.4881099958	behavior
0.4881001850	probability
0.4880633238	paper introduces
0.4880611316	target text
0.4880418310	convert
0.4880418310	advantages
0.4880338712	achieved
0.4879720940	production
0.4879444034	piece
0.4878557257	previous research
0.4878075392	directly
0.4877490549	clones
0.4877490549	octaves
0.4877220703	lexicon
0.4877164970	thanks
0.4876793942	observed
0.4876757497	earlier
0.4875825460	blocks
0.4875287384	perceptually
0.4875287384	classifying
0.4873405926	trained
0.4873236453	solution
0.4871017510	update
0.4870936411	aimed
0.4870296334	acoustic characteristics
0.4869430579	cost
0.4868881220	evaluated
0.4867973473	silence
0.4867692721	special
0.4866082469	facilitate
0.4864720703	entity
0.4863928136	interference
0.4863134152	separate
0.4857108007	propose
0.4857034346	asr errors
0.4856039298	beamforming
0.4854824827	width
0.4854205649	speech representation learning
0.4853850695	high fidelity audio
0.4853831440	community
0.4852790360	reverberation and noise
0.4852276566	effects
0.4850463805	tend
0.4850270761	unsupervised pre training
0.4850088325	remove
0.4850088325	considerable
0.4849015733	include
0.4848971790	proven
0.4848274575	inputs
0.4847345151	retraining
0.4847345151	speeches
0.4846716551	frequencies
0.4846396106	voice over
0.4846204859	biometric systems
0.4845374039	choices
0.4843587279	path
0.4842279183	rolls
0.4842279183	grouping
0.4839270360	syllable
0.4838673752	issues
0.4837646537	intuitive
0.4836678328	difficult task
0.4835936493	melodic
0.4835131108	symptoms
0.4835131108	private
0.4832314000	interesting
0.4832314000	carefully
0.4831286738	respective
0.4831286738	quantify
0.4830227523	natural
0.4830166984	area of research
0.4830051634	categorical
0.4829938075	presenting
0.4829938075	gradually
0.4829938075	avoiding
0.4829938075	implicitly
0.4829938075	drop
0.4829938075	faces
0.4829938075	repository
0.4829016220	neural architecture
0.4826000744	hand
0.4825885097	streaming speech recognition
0.4825642404	self supervised
0.4825607190	addressing
0.4822455797	manner
0.4818265724	simulations
0.4816961728	encourage
0.4815546870	streams
0.4815534769	estimate
0.4815305016	instrumental
0.4815284857	directly generate
0.4813514973	case
0.4813148564	practical applications
0.4811682136	noisy environment
0.4809488081	recall
0.4808923974	strong
0.4808393849	front end
0.4807667657	codes
0.4807343542	metrics
0.4807252871	dcase 2018
0.4806746432	studies
0.4805675742	presence or absence
0.4804084596	attention network
0.4803816869	understand
0.4803517594	classes
0.4802979093	employ
0.4802885210	image recognition
0.4802213519	provided
0.4801823666	training and inference
0.4800774583	personal
0.4799200703	character based
0.4798979877	synthesizing
0.4798142072	vae based
0.4798096686	literature
0.4797305791	back propagation
0.4797272610	leakage
0.4796986119	address
0.4795979704	outperformed
0.4794659479	dataset collected
0.4793926598	music signals
0.4793727260	distinguish
0.4792771839	improvements
0.4791449135	tools
0.4791021542	areas
0.4790523707	scaled
0.4790523707	handling
0.4789339045	activation
0.4789228628	milliseconds
0.4788929072	former
0.4788094133	states
0.4787969829	multi speaker tts
0.4786097300	projection
0.4785961151	teacher models
0.4785951946	discover
0.4785410818	normalized
0.4785291241	convolutional and recurrent
0.4784675700	result
0.4784555993	powerful
0.4784037093	perspective
0.4783314242	producing
0.4783143460	keywords
0.4782194122	musician
0.4780962545	resources
0.4779769854	achieve comparable
0.4779470872	characteristics
0.4778791770	matrices
0.4776766583	easy
0.4776440455	dynamically
0.4775237168	input signals
0.4774986146	average
0.4774002216	~
0.4772685220	non intrusive
0.4772674523	source localization and tracking
0.4771364663	improvement
0.4770676961	details
0.4770524412	cascaded
0.4770438986	remains
0.4769166967	yields
0.4768698096	resulting
0.4766992327	clips
0.4766968504	provide
0.4765429593	operators
0.4765011085	modalities
0.4764964240	lingual
0.4763769102	simulated
0.4763528461	fail
0.4762984328	pronunciation
0.4761819412	embedding vector
0.4761795061	closely
0.4760473801	identifying
0.4758382754	intra
0.4758196075	machine learning algorithms
0.4757553571	preserve
0.4757336092	harmony
0.4757249586	evaluate
0.4756343046	analysis shows
0.4755740312	ideal
0.4753505424	representative
0.4752673133	end to end speech translation
0.4752356000	balanced
0.4752321764	deep convolutional neural networks
0.4751789397	defense
0.4751694206	introduce
0.4751578845	perform
0.4751085095	proposed method outperforms
0.4750268269	reduce
0.4750117525	size
0.4749663212	measure
0.4748804034	levels
0.4748754927	increase
0.4747735769	higher
0.4747537532	neural text to speech
0.4746987871	speaker tracking
0.4746749032	segment based
0.4746686997	recent research
0.4745740312	pretraining
0.4745440374	naturalness
0.4744842494	et
0.4743841282	fundamental
0.4742761761	optimize
0.4742322670	dissonance
0.4742322670	bar
0.4741343841	localization and detection
0.4740423182	energy based
0.4739772878	obtained
0.4739653835	relevant features
0.4739171660	tut sound
0.4738853915	domains
0.4736076265	musicians
0.4736076265	movement
0.4736076265	dependence
0.4736076265	individuals
0.4735583116	missing
0.4734534217	audio fingerprinting
0.4734151541	method achieves
0.4733635821	speech corpus
0.4733592267	voxceleb dataset
0.4729487359	consists
0.4728847970	pairs
0.4727274539	pass model
0.4726425959	speech and text
0.4726394738	scenes and events
0.4723936878	reach
0.4723160578	conducted
0.4722717875	minimization
0.4722717875	expressions
0.4721347523	annotation
0.4720796812	visual context
0.4717742755	\ cite
0.4717502689	translations
0.4716351818	sinusoidal
0.4713438070	rare
0.4713438070	oracle
0.4713438070	compensation
0.4713438070	precise
0.4713140199	benefits
0.4711347860	conduct
0.4710714049	nonlinear
0.4710139592	domain knowledge
0.4709159050	structures
0.4708625976	explored
0.4707020948	outputs
0.4706866189	create
0.4706781325	recognizer
0.4705870792	publicly available datasets
0.4704522130	component
0.4704305871	machine learning methods
0.4704238600	autoencoder based
0.4703116355	discussed
0.4702972521	branches
0.4702972521	opens
0.4702972521	chunks
0.4702972521	differentiate
0.4702972521	perceive
0.4702972521	simplifies
0.4702972521	methodologies
0.4702972521	problematic
0.4702972521	conducting
0.4702972521	topologies
0.4702972521	exceed
0.4702972521	splits
0.4702972521	incorrect
0.4702972521	assignment
0.4702972521	screening
0.4702972521	designs
0.4702972521	specificity
0.4702972521	mentioned
0.4702972521	selectively
0.4702972521	codecs
0.4702972521	considerations
0.4702972521	drastically
0.4702972521	drawback
0.4702972521	organized
0.4702972521	decisions
0.4702972521	costly
0.4702972521	finds
0.4702972521	hypothesize
0.4702972521	localizing
0.4702972521	defining
0.4702972521	adaptively
0.4702972521	spans
0.4702972521	modelled
0.4702972521	complement
0.4702972521	heuristic
0.4702972521	ranked
0.4702681934	deployment
0.4702424351	neural speech synthesis
0.4701405159	distortion ratio
0.4700714049	tempo
0.4700543527	digital signal
0.4699988442	encode
0.4699717413	intended
0.4699237088	labelled
0.4698321729	enabled
0.4695531241	everyday
0.4695531241	moderate
0.4695531241	composing
0.4695531241	thousand
0.4695457905	test accuracy
0.4695349288	non stationary
0.4694206453	voice conversion challenge
0.4694020209	values
0.4693123257	labeled
0.4690738006	spectrogram based
0.4688347429	overlapping
0.4688150625	non invasive
0.4687522473	vocabulary
0.4685710864	word recognition
0.4685519120	widely
0.4685435226	focus
0.4685123534	wide range
0.4683459723	enrolled
0.4683459723	acquire
0.4683459723	dialogues
0.4683459723	grows
0.4683459723	enforces
0.4683459723	acceptable
0.4683459723	phonetically
0.4683459723	slight
0.4683459723	assigning
0.4683459723	induce
0.4683459723	covering
0.4683459723	papers
0.4683459723	preferences
0.4683459723	lacking
0.4683459723	adjust
0.4683459723	attain
0.4683459723	libraries
0.4683459723	constituent
0.4683459723	decompose
0.4683459723	greedy
0.4683459723	passed
0.4683459723	smoother
0.4683459723	loudspeaker
0.4683459723	stationarity
0.4683459723	localized
0.4683459723	allowed
0.4683459723	genuine
0.4683459723	restore
0.4683459723	attenuation
0.4683459723	expand
0.4683459723	qualitatively
0.4683459723	prompted
0.4683459723	scope
0.4683459723	integral
0.4683459723	energies
0.4683459723	cells
0.4683459723	synthesised
0.4683459723	exceeds
0.4683459723	degrading
0.4683459723	extreme
0.4679497388	f
0.4679388900	locations
0.4677405560	variation
0.4677068371	information processing
0.4675309052	threshold
0.4675213289	obtain
0.4675209972	speech to text
0.4673485563	real time
0.4672818554	collect
0.4672090297	speech enhancement algorithms
0.4671298807	speaker verification task
0.4671282778	correspondence
0.4670268525	verify
0.4667755923	capture long term
0.4667578043	days
0.4667578043	thresholds
0.4667578043	uttered
0.4667553148	segments
0.4666902640	extend
0.4666362802	percent
0.4666362802	axis
0.4666362802	elevation
0.4666362802	subsidiary
0.4666362802	angles
0.4666114741	multi channel speech separation
0.4665570848	inversion
0.4664897781	temporal context
0.4664855438	chunk
0.4662396118	variational auto
0.4662285329	greatly
0.4661727819	obtaining
0.4658783665	https
0.4658086696	boost
0.4656981493	implement
0.4656835389	effectiveness
0.4656416385	construct
0.4655901551	created
0.4655487788	yield
0.4655368661	hypothesis
0.4655214838	self attention
0.4654300464	projects
0.4654180039	supervised and unsupervised
0.4653354551	examine
0.4651664687	unseen data
0.4651474140	operates
0.4649398384	add
0.4649271252	forms
0.4648450812	objects
0.4648280550	am
0.4646821756	hybrid models
0.4645555024	employed
0.4645247671	contents
0.4645247671	proposal
0.4645247671	gestures
0.4644265963	training and testing
0.4644135851	fashion
0.4642088385	updated
0.4641650589	needed
0.4640539345	mode
0.4639616173	ways
0.4639594506	strategy
0.4638898384	beneficial
0.4638874919	explore
0.4637993343	chords
0.4637993343	sessions
0.4637993343	synchronization
0.4637448671	asvspoof 2019 challenge
0.4637202885	full band
0.4637088573	unweighted
0.4635332386	considered
0.4635160344	shows
0.4634378878	explores
0.4633365946	procedure
0.4632688882	validate
0.4632165887	allowing
0.4631361733	x
0.4629886760	place
0.4629357411	edit
0.4628933931	annotated
0.4627951978	apply
0.4626072974	includes
0.4625068554	newly
0.4624441265	mainstream
0.4623603006	experiments demonstrate
0.4622892233	speech corpora
0.4622322974	larger
0.4622298120	end to end slu
0.4621820591	curves
0.4621642866	affect
0.4618357418	class labels
0.4617838735	demonstrate
0.4616579322	training and test
0.4614675530	variants
0.4610654808	tapers
0.4610190532	segmented
0.4610190532	affecting
0.4610190532	arbitrarily
0.4610190532	stronger
0.4610190532	yielded
0.4610190532	respond
0.4610190532	theoretically
0.4610190532	reached
0.4610190532	peak
0.4610190532	adjacent
0.4610190532	dilation
0.4610190532	intervention
0.4610190532	switch
0.4610190532	delays
0.4610190532	annotate
0.4610190532	ambiguous
0.4610190532	unreliable
0.4610190532	continue
0.4610190532	convey
0.4610190532	guaranteed
0.4610190532	simplify
0.4610190532	sonic
0.4610190532	reflected
0.4610190532	expressed
0.4610190532	academic
0.4610190532	lacks
0.4610190532	brought
0.4610190532	quantitatively
0.4610190532	threats
0.4610190532	detects
0.4609313539	stable
0.4608637683	outperform
0.4608484029	fine
0.4605985742	preprocessing
0.4605260376	speech and language
0.4604523280	synthesize
0.4604501631	read speech
0.4602959777	per
0.4602389141	called
0.4601486173	look
0.4599857549	ordinary
0.4597151173	progress
0.4596978465	input and output
0.4596499146	feature vector
0.4595413141	planning
0.4595413141	slices
0.4595413141	chance
0.4595413141	outliers
0.4594555806	speaker's
0.4594487583	data for training
0.4594436399	`
0.4593362602	1a
0.4593362602	trial
0.4593362602	assistance
0.4593362602	fide
0.4593362602	unavailable
0.4593362602	rigorous
0.4593362602	obvious
0.4593362602	reporting
0.4593362602	discusses
0.4593362602	necessity
0.4593362602	researched
0.4593362602	mimics
0.4593362602	begin
0.4593362602	guarantees
0.4593362602	bridges
0.4593362602	return
0.4593362602	portions
0.4593362602	highlighted
0.4593362602	optimum
0.4593362602	adversely
0.4593362602	impose
0.4593362602	windowing
0.4593362602	slots
0.4593362602	inevitable
0.4593362602	vice
0.4593362602	physically
0.4593362602	tractable
0.4593362602	accompaniments
0.4593362602	convenient
0.4593362602	suitability
0.4593362602	opposite
0.4593362602	intractable
0.4593362602	discretized
0.4593362602	globally
0.4593362602	drops
0.4593362602	imperfect
0.4593362602	standalone
0.4593362602	suboptimal
0.4593362602	mutually
0.4593362602	compromised
0.4593362602	researches
0.4593362602	audience
0.4593362602	favorable
0.4593362602	aggregating
0.4593362602	interests
0.4593362602	convincing
0.4593362602	contributing
0.4593362602	documents
0.4593362602	motivate
0.4593362602	mitigated
0.4593362602	guidelines
0.4593362602	advantageous
0.4593362602	offered
0.4593362602	extensible
0.4593362602	visible
0.4593362602	shifted
0.4593362602	approached
0.4593362602	expands
0.4593362602	characterizes
0.4593362602	laborious
0.4593362602	1st
0.4593362602	optimizations
0.4593362602	concerned
0.4593362602	pick
0.4593362602	splitting
0.4593362602	execute
0.4593362602	clustered
0.4593362602	inform
0.4593362602	versa
0.4593362602	obstacles
0.4593362602	circuit
0.4593362602	lowest
0.4593362602	safety
0.4593362602	tackles
0.4593362602	prevents
0.4593362602	sized
0.4593362602	falls
0.4593362602	push
0.4593362602	tackled
0.4593362602	network's
0.4593362602	deviate
0.4593362602	alter
0.4592573483	performance gap
0.4592376344	computed
0.4592073357	learned representations
0.4590899188	helps
0.4590302526	doing
0.4589880824	c
0.4589679275	impact
0.4588104268	derive
0.4587231224	end to end speech synthesis
0.4587230923	shown
0.4587146872	observation
0.4585860879	required
0.4585288152	prediction error
0.4582207197	asr performance
0.4581989965	exploit
0.4581709090	audio visual speech enhancement
0.4581210438	truth
0.4580644874	promising performance
0.4580572525	excerpts
0.4580500373	anonymized
0.4580500373	limiting
0.4580500373	quiet
0.4580500373	distorted
0.4580500373	started
0.4580500373	standardized
0.4580500373	uni
0.4580500373	varied
0.4580500373	stored
0.4580500373	accomplish
0.4580500373	finer
0.4580500373	flexibly
0.4580500373	ensures
0.4580500373	hold
0.4580500373	framewise
0.4580500373	summarize
0.4580500373	aggregate
0.4580500373	unrelated
0.4580500373	varies
0.4580500373	implies
0.4580500373	broader
0.4580500373	execution
0.4580500373	causing
0.4580500373	picture
0.4580500373	grouped
0.4580500373	derivatives
0.4580500373	today
0.4580500373	objectively
0.4580500373	links
0.4580500373	protect
0.4580500373	incrementally
0.4580500373	adapts
0.4580500373	closest
0.4580500373	lattices
0.4580500373	names
0.4580500373	foster
0.4580500373	drawbacks
0.4580500373	customized
0.4580500373	abundant
0.4580500373	realize
0.4580500373	products
0.4580500373	determines
0.4580500373	attains
0.4580500373	devise
0.4580500373	healthcare
0.4580500373	outstanding
0.4580500373	meet
0.4580500373	steering
0.4580500373	changed
0.4580500373	unchanged
0.4580500373	traits
0.4580500373	concern
0.4580500373	maximally
0.4580500373	unidirectional
0.4580500373	2nd
0.4579810130	research community
0.4579633153	commercial
0.4578762949	requiring
0.4577978575	analyze
0.4576838072	frequency masking
0.4576348198	structured
0.4574721950	investigate
0.4573767728	demonstrated
0.4567376344	reveal
0.4566642354	dcase 2020
0.4565662270	a comparative study
0.4565251847	texts
0.4565251847	force
0.4561696382	helping
0.4561696382	encodings
0.4561696382	scored
0.4561696382	hyperparameter
0.4561696382	house
0.4561696382	budget
0.4561696382	volumes
0.4561696382	biological
0.4561696382	workflow
0.4561696382	lines
0.4561696382	hypothesized
0.4561696382	tremendous
0.4561696382	triggers
0.4561696382	repeating
0.4561696382	beehive
0.4561696382	reflects
0.4561696382	agglomerative
0.4561696382	adds
0.4561696382	converges
0.4561696382	symbols
0.4561696382	delivered
0.4561696382	resembles
0.4561696382	supporting
0.4561696382	commercially
0.4561696382	eliminates
0.4561696382	hinders
0.4561696382	annotating
0.4561696382	fairly
0.4561696382	indirectly
0.4561696382	windowed
0.4561696382	neighboring
0.4561696382	taper
0.4561696382	resultant
0.4561696382	partly
0.4561696382	representational
0.4561696382	inspiration
0.4561696382	artists
0.4561696382	acting
0.4561696382	generalisation
0.4561696382	comparably
0.4561696382	infrastructure
0.4561696382	offering
0.4561696382	corrected
0.4561696382	recovered
0.4561696382	seek
0.4561696382	reward
0.4561696382	propagate
0.4561696382	reject
0.4561696382	expect
0.4561696382	disadvantages
0.4561696382	transmitted
0.4561696382	linearity
0.4561696382	arise
0.4561696382	concert
0.4561696382	establishes
0.4561696382	ignoring
0.4561696382	subsystems
0.4561696382	aperiodicity
0.4561696382	learners
0.4561696382	clinicians
0.4561696382	articulators
0.4561696382	mounted
0.4561696382	leaving
0.4561696382	friendly
0.4561696382	calibrated
0.4561696382	render
0.4561696382	factorize
0.4561696382	comprise
0.4561696382	functionality
0.4561696382	formed
0.4561696382	confirms
0.4561696382	enrich
0.4561696382	clues
0.4561696382	appealing
0.4560943233	text dependent speaker
0.4560812936	dcase 2019
0.4560259665	quantities
0.4560259665	bars
0.4559866514	showed
0.4559292274	sensory
0.4559292274	limit
0.4559292274	reproduction
0.4559292274	malicious
0.4559292274	imperceptible
0.4559292274	beamformers
0.4558320385	represent
0.4556968892	tags
0.4555544041	n
0.4555096295	collected
0.4552160419	suggest
0.4551926377	learned features
0.4550950955	intents
0.4550950955	materials
0.4550950955	intervals
0.4550950955	neurons
0.4550950955	poses
0.4550950955	queries
0.4550950955	trend
0.4549548365	takes
0.4549406298	non autoregressive
0.4545280442	english speech
0.4544369835	conversion challenge 2020
0.4543015152	agents
0.4543015152	continuously
0.4543015152	compositions
0.4543015152	prototype
0.4542565923	release
0.4538302010	due
0.4536955697	single speaker
0.4533750642	unlabeled
0.4533320385	implemented
0.4531706048	attention based sequence to sequence
0.4528625603	important
0.4526568371	processing methods
0.4526403023	taking advantage of
0.4526101986	providing
0.4525974509	accomplished
0.4525974509	roughly
0.4525974509	expertise
0.4525974509	accelerate
0.4525974509	perturbed
0.4525974509	unconstrained
0.4525974509	discovered
0.4525974509	creates
0.4525974509	investigations
0.4525974509	gaps
0.4525974509	guarantee
0.4525974509	vital
0.4525974509	audible
0.4525974509	posed
0.4525974509	association
0.4525974509	embed
0.4525974509	surpasses
0.4525974509	shifts
0.4525974509	unclear
0.4525974509	separates
0.4525974509	assumes
0.4525974509	azimuth
0.4525974509	runs
0.4525974509	minimizes
0.4525974509	realized
0.4525974509	rarely
0.4525974509	descriptions
0.4525974509	assigned
0.4525974509	treat
0.4525974509	possibilities
0.4525974509	encouraging
0.4525880970	develop
0.4524974821	applied
0.4524231645	comparable results
0.4524076895	attacker
0.4524076895	coarse
0.4524076895	feasible
0.4524076895	studying
0.4524076895	intrinsic
0.4524076895	concatenated
0.4523254181	additional information
0.4523155454	produced
0.4522486740	possibly
0.4522486740	decode
0.4521488526	learns
0.4521377739	visual speech
0.4521133847	categories
0.4514696819	textual
0.4513469096	wavenet based
0.4512281013	input text
0.4512148529	successfully
0.4511074123	lstm network
0.4510342183	target speech
0.4509847346	hence
0.4509368983	far field speech recognition
0.4507057492	augmentation methods
0.4505766618	grams
0.4505766618	generations
0.4505766618	coder
0.4505737897	id
0.4504840451	cases
0.4503646621	end to end asr
0.4502606363	presented
0.4501988610	deep convolutional neural
0.4501150859	translated
0.4499646045	mean opinion
0.4499280864	issue
0.4498353207	conversations
0.4497470138	trained and tested
0.4496609022	capture
0.4494990089	form
0.4491436981	zero shot learning
0.4491373868	sequence to sequence models
0.4490758247	speech and noise
0.4490076130	processed
0.4490063997	category
0.4487491051	annotated dataset
0.4485765610	times faster than
0.4485627295	controls
0.4485608987	synthesizers
0.4483451581	conversation
0.4480755977	emotion prediction
0.4479596757	dcase 2018 task
0.4478680842	e2e models
0.4478234147	neural network acoustic models
0.4476894626	this paper proposes
0.4475661934	t
0.4475619591	potential
0.4475608987	expressiveness
0.4475608987	synchronized
0.4475608987	countermeasure
0.4475608987	procedures
0.4475608987	vocalizations
0.4475608987	selecting
0.4475404976	critical
0.4475332943	account
0.4474285000	pandemic
0.4474285000	subspaces
0.4474285000	pronunciations
0.4474285000	opportunity
0.4474285000	scarce
0.4474285000	progressively
0.4474285000	summarizes
0.4474285000	judgments
0.4474285000	pool
0.4474285000	analyzes
0.4474285000	ahead
0.4474285000	collaboration
0.4474285000	biases
0.4474285000	assign
0.4474285000	couples
0.4474285000	complementarity
0.4474285000	constructs
0.4474285000	serving
0.4474285000	frequent
0.4474285000	reuse
0.4474285000	resemble
0.4474285000	granularity
0.4474285000	breakthroughs
0.4474285000	practices
0.4474285000	drive
0.4474285000	shares
0.4474285000	analysed
0.4474285000	suppressing
0.4474285000	proximity
0.4474285000	turned
0.4474285000	magnitudes
0.4474285000	maintains
0.4474285000	server
0.4474285000	contributed
0.4474285000	susceptible
0.4474285000	unprocessed
0.4474285000	initialize
0.4474285000	demo
0.4474285000	impractical
0.4474285000	gathered
0.4474285000	artefacts
0.4474285000	receives
0.4474285000	raises
0.4474285000	indicator
0.4474285000	adequate
0.4474285000	distinctive
0.4474285000	dictation
0.4474285000	discarding
0.4474285000	experienced
0.4474285000	filtered
0.4474285000	differs
0.4471879230	developed
0.4471570385	enable
0.4469546312	utilized
0.4466134157	l
0.4466094323	runtime
0.4465063273	clip
0.4464105059	prominent
0.4464105059	surface
0.4463402399	produce
0.4463265172	according
0.4462191763	attempt
0.4462017831	compute
0.4461307616	w.r.t
0.4461307616	decide
0.4461307616	builds
0.4461307616	wider
0.4461307616	hyperparameters
0.4461307616	mismatches
0.4461307616	worldwide
0.4461307616	burden
0.4461307616	removed
0.4461307616	insufficient
0.4461307616	digits
0.4461307616	promote
0.4461307616	converge
0.4461307616	analytical
0.4461307616	impacts
0.4461307616	seed
0.4461307616	subtle
0.4461307616	identifies
0.4461307616	regard
0.4461307616	deliver
0.4461307616	nodes
0.4461307616	encourages
0.4461307616	implementing
0.4461307616	located
0.4461307616	imposed
0.4461043568	monolingual data
0.4458752848	gains
0.4458149524	transfer learning approach
0.4457444950	transforms
0.4457444950	solving
0.4457042474	audio
0.4456415543	adapting
0.4456415543	monophonic
0.4456196799	raw
0.4455827807	domain audio separation network
0.4455050083	\ textit
0.4453493950	detection and localization
0.4450218488	performs
0.4450088130	consistently
0.4448491564	under resourced
0.4447696072	confirm
0.4446641417	o
0.4446334080	reported
0.4444885684	interfering speech
0.4443141814	leverage
0.4442945935	confidence
0.4441885263	expected
0.4440951483	extracted
0.4440230183	utilize
0.4440222444	phases
0.4439803742	estimated
0.4438183851	end to end spoken language understanding
0.4437685394	noisy data
0.4437459300	s
0.4435794876	downstream
0.4435612200	self supervision
0.4435566678	last
0.4435552210	robust asr
0.4435433553	processing techniques
0.4434442190	aim
0.4434075081	essential
0.4432214561	works
0.4431435169	capable of generating
0.4431146143	combine
0.4429608987	simpler
0.4429608987	enhances
0.4429608987	submissions
0.4429608987	intonation
0.4429608987	precisely
0.4427999741	speech enhancement methods
0.4426252848	usage
0.4422349002	instead
0.4421727299	challenges
0.4421041540	tts systems
0.4420947763	added
0.4420618568	numerical
0.4420431226	recently shown
0.4420127756	statistically
0.4420127756	similarities
0.4420127756	brings
0.4420127756	averaging
0.4419212783	zero
0.4418505470	embedding features
0.4417891231	channels
0.4417627756	fused
0.4415954712	male
0.4415054864	tested
0.4412941909	must c
0.4412785404	multilingual models
0.4411988429	following
0.4407992452	losses
0.4407558101	designed
0.4405828763	closer
0.4404407483	co
0.4402375028	audio visual
0.4399426189	incorporate
0.4397755170	huge
0.4397222490	generators
0.4397222490	views
0.4397222490	pitches
0.4392534203	investigates
0.4392008369	paired
0.4389908598	leads
0.4389675800	vector and x vector
0.4388014014	addressed
0.4381281085	performed
0.4376243797	training pipeline
0.4375777355	rates
0.4371832617	introduced
0.4367910895	dependencies
0.4367113740	expensive
0.4365758530	senone
0.4365312171	hope
0.4365312171	vanilla
0.4365312171	determining
0.4365312171	lost
0.4365312171	redundant
0.4365312171	abilities
0.4365312171	treatment
0.4365312171	classifies
0.4365312171	viable
0.4365312171	estimators
0.4365312171	reproducibility
0.4365312171	consecutive
0.4365312171	preserves
0.4365312171	transcript
0.4365312171	reaching
0.4365312171	mono
0.4365312171	worse
0.4365312171	visualize
0.4365312171	equally
0.4365312171	speedup
0.4365312171	http
0.4358314892	written
0.4357677929	decoders
0.4356869700	this
0.4355702053	visemes
0.4355058340	promising
0.4354560993	extract
0.4352068783	manual
0.4350032621	five
0.4348047085	build
0.4346808055	want
0.4346166949	studied
0.4345065401	agent
0.4342771227	adopted
0.4342474208	d
0.4342200491	platforms
0.4342200491	modifications
0.4340589315	degraded
0.4340329150	gmm based
0.4339899480	recent advancements
0.4339286389	yet
0.4338522080	towards
0.4337676361	non
0.4337343000	images
0.4337301864	probabilistic model
0.4336806335	audio captioning
0.4333017947	listener
0.4333017947	analyses
0.4331524665	describes
0.4331444044	require
0.4330394907	years
0.4328244867	audio tagging
0.4325535242	difficult
0.4325327852	autoregressive neural
0.4322846429	time domain
0.4321923940	introduces
0.4321309848	non native
0.4320847272	isolated
0.4319855818	khz
0.4318982621	unfortunately
0.4316935151	expert
0.4315748944	modes
0.4313147061	performance evaluation
0.4312934682	brief
0.4312367250	healthy
0.4312367250	emerging
0.4312252720	mean square
0.4311756765	after
0.4306715435	dcase 2020 challenge
0.4304961182	upit
0.4304792653	overall
0.4303051299	addresses
0.4299805024	an open source
0.4298035913	publicly
0.4295990041	representations of speech
0.4293687363	suitable
0.4292374554	experimental results show
0.4289392737	children speech
0.4286642782	large scale dataset
0.4285646332	parallel vc
0.4285282288	this paper presents
0.4285120776	accents
0.4281330201	augmentation techniques
0.4281115354	transcripts
0.4280759132	source signals
0.4279981697	w
0.4279588287	phrases
0.4279482222	synthesis systems
0.4276712418	train and test
0.4276435916	dnn training
0.4274551619	large amounts of
0.4272924193	method called
0.4269260269	self supervised learning
0.4266001012	considering
0.4265650310	second
0.4263691833	third
0.4263284508	streaming and non streaming
0.4263139875	rules
0.4263139875	implementations
0.4263139875	rhythmic
0.4263139875	distortions
0.4263139875	singers
0.4260286820	come
0.4255512189	i vectors
0.4254938401	patterns
0.4253460356	deep learning architectures
0.4252898554	material
0.4251536933	concepts
0.4250411908	t s
0.4249324503	ability
0.4249188935	computer
0.4248173830	verification tasks
0.4247587291	such
0.4247401918	multi speaker speech recognition
0.4247012321	correlations
0.4246273679	though
0.4244930429	speech and music
0.4244402428	branch
0.4243140216	changing
0.4241631980	given
0.4240255907	as
0.4237300141	attention based models
0.4236244881	and accompaniment separation
0.4234174300	\ emph
0.4234168309	temporal classification
0.4233028210	long short
0.4232176471	sample
0.4230481890	achieves comparable
0.4226114109	results obtained
0.4223192811	encoder decoder based
0.4221221522	low rank matrix
0.4218921401	number of microphones
0.4217203156	acoustic and visual
0.4217152362	computation
0.4213489548	error rate reduction
0.4212148067	detection and classification
0.4212109600	hybrid model
0.4211198173	these
0.4208914966	end to end neural
0.4208583623	fixed number
0.4206388466	if
0.4206100529	valid
0.4206100529	teams
0.4206100529	originally
0.4206100529	systematically
0.4206100529	describing
0.4206100529	robustly
0.4206100529	eliminate
0.4206100529	trajectories
0.4206100529	manipulation
0.4206100529	authors
0.4206100529	penalty
0.4206100529	avoids
0.4206100529	satisfactory
0.4206100529	iterations
0.4206100529	choose
0.4206100529	backbone
0.4206100529	interpret
0.4206100529	clusters
0.4206100529	program
0.4204579859	e2e model
0.4202392392	external language
0.4201813462	off
0.4201676953	initialization
0.4200441898	discrimination
0.4198383124	training samples
0.4198343619	targets
0.4197465546	piece of music
0.4197245548	salient
0.4197245548	objectives
0.4197245548	experimentally
0.4196432489	scoring
0.4195575149	token
0.4192074781	melodies
0.4191802109	based vad
0.4189673139	automatic speech recognition systems
0.4186762304	approximately
0.4185575149	distinct
0.4185575149	enabling
0.4183290468	iteratively
0.4183290468	deeper
0.4181555357	high quality speech
0.4180312477	labeled dataset
0.4179001666	truth labels
0.4178855980	human level
0.4178022905	require large
0.4177481946	aims
0.4176839098	sensors
0.4173183182	strong baseline
0.4171575828	hypotheses
0.4170886629	recognition from speech
0.4170416361	first
0.4170402863	speaker diarization system
0.4169953569	versus
0.4169953569	artifacts
0.4169306597	feature sequence
0.4167478235	transformations
0.4167478235	play
0.4167478235	configurations
0.4166881076	gated convolutional
0.4164020378	activations
0.4164020378	dimension
0.4163957973	all
0.4163175629	extremely
0.4163175629	alignments
0.4162893066	while
0.4162777778	v
0.4160575529	recipe
0.4159863205	naturally
0.4159478628	advance
0.4156246304	loudness
0.4156246304	growing
0.4153423730	noise sources
0.4150309091	using
0.4148650799	large datasets
0.4145661014	proposes
0.4144627811	context information
0.4144287289	front
0.4144174061	end to end automatic speech recognition
0.4144164546	robust automatic speech recognition
0.4143582198	ctc model
0.4143132105	sentences
0.4141509416	underlying
0.4140230636	based speech enhancement
0.4138960579	based approaches
0.4138571885	hour
0.4137357415	durations
0.4135742063	then
0.4134373551	annotations
0.4133960983	discriminator
0.4132222888	settings
0.4130309214	right
0.4130016980	training strategies
0.4128960983	subjects
0.4127650767	offline
0.4127088286	richer
0.4127088286	protocols
0.4127088286	held
0.4127088286	agreement
0.4127088286	discriminators
0.4127088286	threat
0.4127088286	leveraged
0.4127088286	overhead
0.4127088286	notable
0.4127088286	averaged
0.4127088286	subsets
0.4127088286	simplicity
0.4127088286	decades
0.4127088286	decreases
0.4127088286	behaviour
0.4125450024	diarization performance
0.4124791818	typically
0.4123621731	scales
0.4122724012	most
0.4121752382	perturbation
0.4121752382	limits
0.4121752382	inherent
0.4121752382	descriptors
0.4117334155	classifiers
0.4116996425	based methods
0.4116262589	regions
0.4113798455	speech samples
0.4113261795	tries
0.4112715072	performance improvements
0.4111995438	k
0.4110778861	g
0.4109624489	secondly
0.4105227913	end to end models
0.4103733887	speech emotion
0.4103456608	time delay
0.4103366154	classification loss
0.4103045839	recorded
0.4102734731	nevertheless
0.4099075882	directions
0.4098843693	contexts
0.4098843693	increasingly
0.4098818523	one
0.4097723359	number of speakers
0.4096378591	waveform model
0.4094582729	term memory
0.4093800178	an attention mechanism
0.4091265714	enhancement and separation
0.4087438419	speech distortion
0.4083519836	end to end tts
0.4083190239	music information
0.4083138590	taken
0.4082130457	model compression
0.4079451463	excellent
0.4078528919	longer
0.4078528919	randomly
0.4076018437	but
0.4066049835	publicly available
0.4065400219	labeling
0.4063658845	notes
0.4063611269	whole
0.4062360448	completely
0.4062360448	observations
0.4062360448	start
0.4060384560	in
0.4059676803	audios
0.4059676803	decrease
0.4057953749	enrollment
0.4056748756	unsupervised manner
0.4056247628	self
0.4054699012	keep
0.4054405602	dimensions
0.4054086117	effort
0.4053604927	y
0.4053330607	acoustic and language
0.4052987144	test data
0.4052679841	emotions
0.4051572235	sampled
0.4051047935	time
0.4050905358	thus
0.4049852780	filterbanks
0.4048129173	the
0.4048043480	generator
0.4046421513	many
0.4045322617	tts model
0.4044614112	presents
0.4043494744	modules
0.4043112571	meeting corpus
0.4042485723	predictions
0.4039476036	neural network models
0.4038754130	i
0.4037407136	signal to interference
0.4036521001	norm
0.4035017905	because
0.4034988131	parallel training
0.4033043480	estimates
0.4029705532	recognition task
0.4029690990	generally
0.4028615848	encoder decoder network
0.4023789942	an
0.4021963276	highly
0.4019049657	noises
0.4017679513	lengths
0.4017679513	occurs
0.4017501521	filters
0.4015739010	left to right
0.4014196373	participants
0.4009775693	acoustically
0.4009775693	behaviors
0.4009775693	implications
0.4009775693	attained
0.4009775693	shorter
0.4009775693	motivation
0.4009775693	solved
0.4009775693	latest
0.4009775693	turns
0.4009775693	plausible
0.4009775693	sufficiently
0.4009775693	dramatically
0.4009775693	computes
0.4009775693	acquired
0.4009775693	fit
0.4009775693	promise
0.4009775693	sophisticated
0.4009030223	detected
0.4008973679	regardless
0.4006791422	j
0.4005588467	on
0.4005112431	one shot
0.4004196373	meaningful
0.4004032172	correlate
0.4001284482	a case study
0.3999199079	female
0.3999087551	freely available
0.3998844902	back end
0.3998168446	technologies
0.3996315697	resolutions
0.3995659966	high fidelity speech
0.3995315697	yielding
0.3995315697	extensively
0.3995315697	severely
0.3995315697	medium
0.3995315697	integrates
0.3995315697	suggested
0.3995315697	modify
0.3995315697	encodes
0.3995315697	poorly
0.3995315697	examined
0.3995280223	databases
0.3995280223	patients
0.3994534230	value
0.3993673198	unimodal
0.3993673198	attribute
0.3990642340	on mobile devices
0.3989439688	apart
0.3987004977	electroencephalography
0.3987004977	optimizes
0.3987004977	supports
0.3987004977	qualities
0.3987004977	concerns
0.3987004977	giving
0.3987004977	individually
0.3987004977	reliably
0.3987004977	paths
0.3987004977	differ
0.3987004977	recognized
0.3987004977	solves
0.3987004977	syllables
0.3986690484	comprising
0.3984852467	researchers
0.3983568158	seeing
0.3982021441	reconstructed
0.3981759279	training process
0.3981278620	instances
0.3981278620	calls
0.3981278620	calculation
0.3980490426	distant speech
0.3979577882	presence of background
0.3978778620	reflect
0.3978778620	severe
0.3978778620	difficulties
0.3978778620	reason
0.3978778620	ideas
0.3978015532	inherently
0.3978015532	evaluates
0.3978015532	preferred
0.3978015532	exists
0.3978015532	timing
0.3978015532	vary
0.3978015532	involve
0.3978015532	facilitates
0.3978015532	transferred
0.3978015532	readily
0.3978015532	setups
0.3978015532	computations
0.3978015532	processor
0.3978015532	slow
0.3978015532	attractive
0.3976339865	reasons
0.3976339865	predefined
0.3976339865	impressive
0.3976339865	tailored
0.3976339865	consideration
0.3976339865	regular
0.3976339865	ignore
0.3976339865	suggesting
0.3976339865	costs
0.3976339865	reveals
0.3976339865	inside
0.3976339865	reasonable
0.3975045206	character error
0.3973818353	analyse
0.3973818353	prove
0.3973818353	assume
0.3973818353	informative
0.3973818353	reaches
0.3973818353	initialized
0.3973818353	coherent
0.3973818353	counterparts
0.3973818353	verified
0.3973259754	tends
0.3972445287	easier
0.3972445287	highest
0.3972445287	decoded
0.3972445287	matched
0.3972445287	balance
0.3972445287	assessed
0.3972445287	surrounding
0.3972445287	running
0.3972445287	desirable
0.3970315697	assumptions
0.3970315697	preference
0.3970315697	competing
0.3970315697	inferred
0.3970315697	occurring
0.3970315697	rapidly
0.3970315697	playback
0.3970315697	valuable
0.3970315697	classified
0.3970315697	modifying
0.3970315697	intelligible
0.3970315697	termed
0.3970315697	composers
0.3970315697	exhibit
0.3970315697	situation
0.3970315697	explain
0.3970315697	artificially
0.3969177411	you
0.3967214578	time frequency
0.3966014276	again
0.3965857908	further
0.3963591807	bidirectional long
0.3959636577	sound detection
0.3958817438	a large scale
0.3958427250	exhibits
0.3958427250	largest
0.3958427250	comprises
0.3958427250	reductions
0.3958427250	configuration
0.3958427250	complicated
0.3958427250	considerably
0.3956027514	b
0.3955119664	successful
0.3951663829	converts
0.3951663829	pipelines
0.3951663829	pose
0.3951663829	meaning
0.3951663829	subject
0.3951663829	offer
0.3951663829	quickly
0.3951663829	overfitting
0.3951663829	argue
0.3949191112	rate reduction
0.3948869672	part
0.3945730889	function based
0.3945126071	both
0.3942968989	attributes
0.3942472982	time domain audio separation network
0.3942358392	paper explores
0.3942239745	i vector
0.3937609827	p
0.3936275137	source model
0.3934578558	separation and recognition
0.3932966721	sequence to sequence model
0.3932766179	this paper describes
0.3931059395	acoustic scenes and events
0.3928696291	attention networks
0.3927620712	mappings
0.3927514427	query by example
0.3927277366	current methods
0.3926139522	accurately
0.3925530896	during
0.3923058200	number of parameters
0.3921886395	accordingly
0.3921787745	meanwhile
0.3920872761	$ \ pm
0.3920665311	student training
0.3919589801	lingual voice conversion
0.3919334115	time frequency domain
0.3915862518	e
0.3913592554	masks
0.3912519165	librispeech test
0.3909303725	users
0.3908491074	three
0.3908360939	acoustic scenarios
0.3907620712	attracted
0.3907620712	affects
0.3907620712	simply
0.3907620712	purposes
0.3907620712	considers
0.3907620712	incorporated
0.3906802148	end to end manner
0.3906344002	constraint
0.3904580721	several
0.3902955535	simultaneously
0.3899797285	joint model
0.3899506819	diverse
0.3899152130	move
0.3897983682	de
0.3895253619	internal
0.3894506819	criterion
0.3894287659	a fully convolutional
0.3893771121	resource language
0.3886806994	operations
0.3883979003	h
0.3883285592	extends
0.3883285592	removing
0.3883285592	formulate
0.3883285592	define
0.3883285592	trains
0.3880904871	do
0.3880449480	speech utterances
0.3879361616	independent low rank
0.3877336554	simulated data
0.3876727989	$ \ beta
0.3875836026	example
0.3875547284	tacotron 2
0.3875073537	feature loss
0.3874850714	detail
0.3873506118	intermediate
0.3870952347	mean
0.3870114196	without sacrificing
0.3863289893	potentially
0.3863289893	extra
0.3862355086	full
0.3861138544	published
0.3859591672	a
0.3858500137	at
0.3857656789	training objective
0.3857554230	classification performance
0.3857164650	better
0.3857119983	least
0.3854548541	mandarin speech recognition
0.3854106684	rather
0.3853680687	system
0.3848073538	i vector extractor
0.3846125913	blind source
0.3840249037	correct
0.3838686328	besides
0.3837871732	audio watermarking
0.3836813365	sub
0.3836206560	findings
0.3834830602	verification task
0.3832595946	indicating
0.3832595946	conclude
0.3832595946	matches
0.3831979236	moreover
0.3831019661	a deep neural network
0.3830979692	transcriptions
0.3827917680	main idea
0.3827848080	automatically
0.3827710340	event localization and detection
0.3826206560	selected
0.3826176559	extent
0.3826176559	chosen
0.3826176559	positions
0.3824078407	few shot learning
0.3821202599	transformer model
0.3820928377	it's
0.3818345871	speech frames
0.3817876402	achieved competitive
0.3817140320	approach achieves
0.3814953420	^ 2
0.3813941977	to
0.3813686053	model's
0.3813686053	obtains
0.3813142261	challenge 2020
0.3812954061	requirements
0.3812334568	questions
0.3810514490	measurements
0.3803168106	audio classification
0.3802851797	novel
0.3802469331	slu model
0.3800514490	separately
0.3799981526	first order
0.3795571529	root mean
0.3794037117	new
0.3792577211	captures
0.3791664202	unsupervised subword
0.3790014176	under
0.3789243877	substantially
0.3787103385	learning based
0.3786026684	adversarial attacks on
0.3785757415	determine whether
0.3783350749	state of art
0.3782624652	plays
0.3782624652	representing
0.3782624652	operation
0.3779119937	also
0.3777137122	seq2seq model
0.3774803286	more
0.3773672029	two
0.3770252334	consequently
0.3767626913	remain
0.3767626913	largely
0.3767626913	groups
0.3766979130	various
0.3766748734	confirmed
0.3762182198	even
0.3762123359	true
0.3761355884	follow up
0.3761187209	word error
0.3761118369	other
0.3759981057	formulation
0.3759981057	degrade
0.3759981057	played
0.3759981057	mismatched
0.3759409120	by
0.3759020468	speaker verification tasks
0.3756809699	synthetic dataset
0.3755297923	related features
0.3755071554	conditional generative
0.3751578201	through
0.3751570061	frameworks
0.3751570061	sizes
0.3751570061	adopt
0.3747886942	acoustic environment
0.3745205756	situations
0.3745205756	involving
0.3745205756	poor
0.3745205756	detector
0.3745205756	identified
0.3744638320	furthermore
0.3743629026	speech generation
0.3742254811	constructed
0.3742254811	share
0.3742254811	adopts
0.3742254811	gained
0.3737820061	released
0.3737820061	offers
0.3737820061	assumption
0.3737104311	including
0.3736230288	training stage
0.3734219150	for
0.3733547536	go
0.3732723068	time frequency bins
0.3731626913	efforts
0.3731626913	extracts
0.3725288009	employs
0.3724044466	applies
0.3719909712	does
0.3717091175	slightly
0.3717091175	degrades
0.3717091175	suggests
0.3717091175	correctly
0.3715094605	once
0.3714541419	indeed
0.3711795164	achieved great
0.3709897542	similar performance
0.3708032709	until
0.3706089522	analyzed
0.3706089522	predicts
0.3706027976	discriminative information
0.3705742146	demonstrating
0.3705742146	incorporates
0.3705742146	calculated
0.3705742146	primarily
0.3705742146	remarkable
0.3705742146	accuracies
0.3705742146	exploited
0.3704552395	toward
0.3704075480	candidate
0.3704075480	leverages
0.3704075480	strongly
0.3704075480	exist
0.3704075480	counterpart
0.3703522017	over
0.3702852538	11
0.3701491021	each
0.3700881931	received
0.3700881931	transformed
0.3699038943	speaker verification system
0.3698580911	audio synthesis
0.3697159706	relatively
0.3695043822	especially
0.3694705335	deep architectures
0.3694588553	people
0.3693073908	features extracted
0.3688577216	testing data
0.3683337820	significant performance
0.3682680934	unseen during training
0.3680112732	proposed framework
0.3675900470	variations
0.3672902988	instance learning
0.3672046351	related information
0.3669154910	input representation
0.3667074583	normally
0.3663575205	any
0.3659892218	efficiently
0.3659316072	baseline systems
0.3657011779	matrix analysis
0.3655740968	non negative
0.3655481406	conversion model
0.3654932433	requires
0.3654245546	work
0.3652862109	audiovisual speech
0.3651884331	despite
0.3651723510	augmentation method
0.3651248933	voice corpus
0.3649603722	audio visual speech recognition
0.3649520745	invariant features
0.3649270110	whereas
0.3647200723	12
0.3645761966	however
0.3645690965	e.g
0.3642265766	effectively
0.3641662942	baseline model
0.3641484895	prediction model
0.3637665608	dcase 2019 task
0.3636882622	enough
0.3635344793	with
0.3633313219	raw audio
0.3631769292	benchmarks
0.3630748337	setup
0.3628378581	later
0.3623115445	exploits
0.3623115445	represents
0.3621213485	automatic music
0.3620418460	little
0.3616462008	day
0.3615472549	i.e
0.3613768356	enhancement network
0.3606173051	recognition rate
0.3606071325	short time
0.3605466110	latter
0.3604693064	prediction task
0.3603560667	why
0.3602078777	acoustic feature
0.3601956416	verification systems
0.3599807419	this paper introduces
0.3597084269	without
0.3592710612	followed
0.3590538380	well
0.3589378435	divided into
0.3589298992	streaming end to end
0.3587871773	enables
0.3586876980	down
0.3586323336	second order
0.3585941802	proposed methods
0.3584561005	although
0.3584286889	recognition and speaker
0.3583406446	investigated
0.3583147656	speech related
0.3581717888	than
0.3580461117	conventional methods
0.3579365445	increases
0.3578594370	self attention mechanism
0.3578514174	available
0.3577236200	combines
0.3576112947	involves
0.3573831070	generates
0.3573384356	attack detection
0.3573115445	schemes
0.3572811338	convolutional recurrent neural
0.3571477601	makes
0.3568569804	based approach
0.3567426645	easily
0.3566226638	transformer models
0.3564711479	event labels
0.3562343390	19
0.3558178870	github.com
0.3558178870	observe
0.3558178870	outperforming
0.3558030814	sub challenge
0.3557652934	together
0.3550961513	sequence learning
0.3548178870	independently
0.3548178870	utilizes
0.3547676633	estimated speech
0.3543829240	one hot
0.3539615804	f score
0.3538914780	demonstrates
0.3537514855	recognition performance
0.3537396397	course
0.3536289520	human speech
0.3535168494	discuss
0.3534761642	increased
0.3532800985	learning framework
0.3530910808	score following
0.3525110591	signal to distortion
0.3524789482	term dependencies
0.3524175620	a single speaker
0.3523090536	latent features
0.3522549992	separation and enhancement
0.3520168494	produces
0.3518781938	waveform based
0.3518004197	between
0.3516703061	out of vocabulary
0.3515885782	different
0.3515581447	showing
0.3512153715	see
0.3510607770	lstm model
0.3510422756	or
0.3509716514	only
0.3509547530	of
0.3508944819	asr models
0.3507076258	from
0.3507034574	re
0.3505947343	learning scheme
0.3501109515	proposed method achieves
0.3499573299	next
0.3498583719	noise components
0.3498546534	four
0.3498151966	real room
0.3495707368	2
0.3486970509	take
0.3484643776	like
0.3482117363	whether
0.3482074292	speaker invariant
0.3481747078	tts models
0.3477245810	detection and classification of acoustic scenes
0.3476522396	currently
0.3475575985	adaptation method
0.3473127252	use
0.3471295677	baseline results
0.3470546846	not
0.3468989653	very
0.3467386760	here
0.3464654607	f measure
0.3460391981	human computer
0.3459324682	1
0.3458730762	embedding extraction
0.3455446964	show
0.3451943194	multilingual model
0.3449321303	therefore
0.3449282845	against
0.3447950876	others
0.3441640211	we
0.3441603013	sometimes
0.3441478777	neither
0.3440128020	few
0.3438644155	help
0.3437165714	beyond
0.3435232070	task 1
0.3432814884	throughout
0.3431648688	challenge 2019
0.3426847374	polyphonic sound
0.3424456800	low signal to noise
0.3424175821	voice trigger
0.3420140020	nearly
0.3419687415	an unsupervised
0.3419555775	recognition challenge
0.3418188772	and
0.3417677229	best
0.3417242534	in recent years
0.3415769220	\
0.3411777972	proposed model
0.3411067229	attention based end to end
0.3410660183	resource languages
0.3409497751	top
0.3409488914	must
0.3409109758	model learns
0.3407481634	above
0.3407439580	experiment results show
0.3405944512	back
0.3405206584	visual representations
0.3404772785	well suited
0.3401582216	audio signals
0.3398992592	recognition tasks
0.3398930533	advances in deep learning
0.3397714554	3
0.3394750803	used
0.3394349568	same
0.3394292130	value decomposition
0.3392899231	out of domain
0.3392611088	separation methods
0.3392137118	our
0.3391601151	datasets demonstrate
0.3390513811	generation method
0.3390507349	full rank
0.3390499623	since
0.3390281180	need
0.3388625357	still
0.3387822530	traditional methods
0.3386774037	spatial features
0.3383868169	end to end automatic speech
0.3382649171	get
0.3380935213	widely used
0.3379764893	across
0.3374775360	recent advances in
0.3371201812	try
0.3370025692	never
0.3369446660	$
0.3368322582	side
0.3366666794	speech to speech
0.3365358128	component analysis
0.3364549000	consider
0.3364287374	particularly
0.3363041523	so
0.3360713103	bottom
0.3360713103	fill
0.3360713103	whenever
0.3360713103	twelve
0.3358855555	it
0.3358394110	associated
0.3356450641	level fusion
0.3355406008	translation task
0.3354464372	estimation accuracy
0.3353747250	this paper investigates
0.3352454668	cnn model
0.3348237187	learning of speech
0.3347442221	comes
0.3346975353	2019
0.3344932629	always
0.3342936650	amount
0.3339345696	single task
0.3339249074	an attention based
0.3335161687	rnn t models
0.3333795662	temporal information
0.3329937861	her
0.3328908971	likely
0.3328042851	last years
0.3327323720	^
0.3325849885	speech technology
0.3324880387	temporal features
0.3324856713	self attention network
0.3321489762	speech content
0.3321204275	conversion performance
0.3320664305	along
0.3317077262	current state of
0.3316630327	less
0.3315834386	into
0.3313988363	conventional method
0.3311169869	effective method
0.3311105408	model architecture
0.3309619457	learning algorithm
0.3305409460	code switching speech
0.3302549913	visual data
0.3302463632	non parallel
0.3301989928	audio visual scene
0.3294313531	7
0.3293868148	not necessarily
0.3293422490	2d
0.3292634360	enhancement systems
0.3289813445	supervised training
0.3289411943	much
0.3285291581	resulting model
0.3284758030	non stationary noise
0.3283604796	often
0.3283224469	vector machines
0.3280206525	non streaming
0.3278341723	deep reinforcement
0.3273207716	getting
0.3273207716	thorough
0.3271892170	conduct experiments on
0.3270347257	signal to noise
0.3269934366	top down
0.3269388246	among
0.3267156670	six
0.3266557341	deep learning framework
0.3264451934	uses
0.3263345665	trade off between
0.3257653384	model adaptation
0.3257534026	10
0.3256838236	conventional asr
0.3256530429	self attention layers
0.3249824510	2020
0.3241735514	discriminative training
0.3241629988	2018
0.3236571014	score level
0.3236293351	robust speaker
0.3235172567	data collected
0.3229222863	speaker labels
0.3227149262	for keyword spotting
0.3226137327	mel filter
0.3226068980	find
0.3224406353	experimental results indicate
0.3221278029	voice synthesis
0.3214970403	provides
0.3214818936	network based
0.3214772874	non linear
0.3207958420	encoder decoder model
0.3207576973	end to end text to speech
0.3207157769	there
0.3204397668	based vc
0.3204362346	us
0.3198118388	level features
0.3196404219	multi task training
0.3192850870	up
0.3192645647	acoustic data
0.3191858497	audio events
0.3191392878	describe
0.3188837389	non parallel data
0.3185493015	speech applications
0.3184713679	noise ratios
0.3184551379	gap between
0.3180353987	voice detection
0.3179498376	4
0.3177670371	coming from
0.3177441541	voice data
0.3176202017	embedding learning
0.3175899732	f1 score of
0.3174784292	obtained results
0.3173201776	large amounts
0.3172411407	mapping based
0.3171629487	generate music
0.3170869317	particular
0.3170774441	the art
0.3168484270	robust automatic
0.3168017589	20
0.3167820130	6
0.3167593346	done
0.3167270716	sounding speech
0.3167042250	contains
0.3165327919	100
0.3163168239	dnn model
0.3163104822	way
0.3161821102	results revealed
0.3159530260	commonly used
0.3159340452	two stage
0.3158416092	necessary
0.3158128395	seen
0.3157920129	5
0.3153528190	method achieved
0.3152125701	test time
0.3150383240	task 4
0.3149906856	this paper addresses
0.3146377931	speech encoder
0.3142058096	experiments conducted
0.3141473189	2017
0.3140412569	generalizes well
0.3139092347	+
0.3138769910	method outperformed
0.3137394673	neural network transducer
0.3136763537	a large margin
0.3134069080	audio data
0.3132016418	for speaker verification
0.3131633601	conditional neural
0.3131305532	appear
0.3130023561	speech recognition system
0.3128230908	input representations
0.3127658139	source speaker
0.3127601188	separation tasks
0.3124893548	significant improvement over
0.3122004335	arrival estimation
0.3121659748	the proposed method
0.3121480635	conduct experiments
0.3118053840	significant improvements over
0.3116385070	far
0.3115619632	when
0.3115018903	insight into
0.3113174860	state of
0.3113133265	sub word
0.3112215055	allows
0.3100723162	made
0.3100439498	alone
0.3100145013	separation systems
0.3100059179	this paper explores
0.3099832557	unsupervised representation
0.3099446851	22
0.3099014093	audio processing
0.3098281136	useful
0.3096987590	encoder network
0.3095818849	voxceleb speaker
0.3093214460	human evaluation
0.3092452159	certain
0.3090018993	synthesis model
0.3089856541	multilingual speech
0.3084716388	supervised acoustic
0.3084457304	corresponding
0.3081390901	audio inpainting
0.3079671095	needs
0.3078177461	containing
0.3077527703	memory network
0.3077002948	model takes
0.3076901820	music style
0.3073770719	music performance
0.3072930676	sequence models
0.3067787737	differences between
0.3067545634	speech segment
0.3065060972	localization method
0.3065005105	text only
0.3063675500	enhancement model
0.3062381955	out
0.3062237845	adaptation data
0.3059308402	a single
0.3058923236	image translation
0.3058801158	2015
0.3057985169	a limited number
0.3051697662	top performing
0.3051122500	convolutional and recurrent neural
0.3050447849	interest
0.3049757238	separation task
0.3048997947	enhancement algorithm
0.3048584633	two pass
0.3043542859	what
0.3043506142	robust speech recognition
0.3042184304	learning strategy
0.3042176461	performance metrics
0.3040890774	non trivial
0.3038393918	8
0.3038154297	an efficient
0.3038051090	an unsupervised manner
0.3036154018	time consuming
0.3034862234	an improved
0.3033115423	time variant
0.3032500193	track 1
0.3031608754	70
0.3029332026	does not require
0.3027315359	3000
0.3026666028	give
0.3023238303	identification accuracy
0.3023011053	the voxceleb speaker recognition challenge
0.3022078541	21
0.3019760481	inner
0.3018571488	found
0.3016209873	indicate
0.3009740041	separated speech
0.3009334672	speech mixtures
0.3005831371	processing applications
0.3005052583	while maintaining
0.3004528221	automatic detection
0.3002007648	about
0.2998271324	proposed technique
0.2997717063	enhancement framework
0.2995477103	attention based neural network
0.2993958934	rnn t model
0.2993368770	make
0.2991950876	possible
0.2990700933	the proposed approach
0.2990307018	end to end approaches
0.2989112128	model based
0.2988098144	1.2
0.2988045073	they
0.2987932383	able
0.2987844433	large scale audio
0.2985920385	speech recordings
0.2983299550	vs
0.2982147735	decoder network
0.2975840643	an end to end fashion
0.2974828293	the art performance
0.2969495974	synthesize speech
0.2969008171	appropriate
0.2965411000	supervised deep
0.2963232163	asr task
0.2956918511	nine
0.2955735565	run time
0.2955338750	improves performance
0.2954769012	doing so
0.2953990189	50
0.2951176032	10 db
0.2949148178	voice based
0.2948918115	every
0.2948853643	95
0.2948853643	ever
0.2948853643	85
0.2948853643	indicated
0.2948853643	200
0.2948853643	concerning
0.2948853643	2021
0.2948853643	36
0.2946956139	the proposed framework
0.2942253678	data distribution
0.2941124474	an overview
0.2940609476	features extracted from
0.2939919033	speaker representation
0.2939221210	diarization error
0.2938509182	audio features
0.2937485510	x vector based
0.2936581126	across languages
0.2936534257	now
0.2936233172	short time objective
0.2934912515	the art methods
0.2934383140	learning based speech enhancement
0.2930745409	another
0.2929440161	propose to apply
0.2928515295	a convolutional neural network
0.2928389358	300
0.2927611401	allow
0.2927360247	some
0.2925868728	generation tasks
0.2924727825	speech analysis
0.2919823958	34
0.2919823958	35
0.2919823958	1000
0.2919823958	65
0.2919823958	2010
0.2919823958	92
0.2918384625	time series
0.2917987913	this paper
0.2917594797	a generative model
0.2914620830	known
0.2913679265	to interference ratio
0.2909396734	5 db
0.2908145054	good
0.2906639203	having
0.2902168784	almost
0.2899297349	23
0.2899297349	wherein
0.2899297349	500
0.2899297349	reasonably
0.2899277512	referred to as
0.2898058307	proposed algorithm
0.2897575734	audio streams
0.2886198550	ten
0.2881132713	language recognition
0.2880964147	changes
0.2880025035	text to speech systems
0.2879359422	transform domain
0.2874757028	baseline methods
0.2874029960	audio recordings
0.2868533209	regarding
0.2867841761	learning process
0.2863893147	experimental results on
0.2863364855	number of sources
0.2862696762	separation problem
0.2861509756	the proposed model
0.2861440740	into account
0.2860858136	placed
0.2860858136	know
0.2859015689	87
0.2859015689	1.0
0.2859015689	33
0.2859015689	808
0.2859015689	2.5
0.2859015689	2.0
0.2859015689	99
0.2859015689	31
0.2859015689	45
0.2858203189	model structure
0.2858054021	name
0.2857731973	music dataset
0.2857156809	first pass
0.2856540221	64
0.2856540221	1.6
0.2856540221	0.1
0.2856540221	gets
0.2856540221	except
0.2856540221	78
0.2856239797	model architectures
0.2854391961	said
0.2848003088	call
0.2847310331	speech recognition models
0.2838742628	|
0.2836848382	baseline system
0.2834583912	aimed at
0.2833796555	the proposed approach outperforms
0.2824328214	time scale
0.2823965497	robust against
0.2818580229	trained model
0.2817363419	well established
0.2816456910	a two stage
0.2815269279	learning tasks
0.2815136986	97
0.2815136986	away
0.2815136986	specify
0.2815136986	68
0.2815136986	48
0.2815136986	exactly
0.2815136986	0.5
0.2815136986	88
0.2815136986	0.3
0.2815136986	98
0.2815136986	otherwise
0.2810604589	second pass
0.2810378230	two dimensional
0.2808395030	deep models
0.2808265771	for automatic speech
0.2808006090	task 2
0.2805036282	the main idea
0.2805012840	enhancement task
0.2802997800	time frequency representations
0.2802020755	based on
0.2801036039	thereby
0.2798125963	source separation performance
0.2792515468	who
0.2789438974	experiments conducted on
0.2787100944	i vector based
0.2786356561	method improves
0.2785905804	near
0.2780712487	speaker detection
0.2777833241	speech representation
0.2776343774	onto
0.2776195886	the proposed method achieves
0.2774762714	deep convolutional
0.2769780812	time varying
0.2768103011	for speech enhancement
0.2767951520	voice conversion system
0.2765768108	adaptation methods
0.2760554642	neural machine
0.2756442748	suffer from
0.2755705348	old
0.2755208619	recent neural
0.2754422060	speech embeddings
0.2753711319	approach shows
0.2750904733	its
0.2749504812	wide variety of
0.2748764259	multi channel audio
0.2748559003	an utterance level
0.2743923611	deals with
0.2742633525	automatic speaker
0.2739543387	multi talker speech
0.2737282857	model called
0.2736122287	real data
0.2731067097	human auditory system
0.2729134603	deep features
0.2728752968	vector machine
0.2727750139	how
0.2726796001	time and frequency
0.2725353284	simulation results
0.2721843451	high quality audio
0.2720250757	verification performance
0.2716391675	end to end speech
0.2715768749	below
0.2715768749	his
0.2713917900	self training
0.2713761735	a challenging task
0.2710219288	attention models
0.2709822115	multi channel speech
0.2703797999	based encoder decoder
0.2703181795	in noisy environments
0.2702943125	plus
0.2701741920	itself
0.2699804590	recent success of
0.2699363871	an encoder decoder
0.2697548393	so far
0.2694410728	audio to score
0.2693649150	word based
0.2692256108	even though
0.2692213414	successfully applied to
0.2686205050	just
0.2685116724	speech synthesis using
0.2684988850	no
0.2684544118	automatic detection of
0.2683600867	rather than
0.2682797362	speech dataset
0.2682214116	using deep neural networks
0.2682211830	2000
0.2676335178	training dataset
0.2672399751	mean average
0.2671879419	over fitting
0.2671061017	speech features
0.2671021150	learn representations
0.2668231288	paper reports
0.2668169272	fed into
0.2666698459	more accurate
0.2666037870	separation network
0.2664871200	13
0.2662404997	real time applications
0.2661880222	their
0.2658527555	dataset demonstrate
0.2655739598	modern deep
0.2655243106	causes
0.2643968468	asr tasks
0.2642418177	doesn't
0.2641842812	audio declipping
0.2641628104	visual and audio
0.2636796001	audio and text
0.2636370897	serious
0.2636370897	actually
0.2636370897	eight
0.2636370897	year
0.2636370897	32
0.2636370897	seems
0.2636370897	outside
0.2636370897	14
0.2634603960	deep networks
0.2633985362	task 5
0.2630539619	an equal error rate
0.2626577752	d vector
0.2625336088	important task
0.2622726371	conversion task
0.2621770651	recorded speech
0.2620330484	the audio domain
0.2619822014	aiming at
0.2616732947	trigger detection
0.2615972041	task 3
0.2615552465	in domain data
0.2614746325	detection and classification of acoustic
0.2614649417	generated speech
0.2613895147	achieve state of
0.2611918251	the previous state
0.2611843939	learning strategies
0.2610881055	scale invariant signal
0.2606864852	average relative
0.2605437258	based voice conversion
0.2604201118	faster than real time
0.2603952224	robust automatic speech
0.2601070905	80
0.2599146069	resource speech
0.2598924660	24
0.2598924660	entirely
0.2598266163	based models
0.2598017759	+ +
0.2596474660	can
0.2595430812	text based
0.2595010363	single channel speech
0.2593425680	based classifier
0.2591676952	enhancement method
0.2589681410	end to end model
0.2589486359	identification task
0.2588917697	speech extraction
0.2587287550	within
0.2585190080	an input
0.2585117596	17
0.2582264992	an ablation study
0.2581218437	low computational
0.2580917028	based method
0.2576796001	audio and video
0.2574940071	360
0.2572184416	audio to score alignment
0.2571049244	tts system
0.2570532786	achieves state of
0.2569348788	caused by
0.2569187657	acoustic parameters
0.2566443760	becoming
0.2566443760	themselves
0.2565599558	proposed techniques
0.2559112271	supervised approach
0.2558767170	usually
0.2558471748	supervised methods
0.2557327752	sound signal
0.2556797203	non local
0.2556530706	nor
0.2556530706	too
0.2556530706	40
0.2556504422	be
0.2555894257	non causal
0.2555701792	a simple
0.2553255377	25
0.2553255377	indicates
0.2550807457	single neural
0.2550726646	recent progress in
0.2550648701	baseline method
0.2550293239	was
0.2547088710	specified
0.2547088710	60
0.2547088710	seven
0.2546504422	which
0.2545959329	before
0.2545813075	spatial audio
0.2545767221	2016
0.2545056105	recent advancements in
0.2544002140	should
0.2543471758	enhancement performance
0.2543008171	clearly
0.2543008171	16
0.2541504422	are
0.2541504422	that
0.2541504422	is
0.2541287031	have
0.2541287031	has
0.2540059704	wide range of
0.2539691625	put
0.2539691625	believe
0.2539495779	visual speech enhancement
0.2539323162	a user study
0.2533881048	paper addresses
0.2533782973	while keeping
0.2533736003	paper shows
0.2532106613	compared to
0.2531460443	minimum mean
0.2531307249	relations between
0.2531139242	the wild
0.2528013455	music domain
0.2527576456	via
0.2526014002	results show
0.2524015117	90
0.2524015117	behind
0.2523413360	enhancement algorithms
0.2522367718	very small
0.2522278457	proposed method improves
0.2522098572	evaluation data
0.2516796001	audio and visual
0.2516011435	being
0.2514880294	based techniques
0.2513198428	asr system
0.2513008171	0
0.2511615554	large dataset
0.2510895752	those
0.2510094748	one dimensional
0.2509690634	deep speaker
0.2508534758	greater than
0.2508002140	upon
0.2507835757	cause
0.2507835757	follows
0.2506287031	been
0.2506161301	described
0.2504189159	neural models
0.2503746944	so called
0.2501411951	sub optimal
0.2501179538	dealing with
0.2500067300	discriminate between
0.2495621471	=
0.2495493736	neural sequence to sequence
0.2495195766	models achieve
0.2494695745	an online
0.2491117102	learning techniques
0.2489372462	speaker independent speech
0.2488143922	&
0.2486578885	without parallel data
0.2479909266	learning algorithms
0.2475792316	improve performance
0.2474661362	detection cost
0.2473005637	ones
0.2472417272	experiments performed on
0.2472201500	trained models
0.2471506174	1d
0.2469848551	song identification
0.2467961525	focuses on
0.2466905480	3d
0.2465202327	own
0.2464701386	quite
0.2462549118	for sound event detection
0.2458972509	around
0.2458369174	audio samples
0.2457930592	range of applications
0.2455787350	embedding methods
0.2455724121	enhancement methods
0.2455424864	training corpus
0.2451292957	18
0.2451292957	already
0.2447602822	suffering from
0.2445032490	9
0.2444539989	etc
0.2440891153	extensive experiments on
0.2440793021	where
0.2436691772	universal sound
0.2435797993	embedding models
0.2434116451	user study
0.2433753438	treated as
0.2432762633	contain
0.2431128487	hybrid dnn
0.2430594939	faster than
0.2429397474	model performs
0.2427959623	had
0.2426762633	cannot
0.2424162853	training algorithm
0.2422338068	learning setting
0.2421261017	15
0.2417678715	data corpus
0.2417107297	network parameters
0.2413826928	vector analysis
0.2413341022	them
0.2412341502	becomes
0.2408575878	any to many
0.2408209566	proposed scheme
0.2399249499	test other
0.2397421455	an end to end
0.2396688078	either
0.2396645680	consists of
0.2394326280	this article
0.2393675894	achieve high
0.2393137798	shown promising
0.2391831464	an utterance
0.2390742608	relation between
0.2387122108	based model
0.2385714840	namely
0.2383992693	neural text
0.2383370784	respectively
0.2380096844	evaluation results
0.2380037450	were
0.2379874168	labeled training
0.2378289214	might
0.2377265116	a data driven
0.2375254509	representations of audio
0.2374780976	depends on
0.2373594038	invariant signal
0.2373246457	recognition problem
0.2370586226	embeddings extracted
0.2368915720	rely on
0.2363878356	outperforms previous
0.2363566957	detection task
0.2362338520	wer improvement
0.2362022557	cross lingual voice
0.2360638575	case study
0.2358269097	could
0.2355455827	suffers from
0.2354353894	different speakers
0.2353850728	focus on
0.2353370784	may
0.2352294330	relies on
0.2347129537	regarded as
0.2346929805	learning approaches
0.2346267888	multiple speaker
0.2346058927	audio files
0.2344558690	maximization algorithm
0.2342634802	inspired by
0.2338444602	time domain audio separation
0.2337847643	depending on
0.2335972432	based features
0.2334726277	30
0.2334726277	gives
0.2334726277	mostly
0.2332057175	detection performance
0.2331054057	fully convolutional neural
0.2330461643	significant improvements in
0.2329768135	sequence model
0.2328015216	audio quality
0.2327774538	test results
0.2325292653	single model
0.2323157542	much larger
0.2321298859	will
0.2321119407	robustness against
0.2319609077	detection methods
0.2318245281	the proposed method improves
0.2316977363	u net architecture
0.2315970171	sound analysis
0.2314408643	audio representations
0.2314389252	time frequency representation
0.2312898436	human auditory
0.2312417736	with skip connections
0.2310886304	the art results
0.2307196101	results demonstrated
0.2306249572	enhancement approach
0.2305234291	whose
0.2303927997	model training
0.2302503700	insights into
0.2302347375	models trained
0.2301164671	important role in
0.2301156461	conversion challenge
0.2300852379	loss functions for
0.2296919382	while preserving
0.2295145833	become
0.2295033118	outperforms existing
0.2294462472	a small
0.2292200834	based se
0.2286878854	vector embeddings
0.2285715468	current state
0.2284784896	processing step
0.2283479166	would
0.2280783037	commands dataset
0.2280518382	mainly
0.2272405431	an effective
0.2265532901	many to many voice conversion
0.2265192019	speech source
0.2263135844	on device
0.2257568676	in many speech
0.2255196716	non standard
0.2248990226	information about
0.2245172821	without requiring
0.2242447898	three dimensional
0.2241574200	different domains
0.2240313009	mean squared
0.2240198475	speech enhancement method
0.2240031050	does not
0.2238659694	past few
0.2238208548	a strong baseline
0.2237856160	end to end fashion
0.2235734318	vocabulary speech
0.2235400347	parametric speech
0.2234710261	on test clean
0.2229447212	a challenging problem
0.2228532574	synthesis models
0.2228131986	baseline models
0.2225492152	music signal
0.2222611910	depend on
0.2220835114	self attention based
0.2220638038	prior work
0.2220186599	challenge 2018
0.2217025975	recent deep learning
0.2216347176	hybrid ctc
0.2213182386	relationships between
0.2212584964	few years
0.2211455978	audio visual dataset
0.2209541253	higher than
0.2205060518	end to end speaker verification
0.2205058516	markov models
0.2204880889	well defined
0.2204085629	distances between
0.2199084084	artificial neural
0.2198959184	achieve competitive
0.2196941780	human like
0.2192756994	a unified
0.2192235595	capture long
0.2190425473	proposed architecture
0.2189614929	achieved state of
0.2188295062	speech database
0.2188211428	the proposed scheme
0.2184183549	an iterative
0.2180925026	the target speaker
0.2175678923	during inference
0.2175277858	machine learning based
0.2172972020	audio visual speech
0.2172341135	neural sequence
0.2170265681	learning models
0.2168225415	audio event
0.2165028794	to distortion ratio
0.2158152857	more discriminative
0.2154836986	number of
0.2154021832	the proposed method outperforms
0.2150693479	ranging from
0.2148226310	these models
0.2143847810	representations learned
0.2142692786	great success in
0.2142584239	for speaker recognition
0.2140370556	recognition models
0.2138212520	consisting of
0.2137315853	diarization system
0.2135127394	to retrieve
0.2133956466	a deep learning based
0.2133466525	this study
0.2132697301	an important role
0.2131858177	the short time fourier transform
0.2130204769	this problem
0.2130191864	available at https
0.2129256861	significant improvement in
0.2128294722	an alternative
0.2125533213	separation method
0.2124731478	perform experiments
0.2121929461	speech synthesis system
0.2121855258	~ \
0.2121853924	difference between
0.2121711822	relative word
0.2121593019	for audio classification
0.2119112534	extracted from
0.2118379175	model trained
0.2116727562	phone error
0.2115939284	time steps
0.2112630126	google speech
0.2111358290	new speakers
0.2110265661	goodness of
0.2110137383	speech spectrum
0.2106880425	an interactive
0.2106841254	previous work
0.2103304918	proposed approaches
0.2103131816	applied to
0.2102835675	network transducer
0.2101004909	art results on
0.2098657343	relative reduction in
0.2097156902	learning architectures
0.2095854854	leads to
0.2095463204	processing algorithms
0.2095344195	model shows
0.2094618441	word error rate by
0.2094033486	trained end to end
0.2093445597	speech enhancement task
0.2093312356	human performance
0.2091162017	while retaining
0.2090333862	network to learn
0.2088763450	do not
0.2084552429	world scenarios
0.2084479693	low resource speech
0.2084210788	learning methods
0.2081261795	use cases
0.2081054374	from multiple speakers
0.2079393898	non parallel training
0.2079375607	acoustic source
0.2079363902	based speech separation
0.2077968757	transfer learning from
0.2074583037	an adaptive
0.2073355772	amounts of data
0.2072922068	model trained on
0.2064244114	$ m
0.2063542451	asr training
0.2060176113	acoustic information
0.2060106188	more realistic
0.2060087918	in addition
0.2059817028	best performing
0.2058143311	specific data
0.2052231289	relationship between
0.2051883259	2020 challenge
0.2049551559	aims at
0.2048964067	\ sim
0.2047603487	time invariant
0.2047083262	recognition model
0.2046183952	the proposed architecture
0.2045251231	attention layers
0.2043607995	to train
0.2043159283	deep learning model
0.2039342656	models trained on
0.2037979505	compared with
0.2037306811	well studied
0.2032266372	dnn based speech
0.2030544442	to learn
0.2029295678	as input
0.2027257719	generated by
0.2027141081	speech input
0.2026886227	end to end spoken
0.2025173612	for acoustic scene classification
0.2025015717	recent deep
0.2020741862	recent progress
0.2017098499	for speech emotion recognition
0.2016302017	to improve
0.2014688281	deep learning based speech
0.2009374815	art accuracy
0.2007998827	2mix dataset
0.2007302796	the first step
0.2005746156	the proposed algorithm
0.2004909344	focused on
0.2003348273	based acoustic model
0.2003145641	a multi modal
0.1999326690	two stage training
0.1999032259	large number
0.1998567215	comparison between
0.1997609528	shared across
0.1995315268	opposed to
0.1994584425	training framework
0.1994167251	large set of
0.1991820268	conditioned on
0.1991610979	asr model
0.1991372913	the ground truth
0.1989589835	an important task
0.1989204762	correlation between
0.1985841261	large number of
0.1984308500	the art result
0.1982562736	the final
0.1980403381	evaluation of speech quality
0.1977884298	more than
0.1976939354	each source
0.1973455938	to image translation
0.1969191041	many to many
0.1967124628	results indicate
0.1961701313	the proposed
0.1961511563	based architecture
0.1961456555	end to end systems
0.1960535171	a semi supervised
0.1960201240	multi speaker speech
0.1960169716	recent advances in deep
0.1959429579	characterized by
0.1959014724	for robust speech recognition
0.1958889788	during training
0.1958822592	end to end approach
0.1958423819	speech recognition task
0.1957669327	mismatch between
0.1955384382	speech enhancement systems
0.1953810835	a deep neural
0.1953625285	evaluation results show
0.1952586738	well known
0.1952406601	a lot
0.1950712041	recent success
0.1949981241	the proposed technique
0.1949416843	faster than real
0.1946015671	viewed as
0.1945012349	perform better
0.1944016786	a novel
0.1934856675	information into
0.1934330750	multichannel audio
0.1930382649	learning approach
0.1930064497	domain features
0.1928982886	as opposed
0.1927788258	extract features
0.1927580927	for environmental sound
0.1922245547	the end of
0.1917286308	to detect
0.1916605118	relative improvement in
0.1914226019	conjunction with
0.1914075215	attention based neural
0.1913302447	speaker classification
0.1913073311	take advantage of
0.1912488730	automatic recognition of
0.1912038900	speech enhancement performance
0.1911173041	independent speaker
0.1907339922	over smoothing
0.1907283116	not clear
0.1906518755	connection between
0.1901852653	the domain of
0.1900616586	data recorded
0.1900146928	art results in
0.1899372323	recent work
0.1898955000	speech synthesis systems
0.1898941684	knowledge about
0.1897807430	monaural speech
0.1896629160	voice conversion using
0.1895481515	from scratch
0.1895300404	classification models
0.1894203489	to extract
0.1893498895	this thesis
0.1892685987	the prediction of
0.1891852653	the voice of
0.1891852653	the level of
0.1891844408	a brief
0.1890746736	head attention
0.1890424082	the network to
0.1889662660	speech transformer
0.1886010294	speech enhancement using
0.1885185987	the purpose of
0.1885185987	the result of
0.1885185987	the style of
0.1883255992	experiment results
0.1883254169	due to
0.1882685987	the outputs of
0.1882514924	acts as
0.1882445279	competitive performance on
0.1881876710	performs better than
0.1881852653	the parameters of
0.1881852653	the length of
0.1881852653	the recognition of
0.1880424082	the knowledge of
0.1879208667	to end automatic speech recognition
0.1879203489	to predict
0.1878317261	improvement over
0.1875185987	a corpus of
0.1875185987	the objective of
0.1874825748	second step
0.1872247664	method based on
0.1872198161	based acoustic models
0.1872073021	found data
0.1870209799	supervised data
0.1869932575	serves as
0.1867911023	to address
0.1866894670	a function of
0.1866496865	both audio and
0.1866324550	obtained from
0.1866188934	to separate
0.1865197174	as well as
0.1862685987	the study of
0.1859996319	an acoustic model
0.1858792020	extract speaker
0.1857849512	good performance
0.1857728004	the generation of
0.1855462305	to generate
0.1854968947	attention based model
0.1853767922	based framework
0.1853060203	trained on
0.1852433395	similarity between
0.1851915703	integrated into
0.1850538278	this work
0.1850228004	the issue of
0.1849384646	single neural network
0.1848830899	a deep learning model
0.1848171316	experiments show
0.1846894670	the identification of
0.1843621752	owing to
0.1843421752	corresponds to
0.1840889547	end to end spoken language
0.1838898127	dominated by
0.1837531348	the help of
0.1836997826	improvements over
0.1836894670	the phase of
0.1836572326	produced by
0.1834244581	previous state of
0.1833710205	in conjunction
0.1833430416	lead to
0.1833035641	significantly better than
0.1832838232	benefit from
0.1831651504	the process of
0.1830222933	the extraction of
0.1830193071	$ n
0.1828887550	end to end learning
0.1826693521	a study of
0.1825623085	prior knowledge of
0.1825500026	conducted on
0.1825203030	distinguish between
0.1824457496	a neural network
0.1824419414	better performance
0.1823037014	more and more
0.1822834371	embedding model
0.1822110585	audio embeddings
0.1821540301	a single model
0.1820804023	the system to
0.1820358615	to perform
0.1820112319	sequence to sequence neural
0.1818869196	the original
0.1816044158	based end to end
0.1815969529	time frames
0.1815500107	this task
0.1815037673	to map
0.1810384150	capable of
0.1807947783	notion of
0.1807934833	to obtain
0.1807718330	followed by
0.1807482856	motivated by
0.1805853138	event recognition
0.1803881535	did not
0.1803718571	formulated as
0.1801994430	input data
0.1800222933	the performances of
0.1798609454	clean audio
0.1798211666	two stages
0.1793629061	relatively small
0.1792245547	the learning of
0.1791651504	the cost of
0.1790139609	in reverberant environments
0.1787623554	an important
0.1787572489	time step
0.1786463731	learning systems
0.1786338763	derived from
0.1783356029	to estimate
0.1782138762	2d cnn
0.1782110547	speech recognition model
0.1781143180	amount of training data
0.1780114342	framework for
0.1779472864	this issue
0.1779214342	such as
0.1777791995	for classification of
0.1777716388	close to
0.1776693521	the framework of
0.1774315937	in order to
0.1773213241	a generative adversarial network
0.1772460543	speech utterance
0.1771269475	the art approaches
0.1770961161	audio waveforms
0.1770914448	three different
0.1768885127	unsupervised speech
0.1768740267	mismatch between training
0.1767423925	investigate different
0.1764412920	performance comparable to
0.1764261335	an investigation
0.1763712736	directly from
0.1763155977	further improve
0.1759702151	n best
0.1759345041	the present study
0.1757511766	sequence based
0.1755011459	converted into
0.1752108992	pitch dependent
0.1750937817	an additional
0.1750225390	proposed approach outperforms
0.1750222774	in terms of
0.1745980946	level feature
0.1745680516	compared to previous
0.1743689269	by introducing
0.1742686016	1 d
0.1742572843	serve as
0.1742044398	the spectrogram of
0.1741781237	speech recognition using
0.1741765825	affected by
0.1739678108	trained with
0.1737771380	loss based
0.1735589868	level representations
0.1732228460	enhancement models
0.1731843961	non target
0.1731696014	proposed models
0.1728853355	neural network model
0.1727755101	target data
0.1726094885	investigate whether
0.1725890274	lower than
0.1725875078	obtained by
0.1725102418	based singing voice
0.1724491509	these methods
0.1723476797	two main
0.1722625329	between speech and
0.1720390105	for automatic speech recognition
0.1718947817	to recognize
0.1718615822	noise model
0.1716864101	two step
0.1714603876	recognition system
0.1714060672	by exploiting
0.1712696328	inference time
0.1711706545	to reduce
0.1709790265	a new dataset
0.1709550734	propose to use
0.1709171658	this approach
0.1708919883	hours of
0.1706693521	the tasks of
0.1706628882	quality than
0.1706613820	compared to conventional
0.1706170364	based data augmentation
0.1705397546	relying on
0.1703774188	data from multiple
0.1703551386	step towards
0.1703080062	$ \
0.1702919446	the encoder and
0.1699186409	experiments on
0.1698730667	jointly trained with
0.1698446640	a pre trained
0.1697865135	an average
0.1697482251	based systems
0.1696584343	best result
0.1690311807	based on deep learning
0.1689850321	and classification of acoustic scenes
0.1689256788	features learned
0.1688675805	the main
0.1687332742	yields better
0.1687147942	speaker recognition challenge
0.1686700073	an ensemble
0.1685468962	for single channel
0.1683548900	efficient way
0.1682601804	achieved by
0.1682029856	experiments indicate
0.1679833715	able to
0.1678912427	to produce
0.1678862998	features from
0.1678355307	performs well
0.1674715430	sound event localization and
0.1672263269	and classification of
0.1672078471	mapping between
0.1671089185	long time
0.1670356865	use case
0.1669288462	evaluated on
0.1669263240	more robust
0.1669203316	to achieve
0.1667934610	combined with
0.1667551660	to solve
0.1664920407	end to end training
0.1664765508	the notion
0.1664255030	visual scene
0.1663306375	approach provides
0.1663115456	between training and
0.1663067057	each utterance
0.1662320541	knowledge into
0.1661642871	a large
0.1661529630	$ k
0.1660142068	corpus show
0.1657841321	lot of
0.1657245760	to build
0.1652044398	the code and
0.1650950802	significantly better
0.1648584728	based loss
0.1648048768	student learning
0.1646704623	these techniques
0.1645779814	a neural network based
0.1641586896	accuracy than
0.1640223092	for use in
0.1638877714	art performance on
0.1637524450	entropy loss
0.1636020533	three tasks
0.1633618939	perceptual evaluation of speech
0.1632140955	gains over
0.1632042462	to convert
0.1631518179	interactions between
0.1630498164	frame by
0.1630299518	explore various
0.1629095113	compared against
0.1626159622	methods like
0.1623666010	verification system
0.1623227823	the training set
0.1621740111	training method
0.1617996191	control over
0.1617815833	the model
0.1616855990	a multi task learning
0.1616706545	to identify
0.1615603655	to evaluate
0.1615332655	system achieves
0.1614047886	focusing on
0.1611484210	time warping
0.1608101966	presence or absence of
0.1605563531	dependencies between
0.1603907089	provided by
0.1599571078	represented by
0.1599544398	a technique to
0.1598080556	very low
0.1597432730	on wsj0 2mix
0.1596157611	drawn from
0.1590763344	model performance
0.1587816903	dependent speaker
0.1586641242	different languages
0.1585053219	smaller than
0.1583916548	learned from
0.1583915159	belonging to
0.1582808466	asr based
0.1580579334	end to end neural network
0.1580458973	simple yet
0.1578812142	by applying
0.1578548984	an auxiliary
0.1577719481	performs better
0.1577291843	for end to end speech
0.1577029281	results than
0.1575587299	generalize well
0.1574597584	neural text to
0.1573848659	two fold
0.1573711367	to noise ratio
0.1573584375	deep speech
0.1573261266	in real world
0.1572670335	audio analysis
0.1572232112	pre trained on
0.1569672187	synthesis system
0.1568664744	better than
0.1567930187	extracted by
0.1567588870	audio generation
0.1567060198	interpreted as
0.1566492157	performance on
0.1565532154	the fly
0.1564933447	leading to
0.1564519888	significantly more
0.1563261257	audio clip
0.1561940472	to create
0.1561639793	datasets show
0.1561609772	improves over
0.1560693326	source separation using
0.1558374953	best performance
0.1558061565	the binaural cues
0.1557484572	deep network
0.1556899901	a preliminary
0.1556028020	of speakers in
0.1555977976	carried out using
0.1555443147	the other hand
0.1555167405	for text independent
0.1554772064	existing state of
0.1552887671	in real life
0.1552009548	end to end automatic
0.1551264044	based speaker recognition
0.1548813662	unknown number of
0.1547346854	distance between
0.1546110705	accuracy over
0.1539657270	much faster
0.1537717457	suitable for
0.1536872564	set of
0.1536168537	of sound events
0.1536030917	audio content
0.1535872043	end to end speaker
0.1535765133	not only
0.1533688934	a strong
0.1533197807	relative transfer
0.1532251357	recognition dataset
0.1531724768	reverberation time
0.1530842204	an audio clip
0.1530776775	time delay neural
0.1528011459	to enhance
0.1527242017	for text independent speaker verification
0.1526980644	to leverage
0.1519903016	belong to
0.1519419866	information from
0.1519027795	much higher
0.1518996758	good quality
0.1517901144	a fully connected
0.1516263405	relative improvements of
0.1515459309	the training phase
0.1515044656	an essential
0.1512078630	correspond to
0.1511458333	kws system
0.1510447933	at least
0.1509172372	speech recognition tasks
0.1509009766	visual speech recognition
0.1508558411	audio waveform
0.1503328990	to develop
0.1501916328	the target
0.1498468295	knowledge from
0.1498013378	method does not
0.1495786837	modeled by
0.1493665932	to classify
0.1491601864	replaced by
0.1491519208	seen during training
0.1489981854	publicly available at
0.1487675581	more efficient
0.1486734793	audio scene
0.1485328639	detection system
0.1485149263	training approach
0.1483145677	stage approach
0.1480258020	network acoustic models
0.1479455828	using eeg
0.1479160762	the same
0.1479090567	point to
0.1476370563	paper provides
0.1474573310	small amount of
0.1471478154	better performance than
0.1468644531	limited number of
0.1468597767	end to end framework
0.1468155735	voices from
0.1468087539	the last few
0.1467756610	used to train
0.1467076852	the research community
0.1466980644	to enable
0.1465498454	shown to
0.1463019384	considered as
0.1462992188	the proposed methods
0.1458893530	more complex
0.1458785627	learning method
0.1458741357	less than
0.1458497158	these features
0.1457386405	to capture
0.1457089974	access to
0.1452581814	each frame
0.1452044937	asv system
0.1448350392	human perception of
0.1448287093	produce high
0.1448073477	outperforms state of
0.1447006590	according to
0.1445995682	tasks like
0.1445886964	generating speech
0.1444817234	perform well
0.1444401004	model achieved
0.1443636711	enhancement system
0.1443535459	the target domain
0.1442042871	second stage
0.1440152954	the student model
0.1438786159	using convolutional neural networks
0.1437915288	word error rate on
0.1431501815	a multi task
0.1426237130	audio source
0.1426208227	consist of
0.1425748124	emotional voice
0.1422506766	the time frequency domain
0.1421758851	amounts of
0.1421487605	for low resource
0.1418947228	\ pm
0.1418075058	the conventional
0.1414768317	a significant improvement
0.1413209560	of arrival estimation
0.1412416925	available online
0.1411115376	achieves competitive
0.1410995981	the entire
0.1409658175	these systems
0.1409252621	attempt to
0.1407326412	system outperforms
0.1406906993	aim at
0.1406479258	a low dimensional
0.1405666957	achieve better
0.1404437726	with respect to
0.1402953595	not seen during training
0.1400880715	related to
0.1400329521	to provide
0.1398506155	variety of
0.1395629701	composed of
0.1394194421	first layer
0.1391865632	connections between
0.1390754677	achieves better
0.1386212927	aims to
0.1386146153	far field speech
0.1385199942	by combining
0.1384496580	able to produce
0.1381504349	for text independent speaker
0.1379087574	interested in
0.1375677075	for end to end speech recognition
0.1375428376	results on
0.1374787724	an arbitrary
0.1373756105	an automatic
0.1372885544	resulted in
0.1368649576	parts of
0.1368124173	speaker verification using
0.1366077456	art end to end
0.1364666136	performance under
0.1364008314	for on device
0.1363985861	this work proposes
0.1362805607	classification based
0.1362608288	detection using
0.1361533267	art approach
0.1361443668	kinds of
0.1360405180	a single neural network
0.1356468838	text to speech system
0.1354795868	other words
0.1353591414	the development set
0.1351601094	the target speaker's
0.1351240667	speech enhancement based on
0.1351003888	successfully used
0.1348888137	applications like
0.1348268531	emerged as
0.1347510647	to understand
0.1347392693	easy to
0.1344250055	data from
0.1341462741	an end to end neural
0.1339562294	coupled with
0.1339511749	more flexible
0.1339436240	to minimize
0.1336004467	simple but
0.1335618758	a common
0.1335083850	end to end system
0.1332457931	the training data
0.1331439253	limited amount of
0.1331051171	different types of
0.1328660678	model trained with
0.1328179376	speaker recognition system
0.1328025151	based on deep neural
0.1327052973	two steps
0.1326938487	all pass
0.1325485309	tend to
0.1324422377	submitted system
0.1324257272	a wide range
0.1323595899	version of
0.1322316328	by adding
0.1321862712	the training process
0.1321832115	as well
0.1320796848	used to generate
0.1318231476	the performance of
0.1317266774	audio stream
0.1317216675	impact on
0.1317175258	look at
0.1316727297	supported by
0.1314826921	to tackle
0.1314444762	mismatch between training and
0.1313962031	online speech
0.1313162136	interaction between
0.1313087692	indistinguishable from
0.1312967348	the baseline
0.1312436103	developed by
0.1311612186	better results than
0.1311529845	to overcome
0.1311367379	the first pass
0.1311306456	types of
0.1309857152	based on convolutional
0.1309710547	a comprehensive
0.1308198569	training phase
0.1306944248	well trained
0.1306701665	these approaches
0.1305850741	for automatic speaker verification
0.1305681392	stage training
0.1303965123	non parallel voice
0.1302034998	other methods
0.1301857259	sub network
0.1301618902	audio representation
0.1300718290	very large
0.1297699085	a feed forward
0.1297074619	type of
0.1296978996	the t f
0.1296778544	robust speech
0.1296145088	for end to end
0.1295546376	the speech signal
0.1294774045	the shelf
0.1294256472	apart from
0.1293482293	produce more
0.1292692472	in most cases
0.1292547433	first stage
0.1291539596	for music source
0.1291247458	all neural
0.1291132911	the fact
0.1290657027	each time frequency
0.1289596454	more specifically
0.1289226504	compared to state of
0.1288487870	built on
0.1286869276	relied on
0.1285212820	thousands of
0.1284955672	large amount of
0.1284251094	refers to
0.1283764404	led to
0.1283647311	to combine
0.1279023723	propose to train
0.1277572946	network to predict
0.1273198037	a recently proposed
0.1273194592	based on deep neural networks
0.1271852465	aim to
0.1269525699	method uses
0.1267916443	represented as
0.1267340672	an image
0.1267080102	music emotion
0.1265297066	sub networks
0.1264389089	a new method
0.1264315049	to handle
0.1261895996	on librispeech
0.1260529442	large amount of data
0.1260168090	without parallel
0.1257701000	able to generate
0.1256893495	based sequence to sequence models
0.1256585554	yet effective
0.1256507482	learned by
0.1255907040	recognition evaluation
0.1254206061	audio effects
0.1253178318	a shared
0.1253084553	range of
0.1249694859	based architectures
0.1246184723	to extract features
0.1245313978	to optimize
0.1245290300	the input
0.1245139262	time aligned
0.1244605638	role in
0.1244223917	to text translation
0.1241653540	deal with
0.1241011996	contribute to
0.1240690256	various conditions
0.1240547418	perceptual quality of
0.1239785632	generalize well to
0.1239590266	outperforms other
0.1238841758	integrated with
0.1238133301	collected from
0.1237824400	speech sounds
0.1237269615	based multi
0.1237071808	representation of speech
0.1237070132	methods in terms
0.1235351240	against adversarial
0.1231615921	these issues
0.1230654313	identification using
0.1229537400	the state of
0.1228877958	for code switching
0.1228777850	applications such as
0.1227656365	the visual modality
0.1227593977	the art systems
0.1226434721	perceptual evaluation of
0.1224896830	detection systems
0.1223307771	able to achieve
0.1223162109	different ways
0.1222201874	compare different
0.1220518520	scores from
0.1219711242	by leveraging
0.1219533865	event detection using
0.1218100856	test clean and
0.1217484778	the proposed network
0.1217144367	by minimizing
0.1216424600	small number of
0.1214756795	part of
0.1214331353	signal into
0.1214273097	emotion recognition from
0.1213690337	for monaural
0.1213413739	an equal error
0.1212416146	refer to
0.1209901916	correlated with
0.1207943548	still challenging
0.1207625533	millions of
0.1206660431	vulnerable to
0.1206650764	approach to
0.1206015133	a fully
0.1205159654	a novel architecture
0.1204352125	neural speech
0.1204056458	to unseen speakers
0.1203703087	based music
0.1202599074	make use of
0.1202034893	audio adversarial
0.1201282808	other domains
0.1199295010	adoption of
0.1199116859	a convolutional
0.1198578808	performance comparable
0.1197927894	emergence of
0.1196119735	taken into
0.1194362529	very effective
0.1194203320	equipped with
0.1192821670	models trained with
0.1192189970	computer interaction
0.1191584650	dataset contains
0.1190390412	comprised of
0.1189674973	for text dependent
0.1187323117	model for
0.1186993269	ability to
0.1186689616	comparable to
0.1186300861	classification system
0.1184537987	error rate by
0.1184438846	two speakers
0.1183122655	loss function for
0.1181237780	speech recognition performance
0.1181139083	factors such as
0.1179824842	these tasks
0.1179693522	very limited
0.1178927556	difficult to
0.1177898861	a convolutional recurrent neural
0.1177894730	used for training
0.1177871585	this work explores
0.1177426218	multichannel speech
0.1177421869	a convolutional recurrent
0.1176878667	based sequence to sequence
0.1173196624	kind of
0.1172534795	art performance
0.1172016394	to facilitate
0.1171757739	audio sources
0.1171297604	separation model
0.1170772768	for robust speech
0.1169655015	a significant
0.1167911364	x vector system
0.1167247840	to determine
0.1167133806	with limited
0.1166978971	a priori
0.1166410170	the use of
0.1165880624	not seen during
0.1163734395	techniques such as
0.1163670065	unseen during
0.1162896623	at https
0.1162827484	very little
0.1162328557	representation of
0.1160251989	overall accuracy
0.1160046413	model accuracy
0.1158663916	a wide
0.1158543957	se system
0.1157708386	alignment between
0.1156209548	more difficult
0.1154934734	a wide range of
0.1154409101	an external language
0.1151523362	for distant speech
0.1150569147	speaker modeling
0.1150347266	model to generate
0.1148460516	performed by
0.1147871985	ease of
0.1147042037	parallel speech
0.1146521035	dataset show
0.1146174594	but also
0.1145940377	the input signal
0.1145072673	applicable to
0.1145027758	experiment on
0.1144799649	performance over
0.1143206152	these limitations
0.1141937837	the art models
0.1141520409	learning model
0.1140257571	the relationship between
0.1139581428	the raw waveform
0.1138014226	the effectiveness of
0.1137198179	an audio signal
0.1137094378	two different
0.1135951927	generated from
0.1135562765	signal based
0.1134378654	tested on
0.1134266371	first step
0.1134192449	similar to
0.1133580108	a new
0.1133195003	set of experiments
0.1132038594	two parts
0.1131471624	on top of
0.1129205747	condition training
0.1127402887	the voxceleb speaker recognition
0.1127086921	to avoid
0.1126606124	characteristics of
0.1126458948	classification using
0.1125783162	prone to
0.1125527229	challenging due to
0.1124362990	by proposing
0.1122749505	analysis of
0.1122122291	embeddings from
0.1118819283	to date
0.1118698319	each language
0.1115501196	able to reduce
0.1114941513	demonstrated on
0.1114815617	vector based
0.1114574345	of interest
0.1114312888	based speech recognition
0.1114056915	not always
0.1113967380	not yet
0.1111872542	taking into
0.1110670306	both objective and subjective
0.1108207380	for real time applications
0.1106689750	the case of
0.1106284854	challenge task
0.1105177519	because of
0.1104888464	even if
0.1103715981	possible to
0.1102024494	effectiveness of
0.1101654873	the converted speech
0.1100283906	in order to improve
0.1099157937	model uses
0.1096227759	to further improve
0.1096063222	the teacher student
0.1095035553	performance compared to
0.1094791653	these two
0.1093524886	so as to
0.1092466723	these new
0.1091734944	the time domain
0.1091425809	input speech
0.1091198919	corrupted by
0.1090571710	challenging due
0.1090411492	along with
0.1090033119	small amount
0.1088565525	the efficacy of
0.1088445749	based end to end speech
0.1088276717	the target language
0.1086456968	a series of
0.1086362787	such as wavenet
0.1086158979	speech spectrogram
0.1085829281	the most important
0.1084743970	training time
0.1084309482	performance than
0.1083535505	leads to better
0.1081558114	for text dependent speaker
0.1080729654	art model
0.1079698042	able to learn
0.1079590496	starting from
0.1079491651	and long short term
0.1078831566	the art neural
0.1078438678	these problems
0.1078206360	improvement compared to
0.1077265609	submission to
0.1075954374	known as
0.1074468979	a transformer based
0.1073317884	current speech
0.1073280413	order to improve
0.1071492698	a key
0.1070775871	first present
0.1070310107	scarcity of
0.1070205747	mixture models
0.1070175078	combination of
0.1069596967	model consists of
0.1069337423	learning method for
0.1069231149	more effective than
0.1067702186	the proposed system
0.1067200940	levels of
0.1066081583	model for music
0.1065235253	algorithm based on
0.1064920156	previous work in
0.1062355806	a variety of
0.1062331649	audio feature
0.1060329114	much more
0.1059702838	sequence to sequence speech
0.1059193790	learn from
0.1058802062	on device speech
0.1058668616	the timit dataset
0.1058093710	improvement on
0.1056264407	multiple sound
0.1056116055	the art performance on
0.1054484506	work proposes
0.1054153600	each layer
0.1052471110	performance of
0.1050850315	to do so
0.1050412308	properties of
0.1049990496	captured by
0.1049727373	mapping from
0.1049596976	and time consuming
0.1048900539	for single channel speech
0.1048823696	on average
0.1048621605	the lack of
0.1048162479	to ensure
0.1048142482	the primary
0.1047946847	subset of
0.1046714314	an unknown number of
0.1045067873	tasks such as
0.1044687465	other hand
0.1044635682	time frame
0.1044332046	in many cases
0.1044086287	classification model
0.1043545311	method for
0.1043211327	a comparative
0.1043046765	solutions for
0.1040865393	by means of
0.1040650294	to make
0.1038710631	designed to
0.1038621605	the field of
0.1037393490	both tasks
0.1036927412	a two step
0.1035702425	model to learn
0.1034275167	very challenging
0.1034182634	over time
0.1033580108	used to
0.1032265677	used to compute
0.1032201040	resulting in
0.1031810512	to assess
0.1031564810	the problem of
0.1031019059	corresponding to
0.1030734635	time frequency representation of
0.1030256604	available at
0.1030148419	an adversarial
0.1028492989	the most popular
0.1026944260	used to evaluate
0.1026803584	best knowledge
0.1026799254	to deal with
0.1026267055	under noisy
0.1025406886	improvement in
0.1022506379	approach based on
0.1022176659	sensitive to
0.1021564810	a set of
0.1020727991	to adapt
0.1019488983	a recurrent neural network
0.1018688818	feasibility of
0.1018294159	this purpose
0.1018148966	well as
0.1017836732	achieve good
0.1016931056	contained in
0.1016877525	nature of
0.1016370335	to represent
0.1015008436	the vocal tract
0.1015004424	streaming speech
0.1014437731	two channel
0.1013956967	a deep
0.1013287583	to compute
0.1012492167	comparison with
0.1012326814	the baseline model
0.1011799263	through experiments
0.1011647649	mapped to
0.1011564810	the task of
0.1011001755	tests show
0.1010972582	measured by
0.1009827706	2019 challenge
0.1009576180	speech recorded
0.1009544835	the current
0.1008012085	recordings from
0.1007814110	model based on
0.1007208963	used for
0.1007095290	aspects of
0.1006593696	a small number of
0.1006516977	architecture for speech
0.1005167315	the equal error rate
0.1003647331	than others
0.1003635776	embedding network
0.1002865853	by employing
0.1002856378	amount of
0.1001074746	a deep convolutional
0.1000660068	any additional
0.1000440024	detection models
0.0999555274	fed to
0.0999518188	the desired
0.0999002446	the art results on
0.0998855807	this work investigates
0.0998621605	the goal of
0.0998470835	the latent space
0.0998301030	driven approach
0.0998165703	speech recognition with
0.0995679770	often used
0.0993963535	a sequence of
0.0992969574	single network
0.0992178475	bag of
0.0991855614	the teacher model
0.0991396126	based on multi
0.0991289196	a fixed
0.0991259007	most existing
0.0990829346	to analyze
0.0989007380	classification accuracy of
0.0988599740	a novel technique
0.0987956966	framework based on
0.0986905541	more challenging
0.0986301454	tool for
0.0985215999	at test time
0.0984852138	up to
0.0984424238	the output of
0.0984231827	able to perform
0.0984216213	a cross modal
0.0983550582	the cocktail party
0.0983445996	previous work on
0.0982788214	this limitation
0.0981849254	an open
0.0981848480	the design of
0.0981238794	utterances from
0.0981201695	to guide
0.0979590638	limited amount
0.0979292114	evaluations show
0.0978979700	the fundamental frequency
0.0978886763	for example
0.0977580791	sound event detection with
0.0976978271	relevance of
0.0976685916	reference audio
0.0976674198	interact with
0.0976219007	a multi
0.0975745395	these embeddings
0.0975436044	a critical
0.0975140887	to reconstruct
0.0974549215	also present
0.0973904828	adapted to
0.0973621605	the robustness of
0.0972749289	used as
0.0971564810	the quality of
0.0971228301	availability of
0.0970081710	a low resource
0.0970081614	applied as
0.0968366656	operating in
0.0967490588	majority of
0.0967041713	consists of three
0.0966986719	two approaches
0.0966800106	a microphone array
0.0966673066	unlabeled audio
0.0965655569	an empirical
0.0964378743	unable to
0.0963743817	real time on
0.0963235241	scale datasets
0.0962004604	a real time
0.0961756675	the synthesized speech
0.0961434542	very deep
0.0960505445	a given
0.0959891943	terms of
0.0959838268	much attention
0.0959311645	two sets of
0.0958883384	the two modalities
0.0958151908	simultaneous speech
0.0957901491	each other
0.0957180158	of arrival
0.0957024289	a stack of
0.0956709943	limited by
0.0955874826	the latent variables
0.0955247136	different approaches
0.0955127213	the frame level
0.0954627496	based speech synthesis
0.0954499307	experiment with
0.0954238190	dataset containing
0.0953699342	across multiple
0.0953513342	spoken by
0.0951461134	audio clips
0.0951143571	an extensive
0.0950358865	neural source
0.0949962673	without increasing
0.0949765678	implementation of
0.0949245210	a complete
0.0949035867	decoder based
0.0948221830	large amount
0.0948014226	the effect of
0.0947408800	the second one
0.0947067269	time consuming and
0.0945490494	used to extract
0.0944185522	also investigate
0.0943888276	many applications
0.0943621605	the presence of
0.0941408556	performance in terms
0.0940451627	generative model for
0.0939352184	all other
0.0937834720	the importance of
0.0937360861	approach for
0.0936921105	further improvements
0.0936388957	a fast
0.0936210895	the context of
0.0935378670	the number of
0.0935233962	from eeg
0.0935093752	instead of
0.0935012694	\ relative
0.0934289366	collection of
0.0932096190	an accuracy of
0.0931150950	unsupervised acoustic
0.0930900793	a novel framework
0.0930282712	an acoustic
0.0930248444	more general
0.0930077361	evaluation of
0.0929616880	to exploit
0.0928839524	widely used in
0.0928297186	a variational autoencoder
0.0927974894	the model performance
0.0927284648	more effective
0.0925702091	auditory system
0.0925586001	used to build
0.0924505349	for deep speaker
0.0924326218	recorded from
0.0924231964	definition of
0.0923892049	optimized by
0.0922419100	the far field
0.0922233816	further improvement
0.0919159422	for monaural speech
0.0919011106	lies in
0.0918757817	many to many voice
0.0918743213	to localize
0.0918467819	based speech
0.0918080894	art models
0.0917766623	recognition using
0.0917414349	side information
0.0917370906	to mimic
0.0916692439	in contrast
0.0915568883	together with
0.0914400774	to increase
0.0913811337	a novel approach
0.0913119141	a well trained
0.0912790857	submitted to
0.0912296893	a combination of
0.0911564810	the impact of
0.0911477046	unsupervised speaker
0.0911039000	amount of data
0.0910663452	the experiment results
0.0910483374	the number of speakers
0.0910466966	the need for
0.0909546027	on par with
0.0908348841	the development of
0.0907174393	across different
0.0906978271	diversity of
0.0906689750	the effects of
0.0905694149	fraction of
0.0904538021	outputs from
0.0903268698	in particular
0.0903197704	a study on
0.0902736376	methods based on
0.0901875496	combinations of
0.0900309695	proven to
0.0899705834	determined by
0.0898686998	coherence of
0.0896882269	a few
0.0895696025	development of
0.0895614914	query by
0.0895509102	proposed multi
0.0894968363	the performance
0.0893843311	original speech
0.0893600528	other languages
0.0890013282	for acoustic scene
0.0889648059	to listen
0.0889178537	to infer
0.0888776674	the art accuracy
0.0888384996	learning framework for
0.0887575033	the first
0.0886323529	and sum
0.0886254958	run on
0.0884011584	a word error rate
0.0883895986	in word error rate
0.0882604385	reduction over
0.0882593747	features such as
0.0881891603	robust to
0.0880898217	an audio
0.0880658212	employed for
0.0879027438	eer on
0.0878902027	an optimal
0.0876071865	the microphone array
0.0876021486	also outperforms
0.0874925987	the audio signal
0.0874114775	the art baseline
0.0873830586	possibility to
0.0873826083	better results
0.0873376202	need to
0.0872357601	a real world
0.0871718017	real time speech
0.0871178725	or not
0.0870986213	used to predict
0.0870858598	to mitigate
0.0870790944	on par
0.0870731490	the edge
0.0870667143	sound sources in
0.0870621605	the potential of
0.0867538627	in contrast to
0.0866971797	a large number
0.0866579487	relative improvement of
0.0866198533	mixed with
0.0864582182	a latent space
0.0863685935	several different
0.0863491844	transcription system
0.0862341135	other approaches
0.0860894855	to alleviate
0.0859620825	for automatic speaker
0.0858923250	recorded by
0.0858683972	methods such as
0.0858160027	for multi modal
0.0857551464	overview of
0.0857518683	the utterance level
0.0857374777	\ beta
0.0856877414	baselines on
0.0856329775	different modalities
0.0855891206	approaches based on
0.0853867527	error rate of
0.0853218970	improved by
0.0853189262	the output
0.0852744043	an important role in
0.0851962050	novel approach
0.0851805831	baseline by
0.0851349663	generative models for
0.0851231340	the art deep
0.0851224299	named as
0.0849657008	a well known
0.0848386025	a non autoregressive
0.0848103792	the standard
0.0847990556	competitive with
0.0846629663	most important
0.0846611704	able to improve
0.0844385256	performed on
0.0844378976	based speaker verification
0.0844284136	the aim of
0.0844081336	the computational cost
0.0843895031	based approach for
0.0843630114	voice conversion with
0.0843318666	the proposed method significantly
0.0843229861	music source
0.0843183646	associated with
0.0841750986	song dataset
0.0840900723	two types of
0.0837877336	the same time
0.0837522308	applicability of
0.0836407484	a wide variety of
0.0835744818	the generated
0.0835743861	adopted as
0.0833582654	attributed to
0.0832301255	for robust
0.0832037544	the current state
0.0831978811	this work presents
0.0831910824	in comparison to
0.0831900094	each task
0.0829577993	at different
0.0829013350	listen to
0.0828901718	each individual
0.0828707337	tested with
0.0828458541	model with
0.0827321063	a number of
0.0826754186	audio only
0.0826049358	musical audio
0.0825038150	the baseline system
0.0825014576	elements of
0.0824771195	better quality
0.0824549379	the most common
0.0824151988	three datasets
0.0823927691	performance on various
0.0823885446	the art techniques
0.0823536318	a state of
0.0822322658	samples from
0.0822166469	a single channel
0.0821491930	fail to
0.0820736831	benefits from
0.0820048127	to maximize
0.0818728486	the word error rate
0.0818543308	various types of
0.0818368990	supervised speech
0.0818368990	speech conversion
0.0817991011	the whole
0.0817838233	representations from
0.0817103235	made available
0.0816752616	models for automatic
0.0816024286	attention mechanism in
0.0815893644	a survey
0.0815259139	tasks show
0.0814829821	the aforementioned
0.0813420723	the experimental results
0.0812067810	both modalities
0.0811856052	frequency representation of
0.0810177951	created by
0.0809960010	a single network
0.0809655903	emphasis on
0.0809385766	techniques based on
0.0809007966	a machine learning
0.0808822651	processed by
0.0808632818	a large number of
0.0807945077	the input data
0.0806873159	between speakers
0.0805624119	as much as
0.0804073866	further show
0.0803871050	various tasks
0.0802777732	words from
0.0802605921	compared to other
0.0802289468	as opposed to
0.0801060156	further propose
0.0800614067	addressed by
0.0800425185	the last
0.0798872107	same language
0.0798342133	a deep convolutional neural
0.0798300540	a detailed
0.0797602295	ways of
0.0796846354	obtained using
0.0796487961	than conventional
0.0796166246	a new algorithm
0.0796108664	a limited number of
0.0795362700	an entire
0.0795313762	output speech
0.0795139954	a frame level
0.0795099632	par with
0.0793431950	consists of two
0.0791923100	used in
0.0791712941	for predicting
0.0791676624	each component
0.0790647000	useful for
0.0790376962	vc system
0.0789964500	two datasets
0.0789778955	than real time on
0.0789337944	wer on
0.0787416272	group of
0.0787083586	in conjunction with
0.0786415132	an output
0.0784671076	theory of
0.0782437863	addressed in
0.0781777666	in fact
0.0781023822	an active
0.0781023062	by comparing
0.0780749315	evaluated by
0.0779840407	for far field
0.0779787156	possibility of
0.0778981714	other state of
0.0778613561	information between
0.0778130883	novel deep learning
0.0778052616	original audio
0.0777880050	in practice
0.0777836924	the long term
0.0777757741	the influence of
0.0776398080	obtained with
0.0775421087	the latent representation
0.0774520592	music using
0.0774176768	than other
0.0773301407	to encode
0.0773284235	respect to
0.0772593461	models for
0.0772405051	systems without
0.0771860220	the above
0.0770389711	computed from
0.0769383626	attention mechanism for
0.0768956329	at hand
0.0768789829	an audio visual
0.0767234638	degree of
0.0766896797	assessment of
0.0765751171	the voxceleb speaker
0.0764163525	audio segments
0.0764075211	sampling from
0.0763767054	an end to end model
0.0763435628	geometry of
0.0763428457	occur in
0.0763225276	impact of
0.0763193080	the learned representations
0.0761844564	the short time fourier
0.0760632732	the basis of
0.0760008275	fails to
0.0759723700	while achieving
0.0759327292	a hierarchical
0.0759244845	attention mechanism to
0.0758536159	improved speech
0.0758291845	estimated by
0.0757298919	accuracy of
0.0756639109	by more than
0.0756520281	allowing for
0.0756395434	detection based on
0.0754997832	to integrate
0.0754190013	at interspeech
0.0754108322	without relying on
0.0753964818	end to end deep
0.0753285455	systems use
0.0753254777	speech datasets
0.0753092666	locations of
0.0751538609	learning for audio
0.0750484487	in adverse
0.0750410998	to synthesize
0.0748964818	neural end to end
0.0748652454	for speech emotion
0.0746091841	the input text
0.0745147257	to preserve
0.0744265501	two major
0.0743907027	attempted to
0.0743833975	more natural
0.0743519918	and speaker similarity
0.0742548027	all three
0.0742288377	per speaker
0.0741636627	generated using
0.0740965800	the speaker characteristics
0.0740480029	the experimental results demonstrate
0.0739989014	the source code
0.0739617867	single system
0.0738688342	also allows
0.0736925174	the input speech
0.0736604044	this way
0.0735468227	the trade off between
0.0734699474	the test set
0.0734573312	most effective
0.0733822793	an encoder
0.0733207823	a relative improvement of
0.0728516899	much better
0.0728025275	a dual
0.0726793667	aspect of
0.0725235418	optimized for
0.0724738430	by adopting
0.0724057318	cope with
0.0721911587	the role of
0.0721371567	in domestic
0.0721202979	adapt to
0.0720280201	text to
0.0719953333	a large amount of
0.0719621414	neural networks for
0.0719315556	to emphasize
0.0719232968	then uses
0.0718976956	the possibility
0.0718318994	models based on
0.0718304019	the success of
0.0717283176	most state of
0.0717255224	different aspects of
0.0716805800	efficacy of
0.0716618099	by utilizing
0.0716137943	the generated speech
0.0715740525	for emotion recognition
0.0715728131	degrees of
0.0715599286	sounds from
0.0715579511	estimated from
0.0715477810	a trade off
0.0714722857	to support
0.0714598447	new state of
0.0713736583	novel framework
0.0713142994	an appropriate
0.0713110677	the equal error
0.0712970920	the evaluation set
0.0711865283	error rate on
0.0711340340	methodology for
0.0711058507	augmented with
0.0710854961	proposed system
0.0710516063	neural networks for speech
0.0710316908	a high quality
0.0710144508	a result
0.0710120937	learns from
0.0709068574	label classification
0.0708565885	this area
0.0708326921	to deploy
0.0707152634	review of
0.0707080634	the ability to
0.0706968252	a sound event detection
0.0705642924	word error rate of
0.0704215381	presence of
0.0704032203	this paper focuses on
0.0703550286	the conventional methods
0.0702276783	different layers
0.0702183269	this work aims
0.0701892180	robustness to
0.0701747968	designed for
0.0700576941	an intuitive
0.0699389245	two separate
0.0699186396	to reach
0.0698340599	in addition to
0.0698205884	two aspects
0.0695775300	a lot of
0.0695581871	features into
0.0694437904	interest in
0.0693268703	defined as
0.0693089370	a hybrid
0.0692812420	the input features
0.0692518920	better understanding of
0.0692504787	the first time
0.0691916977	experiments on two
0.0691839688	hundreds of
0.0691796258	a consequence
0.0691691853	heavily on
0.0691074403	conditioning on
0.0690305590	adversarial training for
0.0690248673	at inference time
0.0689875664	a novel method
0.0687701996	on two datasets
0.0686161377	deep neural networks for
0.0686077664	to implement
0.0685822972	research on
0.0685627545	both visual
0.0685265830	description of
0.0684467743	while still
0.0684250997	the presence or absence
0.0684202802	build on
0.0683326094	adversarial audio
0.0683247480	the low frequency
0.0682127762	to align
0.0681586815	in terms
0.0680986204	implemented by
0.0680043563	utilization of
0.0679211439	then applied
0.0678358026	most common
0.0678130386	extracted using
0.0678022297	not require
0.0677584333	the most
0.0677218961	performance in terms of
0.0677039196	the literature
0.0676926033	several state of
0.0676503389	by incorporating
0.0676221469	the notion of
0.0675436958	by using
0.0674559242	dependent on
0.0672528219	dictionary of
0.0671833171	in combination with
0.0671749361	recorded in
0.0670897777	methods in terms of
0.0670009925	under various
0.0669912643	generation using
0.0669385053	\ times
0.0669283547	to construct
0.0669006952	concept of
0.0667553430	advances in
0.0666376551	regardless of
0.0665503600	the art speech recognition
0.0665191314	to establish
0.0665069495	to assist
0.0665025388	then used
0.0664851690	a model trained
0.0663934211	defined by
0.0663659946	the art deep learning
0.0662620128	novel deep
0.0661970830	the direction of arrival
0.0661562738	even better
0.0661471958	absence of
0.0661366880	to collect
0.0661120427	toolkit for
0.0660718064	neural network based speech
0.0659074322	intended to
0.0658358721	first work
0.0657578293	a speech signal
0.0657204723	2017 challenge
0.0656320284	to apply
0.0655956515	an approach
0.0655591217	a factor of
0.0655058358	helps to
0.0654628915	model to
0.0654575962	introduced by
0.0654550028	the word error
0.0652152500	change in
0.0652055356	this report
0.0651771118	improvement of
0.0651665876	2018 challenge
0.0651419501	predicted by
0.0651142339	sound event detection in
0.0650344961	end training
0.0650344961	sequence speech recognition
0.0649512180	implemented on
0.0649497432	the number of sources
0.0649351336	this goal
0.0649146972	the front end
0.0648195205	to get
0.0647651018	to use
0.0647360625	a general
0.0647352388	studies on
0.0645762562	a simple yet
0.0645648656	a self supervised
0.0644638248	as good
0.0644447255	to incorporate
0.0643477956	from raw
0.0642913882	audio frames
0.0642474656	different tasks
0.0642427039	the training of
0.0642419830	signals from
0.0642315928	the difference between
0.0642016210	an external
0.0641502926	a target speaker
0.0640038150	the previous state of
0.0639026434	the x vector
0.0638396680	suited for
0.0637558358	consistent with
0.0636916813	an attention
0.0636309062	method for speech
0.0635659572	convolutional neural networks for
0.0635511089	weights for
0.0634133776	a song
0.0633894913	an initial
0.0633165721	trained from
0.0632845903	a review of
0.0632416647	a popular
0.0631818021	both approaches
0.0630866091	place of
0.0630702354	paradigm for
0.0630701363	in music information
0.0630618861	an unseen
0.0630235625	each sound
0.0629397544	result in
0.0628852945	by analyzing
0.0628243708	also introduce
0.0627644978	in response to
0.0627443572	the degree of
0.0627394601	fidelity audio
0.0625867926	training data from
0.0625091852	the overall
0.0624959976	for model training
0.0624648206	proposed end to end
0.0624236363	boundaries of
0.0621823357	2018 task
0.0621182797	an existing
0.0620704135	at inference
0.0619872550	an interesting
0.0619760375	four different
0.0619627345	information at
0.0619290029	system uses
0.0619023237	many different
0.0618953505	diagnosis of
0.0617839015	an autoregressive
0.0617552326	contribution of
0.0617404367	the art results for
0.0617260363	different levels of
0.0617128653	operates on
0.0616921295	under different
0.0616898391	best system
0.0616678977	family of
0.0616190803	the objective function
0.0616138379	performance in
0.0615663837	lack of
0.0615338563	the latter
0.0614842545	evaluations on
0.0614647465	the feature extraction
0.0614419390	an enhanced
0.0614148506	or absence of
0.0614091982	a loss function
0.0613595323	and slot
0.0612885083	not available
0.0612786839	variants of
0.0612088281	with varying
0.0611990003	present study
0.0611935539	the possibility of
0.0611272003	many other
0.0611095323	to speak
0.0610111065	sources from
0.0609599723	learning task
0.0609385053	many cases
0.0608712738	an automatic speech recognition
0.0607845706	come from
0.0607463138	encoded in
0.0605897845	area of
0.0605280857	models for speech
0.0604924450	not seen
0.0604329803	most relevant
0.0604080469	two tasks
0.0602707079	people with
0.0601667903	detection and classification of
0.0599759344	a primary
0.0599249539	added to
0.0598960077	minutes of
0.0598667773	music audio
0.0598481370	metric learning for
0.0597564169	novel technique
0.0597563134	conversion system
0.0597274914	occurrence of
0.0597092753	effect on
0.0597017347	in comparison with
0.0597007724	search for
0.0596177822	tens of
0.0595898859	the same speaker
0.0595830192	as inputs
0.0595042102	even more
0.0594924156	system consists of
0.0594281855	the learned
0.0594158182	encountered in
0.0594012180	gap by
0.0592471492	separation system
0.0592302684	to compress
0.0591674572	a typical
0.0591661790	used to learn
0.0590648753	two key
0.0590462349	gap in
0.0590250951	but not
0.0589686755	contains only
0.0589232350	a special
0.0588342160	for music generation
0.0588221332	to investigate
0.0588046889	also includes
0.0586485833	scenario with
0.0585853345	collections of
0.0585745063	a new approach to
0.0585179130	three types of
0.0585032758	measurement of
0.0584419390	also improves
0.0584413805	approach by
0.0584164094	stages of
0.0583986821	an asr system
0.0583432157	comes from
0.0583419650	recorded with
0.0582924374	models without
0.0582875472	available datasets
0.0582392405	automatic recognition
0.0581962906	a total of
0.0581552691	different methods
0.0581523975	to generalize to
0.0581474941	from raw audio
0.0581408938	the current state of
0.0581237306	novel end to end
0.0580427375	reduction on
0.0580424599	utilized in
0.0579841570	proposed network
0.0578149293	advantage of
0.0577288983	the air
0.0576013234	an ensemble of
0.0574374255	variation in
0.0574329803	better understanding
0.0574164094	topic of
0.0574094087	and then
0.0573941096	portion of
0.0573363355	an autoencoder
0.0572567015	five different
0.0571992347	this technique
0.0571342678	this strategy
0.0571311261	quality speech
0.0570762051	the three tasks
0.0570520911	the well known
0.0570412691	or even
0.0570199173	the former
0.0569370446	success in
0.0568538804	an objective
0.0568514147	seen during
0.0568489585	the art performance in
0.0568455234	the accuracy of
0.0568278445	acoustic models for
0.0568276598	influence of
0.0567999128	the absence of
0.0567966649	for end to end asr
0.0567897124	a mixture of
0.0567721350	an analysis of
0.0567597058	propose to
0.0567522736	acoustic model for
0.0567378957	architecture for
0.0566638699	the baseline systems
0.0565026385	the back end
0.0564064056	in many applications
0.0563999418	the art end to end
0.0563372124	a generalized
0.0563197255	a crucial
0.0562431326	a robust
0.0561856940	utility of
0.0561049219	data augmentation for
0.0560199275	important for
0.0560176519	to follow
0.0559917268	most popular
0.0559900745	the separation performance
0.0559852597	the converted
0.0559667903	networks for audio
0.0559660225	by replacing
0.0559389643	solely on
0.0559338645	acoustic word
0.0558877992	for audio event
0.0558291854	also explore
0.0557889165	source separation with
0.0557046698	task learning
0.0556998721	both objective
0.0556930997	applied on
0.0556657385	dataset of
0.0556463268	contributes to
0.0556247240	differences in
0.0556170271	aiming to
0.0555561863	directly on
0.0554821723	role of
0.0554697263	extension of
0.0554608071	the distance between
0.0554580634	a method for
0.0553961801	by integrating
0.0553635357	a unique
0.0553382557	to attend
0.0552308177	fields of
0.0551747050	to speech
0.0551650765	pairs of
0.0551455421	generalize to
0.0551008349	to match
0.0550642372	content from
0.0550309265	structure of
0.0549635083	based approach to
0.0548204338	required for
0.0547936511	the number of parameters
0.0546667438	aligned with
0.0545488656	system achieved
0.0545032758	seconds of
0.0544879440	novel architecture
0.0544844459	a multi label
0.0544247136	an adaptation
0.0544191111	a joint
0.0543961214	time domain speech
0.0543750839	adopted in
0.0542833442	a target
0.0541110239	a pair of
0.0540483574	a speaker embedding
0.0540254769	evolution of
0.0540105281	needs to
0.0539931326	a high
0.0539168036	limitation of
0.0539051071	the enhanced speech
0.0538680854	an estimate of
0.0537697418	the receiver
0.0537032758	details of
0.0536641613	an increase
0.0536335988	idea of
0.0536135555	a mixture
0.0535892090	by performing
0.0535530501	conventional speech
0.0534954820	methods for
0.0534823431	different architectures
0.0534772307	the teacher
0.0534348536	gains in
0.0533880672	tools for
0.0533152753	level speech
0.0532557756	networks for
0.0532512582	approximation of
0.0531967160	the target speech
0.0531291201	the asvspoof 2019
0.0531138285	supervised learning for
0.0529821281	unsupervised learning of
0.0529452680	a comparison of
0.0528463142	a reference
0.0528236458	self attention for
0.0527570060	at scale
0.0526281974	in depth
0.0525674572	a major
0.0525291229	do so
0.0524137909	an absolute
0.0524137909	by humans
0.0523595323	of western
0.0523229581	an alternative to
0.0521574254	to cope with
0.0520969836	form of
0.0520547501	from youtube
0.0520518697	the predicted
0.0520333482	employed to
0.0520063593	generalizes to
0.0520054169	an issue
0.0519112301	done by
0.0519055844	the proposed loss
0.0518858411	a range of
0.0518827375	available data
0.0518629488	of acoustic scenes and events
0.0518539008	novel method
0.0518506692	by explicitly
0.0518330531	the training
0.0517981807	trained language
0.0517603620	deployed in
0.0517548954	choice for
0.0517280588	for detecting
0.0517260704	usefulness of
0.0516733172	the model parameters
0.0516344072	contributions of
0.0515966449	an accurate
0.0515657344	the applicability of
0.0515286218	utilized to
0.0515265214	operate in
0.0514548275	results compared to
0.0514307373	a new approach
0.0513305301	referred to
0.0513206378	validated on
0.0513167360	introduction of
0.0512218690	to utilize
0.0511873276	by optimizing
0.0511758870	progress in
0.0511147109	new architecture
0.0510509897	more suitable for
0.0509451804	a static
0.0508992483	the rise
0.0508692627	series of
0.0508496657	the art results in
0.0506994602	the trade off
0.0506040983	performance of speaker
0.0505859067	demand for
0.0505706434	to carry
0.0504989564	input audio
0.0504833857	to form
0.0504741006	activities of
0.0504363922	a powerful
0.0504103606	system without
0.0503891124	a sequence to sequence
0.0502613804	needed to
0.0502496983	constraints on
0.0502396159	task of
0.0502248198	the corresponding
0.0501213519	exploration of
0.0501185094	capability to
0.0500463506	the art speaker
0.0500431881	requirement for
0.0500289365	method to
0.0500223201	for generating
0.0500132821	locations in
0.0500114027	operating on
0.0499597262	reduced by
0.0498507528	algorithms for
0.0498388998	a neural
0.0498259568	this gap
0.0498240886	the idea of
0.0498240886	the area of
0.0498197255	to distinguish
0.0498160130	a time domain
0.0497962349	implemented in
0.0497605833	an experiment
0.0497562687	principles of
0.0496994085	combination with
0.0496749316	number of training
0.0496242434	then used to
0.0495956845	learning architecture
0.0495937908	a time frequency
0.0495291431	a good
0.0495248053	dependencies in
0.0495131782	for instance
0.0494905565	library for
0.0494326317	a straightforward
0.0493772008	the second
0.0493417762	investigation of
0.0493417762	capability of
0.0492601635	superiority of
0.0492489480	in isolation
0.0492270273	each time
0.0491333116	by providing
0.0490585518	over state of
0.0490544658	for audio source
0.0489538150	the superiority of
0.0489201984	application to
0.0489100955	out of
0.0488862215	by generating
0.0488725210	compensate for
0.0486922277	new method
0.0486794658	the amount of training
0.0486693220	a small amount of
0.0486343186	on timit
0.0485508279	after training
0.0485305644	also found
0.0484995637	cer of
0.0484683093	point of
0.0484585233	core of
0.0484314450	to generalize
0.0483738922	an overview of
0.0483093705	the benefits of
0.0482825732	the other
0.0482092356	assumed to
0.0481191855	discussed in
0.0480960602	study on
0.0480406086	estimation using
0.0480278712	made by
0.0480269074	detection of
0.0479794178	a single neural
0.0479125543	tends to
0.0478898713	the task
0.0478857210	to allow
0.0478448623	extended to
0.0477892913	component of
0.0477698101	probabilities of
0.0477044671	end to end audio
0.0476829255	for developing
0.0476665846	sequence to
0.0476617482	the shape of
0.0476602587	as part of
0.0476301364	developments in
0.0476237238	contrast to
0.0475961816	to validate
0.0475575588	numbers of
0.0475327134	sense of
0.0475247872	pair of
0.0474911424	a new method for
0.0474818878	an extension of
0.0474675611	a novel multi
0.0474654363	a continuous
0.0474630401	basis for
0.0474023388	question of
0.0473094418	signal to
0.0472954888	more suitable
0.0472845903	the existence of
0.0472263373	the art on
0.0471894744	sets of
0.0471394817	same speaker
0.0470993779	the proposed multi
0.0470093705	a subset of
0.0470093705	the validity of
0.0469790629	to discriminate
0.0469351785	a transfer learning
0.0469214850	a large amount
0.0468742741	expected to
0.0468685230	the perceptual quality
0.0468341097	models such as
0.0466289786	increase in
0.0465827452	interpretability of
0.0465755616	strategy for
0.0465539382	the two tasks
0.0465472091	a newly
0.0465259681	a user
0.0464727081	aimed to
0.0464444418	different levels
0.0464120927	dedicated to
0.0463941163	to examine
0.0463166757	different types
0.0462647751	orders of
0.0462473397	developed for
0.0461386097	by taking
0.0460327134	generalizability of
0.0459637479	run in
0.0458139665	to compensate for
0.0457703477	superior to
0.0457571778	an algorithm
0.0457562687	flexibility of
0.0457541126	to help
0.0457284112	audio recording
0.0456341613	by reducing
0.0456129515	trained by
0.0456087662	the usefulness of
0.0455713346	a novel deep
0.0455356660	the art method
0.0455306105	various types
0.0455195194	reported in
0.0454763834	a novel method for
0.0454742741	crucial for
0.0454164277	first time
0.0453682517	location of
0.0453549954	dependency of
0.0453103220	network architecture for
0.0453093705	a new state of
0.0452821966	the time frequency
0.0449114465	input to
0.0448994086	interpretation of
0.0448881779	an improvement of
0.0448195043	view of
0.0447928914	by conditioning
0.0447448373	to update
0.0447306820	the utility of
0.0446367323	to boost
0.0446354451	reduction of
0.0445756994	converted to
0.0445687025	by estimating
0.0445673068	a lightweight
0.0445503938	different from
0.0445039352	validity of
0.0444635352	advantages of
0.0443426947	order to
0.0443212542	channel audio
0.0443203737	an emotional
0.0441644989	restricted to
0.0441224416	deployment of
0.0441048760	to fine
0.0440750056	application of
0.0439920352	an integrated
0.0439435538	a method
0.0439360992	an approach to
0.0439039068	comparing with
0.0438454374	system for
0.0437770990	to reproduce
0.0436836494	attempts to
0.0435468332	property of
0.0435093705	the feasibility of
0.0434993645	reduction in
0.0434863922	to derive
0.0434538150	a database of
0.0434008349	to verify
0.0433952391	same time
0.0433594185	improvements in
0.0433550168	proved to
0.0433203737	an e2e
0.0433149638	variant of
0.0433105196	mixtures with
0.0432784664	an f1
0.0431700201	to speech synthesis
0.0430311584	choice of
0.0429771476	needed for
0.0429338363	a recurrent neural
0.0429333142	new dataset
0.0428803169	comparison of
0.0427599930	for audio source separation
0.0427183064	to quantify
0.0426887987	an english
0.0424694438	a variant of
0.0424159490	characteristic of
0.0424116013	capacity of
0.0424019799	to remove
0.0423803306	the presence or
0.0422475499	extractor for
0.0421939759	to compensate
0.0421110239	the difference in
0.0420924957	problem of
0.0420712218	a word error
0.0420689733	a broad
0.0420524755	use of
0.0419762550	direction of
0.0419738899	involved in
0.0419724400	two novel
0.0418889697	integration of
0.0418566354	for improving
0.0418514616	decoding with
0.0418276159	trained to
0.0417884808	equal to
0.0417665320	decomposition of
0.0417561867	novel method to
0.0417243179	pipeline for
0.0416983589	a machine
0.0416511155	the need of
0.0416212418	a person
0.0415999687	work aims
0.0415625847	categories of
0.0415593705	the ability of
0.0415231222	speaker voice
0.0414818486	algorithm for
0.0414396279	a multi speaker
0.0413978404	by up to
0.0413568332	critical for
0.0413567698	the actual
0.0413454269	the potential
0.0412986739	estimation of
0.0412673023	existence of
0.0412271197	two types
0.0411879049	system based on
0.0411236840	advancements in
0.0410577866	step for
0.0410017890	account for
0.0409944135	end to end multi
0.0408841420	reliability of
0.0408738061	an approach for
0.0408634365	different acoustic
0.0408317719	a generic
0.0407726299	the latent
0.0407175328	to explore
0.0406779952	to run
0.0405231222	based feature
0.0404974743	solution for
0.0404137909	an explicit
0.0404074617	the art speech
0.0404032506	hard to
0.0403701284	and thus
0.0403099519	contribution to
0.0402774399	separation using
0.0402254169	this project
0.0402059829	required to
0.0401876513	conducted with
0.0399870404	an individual
0.0399388090	score of
0.0399271221	importance of
0.0399219182	the creation of
0.0399202971	synthesis using
0.0398698869	a novel approach for
0.0397941163	to approximate
0.0397207026	work presents
0.0397095963	to control
0.0396739221	popularity of
0.0396738640	difficulty of
0.0396386432	benefit of
0.0396228021	issue in
0.0395989029	strategies for
0.0395969358	a clear
0.0395093705	the choice of
0.0394638678	take into
0.0394230963	a modified
0.0394198562	preservation of
0.0394074617	the art model
0.0393748488	different aspects
0.0392841130	a new speaker
0.0392614745	perception of
0.0391457390	variation of
0.0391211325	a self attention
0.0391203120	room for
0.0390997176	the amount of
0.0390101275	the past
0.0389834459	helpful for
0.0389404367	a technique for
0.0389074617	an end to end speech
0.0388699649	a novel method to
0.0388427039	a dataset of
0.0387527002	stack of
0.0386863545	the core
0.0386092669	behavior of
0.0385118625	a closed
0.0385033320	information in
0.0384923049	a considerable
0.0384724020	two modalities
0.0384538150	the difficulty of
0.0384395743	techniques for
0.0382574654	comparing to
0.0382070845	represented in
0.0381916652	variability in
0.0381683641	to highlight
0.0381532614	employed in
0.0379839319	an automatic speech
0.0379761126	capabilities of
0.0379225265	shape of
0.0379219358	a substantial
0.0379013865	to account for
0.0378034032	studied in
0.0376761213	creation of
0.0374828969	to yield
0.0374825355	from unlabeled
0.0374410511	learning approach to
0.0373600214	the task of speech
0.0373494786	to transcribe
0.0373173068	the learnt
0.0372512912	the same model
0.0371952680	the contribution of
0.0371334030	the commonly used
0.0369955439	forms of
0.0368924017	effective in
0.0368791720	a convolutional neural
0.0368786601	to encourage
0.0368752824	equivalent to
0.0368673532	probability of
0.0368653531	the base
0.0368256721	modification of
0.0368176969	for evaluating
0.0367768871	a dataset
0.0366675667	by maximizing
0.0366403647	the application of
0.0366312309	included in
0.0365594564	the superiority
0.0365529021	an example
0.0365120516	the art in
0.0363862674	investigation on
0.0363545918	without using
0.0363213426	understanding of
0.0362487501	observed in
0.0362455650	a quantitative
0.0360956028	mixture of
0.0360926727	means to
0.0360429915	the first work
0.0360289645	functions for
0.0359618356	adopted to
0.0359272045	value of
0.0358841420	construction of
0.0358793909	a stacked
0.0358743179	baselines for
0.0357385849	the introduction of
0.0356248488	most cases
0.0355897235	with respect
0.0355606243	as much
0.0355055244	sum of
0.0354691038	the subsequent
0.0354249294	self attention to
0.0353961761	especially in
0.0352400120	the receptive
0.0352372342	to intent
0.0352319623	the perceived
0.0351169374	the potential to
0.0350935009	effect in
0.0350486458	the correlation between
0.0350157745	provided for
0.0349900699	position of
0.0349321955	the best performing
0.0348862111	usage of
0.0348854694	for creating
0.0348627133	collected in
0.0348158688	from background
0.0347315928	a method of
0.0346774395	solution to
0.0346753607	essential for
0.0346671946	work investigates
0.0346562633	an extension
0.0345976456	a group of
0.0345594564	to add
0.0345454294	the dcase 2019
0.0344394623	to aid
0.0344360469	system using
0.0344317410	likelihood of
0.0343381160	the detection of
0.0343227032	list of
0.0342692057	the strength of
0.0342547431	distribution of
0.0342323381	spectrogram as
0.0341600622	the official
0.0341535195	trends in
0.0341009865	limitations of
0.0340441404	of thousands of
0.0335336040	a collection of
0.0334880139	to extend
0.0334651905	spectra of
0.0334074200	an overall
0.0333192067	presented in
0.0333141358	technique for
0.0332162909	essential to
0.0331952680	the concept of
0.0331158308	phonemes in
0.0330761624	a black
0.0330683093	evaluated with
0.0330664066	introduced to
0.0330384145	method does
0.0330232661	try to
0.0329910322	dataset for
0.0329440528	to simulate
0.0329328395	the problem
0.0327313864	degradation in
0.0327275769	alternative to
0.0326023266	aid in
0.0325112464	for depression
0.0324669859	effective than
0.0324415961	both objective and
0.0324056731	to recover
0.0322998853	a flexible
0.0322237269	research in
0.0321806485	monitoring of
0.0321574584	conversion using
0.0321431401	the first one
0.0320876716	learns to
0.0320325605	the distribution of
0.0319262368	trained using
0.0319041597	the sequence to
0.0319015635	enough to
0.0318819703	an analysis
0.0318433486	the presence
0.0318119859	necessary to
0.0317419100	a pretrained
0.0316559596	difficult for
0.0316446383	the art audio
0.0315431751	amount of training
0.0315360513	network for
0.0315008244	to measure
0.0314963580	to state of
0.0313913626	the input audio
0.0313912746	versions of
0.0313901632	the detection and classification of
0.0313096879	beneficial for
0.0313032215	to disentangle
0.0312627537	eer of
0.0312038803	different time
0.0311282318	voices with
0.0311127133	metric for
0.0310903125	the model on
0.0310903125	the model to
0.0310246125	a novel approach to
0.0310196059	with minimal
0.0309723087	the internet
0.0308728287	potential for
0.0308478732	the gap between
0.0308398853	a compact
0.0308381818	way to
0.0308034492	an ablation
0.0307282408	topic in
0.0307100602	issues in
0.0306388983	to gain
0.0305343181	to fuse
0.0304930902	the nature of
0.0304790690	new training
0.0302676542	a text to
0.0301855164	to resolve
0.0301796107	with skip
0.0300740686	an unknown
0.0299702528	a statistical
0.0299675789	quality of
0.0297706428	need for
0.0297632853	to augment
0.0296325958	a siamese
0.0295605196	benchmark for
0.0295486458	the integration of
0.0295239593	a variant
0.0294552565	explored in
0.0294459838	the best
0.0294400757	achieved with
0.0294200290	introduced in
0.0294187490	a semi
0.0294098485	a system for
0.0293930924	the same data
0.0293548268	measure for
0.0292941404	the reliability of
0.0292412742	to bring
0.0292377964	robustness of
0.0292198099	a result of
0.0291090650	degradation of
0.0291017932	similarly to
0.0290939687	a variety
0.0290482594	the form of
0.0290342463	the analysis of
0.0290297409	the naturalness of
0.0289615633	size of
0.0289519630	make use
0.0288991509	goal of
0.0288919804	top of
0.0288718462	the size of
0.0288703106	to design
0.0288409239	by considering
0.0288041040	to fool
0.0287522811	success of
0.0286709803	crucial to
0.0286487529	labels from
0.0286419905	the dominant
0.0285585156	a representative
0.0285464628	power of
0.0285220435	to replace
0.0285220435	to characterize
0.0284877540	done in
0.0284406658	the number
0.0283726765	ability of
0.0282761056	far from
0.0282741937	a rich
0.0282300602	tested in
0.0281164460	seen as
0.0280903125	the input of
0.0280054570	obtained on
0.0277774992	addition to
0.0277593705	the benefit of
0.0275621413	to discover
0.0275584722	strategy to
0.0275522839	a low
0.0275444945	for measuring
0.0274926317	a custom
0.0273877244	to bridge
0.0273668445	for estimating
0.0273285871	used by
0.0272435110	a great
0.0271019460	effect of
0.0270902292	new approach
0.0270288475	classification based on
0.0269613923	fusion of
0.0269296107	this type
0.0267996059	to select
0.0266902400	framework with
0.0265422241	evaluated for
0.0264930902	the availability of
0.0264703476	to keep
0.0263834383	embeddings for
0.0263593095	for extracting
0.0263439700	dynamic time
0.0263258137	a big
0.0261000497	to find
0.0260888945	the system's
0.0260472011	off between
0.0260336040	a fusion of
0.0259511445	a certain
0.0259482097	the short time
0.0258307654	to translate
0.0257340503	changes in
0.0257184038	a cycle
0.0256278121	fusion with
0.0255621413	a theoretical
0.0255545526	allows for
0.0254930902	the raw audio
0.0254780485	the diversity of
0.0254729315	this problem by
0.0254552565	scenarios with
0.0254135255	result on
0.0253844539	a probabilistic
0.0253369055	of things
0.0252593705	the majority of
0.0252154136	effects of
0.0251549923	potential of
0.0249927355	means of
0.0249911562	the respective
0.0249876947	effective for
0.0249623013	to suppress
0.0249109130	the combination of
0.0248958839	the mismatch between
0.0248359090	trained for
0.0248201211	a method to
0.0248181411	architectures for
0.0248069990	considered in
0.0247710781	augmentation with
0.0247494740	the interaural
0.0247424750	the impact
0.0245646576	the effectiveness
0.0245546107	2019 task
0.0245081709	new method for
0.0244930902	the estimation of
0.0244766131	in capturing
0.0244703476	want to
0.0244389938	database of
0.0244341733	architecture with
0.0244275990	to illustrate
0.0243312247	a proxy
0.0242439900	a 2d
0.0242227591	to prevent
0.0241785805	the feasibility
0.0241604475	to give
0.0241575605	the advantages of
0.0241430668	applied for
0.0240480305	the synthesized
0.0240431751	the use of deep
0.0238237096	ensemble of
0.0237318231	a systematic
0.0236317321	to text
0.0235602072	to interference
0.0235486458	and tested on
0.0235348485	the first to
0.0235208680	a type of
0.0235187908	a generative
0.0234494740	in turn
0.0234389938	field of
0.0233831352	a framework for
0.0233735953	a reliable
0.0233618049	code for
0.0233504541	supervision for
0.0233147833	the performance of speaker
0.0232632853	to maintain
0.0232599080	for building
0.0232451999	to take
0.0232129824	evaluated in
0.0231859085	benefits of
0.0231839387	to calculate
0.0231839387	to answer
0.0230904136	case of
0.0230594564	a piece
0.0230501939	new approach to
0.0230061153	efficiency of
0.0229705058	models do
0.0228979910	the goal
0.0228678729	module to
0.0227835464	the usage of
0.0227475877	to focus on
0.0227178862	tries to
0.0226418373	scheme for
0.0224780485	a lack of
0.0224024914	a universal
0.0222732503	naturalness of
0.0222058857	a challenging
0.0220486458	the power of
0.0220040119	work on
0.0218377992	the ground
0.0218044728	especially for
0.0217756692	a simplified
0.0215743463	the development
0.0215405299	a sequence
0.0215087334	autoencoders for
0.0214567905	an auto
0.0212769120	with up to
0.0211951253	performed in
0.0210810846	to learn from
0.0209473928	precision of
0.0208684712	the direction of
0.0208201211	the fusion of
0.0207587334	strength of
0.0205071238	of up to
0.0204808680	the complexity of
0.0204274017	the behavior of
0.0204266880	the degree
0.0203449105	in detail
0.0200104665	an average of
0.0200031120	measure of
0.0199104475	allow for
0.0198377992	to focus
0.0197184038	a connectionist
0.0196524914	a public
0.0196442890	appropriate for
0.0195449000	a naive
0.0195335223	generation by
0.0194930902	the structure of
0.0194078576	an improvement
0.0192933547	to clean
0.0192808724	sufficient to
0.0192385849	the probability of
0.0192130512	a dictionary
0.0191789681	found to
0.0191527936	issue by
0.0191257093	a baseline system
0.0190150654	a regression
0.0190134301	popular in
0.0189877992	to sequence
0.0189838093	the input to
0.0189631160	the advantage of
0.0188711325	the importance
0.0188588093	the assessment of
0.0188165867	vector system
0.0187819511	problem by
0.0186992211	framework by
0.0186101159	factor of
0.0185187908	a real
0.0185094539	a weighted
0.0182675617	design of
0.0182073595	to noise
0.0181614236	the efficiency of
0.0180976103	a way to
0.0180565588	complexity of
0.0179987110	a gated
0.0178625265	distillation for
0.0178372342	a group
0.0176413277	a non
0.0175665706	of research in
0.0175594564	the flexibility
0.0174421665	useful in
0.0168365859	to outperform
0.0167894847	function based on
0.0167253776	the user's
0.0165433522	a realistic
0.0164505812	demonstrated to
0.0164049975	aim of
0.0163909530	optimized to
0.0163398764	a sequence to
0.0159275541	a tool
0.0158541132	likely to
0.0158528403	a form of
0.0157895597	recognition via
0.0153842989	piece of
0.0153398764	the signal to
0.0153372342	the efficacy
0.0153270223	developed in
0.0152887377	this technical
0.0152739593	the applicability
0.0151565689	seen in
0.0151447922	procedure to
0.0150021665	to describe
0.0148861148	enhancement based on
0.0148240650	expensive to
0.0147203336	a subset
0.0146594125	the need to
0.0144924750	a multilingual
0.0144114236	the limitations of
0.0142188351	mechanism for
0.0140296761	and hence
0.0140068981	mechanism to
0.0139987110	to include
0.0138979910	the idea
0.0138962350	to speed
0.0128927897	an estimate
0.0128927897	to employ
0.0128670793	internet of
0.0125100188	a variational
0.0121685373	even with
0.0120037544	found in
0.0118927897	a bi
0.0114526326	the capability of
0.0113355261	a recurrent
0.0111578267	to conduct
0.0104240415	a full
0.0104103336	a total
0.0101197112	response to
0.0100315783	made in
0.0098670793	left to
0.0094550214	to do
0.0088629017	a set
0.0080845523	help to
0.0078185207	key to
0.0072815783	to consider
