0.9732240383	finite element
0.9728930250	additive manufacturing
0.9710620014	augmented reality
0.9705823930	mixed reality
0.9705223904	dimensionality reduction
0.9685336401	cultural heritage
0.9682558837	artificial intelligence
0.9672953168	inverse kinematics
0.9667696599	monte carlo
0.9665215312	ray tracing
0.9659850287	magnetic resonance
0.9655073985	tone mapping
0.9623213283	gaussian curvature
0.9621773543	point cloud
0.9606675683	heat kernel
0.9583422390	importance sampling
0.9582295266	collision detection
0.9579226685	light field
0.9572361129	vector fields
0.9569125773	physics engine
0.9552268425	reinforcement learning
0.9541735872	motion capture
0.9540490450	gradient descent
0.9522032377	machine learning
0.9509373526	deep learning
0.9501868638	neural networks
0.9496656746	virtual reality
0.9492766995	bas relief
0.9489695238	shortest path
0.9482218451	soft tissue
0.9445969910	visual analytics
0.9438975710	anderson acceleration
0.9436218361	user interface
0.9426469881	magnetic resonance imaging
0.9423129935	temporally coherent
0.9421941513	tensor product
0.9418811121	character animation
0.9411054912	volume rendering
0.9400676165	closed form
0.9396046956	feature extraction
0.9388738738	simply connected
0.9373005902	mobile phone
0.9365959989	convex hull
0.9359486426	transfer function
0.9352548914	virtual environment
0.9348153331	global illumination
0.9339669481	mobile devices
0.9333916606	mobile device
0.9330780899	high fidelity
0.9326734090	differential geometry
0.9314260016	neural network
0.9311162321	nearest neighbor
0.9309992006	style transfer
0.9303548958	performance capture
0.9296892372	medial axis transform
0.9292273684	memory footprint
0.9283174715	ray casting
0.9277405488	facial expression
0.9270512787	persistence diagrams
0.9267271080	vector field
0.9265924590	natural language
0.9257997677	midpoint subdivision
0.9246391212	signed distance
0.9243076709	anti aliasing
0.9237885389	frame rate
0.9228315465	differentiable renderer
0.9221771527	rain streaks
0.9180574482	indoor scenes
0.9168815955	subdivision schemes
0.9162272837	level set
0.9154613745	data structures
0.9142589144	image processing
0.9139335526	light transport
0.9137947861	talking head
0.9127837742	scientific visualization
0.9124032184	participating media
0.9120605864	image compression
0.9115417513	parallel coordinates
0.9088840181	line drawings
0.9084590241	differential equation
0.9074200041	programming language
0.9072478693	differential equations
0.9057416761	virtual environments
0.9055646367	visually pleasing
0.9032175536	semantic segmentation
0.9028751238	path tracing
0.9028271121	semi automatic
0.9015445073	virtual agents
0.8997708292	unsupervised learning
0.8993056661	point clouds
0.8980497852	medial axis
0.8980032265	open source
0.8979957710	memory consumption
0.8964737538	medical imaging
0.8955003347	blood flow
0.8946866395	variational autoencoder
0.8942237743	web pages
0.8935730413	massively parallel
0.8935508446	pose estimation
0.8932995923	geodesic distance
0.8922388680	dynamic range
0.8921294897	facial animation
0.8905789831	data visualization
0.8903152873	generative adversarial
0.8901850328	source code
0.8900383706	generative adversarial networks
0.8879590989	scalar fields
0.8870180725	riemann surface
0.8850445054	graphic design
0.8844243332	` `
0.8842539391	synthetic aperture
0.8838557221	loss function
0.8838347533	isogeometric analysis
0.8831956264	convolutional neural networks
0.8829322293	human body
0.8816141736	geometric algebra
0.8809020695	view synthesis
0.8804234708	data structure
0.8803459443	convolutional neural network
0.8783999807	virtual characters
0.8774758101	point set
0.8766757530	high dynamic range
0.8755792703	tensor fields
0.8749103158	monte carlo integration
0.8747921742	texture synthesis
0.8740166568	processing units
0.8739530700	color palettes
0.8738530853	optimal transport
0.8724529532	b spline
0.8713631386	black box
0.8709370858	decision making
0.8708034811	bounding boxes
0.8707990534	attention mechanism
0.8706289262	information visualization
0.8693311740	functional maps
0.8688848486	computer aided design
0.8678546494	fluid dynamics
0.8675075551	frame rates
0.8673956363	ablation studies
0.8668418037	spatially varying
0.8666265180	ray marching
0.8663629730	shape analysis
0.8662208997	data mining
0.8662179790	radial basis functions
0.8659445118	bezier curves
0.8650734718	mesh denoising
0.8644330151	tool path
0.8619589364	light fields
0.8619046838	super resolution
0.8615969935	convolutional networks
0.8615893617	image editing
0.8599204894	fine grained
0.8592310664	node link
0.8587619667	geometry processing
0.8576169020	shape generation
0.8571993232	generative models
0.8559114240	facial expressions
0.8554896128	significant improvement
0.8554040428	ground truth
0.8532063842	temporal coherence
0.8529318659	human motion
0.8523271881	x ray
0.8519601480	conformal parameterization
0.8516138829	log aesthetic
0.8505046357	geometric modelling
0.8504861169	casteljau algorithm
0.8499872163	fine tuning
0.8493194878	quasi conformal
0.8491698226	general purpose
0.8466718686	spatio temporal
0.8465081605	depth map
0.8453965503	indoor scene
0.8436043859	visual effects
0.8433247440	cloth simulation
0.8432853096	hand drawn
0.8430505153	lighting conditions
0.8426332181	developable surfaces
0.8426089531	topological data analysis
0.8425109676	image generation
0.8421602224	line segments
0.8420945010	direct volume rendering
0.8418604465	single image
0.8409652899	head mounted
0.8408265913	vertex positions
0.8403492912	image synthesis
0.8377626152	implicit functions
0.8370749439	mesh refinement
0.8369019741	encoder decoder
0.8367156497	blue noise
0.8365519716	low dynamic range
0.8359797921	generative adversarial network
0.8351345811	cad models
0.8342975387	simply connected open surfaces
0.8328136653	regular grid
0.8317321018	fully automatic
0.8313289927	surface reconstruction
0.8308850143	deformable objects
0.8305465433	spline surfaces
0.8297209288	topological analysis
0.8294529813	post processing
0.8291980139	image inpainting
0.8290645613	object detection
0.8280793483	basis functions
0.8272388769	neural rendering
0.8268761418	shape matching
0.8257923338	moving least squares
0.8247663221	deep neural networks
0.8240204926	feed forward
0.8235929240	network architecture
0.8234849200	human faces
0.8229251737	long term
0.8221803804	high speed
0.8218619345	light stage
0.8209949580	single view
0.8206938413	data analysis
0.8198370755	boundary conditions
0.8172815938	normal estimation
0.8169357100	higher order
0.8161988671	optimization problem
0.8155411884	fully convolutional
0.8147603339	low cost
0.8145508518	generative model
0.8134688241	color transfer
0.8132231488	times faster
0.8100155590	laplace beltrami
0.8098736662	collision free
0.8092395086	adversarial loss
0.8074163168	video frames
0.8054137973	implicit function
0.8034460209	piecewise linear
0.8027841117	triangle meshes
0.8013754562	functional map
0.8009712588	visualization techniques
0.8009375442	supervised learning
0.8007656687	texture mapping
0.8005978226	digital image
0.7988763299	multi modal
0.7986353059	ezier curves
0.7984984494	local minima
0.7977289869	mesh generation
0.7957684462	extensive experiments
0.7948199173	data set
0.7947336550	deep neural network
0.7943300416	developable surface
0.7933576414	depth estimation
0.7932951884	structured light
0.7928652860	edge preserving
0.7926843326	frame field
0.7919425128	desirable properties
0.7914880280	image quality
0.7912732415	case studies
0.7907797651	shape representations
0.7906295309	laplace beltrami operator
0.7882113782	convolutional neural
0.7872902883	data driven
0.7857134367	low dimensional
0.7856089886	high frequency
0.7850107298	photo realistic
0.7831172849	multi agent
0.7830148346	fine scale
0.7813604626	force directed
0.7803619731	higher dimensional
0.7798130627	least squares
0.7779220964	face images
0.7777283224	low level
0.7761651100	key idea
0.7758726460	low frequency
0.7752408888	computational cost
0.7741134991	deep network
0.7727385083	lower dimensional
0.7720821444	future research
0.7713583704	projection mapping
0.7709686171	high performance
0.7686914946	high resolution
0.7673288398	depth maps
0.7672574315	free surface
0.7671930656	latent spaces
0.7660297965	sharp features
0.7648394506	real life
0.7646619283	free form
0.7645757273	multi person
0.7639510237	control points
0.7628449348	patch based
0.7619006632	motion capture data
0.7618573542	unlike previous
0.7615600983	video synthesis
0.7614940778	recent years
0.7599824354	high dimensional
0.7593958181	point wise
0.7589551384	natural images
0.7581050821	point sets
0.7550773601	large scale
0.7539189113	target image
0.7530308959	critical points
0.7524093302	multi view
0.7523747733	quantitative evaluation
0.7512573804	multi scale
0.7500859441	computational design
0.7498775248	high order
0.7495198636	unified framework
0.7487264210	deep generative models
0.7476264446	data sets
0.7442373304	adversarial training
0.7427228436	graph layout
0.7415447502	free viewpoint
0.7410330503	empirical study
0.7389858665	content aware
0.7382085949	previous works
0.7381494849	small scale
0.7381189817	user interaction
0.7371998491	previous methods
0.7361502644	fluid simulation
0.7332932307	triangular meshes
0.7332110904	training data
0.7317201302	user study
0.7316622781	real images
0.7314163560	physical simulation
0.7302814843	image manipulation
0.7301868522	c + +
0.7259517819	3d printing
0.7249161311	physics based
0.7244364411	low resolution
0.7228740259	user studies
0.7205933158	video sequences
0.7195497096	graph based
0.7179399383	multi level
0.7178692012	synthetic dataset
0.7175257607	existing approaches
0.7166158625	pre trained
0.7144709027	image formation
0.7144616977	sketch based
0.7125642559	differentiable physics
0.7105588765	point normal pairs
0.7100721326	high dimensional data
0.7078481939	motion synthesis
0.7078043569	experiments demonstrate
0.7068028716	euclidean space
0.7057210330	image retargeting
0.7050433109	domain specific
0.7044639867	degrees of freedom
0.7022496685	material properties
0.7019675196	self supervised
0.7001765082	real world
0.6996420595	experimental results demonstrate
0.6967781519	numerical simulations
0.6964649632	light transport simulation
0.6957787039	proposed method
0.6953657219	previous approaches
0.6948555159	deep convolutional
0.6938239996	computer aided geometric
0.6928588890	reference image
0.6924967929	visual quality
0.6907510849	r cnn
0.6904196011	experimental results
0.6891949542	implicit surfaces
0.6881658591	deformation fields
0.6881153889	object categories
0.6879808003	differentiable rendering
0.6875018110	cost effective
0.6872749146	specifically designed
0.6867056950	motion tracking
0.6865147377	rgb d
0.6849105802	human pose
0.6829554285	deep generative
0.6819972753	objective function
0.6810358375	computer aided
0.6803939521	shape classification
0.6799253517	web based
0.6797200563	high level
0.6773858475	computationally efficient
0.6771928762	line of sight
0.6769948315	fine details
0.6765576221	rule based
0.6763335589	human motions
0.6696273093	non rigid
0.6679325490	monocular video
0.6675657003	benchmark datasets
0.6653366059	existing methods
0.6643262420	interactive rates
0.6642922121	multi dimensional
0.6628456728	multiple views
0.6620698137	dynamic scenes
0.6607946819	old photos
0.6607319214	texture map
0.6604484921	input images
0.6604118777	few shot
0.6601943298	physically based
0.6596761608	virtual objects
0.6581134793	feature preserving
0.6578265433	graphics hardware
0.6569632379	haptic rendering
0.6564262923	training set
0.6541195193	data points
0.6526095305	surface patches
0.6518935925	fundamental problem
0.6500832230	deep neural
0.6491549745	promising results
0.6489969025	shape descriptors
0.6489308577	high quality
0.6480999872	adversarial networks
0.6472035776	image denoising
0.6465652677	deep learning based
0.6463439500	visual analysis
0.6463398466	geometric modeling
0.6454074309	computer graphics
0.6449701379	360 \ deg
0.6447426911	adaptive sampling
0.6426377872	feature learning
0.6419460489	parallel rendering
0.6399625822	image smoothing
0.6383579471	position based
0.6368712743	motion sequences
0.6348527206	multivariate data
0.6325104573	graph convolutional
0.6323983909	light weight
0.6321178835	spline curve
0.6306497622	sampling patterns
0.6296939970	numerical experiments
0.6270351160	human face
0.6267161899	dynamic networks
0.6264932837	u net
0.6250139963	higher quality
0.6249305975	image segmentation
0.6242238410	transfer functions
0.6234068023	view dependent
0.6231418170	significantly outperforms
0.6223462472	body shape
0.6221848274	based methods
0.6216750269	short term
0.6199654476	orders of magnitude
0.6198732192	video frame
0.6183183796	multi gpu
0.6178903794	semantic information
0.6178566302	image based
0.6172443009	structure aware
0.6169598676	proof of concept
0.6167069457	real world datasets
0.6154382387	automatically generate
0.6153814465	color space
0.6130241533	visualization research
0.6112023460	k means
0.6108496715	fixed point
0.6054594328	domain decomposition
0.6051221189	synthetic data
0.6044770684	motion transfer
0.6038057809	volume visualization
0.6019826010	approximation error
0.6007768813	learning based
0.5991982084	highly accurate
0.5978200833	ill posed
0.5962627330	qualitative and quantitative
0.5951829567	a long standing
0.5920703644	self intersecting
0.5897688130	rendering techniques
0.5896769132	computational efficiency
0.5884350298	four dimensional
0.5870844170	input point cloud
0.5852121983	generative network
0.5809268928	processing unit
0.5808745037	material parameters
0.5803272471	3 d
0.5802813502	time series
0.5795156826	man made
0.5780325207	traditional methods
0.5761002389	image to image translation
0.5758711321	basis function
0.5753710662	rendering pipeline
0.5749688075	computer vision
0.5739112261	gpu based
0.5734274810	end to end
0.5717122798	b spline curve
0.5709300108	arbitrary number
0.5707821358	qualitatively and quantitatively
0.5707426091	feature points
0.5697610350	point based
0.5692971611	data augmentation
0.5676226112	easy to implement
0.5670633032	latent space
0.5665974906	model fitting
0.5660320157	coarse to fine
0.5644672060	high accuracy
0.5642072713	random field
0.5631756484	resonance imaging
0.5614565972	high end
0.5594846705	depth of field
0.5594348475	visual representation
0.5585717123	neural network architecture
0.5582863788	$ \ mathcal
0.5582439960	rgb image
0.5557521100	graph visualization
0.5556644765	visualization tools
0.5540524552	data driven approach
0.5531877766	\ times
0.5504795009	face reconstruction
0.5500980665	co segmentation
0.5453128045	simply connected open
0.5443527540	real world scenes
0.5430545096	times faster than
0.5423840457	$ \ mathbb r ^
0.5423261448	deep convolutional neural
0.5403015446	existing techniques
0.5379476378	rendered images
0.5354135959	trade off
0.5344745955	\ em
0.5340219493	recent developments
0.5337050019	point cloud data
0.5304510427	partial differential
0.5290277612	$ \ mathbb
0.5288717034	learning based approach
0.5285915584	current methods
0.5271372431	proposed approach
0.5254563139	computer animation
0.5252501459	shape descriptor
0.5250848295	paper proposes
0.5238680323	image decomposition
0.5225606365	significantly faster
0.5217302288	et al
0.5200705265	$ \ acute
0.5186417929	method achieves
0.5182965748	low quality
0.5180304171	quantitative and qualitative
0.5123288846	challenging task
0.5122897458	order of magnitude
0.5120705265	$ \ lambda
0.5117399695	gradient based
0.5108223098	takes as input
0.5101886019	adversarial network
0.5101478587	three dimensional
0.5100386806	optimization problems
0.5098920649	training dataset
0.5084833121	spline curves
0.5079405079	practical applications
0.5072804026	curves and surfaces
0.5055290504	shape deformation
0.5050656832	surface mesh
0.5042342736	^ 2
0.5023483026	raw data
0.5023301165	source and target
0.5003396055	z ^
0.4998974853	publicly available
0.4992828000	case study
0.4976913387	produce high quality
0.4968028660	c ^
0.4949185067	body shapes
0.4942957682	paper presents
0.4931744491	$ \ mathbb r ^ 3
0.4927135118	\ emph
0.4920026248	non euclidean
0.4887495225	digital images
0.4882197990	effectiveness and efficiency
0.4878032033	time cube
0.4865674943	shape reconstruction
0.4865060992	based optimization
0.4859793999	study shows
0.4858216508	spatial and temporal
0.4844818911	self intersections
0.4839754639	n =
0.4827408482	second order
0.4820809360	challenging problem
0.4820710059	based rendering
0.4818071315	input data
0.4809008900	time of flight
0.4800414702	face models
0.4781875403	aided design
0.4779950424	data driven methods
0.4754167262	shape segmentation
0.4753086624	non linear
0.4752816540	graphics applications
0.4740218811	time consuming
0.4721037238	local geometry
0.4696268388	computational fluid
0.4684956275	synthetic and real
0.4677407999	recent advances in
0.4671391223	global and local
0.4652971865	broad range of
0.4651968565	geometry and topology
0.4639141692	end to end manner
0.4625854395	\ mathbb r
0.4614853584	teichm \
0.4612931017	interactive visualization
0.4603872597	two dimensional
0.4601489293	geometric features
0.4594950811	face model
0.4588115269	image content
0.4581311628	visualization tool
0.4564475194	proposed technique
0.4564007760	results demonstrate
0.4562543174	previous techniques
0.4548336718	magnitude faster
0.4538349573	shape features
0.4534031926	vision and graphics
0.4530825623	improvements over
0.4529180997	per pixel
0.4523510938	shape representation
0.4520228543	conditional generative
0.4515101010	graphics processing
0.4509439034	method outperforms
0.4506051160	full body
0.4497573727	adaptive mesh
0.4488530078	distance function
0.4469036832	high resolution images
0.4465423007	speed up
0.4459712262	recent advances
0.4457523645	field of view
0.4454418961	scientific data
0.4453625173	image based rendering
0.4448080065	contrast to previous
0.4439625784	high quality results
0.4428211499	well suited
0.4417867966	screen space
0.4411960561	image pairs
0.4410771870	approach outperforms
0.4407087881	this paper presents
0.4400455737	analysis and visualization
0.4393898527	interactive visual
0.4393059276	volume data
0.4387494326	multiple datasets
0.4386823546	neural network based
0.4371032444	time varying
0.4352186509	non convex
0.4348049364	point of view
0.4339660669	real time
0.4333796655	compared to existing
0.4296946072	optimization framework
0.4265531085	relationship between
0.4260501673	mesh based
0.4256580744	in situ visualization
0.4228121081	visualization and analysis
0.4225023165	\ log
0.4218239533	simulation data
0.4202244131	commonly used
0.4201587219	type 2
0.4198894184	color images
0.4141052289	r ^
0.4127335670	rendering framework
0.4119864459	time dependent
0.4118523538	image to image
0.4093414502	experimental results show
0.4079136390	non experts
0.4074094306	learning based method
0.4071399109	outperforms state of
0.4068010623	g ^
0.4056485185	a level
0.4054554029	applications including
0.4032989043	bottom up
0.4029106394	this paper proposes
0.4007377645	does not require
0.3988635536	rigid registration
0.3975129392	radial basis
0.3958931811	optimization based
0.3915517285	^ 1
0.3907979450	linear time
0.3903289170	\ mathcal
0.3902183713	compared to previous
0.3890466791	visualization systems
0.3870556245	faster than
0.3853369265	a case study
0.3851717126	rather than
0.3843571281	high computational
0.3842029255	novel views
0.3841513761	widely used
0.3827652826	an open source
0.3827590332	proposed framework
0.3809398280	based modeling
0.3800531202	a large dataset
0.3790257438	non trivial
0.3788406336	generalize well
0.3780366777	visualization applications
0.3763102785	$ zier
0.3754888326	wide range of
0.3693999686	even though
0.3684944137	3d point clouds
0.3680702558	non manifold
0.3680670067	method produces
0.3658699792	simulation results
0.3657757267	this paper introduces
0.3638754469	while maintaining
0.3636873828	^ 3
0.3632091996	relation between
0.3628931203	b \
0.3622464595	neural style
0.3612993416	real world data
0.3609058504	volumetric data
0.3587379615	2 d
0.3539377854	available at https
0.3533469126	paper introduces
0.3533469126	paper describes
0.3530698164	a wide variety
0.3527042161	n +
0.3520242185	wide variety of
0.3519941708	similarity between
0.3517101964	proposed algorithm
0.3506797609	learning approach
0.3503160281	component analysis
0.3500595352	into account
0.3496763786	does not
0.3496233197	optimization algorithm
0.3481077292	3d point cloud
0.3476155374	recent developments in
0.3469119840	aiming at
0.3467284655	large amount
0.3454516542	so called
0.3452854627	cope with
0.3442396699	a single image
0.3441827172	deep learning approach
0.3433743850	aimed at
0.3431675390	in recent years
0.3420512119	aims at
0.3409942930	this paper describes
0.3381411103	interactions between
0.3380197203	learning framework
0.3365838705	once trained
0.3359631104	source image
0.3356441605	$ \
0.3328094086	prior work
0.3319949119	shape space
0.3319623038	\ c
0.3306356937	graphical user
0.3294325300	distances between
0.3277950704	+ +
0.3277261039	3d printed
0.3263425983	important role in
0.3253737701	results indicate
0.3245838286	gap between
0.3239898561	rely on
0.3221488923	color style
0.3213365975	input image
0.3210696442	differences between
0.3209323925	easy to use
0.3196126394	the art methods
0.3187331677	divided into
0.3187311513	$ q
0.3184235960	run time
0.3181467974	geometric information
0.3175752237	so far
0.3166138392	target images
0.3164293713	this article
0.3142122875	insight into
0.3140955958	to date
0.3123485004	correspondences between
0.3108037271	direct volume
0.3102224830	rendering technique
0.3101972136	human like
0.3101866618	the laplace beltrami
0.3100691759	no reference
0.3094657861	a deep neural network
0.3093509153	analysis tasks
0.3081182873	current state of
0.3078981696	from monocular video
0.3063439407	optimization method
0.3062050136	an input image
0.3061723675	$ h
0.3060771668	n ^
0.3044030737	the key idea
0.3041111500	\ textit
0.3030327475	$ m
0.3023109436	network to learn
0.3015316100	achieves state of
0.3013103300	a deep learning based
0.3012402878	user specified
0.3007087016	color image
0.2989847091	3d shape reconstruction
0.2987958701	$ g
0.2976000655	the laplace beltrami operator
0.2965107675	based approaches
0.2956119018	visualization of large
0.2952728046	3d human pose
0.2939842051	more accurate
0.2939504767	correlation between
0.2919331417	caused by
0.2919275543	the input image
0.2909743079	non uniform
0.2903397013	moving least
0.2896198293	model parameters
0.2890270828	do not
0.2878233629	previous work
0.2860677119	a user study
0.2859079642	current state
0.2847519583	^ n
0.2840504431	feature space
0.2839358546	well known
0.2837971666	parameter space
0.2823024542	generate realistic
0.2821722655	per second
0.2816401678	real data
0.2815779478	$ o
0.2811277122	information about
0.2801103185	the proposed method
0.2798850492	while keeping
0.2795407338	a large scale
0.2792652523	rendering methods
0.2786385943	large number of
0.2774032390	the fly
0.2772928742	extensive experiments on
0.2758864531	the shelf
0.2703692800	previous state of
0.2676393572	the art results
0.2663460803	well established
0.2661956299	the simulation of
0.2657950155	try on
0.2653775725	image data
0.2653125256	a neural network
0.2651387071	based approach
0.2639103617	without requiring
0.2616289479	a generative model
0.2613556299	for use in
0.2613368738	characterized by
0.2603372211	relationships between
0.2596956299	the style of
0.2591956299	the space of
0.2585355125	a low dimensional
0.2576956299	the shape and
0.2571956299	the cost of
0.2562655975	range of applications
0.2557178898	computer vision and graphics
0.2549689640	defined over
0.2549607854	r ^ 3
0.2546428324	this paper
0.2543522264	the proposed algorithm
0.2539054578	design space
0.2534257112	suffer from
0.2524015255	an important role
0.2521956299	the shape of
0.2521300563	$ n
0.2518605285	inspired by
0.2515793169	take advantage of
0.2514787998	$ \ mathbb r
0.2511956299	the accuracy of
0.2501956299	the effects of
0.2501956299	the geometry of
0.2491956299	the design of
0.2479742921	a wide range
0.2475756385	generated images
0.2471166450	2d and 3d
0.2454174536	efficient method for
0.2451956299	the results of
0.2451461818	conformal parameterization of
0.2448244611	representation of 3d
0.2448103182	based approach for
0.2443397787	computer generated
0.2443306421	the help of
0.2433021025	approach significantly
0.2430316125	at test time
0.2417129256	bounded by
0.2416903968	linear system
0.2407026699	the art
0.2406377470	level of detail
0.2405289632	the study of
0.2400774963	affected by
0.2398836584	an order of magnitude
0.2393793932	non local
0.2382806421	the basis of
0.2380289632	the solution of
0.2378622965	the method of
0.2372972882	in situ
0.2371459268	based image
0.2366970727	use cases
0.2360554522	the challenging
0.2360289632	the geometry and
0.2360026503	on par
0.2353556299	the task of
0.2350222965	the importance of
0.2336057022	$ s
0.2334389632	in 2d and
0.2332365512	less than
0.2328436744	geometric design
0.2328307183	effective method
0.2325554563	reconstruction from
0.2323556299	the process of
0.2299924333	experiments show
0.2291632202	distance between
0.2280452216	realistic images
0.2276406623	the human visual
0.2268480812	each vertex
0.2266128623	correspondence between
0.2264032470	visual analysis of
0.2262587035	based techniques
0.2258986845	$ d
0.2254579423	obtained by
0.2247646126	a lot
0.2244424133	control over
0.2239770519	produced by
0.2228742517	360 \
0.2227807858	depending on
0.2207952344	a single
0.2205533836	relies on
0.2205382520	existing state of
0.2193727592	computer graphics applications
0.2190066535	$ k
0.2188196823	recent work
0.2186003205	on top of
0.2170116159	focus on
0.2159937705	focuses on
0.2150312037	state of
0.2146402479	image translation
0.2143765567	synthetic and real data
0.2132901061	during training
0.2127845744	represented as
0.2127454893	$ p
0.2127091390	number of
0.2111640322	novel view
0.2099028743	interactive visualization of
0.2080927007	dealing with
0.2076483693	based on
0.2067312874	do so
0.2064103970	this thesis
0.2063160960	complex 3d
0.2056710942	significantly more
0.2052614812	relying on
0.2050294724	3d data
0.2048796677	leads to
0.2046890504	computation time
0.2044432487	focused on
0.2043327108	n \
0.2040042986	first order
0.2033734993	the theory of
0.2032883001	integrated into
0.2031922694	the art approaches
0.2031756165	m \
0.2031661596	topological data
0.2031587278	the proposed framework
0.2019564417	learning model
0.2009051226	serves as
0.2001097976	a challenging task
0.1996916924	space time
0.1989981537	a training
0.1986234993	the appearance of
0.1985997824	induced by
0.1984837293	3d shape
0.1979401549	deal with
0.1978929880	as possible
0.1972965431	ranging from
0.1970941035	method to generate
0.1965106279	the art techniques
0.1959953598	to generate realistic
0.1954990109	as well as
0.1946083301	both synthetic and real
0.1941050751	a convolutional neural network
0.1926936491	3d shapes
0.1926596244	the art algorithms
0.1922377245	focusing on
0.1920138309	motivated by
0.1917281579	motion data
0.1911972052	while preserving
0.1909374122	represented by
0.1897904797	the facial
0.1865522830	network trained
0.1855293136	in addition
0.1851977864	this end
0.1850212329	convolutional network
0.1848240376	the generative
0.1847787454	compared to
0.1839474833	responsible for
0.1835805198	enabled by
0.1834151194	the presented approach
0.1819703479	3d meshes
0.1811579128	the original image
0.1802436163	an important role in
0.1788610571	a deep learning
0.1779514478	in terms of
0.1776548760	according to
0.1775575057	widely used in
0.1768971867	diverse set of
0.1760095934	small number of
0.1759964832	majority of
0.1759667066	the art performance
0.1757144526	geometric data
0.1757045289	the sampling
0.1751295210	does not rely on
0.1744580162	trained on
0.1736279667	an extension of
0.1734383440	performance than
0.1732684918	learning techniques
0.1731337347	synthesis methods
0.1729110518	further improve
0.1720623716	transport simulation
0.1715000733	results show
0.1712987878	more than
0.1712180702	to face
0.1711624883	to point
0.1708190665	the medial axis
0.1706277004	conditioned on
0.1701229573	with respect to
0.1699514478	in order to
0.1697283042	a deep
0.1694093942	two stage
0.1692072764	based approach to
0.1688294254	a wide range of
0.1686838581	an end to end
0.1686760820	3d human
0.1684859872	dimensional data
0.1683457441	high dynamic
0.1680859701	based method
0.1678942837	due to
0.1678020948	serve as
0.1677741738	lead to
0.1669635344	open surfaces
0.1667590461	a data driven
0.1665149892	not only
0.1659789456	3d city
0.1654069895	art methods
0.1652213217	simple yet
0.1648124160	experiments on
0.1644264884	able to
0.1638134109	interaction between
0.1636556470	by applying
0.1632894326	3d model
0.1632777856	an image
0.1631942420	applied to
0.1622893944	the fast
0.1614779545	this chapter
0.1610373882	depends on
0.1608063530	followed by
0.1606171015	different ways
0.1592894196	and further
0.1589670553	method uses
0.1587973838	the implementation
0.1586723026	fraction of
0.1586723026	absence of
0.1586544254	the range
0.1579242129	generated by
0.1578838763	a survey
0.1574606657	correspond to
0.1573344857	consisting of
0.1572455478	efficient way
0.1569716546	original image
0.1567695097	at least
0.1564296493	3d mesh
0.1563805875	extracted from
0.1546860514	achieved by
0.1543547683	large dataset of
0.1543015097	interact with
0.1539129300	lot of
0.1535273141	an important
0.1534404637	tend to
0.1531746833	aims to
0.1531607442	an efficient
0.1529345864	a learning based
0.1526897566	a wide variety of
0.1523990464	determined by
0.1519609050	3d objects
0.1510570535	the state
0.1509050572	based representation
0.1506755569	a challenging problem
0.1505476440	the hessian
0.1505124468	an example
0.1502656735	an interactive
0.1501268164	part based
0.1497648281	consists of two
0.1497551316	human visual
0.1491929867	corresponds to
0.1483368422	method based on
0.1483135016	an application
0.1482040315	one or more
0.1468778834	a large variety of
0.1467488127	variety of
0.1467475226	applications such as
0.1463244325	a large number
0.1461867793	large dataset
0.1460416865	this work presents
0.1459081253	learning methods
0.1448892693	compared with
0.1443540437	model based
0.1434721476	an online
0.1433276516	3d hair
0.1430925534	3d scene
0.1430153194	set of
0.1430054991	the parallel
0.1424295045	an average
0.1423512965	to generate
0.1410311453	defined by
0.1404858288	a fully
0.1390920071	based shape
0.1390420753	the other hand
0.1388669973	art algorithms
0.1383659216	directly from
0.1381819141	resolution images
0.1377573875	capable of
0.1367035068	driven approach
0.1356472088	compared to other
0.1353259573	the ground truth
0.1352366585	3d reconstruction
0.1351230132	reconstruction method
0.1350158022	learned from
0.1348525372	an unsupervised
0.1346387745	derived from
0.1336851873	to present
0.1335957380	in geometry processing
0.1333147213	consist of
0.1332240594	resolution image
0.1332160325	such as
0.1327012365	first step
0.1322744852	renderings of
0.1317338146	method for
0.1314469305	by introducing
0.1310732500	for large scale
0.1309454362	kinds of
0.1309363510	to end
0.1306959036	created by
0.1305183514	at different
0.1302541364	efficient algorithm
0.1295444364	but also
0.1295422564	obtained from
0.1295126789	objects into
0.1294072848	an object
0.1292392676	3d models
0.1285387096	quality results
0.1284547085	better than
0.1283770663	a part
0.1278294130	thousands of
0.1276999219	a high quality
0.1272933775	more robust
0.1266951584	method does not
0.1262274412	learning models
0.1259411797	characterization of
0.1255195530	two main
0.1249631081	3d scenes
0.1248438671	the proposed approach
0.1246903080	dimensional space
0.1245613173	to create
0.1245564779	a simple
0.1244394595	cloud data
0.1240941295	a sequence
0.1236888530	an empirical
0.1236804595	a high resolution
0.1233047095	from motion
0.1232199866	starting from
0.1231828669	a point cloud
0.1226230730	hundreds of
0.1225741174	the digital
0.1225629484	each frame
0.1225391549	the evaluation
0.1225230147	a hybrid
0.1223692718	further research
0.1221837521	proposed model
0.1219438613	on real world
0.1217131741	a set of
0.1216995221	information from
0.1215975388	yet effective
0.1211430466	the frame
0.1209786280	available at
0.1204359319	a variety of
0.1201308763	more efficient
0.1198780490	simple to
0.1194825747	dynamic point
0.1194535913	guided by
0.1193628994	an improved
0.1192717919	a robust
0.1185947955	this work
0.1178945928	3d object
0.1178799149	network based
0.1178017407	autoencoders for
0.1172989839	framework for
0.1172791781	many applications
0.1170727676	a very
0.1170703068	an efficient method
0.1169915219	deep learning to
0.1168301056	in conjunction with
0.1167303240	an immersive
0.1166976601	to color
0.1166017978	approach based on
0.1165495951	each other
0.1163643344	the point cloud
0.1162363649	a diverse
0.1155559069	efficient method
0.1152656401	a convolutional neural
0.1148353295	by means
0.1146381795	to learn
0.1146036534	across multiple
0.1145332306	a multi scale
0.1143813335	a new algorithm
0.1142523669	captured by
0.1132581924	this gap
0.1123824714	while providing
0.1122869246	performed by
0.1120777718	the use of
0.1120250078	subset of
0.1117379233	both qualitatively
0.1114985632	based simulation
0.1108904786	restricted to
0.1107770376	the current state
0.1107725305	capture data
0.1104710565	to achieve
0.1104169607	converge to
0.1103778037	a range
0.1103533639	many fields
0.1101662026	role of
0.1100834838	combined with
0.1092503506	these issues
0.1089530996	supported by
0.1088402761	able to generate
0.1087256937	used to compute
0.1087140748	easy to
0.1086649732	close to
0.1081351775	methods for
0.1079626609	the real world
0.1078553771	this issue
0.1077564105	a novel
0.1077235778	3d scanning
0.1076130644	a deep neural
0.1070029835	leading to
0.1066563979	realization of
0.1059997416	approach for
0.1057445591	performance on
0.1052634583	a comprehensive
0.1051324363	an evaluation
0.1049489795	convolutional neural network to
0.1046818362	these methods
0.1045643844	directly on
0.1043802989	make use of
0.1040186052	suitable for
0.1040144292	the original
0.1040111051	the problem of
0.1038278349	kind of
0.1037409301	access to
0.1036972250	in practice
0.1036510732	an arbitrary
0.1034747190	added to
0.1033235069	the depth
0.1030604071	consists of
0.1030367497	methods based on
0.1029963898	pairs of
0.1029145734	compatible with
0.1024908254	an effective
0.1023232749	from point clouds
0.1021974057	an approach
0.1021572605	the neural network
0.1017968544	presented approach
0.1016995546	a problem
0.1011487883	assumptions on
0.1011102716	more general
0.1006325203	an input
0.1004532072	an alternative
0.1003692869	the superiority
0.1003260204	a generalization
0.1000405425	difficult to
0.1000166543	the superiority of
0.0999050587	3d face
0.0998944007	the same
0.0996260150	the material
0.0993841033	the wild
0.0993015378	under different
0.0992656319	the most important
0.0989978017	across different
0.0989775840	more realistic
0.0988840540	algorithm for
0.0988299256	to improve
0.0983530094	a 3d shape
0.0982241922	versions of
0.0981631417	driven by
0.0979691088	as input
0.0977501538	while still
0.0977260204	a pair
0.0977169730	tool for
0.0977031839	the input
0.0973039017	a group
0.0972937520	better results
0.0971585690	based on deep
0.0967639916	test time
0.0967471470	neural networks for
0.0967032768	structure from
0.0966387063	various applications
0.0965600945	the state of
0.0965484313	a design
0.0961061160	to compute
0.0955126336	mapping from
0.0953583853	so as to
0.0951580066	the inner
0.0950604964	provided by
0.0950558385	more effective
0.0945902115	2d images
0.0945240806	approach to
0.0943042784	a novel algorithm
0.0942161290	benefit from
0.0941843371	neural network to
0.0941723318	defined on
0.0940474010	these approaches
0.0934626702	the proposed
0.0934018119	by exploiting
0.0933693184	much more
0.0933370565	the availability
0.0933361937	robustness to
0.0930645750	novel deep learning
0.0929088883	new dataset
0.0929086798	3d volume
0.0927761888	and real data
0.0927686856	works on
0.0925173862	to produce
0.0921866830	assessment of
0.0917737342	novel framework
0.0916770650	visualization of
0.0916602210	a small number of
0.0915207962	the existence
0.0912003506	role in
0.0909472199	a web
0.0905529596	the tool
0.0902782789	allows users to
0.0902720105	to address
0.0901128903	a generative
0.0899562836	to obtain
0.0898121585	this approach
0.0895879313	a virtual reality
0.0894869154	neural network for
0.0893950806	a series of
0.0890379808	a general
0.0888871768	3d shapes from
0.0886801812	platform for
0.0883672822	this problem
0.0882942447	in contrast to previous
0.0878800758	by combining
0.0875205313	images from
0.0872721029	the only
0.0871593073	behavior of
0.0870037769	the number of
0.0868983316	a lot of
0.0868371041	generalize to
0.0866472064	the existence of
0.0866129138	at most
0.0865185181	to navigate
0.0864820155	an analysis of
0.0864502960	approach allows
0.0864325118	the loss
0.0863713122	the training data
0.0858761386	the size of
0.0857185667	a computational
0.0856938321	tools for
0.0856442675	the quality of
0.0855533927	for point cloud
0.0853683016	report on
0.0851449047	results on
0.0850351186	description of
0.0847242017	by adding
0.0847238183	the authors
0.0845119037	this task
0.0845068925	adapted to
0.0840265420	an automatic
0.0839898588	the feature
0.0837659876	3d modeling
0.0837019227	to frame
0.0835901566	used to generate
0.0832206969	introduced by
0.0831490436	the user's
0.0830201816	the creation
0.0828098891	operate on
0.0827714079	out of
0.0827587559	an intuitive
0.0827114470	for 3d shape
0.0823978143	a target
0.0823455567	meshes with
0.0819283326	evaluated on
0.0818645187	a new
0.0817373748	progress in
0.0816656556	through extensive
0.0816517625	the experimental results
0.0813820756	3d indoor
0.0813485641	of training data
0.0813408163	interested in
0.0812990938	methods in terms of
0.0811289186	the difference between
0.0810253228	advantage of
0.0810192991	as well
0.0810182915	the field of
0.0809209789	the ability
0.0807238016	the target
0.0807210755	only one
0.0802511323	measured by
0.0801268688	between two
0.0800787159	ease of
0.0800188682	the context of
0.0795817117	the log
0.0794571686	an initial
0.0794257341	the complexity of
0.0793967260	in combination with
0.0793580576	along with
0.0792735077	by means of
0.0789687107	an artist
0.0788729538	the topology
0.0787317604	a simple yet
0.0785128399	different types of
0.0781520309	prone to
0.0781083577	of handling
0.0779730174	to provide
0.0778094410	the normal
0.0777513734	between pairs
0.0777407065	technique for
0.0777175483	to state of
0.0776987030	a variety
0.0776444194	the analysis of
0.0775455038	applicable to
0.0774737108	this study
0.0772951804	a flexible
0.0772014248	consistent with
0.0770807693	levels of
0.0770115518	method by
0.0769726769	derivation of
0.0769244663	by leveraging
0.0768599909	back to
0.0767490429	the most common
0.0766873710	advances in
0.0763895396	to introduce
0.0763876034	at https
0.0763538974	in computer graphics
0.0760855159	generated from
0.0760066696	as much
0.0757081464	related to
0.0756723127	the generation of
0.0756562734	an adversarial
0.0755497529	algorithms for
0.0751567758	range of
0.0748319443	conjunction with
0.0748019188	diversity of
0.0745419055	the effectiveness of
0.0743976894	a novel framework
0.0740289445	this method
0.0738253024	the ability of
0.0737613036	similar to
0.0737106617	an algorithm
0.0736546139	the user to
0.0735049967	an overview of
0.0734147720	analysis of
0.0731577439	composed of
0.0731123511	the research
0.0731072124	the possibility of
0.0730843233	considered as
0.0730549437	to process
0.0729320598	a method of
0.0727980437	these challenges
0.0725569147	ability to
0.0723071654	the user
0.0720723505	identification of
0.0720147734	the watermark
0.0719403877	the texture
0.0715115463	account for
0.0714174661	to solve
0.0713068749	most existing
0.0710367993	method on
0.0710144944	the case of
0.0710070859	mapped to
0.0709348321	a face
0.0707968212	to tackle
0.0706632286	novel technique
0.0706158338	not require
0.0701096053	the final
0.0700862980	to study
0.0700565009	the rest of
0.0700422453	the linear
0.0698775613	result in
0.0697020714	fail to
0.0696081839	lies in
0.0692914061	different levels
0.0688378356	learns to
0.0684542505	evolution of
0.0684167204	in particular
0.0683127400	the efficiency of
0.0682248586	designed to
0.0679449378	a sequence of
0.0679044892	sequences with
0.0677571082	a real time
0.0676835025	or even
0.0676712752	a focus
0.0675231439	a comparison
0.0674820996	the effect
0.0674366248	coupled with
0.0671846372	a group of
0.0671250630	tested on
0.0670653049	collection of
0.0669675175	over time
0.0667974139	a synthetic
0.0662843385	used to
0.0661941310	to enable
0.0659726948	not rely on
0.0659100889	a new approach
0.0658961911	sum of
0.0658535291	by providing
0.0658016131	an iterative
0.0657336142	more complex
0.0657050205	results from
0.0656793836	two different
0.0655783820	classes of
0.0655743636	a compact
0.0655116045	the presence of
0.0655097226	the main
0.0655072132	the impact of
0.0654849794	spectrum of
0.0653936232	class of
0.0652798970	difference between
0.0652231892	the flow
0.0651546608	in real time
0.0650692316	a dual
0.0650479073	attempt to
0.0649028830	an adaptive
0.0648713921	the current state of
0.0648497928	the surface
0.0648442882	a diverse set of
0.0644195124	amount of
0.0642981591	runs in
0.0641997851	lack of
0.0639397269	the detection
0.0637387245	top of
0.0636236201	family of
0.0635993107	overview of
0.0635951551	to quantify
0.0635572539	the number
0.0635325861	different parts
0.0634997599	art in
0.0633741817	to predict
0.0633520483	amounts of
0.0633477629	a 3d
0.0633307115	an optimal
0.0630442576	reconstruction of
0.0629862738	taken from
0.0626822415	each point
0.0625163926	the evolution
0.0625029418	the desired
0.0624967626	to train
0.0624069950	aspect of
0.0623780294	a canonical
0.0623042661	comparison with
0.0622565030	improvements in
0.0621184383	an additional
0.0621169929	differences in
0.0620357565	a new dataset
0.0619568714	the heat
0.0616716045	a method to
0.0616269782	level of
0.0616236885	to ensure
0.0615004186	the effect of
0.0614382872	to extract
0.0613086381	the smooth
0.0612496143	three different
0.0611134780	representation of
0.0610607044	most common
0.0610057673	a dataset of
0.0609772753	the output
0.0609132639	input 3d
0.0608543643	the trained
0.0606685025	a number
0.0605702099	the complex
0.0605687231	understanding of
0.0605377547	comparison of
0.0604612276	images with
0.0602818004	capabilities of
0.0602564002	the interactive
0.0600425447	implementation of
0.0599216077	ubiquitous in
0.0596251910	than previous
0.0596002354	computed by
0.0595602985	variant of
0.0594919908	by minimizing
0.0594582872	to synthesize
0.0594123540	a graph
0.0593144979	a subset of
0.0591900926	synthesis using
0.0589487840	the end
0.0588402557	combination of
0.0585075945	to support
0.0583448740	used for
0.0583389294	designed for
0.0583089003	not exist
0.0582717671	degrees of
0.0582228043	notion of
0.0580502344	novel neural
0.0577887356	an open
0.0577809981	learning to
0.0575657673	the goal of
0.0575567558	guaranteed to
0.0575522107	than state of
0.0575430000	other state of
0.0575120106	embedded in
0.0574549585	the performance of
0.0573751868	employed to
0.0573310417	two components
0.0572070582	a novel approach
0.0572048312	trained with
0.0571673104	to deal with
0.0570476409	distribution of
0.0569912142	reconstructed from
0.0569125265	type of
0.0568400937	a combination of
0.0568135269	the utility
0.0567907089	a fundamental
0.0566809079	the problem
0.0564805319	a special
0.0563875114	a range of
0.0563550814	the notion of
0.0562312454	a large
0.0562080617	to enhance
0.0561976368	a complete
0.0561892029	between shapes
0.0561627864	equivalent to
0.0561054124	properties of
0.0560858450	the surface of
0.0560637089	a novel technique
0.0559488331	problem of
0.0559049179	the center
0.0558193901	of flight
0.0557874333	the aim of
0.0557138983	the recent
0.0556668760	the surrounding
0.0555469974	the pixel
0.0554432638	to evaluate
0.0554098098	efficacy of
0.0553941828	possibility of
0.0553351494	video to
0.0553082023	a dataset
0.0551953574	associated with
0.0551473753	a pair of
0.0550690843	the choice
0.0550048103	other methods
0.0549708752	combinations of
0.0549248722	aspects of
0.0548858666	face to
0.0544496430	to determine
0.0542739340	this purpose
0.0542242471	sense of
0.0539806292	pipeline for
0.0539387141	availability of
0.0538586237	these features
0.0537977818	novel approach to
0.0537145998	an appropriate
0.0535466154	comparable to
0.0534688975	the boundary
0.0534410435	also present
0.0533014598	to estimate
0.0532730108	to minimize
0.0532303781	resulting in
0.0530778738	scans of
0.0530419049	presence of
0.0529329319	than existing
0.0528782047	interactive 3d
0.0528398549	novel approach for
0.0526196818	density of
0.0526100074	the local
0.0523902441	different levels of
0.0523107975	success of
0.0522569506	by utilizing
0.0522080617	to avoid
0.0521370303	form of
0.0521179826	evaluation of
0.0521011527	an essential
0.0520549495	a generalization of
0.0520338397	a novel deep
0.0519595504	the world
0.0518840957	these models
0.0518666454	to infer
0.0518575860	better performance
0.0517269408	this report
0.0516252653	an improvement
0.0516222031	used in
0.0515540038	the feasibility of
0.0515420743	a person
0.0515111457	of 3d shapes
0.0514873754	effectiveness of
0.0514618344	to overcome
0.0513637827	the art methods in
0.0513104780	different types
0.0512150596	this leads to
0.0512117154	version of
0.0511915133	nature of
0.0510946141	a common
0.0510251242	center of
0.0510023279	an optimized
0.0508084343	an end
0.0507537106	the core of
0.0505881127	by comparing
0.0505278282	the 3d
0.0505173930	the need for
0.0503932876	also introduce
0.0503155568	to perform
0.0502996068	from raw
0.0501376636	the computation of
0.0501207439	adapt to
0.0499558837	in computer vision
0.0499421353	the presence
0.0498891871	sets with
0.0497007145	in conjunction
0.0496522538	the purpose of
0.0496306549	by optimizing
0.0495436449	generation of
0.0495409388	further demonstrate
0.0495006151	on average
0.0494148188	suited for
0.0493094481	to reconstruct
0.0492854666	a result
0.0490223465	introduction of
0.0489296296	limitations of
0.0489114361	this challenge
0.0488687535	realistic 3d
0.0487042049	to deal
0.0486907338	this allows
0.0485911593	interactions with
0.0484547975	a series
0.0484085789	other hand
0.0483938450	to see
0.0482567743	well as
0.0482107077	the current
0.0481810440	impact of
0.0481619926	an implicit
0.0481415723	to represent
0.0481163227	to use
0.0481076443	then used to
0.0480989750	the fact
0.0480723129	existence of
0.0478880675	modification of
0.0478516963	orders of
0.0477048654	the ability to
0.0477024049	a collection of
0.0476971278	to build
0.0476566919	this research
0.0476564657	algorithm to
0.0474125895	a novel method
0.0473824742	objects from
0.0473329930	an overview
0.0473131482	generalizes to
0.0472528786	numbers of
0.0472294319	an integral
0.0470675433	to reduce
0.0470614085	an order
0.0470121430	to understand
0.0470108993	a consequence
0.0467573932	with varying
0.0465248481	these problems
0.0465060991	the same time
0.0461546511	types of
0.0461524665	the benefits of
0.0460084875	used as
0.0458103209	a patch
0.0457831404	with minimal
0.0457441301	a family
0.0456974491	the circle
0.0456908237	to noise
0.0456384422	to construct
0.0455930880	a wide
0.0455916389	a particular
0.0454893416	a large number of
0.0454270359	topic in
0.0453097405	to control
0.0449477245	selection of
0.0448501035	exploration of
0.0447323932	to encode
0.0445659297	an extensive
0.0445186420	a given
0.0445161028	the creation of
0.0444857589	a number of
0.0444356646	array of
0.0444202688	to select
0.0442841094	a semantic
0.0442641310	to capture
0.0442558663	coarse to
0.0442247588	to adapt
0.0441443715	a method
0.0441262276	the intersection
0.0441130295	the effectiveness
0.0440954737	these techniques
0.0440548436	generation from
0.0440094179	a linear
0.0438410371	the applicability of
0.0435734099	the set of
0.0435516038	choice of
0.0434945027	such as image
0.0434220083	to render
0.0433265831	for instance
0.0431929582	a collection
0.0431833095	vertices of
0.0431490139	in contrast to
0.0429552737	to realize
0.0428455970	the degree of
0.0428153754	combination with
0.0427861948	in turn
0.0427623250	series of
0.0427417625	a low
0.0425954556	the development of
0.0424978899	a few
0.0423973764	to recover
0.0423648564	a user
0.0422121023	the key
0.0420255545	property of
0.0419659633	to leverage
0.0419151298	while also
0.0418599380	the definition of
0.0418402981	this problem by
0.0417750894	the most
0.0416763820	the usual
0.0416660663	into two
0.0415736939	to implement
0.0414672876	a case
0.0414527202	from 2d
0.0414487755	known as
0.0414291261	alternative to
0.0413991228	by using
0.0413989970	a set
0.0413859524	a new method
0.0413828667	the entire
0.0412652374	for example
0.0411517171	features from
0.0411049464	this way
0.0408764160	essential for
0.0408540922	a novel approach to
0.0407890262	to handle
0.0407839695	many different
0.0407507321	to demonstrate
0.0406593053	the visualization of
0.0406524156	a specific
0.0405190994	usage of
0.0404934215	needs to
0.0404735303	new method
0.0404162647	the usefulness of
0.0403801729	proof of
0.0403360291	the need
0.0403349294	the utility of
0.0403154883	as part of
0.0403027504	the proposed system
0.0402530660	this framework
0.0400133591	of 3d objects
0.0398906910	variants of
0.0398811373	the discrete
0.0398308596	perception of
0.0398197162	a convolutional
0.0398060466	to explore
0.0395801453	built on
0.0395756285	to identify
0.0395509127	to apply
0.0395220175	a fixed
0.0395071402	in addition to
0.0394543308	the first time
0.0394300431	to guide
0.0393600043	a small
0.0393325681	shapes from
0.0393023453	approach on
0.0392512095	or not
0.0392052304	and easy to
0.0391101450	the lack of
0.0389108568	further show
0.0387949365	both synthetic and
0.0386950480	sequence of
0.0386894709	the advantages of
0.0386078870	feasibility of
0.0385719597	a novel approach for
0.0385483695	a broad
0.0383737465	this project
0.0382935649	comparisons with
0.0382669191	the whole
0.0382022648	a digital
0.0381455563	extension of
0.0380565230	this network
0.0380102783	2d image
0.0380045621	a challenging
0.0379469355	3d volumetric
0.0378974775	usefulness of
0.0378886211	a visual
0.0378840389	a semi
0.0378542362	the obtained
0.0376986310	3d surface
0.0376152701	signal to
0.0376092278	to accelerate
0.0375075504	this goal
0.0374822910	application to
0.0374674958	the rapid
0.0374489082	the concept
0.0373182246	idea of
0.0372924631	an accurate
0.0372712380	allows for
0.0372077823	3d pose
0.0371894462	curves with
0.0371696188	the first
0.0371098618	to embed
0.0371042080	an approach to
0.0370283424	estimation from
0.0369695377	captured with
0.0369583537	the gap
0.0368541771	to detect
0.0368313638	together with
0.0368028918	involved in
0.0367655614	to maintain
0.0366662647	a novel method for
0.0366595819	a framework
0.0365980531	expected to
0.0365837606	to facilitate
0.0365676410	a high
0.0363693952	the internal
0.0363377524	to convey
0.0363204865	aid of
0.0362580262	shown to
0.0362480696	need to
0.0362466517	the overall
0.0361126301	superiority of
0.0360583805	the idea of
0.0359968463	to store
0.0359309829	a unified
0.0359022936	done by
0.0358012289	a real
0.0357499635	a key
0.0355898810	search for
0.0355809410	a method for
0.0353486717	a state of
0.0352827571	applicability of
0.0352452454	many computer
0.0351642488	allowing for
0.0351511374	solution for
0.0351371387	an existing
0.0350988475	3d motion
0.0350497633	a spline
0.0350439631	this enables
0.0350310920	the power of
0.0349139849	research in
0.0348948617	model for
0.0348616821	the medial
0.0348068900	encoded in
0.0347652995	also provides
0.0347201673	new framework
0.0346679652	research on
0.0345860607	the sense
0.0345413329	rest of
0.0344173795	with state of
0.0343546554	a path
0.0342002043	by solving
0.0341068929	present in
0.0340545258	the construction of
0.0340332361	an optimization
0.0339541080	3d point
0.0339527512	a new method for
0.0338239446	calculation of
0.0337555199	scheme for
0.0337009046	the latent space
0.0336611750	up to
0.0336218949	formulation of
0.0335829314	the concept of
0.0335702630	relative to
0.0335547002	structure of
0.0335231903	part of
0.0334909995	strategies for
0.0334797730	to remove
0.0334702257	and robustness of
0.0334440757	interpretation of
0.0334435675	independent of
0.0334428528	methodology for
0.0333491531	used by
0.0332853726	the well known
0.0332650498	to find
0.0332603148	of up to
0.0329771533	for solving
0.0328910732	method does
0.0328639876	respect to
0.0327076373	to integrate
0.0326865743	the literature
0.0325122701	this context
0.0324461506	a tool
0.0323492865	deformation of
0.0322902296	separation of
0.0322489549	to assess
0.0322457510	representation for
0.0322415983	for evaluating
0.0321762545	the properties of
0.0321724265	definition of
0.0321286760	novel method
0.0321166982	of interest
0.0320797025	to interpolate
0.0320652894	in order
0.0320375724	instead of
0.0320323111	a technique for
0.0319862281	an extension
0.0318804621	characteristics of
0.0318518036	for estimating
0.0317568743	performed on
0.0316272554	network on
0.0315330534	by considering
0.0315031803	support for
0.0314375413	an algorithm for
0.0313999606	studied in
0.0312197327	the representation of
0.0312111479	to visualize
0.0311302372	specified by
0.0310958349	limited to
0.0310142420	hard to
0.0309732758	integrated in
0.0308925914	results than
0.0307860005	this technique
0.0307310650	flexibility of
0.0307218434	review of
0.0307042442	useful for
0.0306444058	a fast
0.0305628587	performance of
0.0304468947	this leads
0.0304355331	a way to
0.0303769102	this process
0.0302999455	improvement in
0.0302464883	problem by
0.0301751667	text to
0.0300781113	increase in
0.0300534231	to simulate
0.0299335024	approach by
0.0299314476	applied on
0.0298523399	the last
0.0296128391	developed for
0.0294954116	to get
0.0294432627	in contrast
0.0293396217	to draw
0.0292975247	obtained with
0.0292961845	integration of
0.0292345168	a framework for
0.0291232376	models from
0.0290673386	a system for
0.0288578540	the various
0.0287728167	a closed
0.0287561587	a projection
0.0287155402	the first to
0.0286411384	quality of
0.0285716715	developments in
0.0284541231	novel approach
0.0282116897	image from
0.0281866586	a short
0.0281645258	try to
0.0281618792	required to
0.0280990728	the form of
0.0280756374	complexity of
0.0280508650	the possibility
0.0280228307	transfer of
0.0279601467	a good
0.0279155066	the rest
0.0278529036	for reconstructing
0.0278003835	shape from
0.0277236155	the application of
0.0277169365	the availability of
0.0276745582	development of
0.0276579708	evaluation on
0.0276310465	the question
0.0276300143	different from
0.0275883970	the way
0.0274382087	application of
0.0272373462	to make
0.0272274103	a pixel
0.0271640556	partition of
0.0270065898	the relationship
0.0269694618	the robustness of
0.0268937487	a promising
0.0268937487	a database
0.0268146499	as compared to
0.0267056389	library for
0.0266232758	composition of
0.0266192795	a learned
0.0265697195	the integration of
0.0265515475	and qualitatively
0.0265268306	and efficiency of
0.0264964804	usability of
0.0264294340	architecture to
0.0263681821	two novel
0.0261448798	an encoder
0.0260237705	a powerful
0.0259911052	the past
0.0258631771	model on
0.0258180866	system for
0.0257777953	a graphical
0.0257714926	the advantage of
0.0256982911	the structure of
0.0256649283	proposed system
0.0256630931	the usefulness
0.0256630931	the efficacy
0.0256405374	advantages of
0.0256241000	to generalize
0.0255877875	to help
0.0255557955	stages of
0.0254792067	an output
0.0254452020	order to
0.0253451501	the possible
0.0252721329	many of
0.0252627919	to establish
0.0252269807	the interior
0.0252179320	the intended
0.0251595788	a major
0.0251494464	the capability of
0.0251048659	optimized for
0.0250596364	location of
0.0250540626	a rich
0.0250436828	a probabilistic
0.0250000744	an augmented
0.0249757991	a statistical
0.0249550387	extended to
0.0247279325	influence of
0.0247190624	this area
0.0246957215	the results show
0.0246835225	variation of
0.0246728817	need for
0.0245601349	the latter
0.0244429485	a numerical
0.0242089839	to reveal
0.0241985462	for computing
0.0240666092	to allow
0.0240574348	to manipulate
0.0240175374	especially for
0.0240014380	for decades
0.0238678220	first time
0.0238260553	for generating
0.0237989574	the actual
0.0237369049	to measure
0.0235881957	generalization of
0.0235135922	method allows
0.0234509719	to edit
0.0234509719	to add
0.0232702169	a family of
0.0232173172	important for
0.0232132138	a finite
0.0231713177	surfaces with
0.0230744580	the minimum
0.0230488440	construction of
0.0229956370	to investigate
0.0229915807	feature of
0.0229237786	new algorithm
0.0228207605	changes in
0.0227462291	to develop
0.0226976395	a challenge
0.0226568040	to fill
0.0226548555	method to
0.0225856461	a strong
0.0224571320	a separate
0.0223707256	the amount of
0.0223192938	effective for
0.0222407141	a significant
0.0221971991	found in
0.0221755068	for creating
0.0221501021	to approximate
0.0220921566	utility of
0.0218710102	the characteristics of
0.0218708701	concept of
0.0218654035	purpose of
0.0218343009	issue of
0.0218014353	use of
0.0217598561	proposed by
0.0217543568	in fact
0.0217295225	the available
0.0217212145	a smooth
0.0216789088	network to
0.0216439176	a popular
0.0216354231	challenge in
0.0215777163	aim to
0.0215636654	network for
0.0215179278	to extend
0.0214899970	enough to
0.0214681716	interaction with
0.0214232636	a weighted
0.0214104493	superior to
0.0213922767	the estimated
0.0213388572	visualisation of
0.0212589958	model as
0.0212465584	the difficulty
0.0211757689	the efficacy of
0.0210680082	surfaces from
0.0210514984	with respect
0.0210497955	strategy for
0.0210272104	a simple and
0.0209617686	required for
0.0208380097	treatment of
0.0208265241	field of
0.0207666114	the need to
0.0206313283	for analyzing
0.0206146322	creation of
0.0206063937	to validate
0.0205847685	the core
0.0205506716	training of
0.0205179005	for synthesizing
0.0204940859	a theoretical
0.0204879375	new approach
0.0204391854	to preserve
0.0203046203	the best
0.0202697897	to analyze
0.0201793404	an rgb
0.0201344042	a lightweight
0.0200796501	scale of
0.0199971976	to match
0.0199767423	proposed for
0.0199420614	ability of
0.0199302598	terms of
0.0198405694	a variational
0.0198260011	to derive
0.0196927477	the normalized
0.0194764959	to exploit
0.0194632881	implemented in
0.0193134204	value of
0.0193021909	same time
0.0191744402	technique to
0.0191466788	so as
0.0190902679	to sample
0.0190529238	the observed
0.0189284526	in computer
0.0188803783	dataset with
0.0186949972	a hierarchical
0.0186778659	to combine
0.0185936396	parameterization of
0.0185811586	corresponding to
0.0185448673	pair of
0.0185260294	measure for
0.0184869205	the primary
0.0184789743	methodology to
0.0184272798	a long
0.0183881531	useful in
0.0183849725	power of
0.0182027515	case of
0.0181751241	to optimize
0.0181751241	to increase
0.0181577442	a typical
0.0181264110	a differentiable
0.0180928828	degree of
0.0180919186	a controlled
0.0180680649	to run
0.0179931869	dataset for
0.0179671917	available on
0.0179209466	to define
0.0178516333	to track
0.0178344708	a perceptual
0.0178189752	even for
0.0177693107	to illustrate
0.0177514168	addition to
0.0177116543	done in
0.0176912335	way of
0.0176629288	a consistent
0.0175993191	a spherical
0.0173792699	a regular
0.0173580879	performed in
0.0173531431	connectivity of
0.0173417323	to fit
0.0173277461	database of
0.0173025973	scenes with
0.0172701854	computed in
0.0172047507	means of
0.0171684525	described in
0.0171515050	a dense
0.0170016291	a different
0.0169951245	way to
0.0168116543	necessary to
0.0168068104	capability of
0.0168028393	change of
0.0167825700	image into
0.0167820115	effect of
0.0166991294	computation of
0.0166188633	a generic
0.0166160560	robustness of
0.0166148609	even with
0.0166050275	extraction of
0.0165882820	available for
0.0165381741	reduction in
0.0164766667	a critical
0.0164672407	a detailed
0.0164317066	to compare
0.0164180230	a coarse
0.0162157273	a clear
0.0162040031	a planar
0.0161461466	contrast to
0.0160894515	a unique
0.0159004614	aim of
0.0157331844	one or
0.0156268200	for visualizing
0.0155987207	architecture for
0.0155460294	introduced to
0.0154964025	efficiency of
0.0154214826	contribution of
0.0154006514	context of
0.0153522080	the above
0.0153382132	dataset of
0.0150503708	goal of
0.0150017175	benefits of
0.0149390223	the next
0.0149104893	possible to
0.0148328480	size of
0.0146242022	system on
0.0146165717	layout of
0.0143887749	especially in
0.0143810438	or more
0.0143673605	a way
0.0141074804	trained to
0.0136727857	important to
0.0135187809	available to
0.0133833568	need of
0.0127583534	to describe
0.0126727857	distance to
0.0126507211	two or
0.0121513919	interest in
0.0120956861	interface for
0.0120873485	to take
0.0120613330	work on
0.0120162792	allow for
