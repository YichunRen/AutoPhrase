0.9680236500	sina weibo
0.9674464930	stack overflow
0.9659091856	south african
0.9658213967	stock market
0.9636659995	supreme court
0.9629227809	indus script
0.9627401880	ted talks
0.9626826857	west african
0.9619380233	bipolar disorder
0.9617974250	diacritic restoration
0.9613115600	spell checker
0.9610594530	sexual harassment
0.9610455798	adverse drug reactions
0.9605274326	amazon mechanical turk
0.9603193621	message passing
0.9600163700	alzheimer's disease
0.9595956221	heart failure
0.9587594500	stochastic gradient descent
0.9585781832	majority voting
0.9582388358	eye movement
0.9580312037	electronic health record
0.9573714906	facial expression
0.9572730871	digital humanities
0.9567625320	adverse drug reaction
0.9566793254	suicidal ideation
0.9566061095	teacher forcing
0.9563979155	scheduled sampling
0.9559646922	lambda calculus
0.9555263589	clinical trials
0.9553657207	brazilian portuguese
0.9552468723	prepositional phrase attachment
0.9549836062	humanoid robot
0.9546508838	support vector machines
0.9546076058	microsoft cortana
0.9545200031	quantum mechanics
0.9544430978	european union
0.9542782765	random forest
0.9542539473	alexa prize
0.9541298810	variational autoencoder
0.9538026573	wall street journal
0.9536634834	variational autoencoders
0.9536409848	gibbs sampling
0.9536012922	densely connected
0.9535968014	united nations
0.9535067155	maximum likelihood estimation
0.9533670409	tensor product
0.9533339462	openai gpt
0.9531261819	african american
0.9527684632	keyword spotting
0.9527649074	edit distance
0.9527533426	virtual assistants
0.9524511963	breast cancer
0.9524097155	maximum likelihood
0.9523196351	collaborative filtering
0.9522962317	zipf's law
0.9521305187	mechanical turk
0.9519807891	icd coding
0.9519498219	morphological analyzer
0.9517863309	knowledge base
0.9516584887	locality sensitive hashing
0.9515130399	correlation coefficient
0.9512770992	quantifier scope
0.9512725967	lottery ticket
0.9511998552	lambek calculus
0.9510503177	random forests
0.9510405466	artificial intelligence
0.9508372923	mel spectrogram
0.9506325222	max pooling
0.9506076058	roget's thesaurus
0.9503719105	customer service
0.9502240161	tac kbp
0.9501903532	latent dirichlet allocation
0.9500704681	dirichlet allocation
0.9497921900	selectional restrictions
0.9496674263	reward shaping
0.9492551915	naive bayesian
0.9492045807	naive bayes
0.9491689059	conspiracy theories
0.9488687348	integer linear programming
0.9487130257	permutation invariant
0.9486992111	situational awareness
0.9486391496	urban dictionary
0.9484959519	random walk
0.9481743314	regular expression
0.9478526725	social sciences
0.9477924590	gated recurrent unit
0.9477833689	coreference resolution
0.9477510585	lexicalized tree adjoining
0.9477355312	maximum entropy
0.9475429471	voice cloning
0.9474973355	sliding window
0.9474164308	cultural heritage
0.9473997785	fine grain
0.9473759804	optimal transport
0.9473072317	visually grounded
0.9472418713	referential game
0.9472303098	radiology reports
0.9471081591	privacy policies
0.9471030198	massively multilingual
0.9470884289	mobile devices
0.9468990673	conditional random field
0.9468437633	clinical trial
0.9465635439	dimensionality reduction
0.9465272595	mel frequency cepstral coefficients
0.9465229499	voice conversion
0.9463262272	frobenius algebras
0.9461719570	plagiarism detection
0.9461056993	voice assistants
0.9460777036	categorial grammar
0.9459918654	wasserstein distance
0.9458577977	nearest neighbor
0.9458029662	episodic memory
0.9457992177	modern standard arabic
0.9457140029	song lyrics
0.9457051520	named entity
0.9455911620	penn treebank
0.9455056480	posterior collapse
0.9453549883	coreference resolvers
0.9451622528	reading comprehension
0.9450861794	discussion forums
0.9446747464	electronic health records
0.9445225008	nearest neighbors
0.9445130502	matrix factorization
0.9443307175	rumour stance
0.9442582491	logistic regression
0.9442076915	morphological inflection
0.9441537402	differential privacy
0.9439982537	multilayer perceptron
0.9438051286	constraint logic programming
0.9438001210	weakly supervised
0.9437946510	optical character recognition
0.9437671966	reinforcement learning
0.9437602703	icd codes
0.9436445415	latin script
0.9435864916	high school
0.9435233542	euphonic conjunctions
0.9435229745	american sign language
0.9434015313	singing voice
0.9433598790	experience replay
0.9432580408	spurious correlations
0.9432465376	noun compound
0.9431871093	relation extraction
0.9431869623	fuzzy logic
0.9431728235	slot filling
0.9431352134	multiword expressions
0.9429429130	amazon alexa
0.9427816721	gradient descent
0.9427587543	emergent communication
0.9427471281	cycle consistency
0.9425454160	gaussian mixture
0.9424327299	proof nets
0.9423623518	turing test
0.9423396112	egyptian arabic
0.9423262099	distant supervision
0.9423026656	connectionist temporal classification
0.9422297164	mental health
0.9421262202	stack exchange
0.9420611783	word sense disambiguation
0.9420031026	named entity recognition
0.9417446624	nearest neighbour
0.9414804598	bilingual lexicon induction
0.9412326057	authorship attribution
0.9411821065	electronic medical records
0.9409915748	lexically constrained
0.9409129845	surface realization
0.9407896674	inflectional morphology
0.9407526106	stock price
0.9406519514	high fidelity
0.9406206704	ms marco
0.9405934844	tensor decomposition
0.9405886300	emotional arcs
0.9405103693	minimum bayes risk
0.9402515510	author profiling
0.9400465387	ms coco
0.9399729537	regular expressions
0.9397661157	combinatory categorial grammar
0.9396289632	question answering
0.9396006080	beam search
0.9395773872	spelling correction
0.9395570049	referring expression
0.9394735603	tensor factorization
0.9393028413	knowledge bases
0.9392940146	disfluency detection
0.9390424464	random walks
0.9389297752	bias mitigation
0.9386978997	essay scoring
0.9386519122	presidential election
0.9386003917	pushdown automata
0.9385063602	upper sorbian
0.9384843907	raw waveform
0.9383631905	exact match
0.9383031064	factual correctness
0.9382437449	differentially private
0.9382394029	parameter sharing
0.9380499758	referring expressions
0.9379854792	command line
0.9379459541	climate change
0.9379282852	finite state automata
0.9377935332	levenshtein distance
0.9377566087	nonparametric bayesian
0.9377259540	sequence labeling
0.9377020488	reverse engineering
0.9375706426	noun compounds
0.9375374459	visual genome
0.9373930393	common crawl
0.9371533116	software engineering
0.9371131830	bilingual dictionary induction
0.9370224247	conditional random fields
0.9369516810	subject matter
0.9369230749	morphological reinflection
0.9368430660	pointwise mutual information
0.9367715955	shortest path
0.9367346063	vector space
0.9367121819	latent variable
0.9366974556	roman urdu
0.9366384055	theorem proving
0.9365023410	cold start
0.9364604581	byte pair encoding
0.9363726680	mel frequency cepstral
0.9363522563	optimality theory
0.9360970102	textual entailment
0.9360133597	abstractive summarization
0.9359509171	exponential family
0.9359482167	personal assistants
0.9357007966	false positive
0.9356880561	lip reading
0.9356477308	hidden markov model
0.9356088416	privacy preserving
0.9355206888	denoising autoencoder
0.9354211030	depressive symptoms
0.9353438638	cnn daily mail
0.9353173150	sequence labelling
0.9353131267	discharge summaries
0.9352627130	societal biases
0.9352561608	amortized variational
0.9351961853	constraint satisfaction
0.9351874754	power law
0.9349543520	pathology reports
0.9349280353	personality traits
0.9347954458	minimally supervised
0.9347019893	finite state transducers
0.9346842009	amazon alexa prize
0.9346820715	elastic weight consolidation
0.9346476585	expectation maximization
0.9346155493	handwriting recognition
0.9345537060	microsoft coco
0.9344278559	brain activity
0.9343031072	instructional videos
0.9342851400	recurrent neural network
0.9342167636	scientific publications
0.9340983498	grammar induction
0.9339917513	noise contrastive estimation
0.9339737562	cosine similarity
0.9338639586	linear logic
0.9338166416	data augmentation
0.9337262815	typed feature structures
0.9335633556	noun phrases
0.9335284214	lattice rescoring
0.9334702587	compact closed
0.9331524978	political ideology
0.9330358982	string kernels
0.9329716198	knowledge graphs
0.9329400279	stanford sentiment treebank
0.9328833982	google translate
0.9328441591	nigerian pidgin
0.9328216038	personality trait
0.9327423293	social networking
0.9326807312	test suite
0.9325255955	argumentation mining
0.9324802702	wavenet vocoder
0.9324284827	evaluation understudy
0.9323939770	skip connections
0.9322649937	morphologically rich
0.9321539749	gated recurrent units
0.9319626605	dialect identification
0.9318533158	description logics
0.9318246015	tree adjoining grammars
0.9317300235	hidden markov models
0.9316442692	nucleus sampling
0.9316415985	hilbert space
0.9315304966	dynamic programming
0.9313661229	globally normalized
0.9311209763	combinatory categorial
0.9307376743	rhetorical structure theory
0.9306199355	theorem prover
0.9305822176	sentiment analysis
0.9305820944	opinion mining
0.9305750968	connectionist temporal
0.9305216600	social media
0.9305038056	feature selection
0.9304814769	super characters
0.9304364188	image captioning
0.9302195590	shared task
0.9301373457	weak supervision
0.9299590965	pos tagger
0.9298937909	gumbel softmax
0.9298775210	finite state transducer
0.9298728254	mildly context sensitive
0.9297439313	claim verification
0.9296622355	low latency
0.9296386220	clinical notes
0.9296140838	satirical news
0.9296075633	iterative refinement
0.9294613877	trec covid
0.9292255685	dependency parsing
0.9291308588	generative adversarial nets
0.9291276854	writing styles
0.9291052455	attempto controlled english
0.9291028893	gender stereotypes
0.9290510152	named entities
0.9289428800	spoken term detection
0.9289249071	search engine
0.9288114776	policy makers
0.9285990441	gaze behaviour
0.9285050100	exposure bias
0.9284606477	united states
0.9284551212	information retrieval
0.9284523392	monte carlo
0.9284330586	batch normalization
0.9282906564	statistical significance
0.9282833205	computational linguistics
0.9282654048	cohen's kappa
0.9279842624	distantly supervised
0.9279784460	conversational telephone speech
0.9279437790	abductive reasoning
0.9279113026	copying mechanism
0.9278484474	movie subtitles
0.9277953450	active learning
0.9277256060	imdb movie
0.9275109116	science exam
0.9274940762	analogical reasoning
0.9273520486	anti spam
0.9273465328	noun phrase
0.9273344009	early warning
0.9271069705	grammatical error correction
0.9266579880	broad coverage
0.9264816416	selectional preferences
0.9263747905	swiss german
0.9263670700	inverse reinforcement
0.9262370244	decision trees
0.9261850979	definite clause
0.9261758506	convolutional neural networks
0.9261117944	equal error rate
0.9260487803	pointer generator
0.9259605290	dinner party
0.9258971854	pubmed abstracts
0.9258255299	nist sre
0.9257294854	citation recommendation
0.9256420385	plot synopses
0.9255083970	decision tree
0.9254496755	ordinally forgetting
0.9254057951	lessons learned
0.9253676479	gradient boosting
0.9253626793	dialog act
0.9253442482	weight sharing
0.9253285887	game theoretic
0.9252437483	pos tags
0.9251738875	elementary discourse units
0.9251635191	past tense
0.9251147364	pearson correlation
0.9249121393	automatic speech recognition
0.9248867907	deep learning
0.9248098250	filter bank
0.9247928492	support vector machine
0.9247691039	tree adjoining grammar
0.9245713072	pos tagging
0.9244329221	idiomatic expressions
0.9242988204	dynamic routing
0.9241547529	fake news
0.9241286765	story ending
0.9241102430	blog posts
0.9240279729	customer reviews
0.9239006109	sql queries
0.9238635229	micro blog
0.9236498930	abusive behavior
0.9236470996	pronoun resolution
0.9236465915	markov chain
0.9236373887	minimalist grammars
0.9236187254	code switching
0.9235852837	machine reading
0.9235554392	genetic algorithm
0.9234842925	link prediction
0.9233639160	matrix multiplication
0.9233042174	cognitive impairment
0.9231334776	adversarial attacks
0.9230899835	synonym discovery
0.9230815598	bandit feedback
0.9229622514	coronavirus disease
0.9229096481	suicide risk
0.9228116469	google assistant
0.9225980973	euclidean space
0.9225829789	discourse markers
0.9225592131	early stage
0.9224290715	classical chinese poetry
0.9223317982	constituency trees
0.9223196558	late fusion
0.9223017302	density matrices
0.9221885702	vocal tract
0.9221293708	generative adversarial networks
0.9220462619	linguistically informed
0.9219529907	bidirectional lstm
0.9218777716	long short term memory
0.9218605243	dysarthric speech
0.9217688146	deception detection
0.9214236641	mover's distance
0.9213688807	sense inventory
0.9212422125	knowledge graph
0.9211895280	greedy decoding
0.9211452442	paraphrase identification
0.9211006289	disease outbreaks
0.9210141995	neuro symbolic
0.9209559793	source separation
0.9208983407	semantic role
0.9208656198	dynamic oracle
0.9208552987	tv series
0.9208467189	scientific papers
0.9207947670	cnn dailymail
0.9207304062	machine reading comprehension
0.9206604746	dialogue act
0.9206397920	minimum description length
0.9205949090	speech recognition
0.9205470996	anaphora resolution
0.9204312341	zipf mandelbrot
0.9202659478	xlm roberta
0.9201160581	latent dirichlet
0.9201074237	fact checking
0.9199937848	news articles
0.9199426260	cognitive science
0.9198880750	multinomial naive bayes
0.9198325425	derivational morphology
0.9198267530	recurrent neural networks
0.9198217838	ccg supertagging
0.9197515936	digital assistants
0.9196830049	indo european
0.9195988062	capsule networks
0.9195720211	consumer health
0.9195142039	pronominal anaphora
0.9194290580	principal component analysis
0.9193908004	test suites
0.9193578849	mobile phones
0.9193238781	transitive verbs
0.9191388347	minimum risk training
0.9188620600	negative sampling
0.9187165330	machine translation
0.9183990114	bilinear pooling
0.9181931057	hate speech
0.9181904089	evaluation campaign
0.9181104793	dialogue management
0.9180897730	scene graphs
0.9180348187	breaking news
0.9179444618	topic modeling
0.9178330688	word embedding
0.9177584720	cyberbullying detection
0.9176809534	rumor detection
0.9176018161	user simulator
0.9175896675	doctor patient
0.9175524957	logical forms
0.9175369808	denoising autoencoders
0.9175199373	knowledge distillation
0.9175193569	copy mechanism
0.9174921524	domain adaptation
0.9174288413	linguistic typology
0.9173130260	voice activity detection
0.9171053451	universal dependencies
0.9170412974	winograd schema
0.9168872174	label smoothing
0.9167948982	commonsense reasoning
0.9167944268	music genre
0.9167451842	abstract meaning representation
0.9165928774	extractive summarization
0.9165881463	answer selection
0.9165687909	canonical correlation analysis
0.9164052457	public health
0.9163462339	entity linking
0.9162568709	scaling laws
0.9162217934	ticket booking
0.9162200413	source code
0.9161219395	racial bias
0.9161198532	mimic iii
0.9159576298	teacher student
0.9159340464	singing synthesis
0.9159173761	naming game
0.9158500161	cocktail party
0.9158329863	user interface
0.9157396423	dialogue acts
0.9157023756	disease diagnosis
0.9156947567	neural networks
0.9156577213	reference resolution
0.9156293990	historical linguistics
0.9156257562	referential games
0.9156129934	hotel reviews
0.9156018987	finite state
0.9154717108	agglomerative clustering
0.9154353732	opinion spam
0.9153761735	hyperbolic space
0.9152329831	logic programming
0.9152316039	adverse drug
0.9151639402	tree adjoining
0.9147711373	unit discovery
0.9146901976	sql query
0.9146723738	pitch accent
0.9146183533	dependency parsers
0.9145322465	weighting schemes
0.9145304738	parallel corpus
0.9145040577	big data
0.9144486067	reverse dictionary
0.9144208065	dialectal arabic
0.9142832063	roc auc
0.9142216503	human parity
0.9142157900	turing machine
0.9141760318	context free grammar
0.9141566757	spatio temporal
0.9141301029	cmu mosei
0.9141129114	autoregressive predictive coding
0.9139956767	error rate
0.9139705229	long tail
0.9139686657	google books
0.9139364588	fully connected
0.9138760633	fact checkers
0.9138644260	catastrophic forgetting
0.9138089779	cross entropy
0.9137414547	patent claim
0.9137194327	response generation
0.9135388998	multi genre broadcast
0.9134821093	action spaces
0.9133304265	cnn dm
0.9133057564	speaker verification
0.9131527172	graph convolutional networks
0.9129857116	knowledge base completion
0.9129234662	abuse detection
0.9128750322	text mining
0.9127990636	categorical compositional distributional
0.9127898251	lf mmi
0.9127862959	check worthy
0.9126960794	belief tracking
0.9126367870	semantic role labeling
0.9125304375	vector spaces
0.9125068545	natural language
0.9124230975	sequence transduction
0.9123506549	forward backward
0.9122927254	factual consistency
0.9121250825	white box
0.9120509401	dictionary definitions
0.9119236148	keyphrase extraction
0.9118988840	slot tagging
0.9118631194	linear discriminant analysis
0.9118309450	cyber security
0.9116070794	max margin
0.9115553065	kl divergence
0.9114401957	vq vae
0.9113586551	product titles
0.9112752640	court decisions
0.9112496938	masked language modeling
0.9112409041	short term memory
0.9111412926	lexical functional grammar
0.9110515820	structured prediction
0.9109344607	scaled dot product
0.9106715010	low rank
0.9105760448	categorial grammars
0.9104458969	online shopping
0.9103693356	customer support
0.9103521552	news headlines
0.9101619007	convolutional neural network
0.9100902415	bilstm crf
0.9100268269	inductive biases
0.9099097711	actor critic
0.9098706380	noisy channel
0.9098666968	subword units
0.9098613422	cosine distance
0.9098179271	bidirectional lstms
0.9096873896	answer set programming
0.9096813850	customer feedback
0.9096478985	cycle consistent
0.9096081519	code mixing
0.9096073670	weighting scheme
0.9095135166	imitation learning
0.9094926314	graphical user interface
0.9094393184	variational inference
0.9093516999	scientific articles
0.9092874762	transfer learning
0.9092738972	indian languages
0.9092458140	shallow fusion
0.9092120801	factoid questions
0.9090384415	verb phrase
0.9089391465	machine comprehension
0.9086820420	distributional semantics
0.9086667037	major obstacle
0.9086462913	relative clauses
0.9086420790	user interfaces
0.9086292351	hidden markov
0.9086104581	shift reduce
0.9085204190	scholarly articles
0.9082794422	np complete
0.9082772867	conversational agents
0.9081934853	semantic similarity
0.9081835999	image caption
0.9080794998	humor detection
0.9080276436	machine learning
0.9080106604	scene graph
0.9079896846	mutual information
0.9079686367	pos tag
0.9079306066	neural network
0.9079164907	low resource
0.9078476684	protein protein interaction
0.9077430893	determinantal point
0.9077052406	human trafficking
0.9076909208	anomaly detection
0.9076342473	web page
0.9074230144	macro f1
0.9074131456	intensive care
0.9073964401	phrase table
0.9073784237	chit chat
0.9073238969	mandarin chinese
0.9072919679	intent detection
0.9072912821	check worthiness
0.9072498178	wall street
0.9072439406	health care
0.9071851214	cue phrases
0.9071496416	event extraction
0.9071148461	kullback leibler divergence
0.9069737961	amazon reviews
0.9069709240	predictive coding
0.9068292140	hate speech detection
0.9067677680	combinatorial optimization
0.9067214912	posterior regularization
0.9067060934	speaker recognition
0.9066791658	discontinuous constituency
0.9066459170	constituency parsing
0.9065373850	linguistically motivated
0.9064597777	cmu mosi
0.9062480570	broadcast news
0.9061979185	pretrained transformers
0.9061762202	chinese poetry
0.9061433832	web services
0.9061196058	mixed initiative
0.9058846222	error correction
0.9058673373	natural language processing
0.9058010605	indo aryan
0.9056635906	user satisfaction
0.9055538396	movie scripts
0.9055376297	prepositional phrase
0.9053326750	suggestion mining
0.9052277543	web service
0.9052136705	short answer grading
0.9051989312	speaker diarisation
0.9051554024	skip gram
0.9050954066	peer review
0.9050392898	stance detection
0.9049015856	markov chains
0.9048252813	bidirectional long short term memory
0.9048113170	irony detection
0.9045907231	authorship verification
0.9045893228	receptive field
0.9045722663	african languages
0.9045338459	variational auto encoders
0.9043204717	propaganda detection
0.9043032617	shortest dependency path
0.9042296030	state tracking
0.9041722144	abstractive text summarization
0.9041184301	sponsored search
0.9041157982	sequence tagging
0.9034374696	bilingual dictionaries
0.9032896310	finite automata
0.9032273254	drug drug
0.9032237868	large vocabulary continuous speech recognition
0.9031239007	common sense
0.9030615167	commonsense knowledge
0.9028851858	aspect term extraction
0.9026805951	semantic parsing
0.9025745035	emotion recognition
0.9024474245	digital libraries
0.9023770340	recognizing textual entailment
0.9023401662	synthetically generated
0.9023003415	policy gradient
0.9022822010	post editing
0.9022714499	dependency parser
0.9022539147	cross modal
0.9022228898	round trip
0.9022068482	multi layer perceptron
0.9020741133	restaurant reviews
0.9020695080	visual storytelling
0.9020572860	movie review
0.9020312633	customer care
0.9020192442	word senses
0.9019820014	mitigating gender bias
0.9019378254	long short term
0.9019178592	subword segmentation
0.9018966396	patient notes
0.9018026915	dialogue state tracking
0.9014393807	phylogenetic inference
0.9013739953	public opinion
0.9012864818	web pages
0.9012601496	vqa cp
0.9011726074	movie reviews
0.9011422943	generative adversarial network
0.9011251294	future directions
0.9011023571	user engagement
0.9009336219	inflected forms
0.9009229638	wordnet synsets
0.9008275968	clarifying questions
0.9008127681	pos taggers
0.9007515080	worst case
0.9006356703	trending topics
0.9005804549	information extraction
0.9005571635	coreference chains
0.9004815988	jensen shannon
0.9004113158	state tracker
0.9003829292	speaker diarization
0.9003146362	logical form
0.9002537469	scientific literature
0.9002494395	activation functions
0.9002048122	batch sizes
0.9001426845	continuous space
0.9001050550	spam detection
0.9000785180	adversarial attack
0.9000475596	tf idf
0.8998598058	video description
0.8997972007	lower bound
0.8996927729	semi autoregressive
0.8996544334	fact checked
0.8995917388	performs competitively
0.8994736901	general purpose
0.8993462153	social media posts
0.8992459091	variational auto encoder
0.8992021074	prosodic morphology
0.8988647107	word embeddings
0.8986786758	factoid question answering
0.8986484622	artificial neural networks
0.8985510727	dual encoder
0.8985045862	multi party
0.8983379242	small footprint
0.8983128369	style transfer
0.8981660493	user feedback
0.8981088502	adversarial training
0.8980845030	speech repairs
0.8980554618	causal effects
0.8978075909	visual question answering
0.8977824024	contextualized word embeddings
0.8977321970	persona chat
0.8976579712	distance metric
0.8976549391	positional encoding
0.8976240100	micro f1
0.8973638504	silver standard
0.8972530559	context free grammars
0.8972337464	political science
0.8971332194	indic languages
0.8971192751	recurrent neural network transducer
0.8967145086	photo realistic
0.8966462151	headline generation
0.8966427769	fundamental frequency
0.8966415937	cultural evolution
0.8964870696	semantic relatedness
0.8964736986	multi talker
0.8964458296	cepstral coefficients
0.8964431230	skip thought
0.8964076451	turn taking
0.8963843214	prohibitively expensive
0.8963734457	unanswerable questions
0.8963309203	relevance feedback
0.8962294407	paragraph vector
0.8962108990	abstractive summaries
0.8961962304	taxonomy induction
0.8961278805	word mover's distance
0.8960851144	latent semantic analysis
0.8960823037	inductive bias
0.8960631235	arabic script
0.8960366790	pretrained language models
0.8959363832	relative position
0.8959133207	e2e asr
0.8958876167	participating teams
0.8958769099	compositional distributional
0.8958633014	speech synthesis
0.8957802723	left corner
0.8957384037	policy optimization
0.8957158905	black box
0.8956887944	video captioning
0.8955790629	grammatically correct
0.8955572860	hypernymy detection
0.8955180973	supporting facts
0.8955086582	semi markov
0.8954332826	sign language
0.8953271382	mortality prediction
0.8952808095	audio visual scene aware dialog
0.8951893945	accented speech
0.8950203524	kneser ney smoothing
0.8949609014	dialog state tracking
0.8949031741	morphologically rich languages
0.8948926386	transformer xl
0.8948861792	small footprint keyword spotting
0.8948752793	pointer network
0.8948572625	intent classification
0.8948509818	floating point
0.8948035452	dependency treebank
0.8947996997	dot product
0.8947860939	natural language inference
0.8947790893	depression detection
0.8947722073	formal semantics
0.8947194309	readability assessment
0.8946864353	singing voice conversion
0.8946582098	online discussions
0.8946567630	product reviews
0.8946210463	semi supervised
0.8945815473	deep neural networks
0.8945720106	movie description
0.8944710225	hidden state
0.8944105337	tag sets
0.8943426575	convolutional networks
0.8941572350	text simplification
0.8941143148	error recovery
0.8939648419	capsule network
0.8939637810	multi tape
0.8939090177	online communities
0.8938990923	trf lms
0.8938368338	word segmentation
0.8937751090	entity type
0.8936984841	speaker's voice
0.8936848217	complex cepstrum
0.8936404232	eye tracking
0.8936234601	clinical narratives
0.8936071474	en fr
0.8935959413	code mixed
0.8935376654	spoken language
0.8934395027	multiple choice
0.8933999336	peer reviews
0.8933932677	mode collapse
0.8933402816	latent spaces
0.8932411169	personal narratives
0.8931968598	single pass
0.8931922326	european parliament
0.8931733884	higher order
0.8931662358	sentence boundary
0.8931657015	posterior probabilities
0.8931389539	mini batch
0.8930391570	wikipedia pages
0.8930262937	query expansion
0.8928814631	weight matrices
0.8928803418	discourse coherence
0.8928338621	phone recognition
0.8927944518	crowd workers
0.8927773782	prior art
0.8927507627	linear regression
0.8926944917	gaussian process
0.8926208501	product title
0.8925938899	unification grammars
0.8925033768	null hypothesis
0.8924360951	passage retrieval
0.8923796400	media outlets
0.8923545826	ted lium
0.8922031345	visually impaired
0.8921470017	bayesian optimization
0.8920508229	event trigger
0.8919088290	chart parsing
0.8918677282	digital library
0.8918218908	lifelong learning
0.8918063064	correlation coefficients
0.8916217406	long range
0.8915743635	sarcasm detection
0.8914201604	open access
0.8913914832	vanishing gradient
0.8913621624	keyword extraction
0.8913256438	gold standard
0.8912966754	social networks
0.8911861530	percentage points
0.8911771786	systematic generalization
0.8910445705	medical imaging
0.8910299215	annotation artifacts
0.8910155403	encoder decoder
0.8909691797	biomedical literature
0.8908910721	years ago
0.8908194173	untranscribed speech
0.8907962540	inverse document frequency
0.8907640209	semantic textual similarity
0.8906212289	raw audio
0.8906087667	data mining
0.8906020865	contextualized word representations
0.8905067515	dialog history
0.8904635486	class imbalance
0.8904442699	19th century
0.8903902438	parametric speech synthesis
0.8900989604	average precision
0.8900910145	annotation schema
0.8900432270	open source
0.8899358440	acoustic modeling
0.8898671701	adversarial examples
0.8898668072	event detection
0.8898259074	open domain
0.8898052760	majority vote
0.8897643228	paramount importance
0.8897139263	suicide notes
0.8895694699	paragraph vectors
0.8894904436	random fields
0.8894824110	dialogue manager
0.8894727113	distributional thesaurus
0.8894261838	discourse connectives
0.8893911871	hyperparameter tuning
0.8892431501	short stories
0.8892143809	protein protein interactions
0.8892045293	propaganda techniques
0.8891911683	hierarchical dirichlet process
0.8891210703	rnn transducer
0.8890463322	speech acts
0.8890169980	masked language model
0.8889857168	sentence pair
0.8889666087	voynich manuscript
0.8889654378	knowledge graph completion
0.8889206872	linear chain
0.8889194430	factoid question
0.8888645482	kullback leibler
0.8888550442	bilingual lexicons
0.8887534339	entity typing
0.8886709669	overlapped speech
0.8886650956	bridging anaphora resolution
0.8884914686	recurrent unit
0.8884788148	semantic orientation
0.8884462688	siamese networks
0.8883909262	pragmatic inferences
0.8883010606	adjective noun
0.8882835812	chinese word segmentation
0.8882685425	gender bias
0.8882388488	i2b2 va
0.8881772958	cross situational
0.8879219986	quality estimation
0.8878604674	wmt14 en
0.8878436943	upper bound
0.8878023627	machine readable
0.8877576554	recent advances
0.8877195669	programming language
0.8876908091	wide coverage
0.8876732421	context sensitive
0.8876408194	bilingual dictionary
0.8876027852	randomly initialized
0.8875619533	fact verification
0.8874453598	pointer networks
0.8874417175	american english
0.8872724841	affective computing
0.8872483433	information theoretic
0.8872233122	information seeking
0.8872137761	argument mining
0.8871770011	novelty detection
0.8871335064	software development
0.8871274494	clickbait detection
0.8869751786	natural language understanding
0.8869747468	constituent trees
0.8869645482	diacr ita
0.8869638790	punctuation marks
0.8869533836	bioasq challenge
0.8869276297	emotion intensity
0.8868858259	lexical acquisition
0.8868056579	dialog acts
0.8867708983	crowd powered
0.8867420559	pattern matching
0.8867239764	speech recognizers
0.8867019047	set expansion
0.8866827929	poorly understood
0.8866753087	great promise
0.8865512805	conversational ai
0.8865386976	slot values
0.8865143406	computationally efficient
0.8864894594	disaster response
0.8864580320	wikipedia articles
0.8864230348	proper nouns
0.8864059193	auto encoding
0.8863856585	msr vtt
0.8863113902	daily lives
0.8862978815	kneser ney
0.8862694362	grammatical gender
0.8861967251	numerical reasoning
0.8861692749	decision lists
0.8861262322	normal form
0.8860801839	word sense induction
0.8860520203	dialog management
0.8860452526	knowledge base construction
0.8859091350	inter rater
0.8858622608	word order
0.8857982873	computationally intensive
0.8857801450	programming languages
0.8857670851	false positives
0.8857587616	term frequency inverse document frequency
0.8855920094	naturally occurring
0.8855427665	multi modal
0.8854336932	vector quantized
0.8853976515	language modeling
0.8853964657	mutual exclusivity
0.8853650501	writing style
0.8853550745	labor intensive
0.8852507854	spoken language understanding
0.8852167571	test collections
0.8851732309	dialogue systems
0.8851662765	winograd schema challenge
0.8851044003	bi gru
0.8850416595	cross domain
0.8849710740	graph convolutional network
0.8848159769	crowd sourced
0.8847301242	user experience
0.8847263517	medical codes
0.8846364832	navigation instructions
0.8845638311	query logs
0.8845016776	schema induction
0.8844793082	dependency trees
0.8844279836	keyphrase generation
0.8844217928	random seeds
0.8843363319	gold standards
0.8841928693	proper names
0.8840895815	tree bank
0.8840033724	low resourced
0.8839597631	personal attacks
0.8839572274	working memory
0.8839253261	financial news
0.8838983747	user preferences
0.8837222820	clinical records
0.8836564665	bounding box
0.8836257938	daily mail
0.8835353786	entity recognition
0.8834615131	continual learning
0.8833858416	linear transformation
0.8833395008	sentiment classification
0.8832978815	pitman yor
0.8832837741	boundary detection
0.8831900071	hyper parameter
0.8831678166	polarity lexicons
0.8831553505	la langue
0.8831550955	word sense
0.8831486595	speech enhancement
0.8831465201	sentiment polarity
0.8831343162	visual grounding
0.8830267149	language acquisition
0.8828936366	glottal flow
0.8828800456	python package
0.8828651623	news stories
0.8828530665	amazon mechanical
0.8828525029	speech act
0.8827110012	wake word
0.8827004168	crowd sourcing
0.8826896029	arabic dialects
0.8826462812	rhetorical structure
0.8825551533	slavic languages
0.8825473289	large scale
0.8825064889	dimension reduction
0.8824294358	cloze test
0.8823802624	typologically diverse
0.8823684985	multi hop
0.8822038736	batch size
0.8821935682	review rating prediction
0.8820728594	decision support
0.8819565575	statistical machine translation
0.8817796255	paraphrase generation
0.8817566192	web tables
0.8816537626	multitask learning
0.8815101738	exponential growth
0.8814850747	temporal ordering
0.8814275285	lexical entries
0.8813032181	text categorization
0.8812207588	long tailed
0.8811489697	code switched
0.8810835482	dnn hmm
0.8810685196	large margin
0.8810444907	term frequency
0.8810077650	modified kneser ney
0.8809571530	predicate argument structure
0.8808139854	straight forward
0.8807597097	indirect supervision
0.8807304931	classical chinese
0.8806935882	2nd place
0.8806718679	mental states
0.8805904649	street journal
0.8805585609	protein protein
0.8804228378	word error rate
0.8803922192	recurrent neural
0.8803673784	emphasis selection
0.8802773689	medical records
0.8802491881	noise reduction
0.8802341718	federated learning
0.8800882089	relation linking
0.8800113522	decision making
0.8799356629	artificially generated
0.8799308859	networking sites
0.8798788739	projective dependency
0.8798124747	vaw gan
0.8796988277	document collections
0.8796834811	restaurant reservation
0.8796305931	vector cosine
0.8796186378	pronunciation lexicon
0.8794929662	image captions
0.8794655717	embedding spaces
0.8792972307	link grammar
0.8792829482	bi directional
0.8792158819	recurrent networks
0.8791057268	context dependent
0.8790923959	children's speech
0.8789978991	attachment score
0.8789672453	spelling error correction
0.8789426191	memory augmented
0.8788059961	success rate
0.8786575404	contextualized embeddings
0.8784776215	facial expressions
0.8783911228	multi faceted
0.8783647942	agglutinative languages
0.8783520656	fingerspelling recognition
0.8783173943	layer normalization
0.8782826929	ad hoc
0.8782004788	virtual agents
0.8781077383	domain specific
0.8781048105	visual dialog
0.8780860835	book reviews
0.8779688242	auto completion
0.8779354908	stylistic variation
0.8779233752	widely accepted
0.8778382587	starting point
0.8776225238	amr parsing
0.8774770955	multi granularity
0.8774239583	short texts
0.8771469130	label propagation
0.8769666514	word vectors
0.8769314985	log linear
0.8769280314	lexical cohesion
0.8769060054	paraphrastic sentence
0.8768899602	black box attacks
0.8768333992	error reduction
0.8767248806	radiology report
0.8767150527	toxic comments
0.8766144989	curriculum learning
0.8766126991	romance languages
0.8765696985	awd lstm
0.8765442005	model agnostic meta learning
0.8765053025	roman script
0.8764042677	tabular data
0.8763941258	energy consumption
0.8763706653	span extraction
0.8763430694	likelihood ratio
0.8763411288	quality assessment
0.8763103777	gaussian processes
0.8762792121	graph convolution
0.8761792674	proof theoretic
0.8758856670	light weight
0.8758542968	shallow parsing
0.8756376559	clarification questions
0.8755372604	feature engineering
0.8751946081	fuzzy clustering
0.8751547345	arabic dialect identification
0.8750169818	pr auc
0.8750051816	attention heads
0.8749686917	predictive power
0.8747958626	phoneme recognition
0.8747690995	aspect category
0.8747177381	latent space
0.8746664401	multi layered
0.8746126070	chart parser
0.8746095963	lower bounds
0.8745855820	stochastic gradient
0.8745832673	single channel
0.8745533530	log likelihood
0.8743413947	short text
0.8740824944	auto encoder
0.8740017585	domain shift
0.8739980635	math word problems
0.8738996677	probabilistic graphical
0.8738685498	external knowledge
0.8738510460	tree structures
0.8737782470	grammatical error
0.8736704513	video clips
0.8736684056	semantic roles
0.8735633649	black boxes
0.8732759722	privacy policy
0.8732551084	machine translated
0.8732432388	parallel corpora
0.8732321927	penn discourse
0.8727993962	lexical chains
0.8727192702	math word problem
0.8727171103	schema guided
0.8727104273	feature extractor
0.8727067742	case based reasoning
0.8726873227	dialog policies
0.8726315891	syntax aware
0.8726041132	driving force
0.8725765546	knowledge transfer
0.8724869212	speaking style
0.8724097908	memory footprint
0.8723129859	mental lexicon
0.8722369661	relation extractors
0.8722090313	margin softmax
0.8720941287	kg completion
0.8720229645	health records
0.8719675735	chinese character
0.8719475732	byte pair
0.8718358852	star rating
0.8717742117	topological data analysis
0.8717575754	cross lingual
0.8716195455	conversational agent
0.8715977054	spontaneous speech
0.8715906322	emergent languages
0.8714126863	bi lstm
0.8714083318	object oriented
0.8714065664	electronic health
0.8712783783	semantic change
0.8710382168	context aware
0.8709894928	recommender systems
0.8709641549	language understanding
0.8708853705	question answer pairs
0.8707642312	user generated content
0.8706488060	medical notes
0.8705450571	constituent parsing
0.8705231823	deep neural network
0.8705127783	subject verb
0.8703600248	recurrent units
0.8702756708	relationship extraction
0.8701739561	entity disambiguation
0.8700577714	representational power
0.8700510596	fair comparison
0.8700499498	question generation
0.8700493337	document retrieval
0.8697611969	microsoft research
0.8697117958	semi structured
0.8696261161	keyword search
0.8695578910	video clip
0.8695569596	specially designed
0.8694522479	generative adversarial
0.8693553096	medical concept
0.8692872926	literary texts
0.8692109376	subword level
0.8691902709	independence assumption
0.8691712627	largely unexplored
0.8691158350	grammar checking
0.8691001356	facebook posts
0.8690657720	language identification
0.8688741103	softmax layer
0.8688738572	word mover's
0.8687526313	fake news detection
0.8687316019	distributed word representations
0.8687089697	chemical disease
0.8685807198	sense disambiguation
0.8685564930	intermediate layers
0.8685528276	progress notes
0.8685421501	fall short
0.8685306424	compositional generalization
0.8685208403	inter sentential
0.8685207000	annotation tool
0.8684686727	image description
0.8684222905	feed forward
0.8683888082	deep residual
0.8683590509	text infilling
0.8682747435	subject predicate
0.8682282092	probability distribution
0.8682131879	goal oriented
0.8680285052	locally normalized
0.8679661439	key phrase
0.8679062684	multilingual bert
0.8678464940	youtube videos
0.8678152978	feature extraction
0.8677776824	image descriptions
0.8675677620	closed form
0.8674956806	evidence based medicine
0.8674901400	content selection
0.8672566721	sentence compression
0.8671425212	multi channel
0.8670370897	user intent
0.8669925031	auto encoders
0.8669273100	predicate argument
0.8668936036	sentence encoders
0.8668304445	spoken dialogue systems
0.8668021759	20th century
0.8667634937	endangered languages
0.8665911470	semantic parsers
0.8665849231	lexical semantic change detection
0.8665532824	human robot interaction
0.8664963465	deep reinforcement learning
0.8663583965	spanning tree
0.8663373007	recall rate
0.8663309662	code completion
0.8662886305	speaker separation
0.8660606661	sentence splitting
0.8660438933	explicitly stated
0.8660109798	procedural knowledge
0.8659204697	universal schema
0.8659192962	conditional variational autoencoder
0.8658474675	residual connections
0.8658168158	element wise
0.8658044448	social distancing
0.8656303991	emoji prediction
0.8653902964	key phrases
0.8653871950	meeting summarization
0.8653700631	semantic hashing
0.8653239759	speech recognizer
0.8652465397	discriminant analysis
0.8651047038	response selection
0.8650563278	prior polarity
0.8650470051	acoustic modelling
0.8649955472	neural machine translation
0.8648712296	event causality
0.8647934886	community members
0.8647815481	conceptual spaces
0.8646917123	open vocabulary
0.8646569826	multi document summarization
0.8644262547	indo european languages
0.8644145334	systematic review
0.8644123073	weighted sum
0.8641601680	negative examples
0.8641225987	document level
0.8640537498	additive composition
0.8640347026	computational argumentation
0.8639589187	grammatical errors
0.8639029799	content preservation
0.8638150856	intelligent agents
0.8637770244	image editing
0.8637143858	story generation
0.8636347031	relation paths
0.8636159187	social network
0.8634070684	offensive language
0.8633702416	event coreference
0.8633084590	poetry generation
0.8630065310	technical report
0.8629993046	multi granular
0.8629943767	remarkable progress
0.8629704318	fine grained
0.8628308438	aspect term
0.8627680637	morphological disambiguation
0.8627596696	computationally expensive
0.8626167646	dialogue policy
0.8625556812	word segmenter
0.8624338110	masked language models
0.8623288664	ancient chinese
0.8622490188	f1 score
0.8620668320	confidence estimation
0.8620654189	lexical semantics
0.8619876551	academic papers
0.8618567170	yelp reviews
0.8618363475	morphological analysis
0.8617931526	metaphor detection
0.8617853455	hand coded
0.8616347904	neural programmer
0.8615832737	search engines
0.8615335173	mention detection
0.8615214003	paying attention
0.8614519556	relational reasoning
0.8613299608	low cost
0.8612924198	text summarization
0.8612607923	text generation
0.8612090778	vice versa
0.8611680193	clinical concept extraction
0.8611467821	grammatical correctness
0.8611416357	spoken dialog
0.8611129347	dependency treebanks
0.8610627463	real valued
0.8610569894	rating prediction
0.8608523521	surface forms
0.8608422372	parse tree
0.8608383282	sequential decision making
0.8608299807	residual connection
0.8608119424	term extraction
0.8607351669	pseudo labeling
0.8606531611	dynamical systems
0.8605752484	lexical ambiguity
0.8605572742	encoder decoders
0.8602781480	tree structured
0.8602192623	daily life
0.8601590131	attentive pooling
0.8600074464	communicating agents
0.8599945840	extreme summarization
0.8598555996	causal explanation
0.8598396338	definition modeling
0.8598319751	paraphrase detection
0.8598084997	language modelling
0.8597965354	building blocks
0.8597943373	entity centric
0.8597802993	natural language instructions
0.8597417997	distantly supervised relation extraction
0.8594183518	lexical resources
0.8593948984	multi dimensional
0.8592991965	offensive language identification
0.8592885749	aspect extraction
0.8590964054	conversational search
0.8590554995	neural transducer
0.8586843952	rich morphology
0.8586246783	case frame
0.8586039765	natural language generation
0.8585513565	convolutional network
0.8585036909	word substitutions
0.8584579277	dialogue agents
0.8582954510	offensive content
0.8580607309	multi view
0.8579771272	machine transliteration
0.8579553841	contextualized representations
0.8578800258	false claims
0.8578484366	web search
0.8577744842	tree structure
0.8576317374	edge devices
0.8576033619	crisis events
0.8575967463	gained popularity
0.8575669328	search personalization
0.8575294669	lexical cues
0.8575154794	design choices
0.8573312304	user friendly
0.8573218825	automatic post editing
0.8572554059	voice search
0.8571869669	hypothesis testing
0.8571232391	abstract meaning
0.8570912603	unknown words
0.8570725933	lexical simplification
0.8570453904	sentence simplification
0.8570008502	rapid progress
0.8569542426	professional translators
0.8569227997	massive text corpora
0.8567840954	comparable corpora
0.8567115099	automated essay scoring
0.8565926661	discourse structure
0.8565105365	absolute gain
0.8563684606	conversation flow
0.8563332022	external memory
0.8563136925	layer wise
0.8560953139	natural language interfaces
0.8560281879	foreign language
0.8559079540	user profiles
0.8556197676	explainable ai
0.8556133370	window size
0.8555772628	posterior probability
0.8553868561	task completion
0.8553771067	document classification
0.8553440078	position aware
0.8553010324	fine tunes
0.8552606717	political speeches
0.8551929594	character level
0.8551866903	speech separation
0.8551414931	text classification
0.8551267935	average pooling
0.8550830981	speaker attributed
0.8550816357	demographic attributes
0.8550705857	ccg parsing
0.8549920747	variable length
0.8549770795	dynamic fusion
0.8548251251	speaker identification
0.8547525487	rule based
0.8547207159	paradigm completion
0.8546647619	parse trees
0.8546359655	attentional encoder decoder
0.8546110958	timeline generation
0.8545986386	performs comparably
0.8543533209	post edits
0.8543205564	chinese characters
0.8542377800	word similarity
0.8542309765	reciprocal rank
0.8541546508	relation classification
0.8540875734	frame rate
0.8539860970	contrastive estimation
0.8539182743	type logical
0.8538846752	transformer transducer
0.8538104451	stop words
0.8537888664	ambiguous pronouns
0.8537460963	language grounding
0.8536931477	post hoc
0.8535687214	goal oriented dialog
0.8534466169	linked data
0.8534440306	query focused
0.8533577832	hyper parameters
0.8532571860	entity alignment
0.8532490025	logic rules
0.8531887653	latent variables
0.8531071545	uni directional
0.8530447982	speech commands
0.8530243256	online forums
0.8530083041	sentence embeddings
0.8529921137	lexical entailment
0.8529731965	discourse relations
0.8529416539	text analytics
0.8528944058	emotion classification
0.8527773559	simultaneous translation
0.8526593215	extensive experimentation
0.8526492537	emotion analysis
0.8526279026	gated recurrent
0.8526193712	google news
0.8525949688	remains unclear
0.8525455081	sequence modeling
0.8524160479	probability distributions
0.8523957493	compound nouns
0.8523226227	knowledge base population
0.8522657169	goal oriented dialogues
0.8522210163	multi agent reinforcement
0.8522071060	linked open data
0.8520664570	latent tree
0.8520419970	compositional semantics
0.8520045718	social biases
0.8519776261	conditional variational
0.8518678101	semantic matching
0.8518056380	news article
0.8516989387	spoken dialogue
0.8516277912	parallel sentences
0.8516078475	error detection
0.8515289849	fold cross validation
0.8515093978	native speakers
0.8513792130	bottleneck features
0.8513483310	offensive language detection
0.8512958514	query focused multi document
0.8512924577	computational overhead
0.8511923475	inter modal
0.8511089107	topic modelling
0.8510481220	phrase based smt
0.8510318503	morphological segmentation
0.8509877535	word alignment
0.8509528026	spelling errors
0.8509468117	gating mechanism
0.8509278501	representation learning
0.8508735207	faster convergence
0.8507944977	glue benchmark
0.8506339995	world wide web
0.8506315318	dependency grammar
0.8506301175	multi agent
0.8503896595	extensively studied
0.8503735225	document summarization
0.8503735207	convergence speed
0.8502294611	code mixed indian
0.8501488725	multiple choice questions
0.8501303521	language agnostic
0.8500539217	intra sentential
0.8499019042	exploratory analysis
0.8498975476	speaker independent
0.8498645935	ablation studies
0.8498452174	cloze style
0.8497352942	role labeling
0.8496081430	task oriented dialog
0.8495154820	mono lingual
0.8495085018	newly introduced
0.8494172302	inter annotator
0.8493813420	program synthesis
0.8493483468	meta learning
0.8493412940	supervised learning
0.8493275081	word vector
0.8492574622	toxicity detection
0.8491837135	english hindi code mixed
0.8491653720	high precision
0.8490424808	fourier transform
0.8489297680	wmt14 english german
0.8488790820	visual scenes
0.8488354318	contrast sets
0.8487292154	online debates
0.8486323563	lstm lm
0.8485947652	charge prediction
0.8485605662	sentence level
0.8485349505	causal inference
0.8483859019	newly collected
0.8483766193	patient care
0.8481706308	world knowledge
0.8481388587	morphological tagging
0.8481297520	polarity classification
0.8480479317	cutting edge
0.8480389647	lattice lstm
0.8480041391	mental state
0.8479772985	unlabeled data
0.8479239346	post edited
0.8477991665	wer reduction
0.8477201769	causal relations
0.8476152910	recent surge
0.8476049346	bi lstms
0.8474625279	grammar formalism
0.8474518917	text normalization
0.8473400977	text independent speaker verification
0.8473387526	polysemous words
0.8472546073	statistical physics
0.8472126960	general debate
0.8471869876	error propagation
0.8471722444	sensitivity analysis
0.8469824141	multi head
0.8469102501	bidirectional encoder
0.8469048583	partially aligned
0.8468304664	memory networks
0.8468223803	newspaper articles
0.8466426131	discourse phenomena
0.8466274669	constraint logic
0.8466237120	closed world
0.8465470569	widely adopted
0.8465011013	query auto completion
0.8463919921	scoring function
0.8463483616	speaking styles
0.8463188379	author identification
0.8461352198	phrase grounding
0.8461121560	highway networks
0.8459457437	notoriously difficult
0.8458604065	arithmetic word
0.8458074705	man machine
0.8457364743	caption generation
0.8456949314	quantum theory
0.8456768963	web interface
0.8454428871	political bias
0.8453993761	lattice free
0.8453666388	random indexing
0.8452833602	open information extraction
0.8452601931	hindi english
0.8452235804	soft attention
0.8451701597	abusive language
0.8450587666	gaining popularity
0.8450570353	energy based
0.8449055672	knowledge gaps
0.8448939030	toxic content
0.8448414065	mt dnn
0.8448291334	expressive power
0.8447950521	task oriented
0.8445184511	gender neutral
0.8444977032	sentiment orientation
0.8444645068	objective function
0.8443445576	arabic dialect
0.8443282444	spatial relations
0.8443071653	pass phrase
0.8443000541	child directed speech
0.8442888722	model compression
0.8442678193	neural machine
0.8441704736	hidden states
0.8441639559	micro blogging
0.8441176594	biomedical named entity recognition
0.8440738277	selective attention
0.8439700617	speech signals
0.8438971660	hand engineered
0.8438914707	spatial reasoning
0.8438460364	micro averaged
0.8438181387	sound change
0.8438122707	syntax agnostic
0.8436994911	finer grained
0.8436700237	singular value decomposition
0.8436087670	lexicon induction
0.8435834340	negative polarity
0.8434782296	syntactic structure
0.8434121669	tensor networks
0.8433894340	morpho syntactic
0.8433568367	multi grained
0.8432962655	sparse coding
0.8432746841	background knowledge
0.8432724144	domain invariant
0.8432366813	high dimensional
0.8431452732	word formation
0.8430285546	speaker identities
0.8430208767	conditional probabilities
0.8429948680	lstm crf
0.8429198597	evolving events
0.8429080035	heavy tailed
0.8427717618	head driven
0.8426194101	dialogue generation
0.8426134644	high recall
0.8424683572	tag recommendation
0.8423244673	open ended
0.8422143458	language processing
0.8421578212	weight matrix
0.8421517975	discourse treebank
0.8421468151	convolution neural network
0.8420550964	polarity detection
0.8419388261	sinhala language
0.8418139159	vector representation
0.8417151779	punctuation prediction
0.8416270208	deceptive opinion
0.8416181370	stance classification
0.8415830483	emotion detection
0.8415216345	adversarially trained
0.8415158363	extremely low resource
0.8415150006	comment generation
0.8413384961	gaussian prior
0.8413237602	crowdsourced workers
0.8412998932	missing links
0.8412521760	multi domain
0.8412242485	prosody transfer
0.8411394732	reasoning chains
0.8411034615	manually curated
0.8410949880	word association
0.8410899473	compositional distributional semantics
0.8410759754	fake reviews
0.8410397421	reducing gender bias
0.8409845067	large vocabularies
0.8409009361	phrase based
0.8408983930	visual reasoning
0.8408941960	subject verb agreement
0.8408793922	disentangled representations
0.8407660595	single modality
0.8407351552	task agnostic
0.8406059547	free word order
0.8405331234	database schema
0.8404210241	community question answering
0.8404109179	topic coherence
0.8403863454	prior knowledge
0.8402180866	byte level
0.8402164349	speech waveform
0.8401955254	ranked list
0.8401761294	inter annotator agreement
0.8401417279	privacy concerns
0.8399957543	contextual embeddings
0.8399693781	counter intuitive
0.8399439689	takes place
0.8398956747	fine tuning
0.8398362769	voice controlled
0.8397647684	contrastive learning
0.8397413818	fixed length
0.8397022606	occur frequently
0.8396875773	data selection
0.8396539314	recursive neural network
0.8396058637	native language
0.8395748039	context free
0.8395345442	online reviews
0.8395272351	lexico semantic
0.8395197560	news headline
0.8394316029	cloze style reading comprehension
0.8394312806	myanmar sentences
0.8394206153	semantic frame
0.8392174402	multi lingual
0.8392080862	pos tagged
0.8391970759	carefully selected
0.8391258302	literature review
0.8390970504	predicate argument structures
0.8389914376	unsupervised domain adaptation
0.8389841393	single document summarization
0.8389569830	latent topics
0.8386325327	chest x ray
0.8385698585	linear transformations
0.8385400530	universal sentence encoder
0.8384299164	rouge scores
0.8384076642	aspect based sentiment analysis
0.8383527659	carefully designed
0.8382963936	likelihood estimation
0.8382621939	memory network
0.8382381527	passage ranking
0.8381939823	forward pass
0.8381657214	monotonic attention
0.8380909895	edit operations
0.8380084510	github.com facebookresearch
0.8378904340	extractive summaries
0.8378458564	long standing
0.8377955679	relevance propagation
0.8376879174	dynamic time warping
0.8376438385	york times
0.8375143007	category theory
0.8374114735	user generated
0.8374071864	taxonomy construction
0.8372460519	grammatical relations
0.8372373653	conditional text generation
0.8371698275	ground truth
0.8371323569	data collection
0.8370617541	dialogue state tracker
0.8369254721	recursive neural networks
0.8369034201	argument structure
0.8368377408	log loss
0.8367917697	auto regressive
0.8367174537	bi lstm crf
0.8365757044	gender differences
0.8365113022	phone error rate
0.8364495728	sentiment lexicons
0.8364356880	common ground
0.8363519804	streaming asr
0.8363347454	vietnamese language
0.8362269134	lstm lms
0.8358866264	rare word
0.8357356993	type logical grammars
0.8357070062	entity extraction
0.8356348942	knowledge graph embedding
0.8356319591	human robot
0.8356233400	neural net
0.8356191276	temporal relations
0.8354196771	natural disasters
0.8353592790	entity resolution
0.8351927675	open world
0.8350800816	multi tasking
0.8350634041	high order
0.8350213800	semantic web
0.8349940388	linear interpolation
0.8349616445	speaker adaptation
0.8348923613	personal health
0.8348407332	neural language models
0.8348397355	event coreference resolution
0.8347109685	dnn based
0.8346884266	visual captioning
0.8346589495	multi turn
0.8346331650	distance measures
0.8345509896	transition based
0.8345185485	noun modifier
0.8344808662	ubuntu dialogue corpus
0.8344577316	speech emotion recognition
0.8343800899	biomedical texts
0.8342555342	distributional hypothesis
0.8341805667	document ranking
0.8341208629	semi automatic
0.8341177909	word analogy
0.8341110272	short text conversation
0.8340288060	performs poorly
0.8339672081	language independent
0.8337630221	frequency cepstral coefficients
0.8337494870	deep bidirectional
0.8335860761	distributed representations
0.8334906621	sequence generation
0.8334286495	hierarchical recurrent
0.8333815917	conversational recommender
0.8333745142	news recommendation
0.8331816678	pattern based
0.8329997352	clinical text
0.8329907146	knowledge acquisition
0.8329883211	hindi english code mixed
0.8329806790	pragmatic reasoning
0.8329629233	tamil language
0.8328667227	event argument
0.8328298493	story cloze
0.8328133400	multi class
0.8327183208	judgment prediction
0.8326990862	iterated learning
0.8326597279	abusive language detection
0.8326581624	clinical reports
0.8326338218	similarity measures
0.8326278524	sentence ordering
0.8326114564	temporal expressions
0.8325946725	evaluation metrics
0.8325801591	multi task
0.8325016802	malayalam english
0.8324234185	frame induction
0.8323869598	general knowledge
0.8323224143	cross modality
0.8322931718	news media
0.8322572660	procedural text
0.8321615802	phoneme level
0.8321410787	flickr30k entities
0.8320565347	cross validation
0.8320182422	dependency tree
0.8320171346	exemplar based
0.8317688577	pattern recognition
0.8317278110	hindi language
0.8315834225	unification based
0.8315669374	concept extraction
0.8315309784	square error
0.8315125905	synthesized speech
0.8314196650	crisis response
0.8313875585	sentence embedding
0.8312576190	audio visual
0.8311648456	pointer generator network
0.8311150301	surrounding context
0.8310523817	closed domain
0.8310331889	online social networks
0.8309867285	language varieties
0.8308995688	multilingual societies
0.8308820617	dialog policy
0.8307919107	long form
0.8306940792	phrase structure
0.8306242629	centrality measures
0.8306199219	greedy search
0.8304951821	discourse relation
0.8304939546	task oriented dialogs
0.8304292952	semantic search
0.8303225493	challenges faced
0.8302584510	github.com lancopku
0.8302536251	lexico syntactic
0.8302428068	statistical machine
0.8301978348	additive noise
0.8301975472	dialogue state
0.8301965066	completely unsupervised
0.8301195409	fully differentiable
0.8300501705	open sourced
0.8299927076	mixture model
0.8298376435	posterior distribution
0.8298133144	dialogue agent
0.8297696397	scene text recognition
0.8297587357	neural architecture search
0.8297194069	statistically significant
0.8296639058	conversational systems
0.8294842320	biomedical relation extraction
0.8294409521	dialog systems
0.8294217980	argument component
0.8293900343	natural language commands
0.8293701464	quantitative linguistics
0.8293527843	tree master
0.8293160355	trivial events
0.8293055901	nested ner
0.8292585154	wikipedia article
0.8292283217	drug reactions
0.8291509170	reward function
0.8291474433	randomly selected
0.8291178237	spoken content
0.8290930173	text matching
0.8290901829	times faster
0.8290468711	resource constrained
0.8288157291	extractive text summarization
0.8287836747	vary greatly
0.8287816347	language change
0.8285565095	english wikipedia
0.8285516450	topic models
0.8285468580	markov decision
0.8284128214	universal dependency
0.8282363448	paraphrase database
0.8281872060	high confidence
0.8281157635	indian language
0.8279042423	multi modality
0.8278982335	semantically informed
0.8278475985	ehr notes
0.8277857998	spectral clustering
0.8277810723	image retrieval
0.8276881983	wsj0 2mix
0.8276834853	tensor train
0.8275776058	multi source
0.8275533015	adaptive computation
0.8275520122	rewrite rules
0.8275020170	subword unit
0.8274786955	knowledge intensive
0.8274776363	syntactic dependencies
0.8273827581	resourced languages
0.8273683522	case study
0.8272657778	multi label
0.8272477812	english romanian
0.8271988605	opinion target
0.8271951643	morphologically complex
0.8271909670	text editing
0.8270965736	syntactic structures
0.8270161951	personal assistant
0.8269718539	long term
0.8269570236	formality style
0.8269511745	code assignment
0.8269499673	absolute improvement
0.8267402921	frame identification
0.8266413200	distant speech recognition
0.8265655527	dependency structures
0.8265250261	common sense reasoning
0.8264794447	document clustering
0.8264692701	open source software
0.8264547132	multi head attention
0.8264461635	attribute values
0.8263486793	discourse representation structures
0.8263386418	dual learning
0.8262785248	github.com thunlp
0.8262613911	tag set
0.8262006483	implicit discourse relation
0.8261793900	amr graphs
0.8261660339	evaluation protocol
0.8261056065	cross sentence
0.8260421937	convolutional neural
0.8260273248	human beings
0.8259892276	sense definitions
0.8259838765	word level
0.8258885101	text dependent speaker
0.8258094433	loss function
0.8257938902	long documents
0.8256834081	long term memory
0.8256590155	instance level
0.8256215219	abstract machine
0.8256140829	word length
0.8255403755	high quality
0.8254965357	counterfactual learning
0.8254791215	trade offs
0.8254662144	standard deviation
0.8254003623	task oriented dialogue
0.8253895903	meaning representation
0.8253027040	fewer parameters
0.8252646943	closely related
0.8252624985	pair wise
0.8252179155	semantic spaces
0.8250888117	video story
0.8250843419	multi speaker
0.8248966914	synthetic data
0.8248774407	activity recognition
0.8248263688	attention mechanisms
0.8248117996	patent claims
0.8247886264	knowledge integration
0.8247593313	long short term memory networks
0.8247533540	domain adaptive
0.8247226284	action space
0.8245091420	sentence representations
0.8244765459	attracted considerable
0.8243943679	contextualized word
0.8243143427	bi directional lstms
0.8243007119	pass decoding
0.8242593464	mixed effects
0.8242414684	speaker adapted
0.8241955995	typologically diverse languages
0.8241751807	early detection
0.8241577367	discourse parsing
0.8241496743	web crawled
0.8241367189	research papers
0.8239771969	query intent
0.8239669000	legal documents
0.8237526229	micro reviews
0.8236230280	sequence prediction
0.8235975100	semantically similar
0.8235444464	metric learning
0.8235190926	tensor network
0.8232624978	belief state
0.8231975187	bleu points
0.8231333111	activation function
0.8230896018	relational databases
0.8230660234	similarity measurement
0.8230242452	discourse segmentation
0.8229683796	deep reinforcement
0.8228192791	binary code
0.8228130895	gender prediction
0.8228127182	minimal supervision
0.8227127639	error rates
0.8226205295	data science
0.8225812306	publicly released
0.8224715205	phoneme sequences
0.8223973380	data scarcity
0.8223694503	semi supervision
0.8223659270	scene aware dialog
0.8223216743	word ordering
0.8222518443	text based games
0.8222093046	discourse analysis
0.8221238398	audio visual scene aware
0.8220715744	cost function
0.8220419440	widely recognized
0.8220245095	audio recordings
0.8219195970	perform comparably
0.8218785715	duplicate question
0.8218607073	rapidly growing
0.8217815862	babi dialog
0.8216387888	query rewriting
0.8214936442	deep contextualized
0.8214670460	text streams
0.8214614709	online abuse
0.8214275829	domain experts
0.8214240513	fine grained entity typing
0.8212840823	semantic drift
0.8212216835	hot topic
0.8211943109	multi stage
0.8211776966	memory cells
0.8211531765	chinese literature
0.8210853553	language learners
0.8210645292	verb object
0.8210557822	sentence completion
0.8209523158	word error rates
0.8209076642	transformer big
0.8207329987	word representations
0.8206874672	open set
0.8206173268	bayesian model
0.8206163956	label wise
0.8205524486	adversarial perturbations
0.8205428154	error prone
0.8203905843	information bottleneck
0.8203814673	artificial agents
0.8203180631	hybrid dnn hmm
0.8203162836	speaker identity
0.8202932181	markov models
0.8201893887	domain independent
0.8201357807	document summarisation
0.8201200317	constraint based
0.8200468642	complex valued
0.8200186053	token level
0.8199117066	stanford question answering dataset
0.8198646468	forum posts
0.8198147812	parallel texts
0.8197956815	dialogue responses
0.8196888117	question answer
0.8196390984	twitter data
0.8196321417	speaker role
0.8196034543	knowledge representation
0.8195652791	meaning preservation
0.8195486018	debiasing methods
0.8195067853	reconstruction loss
0.8194497408	generalization capability
0.8193871804	similarity measure
0.8193762271	neural trf lms
0.8193524974	character based
0.8192440802	voice quality
0.8192040033	pointer generator networks
0.8189902517	natural sounding
0.8189551128	high volume
0.8189243681	referring expression generation
0.8186386322	feedforward neural network
0.8186318864	feature extractors
0.8185907403	acoustic cues
0.8185675496	noisy labels
0.8183280021	social media platforms
0.8181564234	journal articles
0.8180439499	skip grams
0.8180430593	relative wer reduction
0.8180369867	multi microphone
0.8178296104	disaster related
0.8177783428	tree lstm
0.8177740917	span identification
0.8176583628	handwritten text
0.8175289998	recurrent network
0.8175288553	word meanings
0.8174998786	low dimensional
0.8174457261	product review
0.8174053341	logical reasoning
0.8173458712	distractor generation
0.8171713492	f1 scores
0.8171412907	audio word2vec
0.8170814949	conversational speech
0.8169925377	spelling error
0.8168748513	bayes risk
0.8168273834	model selection
0.8168192764	global pandemic
0.8167850474	logical inference
0.8167484436	resource poor
0.8166368481	transformer based
0.8166148042	multi choice
0.8165799158	extractive qa
0.8165698873	medical diagnosis
0.8165477127	video summarization
0.8165327350	hand crafted
0.8165004028	word vector representations
0.8164947511	dialogue response generation
0.8163567090	conceptually simple
0.8163408386	darwin
0.8162454339	external knowledge bases
0.8161725338	foreign languages
0.8161348960	impressive progress
0.8160921873	noisy environments
0.8159896335	european languages
0.8159804481	convolutional layers
0.8159406680	semantic indexing
0.8156941091	temporal relation
0.8156882453	rewriting systems
0.8156233575	recursive neural
0.8156172340	answer extraction
0.8155855190	conversation history
0.8155133317	monolingual corpora
0.8152179946	stacking networks
0.8151118814	grammar formalisms
0.8149955566	multi level
0.8149937780	bidirectional lstm crf
0.8149252759	ppi extraction
0.8147764442	biomedical ontologies
0.8147201001	sentence fusion
0.8147011952	ultimate goal
0.8146026626	semantic units
0.8145981270	data driven
0.8145895558	generated text
0.8145666114	stanford natural language inference
0.8145425767	semantic parser
0.8145253756	news websites
0.8144479935	segmental models
0.8143543687	fully unsupervised
0.8142509549	synthetic speech
0.8142362693	triple extraction
0.8141443527	widely studied
0.8140719874	aspect level sentiment classification
0.8140703613	majority class
0.8139723134	knowledge enhanced
0.8139382505	action sequences
0.8138897376	automatically generating
0.8138804779	german language
0.8138783202	text processing
0.8138640455	adversarial samples
0.8137623199	named entity linking
0.8136811421	language drift
0.8136471146	adversarial learning
0.8136199621	context windows
0.8134168557	directed acyclic
0.8132482320	language games
0.8131325855	svm classifier
0.8131170228	ainu
0.8129970714	character recognition
0.8129393010	user reviews
0.8129211168	sentence segmentation
0.8128340818	sentimental analysis
0.8127728836	` `
0.8127698119	arab
0.8127698119	chomsky
0.8127698119	albanian
0.8126799426	individual neurons
0.8125024351	approximate inference
0.8123240604	neural mt
0.8123173337	cost sensitive
0.8123162367	pivot language
0.8122735511	maltese
0.8121993670	australian
0.8121743222	newly released
0.8121305744	transductive learning
0.8120542933	preprocessing step
0.8120343490	open domain dialogue
0.8119860863	choice reading comprehension
0.8119177585	acoustic models
0.8118274005	open domain question answering
0.8117366771	simultaneous speech
0.8117246334	positional information
0.8115737323	span prediction
0.8115264044	uncertainty estimates
0.8114857868	factual knowledge
0.8113863697	distributional measures
0.8113274677	incremental learning
0.8112499024	factor analysis
0.8112035238	hausa
0.8111837234	autoregressive models
0.8111583326	domain agnostic
0.8111323601	cross lingual transfer
0.8111272390	large vocabulary continuous speech
0.8111244630	iranian
0.8111053891	canadian
0.8111053891	latvian
0.8110731494	single hop
0.8110300007	test case
0.8109246639	sentiment lexicon
0.8108296398	grid search
0.8107908766	real life
0.8106758745	language archives
0.8106266745	arabic language
0.8106137653	sentiment prediction
0.8105982961	annotation costs
0.8105719515	implicit discourse relations
0.8104734643	pronoun disambiguation
0.8104372331	machine speech chain
0.8103983145	bi directional lstm
0.8103811023	key insight
0.8103128053	pseudo labels
0.8102980443	rnn lm
0.8102927550	paragraph level
0.8102366686	lexical features
0.8101916979	valuable insights
0.8101621873	automated essay
0.8101586345	manually annotated
0.8101261453	lattice based
0.8101166060	dual attention
0.8100438552	annotator agreement
0.8100417211	khmer
0.8099975134	semantically valid
0.8099462271	entity normalization
0.8099216241	semantic interpretation
0.8098487522	event centric
0.8096803289	feature fusion
0.8094450182	neural ir
0.8093815229	demographic groups
0.8092996394	social science
0.8090893341	cloze task
0.8090659651	coarse grained
0.8090256600	pointwise mutual
0.8089897552	translation lexicons
0.8089425292	hmm based
0.8088874491	model's predictions
0.8088279398	cross document
0.8087443648	word discovery
0.8087218972	temporal reasoning
0.8087062782	markov model
0.8086043909	short term
0.8085342989	biomedical domain
0.8085179790	chicago
0.8084454805	knowledge grounded
0.8084432092	meta embeddings
0.8083755324	neural nets
0.8083652900	neural vocoder
0.8081863279	multi agent communication
0.8081057680	distributional similarity
0.8079844303	bayesian inference
0.8079606394	english french
0.8078914046	annotation guidelines
0.8078855599	malagasy
0.8078619780	scientific abstracts
0.8077893697	sense tagged
0.8077682219	sample efficiency
0.8077655609	semantically equivalent
0.8077496011	topic detection
0.8077393503	multi step
0.8076988957	cross entropy loss
0.8074485726	newly created
0.8073100329	unsupervised learning
0.8072417387	user profile
0.8071234271	discussion forum
0.8069928857	interactive predictive
0.8069907063	mary
0.8068477356	recent developments
0.8067614588	grounded dialogues
0.8066762253	human judges
0.8066699866	multi scale
0.8066266404	information technology
0.8065720889	tail entity
0.8064044978	high coverage
0.8063584972	deep recurrent
0.8063360595	language technology
0.8062198915	armenian
0.8061496285	hierarchical attention
0.8058620740	beam search decoding
0.8057028974	multi document
0.8056763864	semantic composition
0.8056762374	multi hop reasoning
0.8056627239	native language identification
0.8056314168	topic discovery
0.8056262928	cross attention
0.8054137273	english german
0.8053977774	spam filtering
0.8052512532	semantic relations
0.8051466045	babylonian
0.8050902478	gate mechanism
0.8050695237	communication protocols
0.8050028927	language evolution
0.8049113686	bert base
0.8049070452	research directions
0.8048818116	reading difficulty
0.8047233425	multimedia content
0.8045686875	perform poorly
0.8045651864	multi turn dialog
0.8045389881	extractive document summarization
0.8045383344	meaning representations
0.8045285521	multi genre
0.8044632686	concept hierarchy
0.8044476703	cross lingually
0.8043885719	label embedding
0.8043539108	neural symbolic
0.8041859723	sentence encoding
0.8041563137	jordan
0.8041086611	morpheme level
0.8040858270	bulgarian
0.8040490164	bidirectional recurrent
0.8040213541	question pairs
0.8039957084	reasoning paths
0.8036167538	country level
0.8035820923	scientific paper
0.8034844918	number agreement
0.8034809373	relative frequencies
0.8034401451	linguistic annotation
0.8033975799	assamese
0.8033556310	entity types
0.8032658512	ambiguity resolution
0.8032135922	semantic tagging
0.8030703617	conversational recommendation
0.8030145915	online hate speech
0.8029827961	information overload
0.8029459080	code snippets
0.8028409094	content analysis
0.8028288132	medical practitioners
0.8025290486	acoustic model
0.8024783995	visual context
0.8024566890	relation prediction
0.8024446849	word prediction
0.8024400950	english hindi
0.8024188542	relational knowledge
0.8023632527	cornell
0.8023472575	hybrid ctc
0.8023314048	sentence representation
0.8022959812	sentiment polarities
0.8020113679	bilingual lexicon
0.8019993468	application areas
0.8018415994	unpaired text
0.8018211209	hybrid model
0.8018122069	challenge dataset
0.8017900152	artificial neural network
0.8017894032	macro averaged
0.8017583694	phrase level
0.8017492167	user behavior
0.8017095221	controversial topics
0.8016450165	universal grammar
0.8015926472	test sets
0.8014794484	sentence alignment
0.8014485916	relation tuples
0.8014402519	news sources
0.8014172276	resource intensive
0.8014127047	word frequency
0.8013444917	distributional semantic
0.8013299198	clean data
0.8012810793	dialog state
0.8012693507	biomedical text
0.8012173671	large vocabulary
0.8011542547	multimodal sentiment analysis
0.8011297891	transcribed speech
0.8010920206	topic aware
0.8009406362	previously unseen
0.8009205657	vqa models
0.8008480898	search queries
0.8008426674	code snippet
0.8008378359	nli models
0.8006685220	biomedical text mining
0.8005236320	wide spread
0.8005048367	soap
0.8004877844	london
0.8004763666	abstract syntax
0.8003405465	word importance
0.8003312489	association rules
0.8003199051	long distance
0.8002911179	sense embeddings
0.8001319066	visual modality
0.8001151813	language model
0.8000983762	emotional speech
0.8000464045	lewis
0.8000416428	meta learner
0.7999856839	information processing
0.7999442382	human evaluators
0.7999358702	catalan
0.7999218031	speech perception
0.7998365198	sindhi
0.7998190594	sentence boundaries
0.7997729561	human raters
0.7997056295	distributed representation
0.7997012703	tang
0.7996505199	elmo embeddings
0.7996429999	raw speech
0.7995455326	visual question
0.7995045053	lexical semantic
0.7994240273	punjabi
0.7993391064	semantic annotation
0.7991925465	loss functions
0.7991703309	graph neural networks
0.7991368731	parameter estimation
0.7990583015	auxiliary task
0.7990299555	entity descriptions
0.7988981366	task oriented dialogues
0.7988835367	complex word identification
0.7988779153	riemannian
0.7987483220	thesis presents
0.7986410395	cambridge
0.7986154499	structure aware
0.7985730389	teacher student learning
0.7985559857	portuguese language
0.7985434120	bernoulli
0.7985361425	ctc based
0.7984990721	text attribute transfer
0.7984972948	low resource languages
0.7983746605	multi sense
0.7983550497	sparql queries
0.7981733188	beam size
0.7981493224	chinese poetry generation
0.7981481011	retrieval based chatbots
0.7980655957	answer span
0.7980502857	pretraining data
0.7980410949	image caption generation
0.7978919039	pre training
0.7978887029	opinion summarization
0.7978499572	data set
0.7977790392	contextualized representation
0.7976893019	weakly labeled data
0.7976770046	emotion lexicon
0.7976563325	spoken english
0.7975975053	human readable
0.7975651687	manually annotate
0.7975174262	image segmentation
0.7974542876	confusion networks
0.7974131791	vocabulary mismatch
0.7973246614	document representations
0.7973119421	speech processing
0.7973089858	discourse parser
0.7970817903	human computer interaction
0.7970246912	multimodal fusion
0.7969908842	slot labeling
0.7969742790	spearman's
0.7969479593	coherence modeling
0.7968760250	phoneme error rate
0.7968210090	visual qa
0.7967311469	user defined
0.7965602444	word relatedness
0.7965221578	dynamic semantics
0.7965070978	previously published
0.7964815915	automated fact checking
0.7963369839	reward signal
0.7962791610	text segmentation
0.7962595972	sentence structure
0.7962562838	rare words
0.7961966885	grammar development
0.7960642454	multi sentence
0.7960554680	term frequency inverse
0.7960017762	character error rate
0.7958820631	web scale
0.7958806917	dialog policy learning
0.7958465705	model agnostic
0.7957813722	trans dimensional random
0.7957622916	modern chinese
0.7956772039	apache
0.7955272607	public mood
0.7955219809	danish
0.7954402401	controlled natural language
0.7954292110	script knowledge
0.7954240652	annotation schemes
0.7954126302	semantic complexity
0.7953783614	human rationales
0.7953187176	test collection
0.7953069906	partially annotated
0.7952934512	multi resolution
0.7952558363	aspect detection
0.7952489846	contextual word
0.7952226903	distributional semantic models
0.7951011783	dialog generation
0.7950869614	video question answering
0.7950394077	automated assessment
0.7949478084	widely applicable
0.7948923102	graph convolutional
0.7948717613	romanian english
0.7948471980	attention mechanism
0.7947134147	fully supervised
0.7945786707	lstm rnn
0.7945705230	controllable text generation
0.7943512619	filipino
0.7943241201	bilingual word embeddings
0.7943108582	multi task learning
0.7942197784	news translation task
0.7942193854	commonsense inference
0.7941827821	visual features
0.7940565776	information flow
0.7940255214	natural language explanations
0.7939423247	resource rich
0.7937537015	ukrainian
0.7936986110	thai
0.7936381171	positive negative
0.7935609843	short duration
0.7934966955	multi criteria
0.7934350909	fine tune
0.7934024334	medical documents
0.7933456036	document context
0.7932013727	perform competitively
0.7932012730	americans
0.7931405679	fully automatic
0.7930955991	dirichlet process
0.7930790377	closed set
0.7929715018	imbalanced data
0.7929436387	automatic text summarization
0.7928659555	head driven phrase
0.7928318110	emerging entities
0.7927846583	english spanish
0.7926675986	manually labeled
0.7926121208	single speaker
0.7925850853	principal component
0.7925523202	inter sentence
0.7925318384	shown promise
0.7925166085	irish
0.7924492406	vector embedding
0.7923655109	political discourse
0.7923139778	asian
0.7922734417	bleu score
0.7921866864	tibetan
0.7921688775	speaker embedding
0.7921530551	ubuntu dialogue
0.7921422999	nvidia
0.7921232571	malayalam
0.7920828183	multi relational
0.7919802488	complex networks
0.7919545831	vocabulary size
0.7919403357	william
0.7917739352	language families
0.7916716193	attention based
0.7916264565	word analogies
0.7913758558	entity representations
0.7913720200	nlp tools
0.7913503647	usa
0.7912791803	macro average
0.7911973594	transformer decoder
0.7911670284	heterogeneous information
0.7909233143	preference learning
0.7908754076	speech transcripts
0.7907935397	attention layers
0.7907443181	parameter free
0.7906995000	vector space model
0.7906859618	aspect sentiment classification
0.7906820715	latent topic
0.7906803691	response ranking
0.7906544672	bilingual corpora
0.7906491203	lexical choice
0.7905924633	spanish english
0.7905502215	automatic speech
0.7904357669	media platforms
0.7903610290	auxiliary tasks
0.7903457837	multi word expressions
0.7903252732	emergent language
0.7903187715	cross language
0.7901881166	related tweets
0.7900914246	annotated corpus
0.7900441877	multi stream
0.7900210829	wavenet
0.7898600956	lexical database
0.7898565294	video summary
0.7898346888	ontology based
0.7897920785	articulatory features
0.7896159690	church
0.7895130806	obama
0.7894746315	input output
0.7894390753	legal texts
0.7893737158	test bed
0.7893579943	neural module networks
0.7893443401	similarity metric
0.7892897611	term set expansion
0.7892537163	text independent speaker
0.7892383354	fixed size
0.7892327274	times fewer parameters
0.7892281359	fixed point
0.7892268195	latent tree learning
0.7891890697	data hungry
0.7889976150	attention layer
0.7888998661	national corpus
0.7888802226	translation directions
0.7888795621	bi directional long short term memory
0.7888472674	visual relationship
0.7887746702	approximate nearest
0.7887705902	word usage
0.7885805797	social scientists
0.7885552683	personal names
0.7885147389	gujarati
0.7885147389	kannada
0.7884553952	question answering systems
0.7884093064	word frequencies
0.7884018531	bi modal
0.7882179958	meta embedding
0.7881597629	monolingual data
0.7881538325	low precision
0.7881024858	canada
0.7880069877	john
0.7880000817	noisy text
0.7879267185	euclidean
0.7879261479	emotional state
0.7877858017	policy learning
0.7877432664	textual descriptions
0.7877122936	real world
0.7877064252	architecture search
0.7876714328	post processing
0.7876270404	sentence classification
0.7875545203	surface form
0.7874994456	tagging scheme
0.7874770405	uk
0.7874445239	word list
0.7874173816	conversational context
0.7873476914	long document
0.7871897577	sequence level
0.7871554769	igbo
0.7871347405	function tagging
0.7871333216	tree grammars
0.7870425106	negative samples
0.7869618944	fully convolutional
0.7867952161	shakespeare
0.7867304685	test set
0.7867078777	aspect sentiment
0.7866857075	explanation methods
0.7866409989	publicly release
0.7865826698	mandarin english
0.7864501884	tacotron
0.7864165218	gated attention
0.7863148833	random variables
0.7862164112	cross view
0.7861859961	native english speakers
0.7861294368	croatian
0.7860841709	noise robust
0.7858563716	great success
0.7858470929	augmented data
0.7858289666	sentence selection
0.7858231093	natural questions
0.7857030167	lexical resource
0.7856718490	recently released
0.7856536990	increasing popularity
0.7856188098	dialog models
0.7855901403	word pair
0.7855563495	substantial gains
0.7855508464	chinese classical
0.7854750174	sentence matching
0.7854658121	expert annotations
0.7854016366	qa srl
0.7853308944	word confusion
0.7851339292	association test
0.7850826141	science questions
0.7850509844	reconstruction error
0.7850023506	hidden layer
0.7849908755	negation detection
0.7849728916	open domain dialogue systems
0.7849315430	text sequences
0.7848972509	social bias
0.7848804059	multi layer
0.7848480678	sinhala
0.7847498757	sentence similarity
0.7846880747	lexical rules
0.7846082441	shared tasks
0.7845263854	answer generation
0.7844430969	dravidian
0.7844258823	knowledge driven
0.7843899065	sample efficient
0.7843396293	event pairs
0.7843109035	linguistic properties
0.7842671865	named entity disambiguation
0.7842636594	margin loss
0.7840833044	u.s
0.7840495826	entity mentions
0.7840080858	tagged corpus
0.7839564620	maximum mutual information
0.7838934735	generalization ability
0.7837977969	multi aspect
0.7837290318	flickr30k
0.7837220599	grounded language learning
0.7837025398	cross platform
0.7836984780	graph embeddings
0.7836115403	online debate
0.7835744631	user item
0.7835270544	social media users
0.7834840769	online news
0.7834825861	meaning space
0.7833477339	written text
0.7832612574	transformer models
0.7832591291	knowledge aware
0.7832081620	comparative studies
0.7831951490	kb qa
0.7830632942	macro f1 score
0.7830481804	case studies
0.7830411888	pre processed
0.7830299693	ensemble learning
0.7829659724	causality detection
0.7829044234	financial domain
0.7828763353	label noise
0.7828570712	sentence planning
0.7828489052	fine tuned
0.7828422477	natural language interface
0.7827866715	human translators
0.7827153466	multi perspective
0.7827132716	attention weights
0.7825521147	external sources
0.7825144912	rank frequency
0.7824445709	image text
0.7822911376	user query
0.7822650945	trump
0.7822373083	data oriented parsing
0.7821294714	pivot based
0.7817992360	text similarity
0.7817803287	clinical practice
0.7817339178	error analysis
0.7817050903	japanese english
0.7816865521	selection strategies
0.7816816804	document categorization
0.7816467665	low frequency
0.7815368369	marathi
0.7815050435	authorship analysis
0.7814526346	multilingual nmt
0.7812911956	information maximization
0.7811942402	norm based
0.7811879931	linguistic structure
0.7811221659	adversarial evaluation
0.7810610868	linear svm
0.7810513286	online review
0.7809417156	acoustic patterns
0.7809383172	weighted finite state
0.7808968574	language generation
0.7808568290	free text
0.7808476851	online social
0.7808335060	term weighting
0.7808161852	detecting hate speech
0.7808097109	computational social science
0.7807847294	word recognition
0.7807830452	medical concepts
0.7807767033	opinion words
0.7806874621	viterbi
0.7806341069	speech chain
0.7805784282	domain knowledge
0.7805179067	image regions
0.7802284769	rhetorical relations
0.7801948760	bengali english
0.7801821433	knowledge sources
0.7801506559	human intelligence
0.7801457687	annotation tools
0.7801452895	error types
0.7801228714	pubmed
0.7800254393	poem generation
0.7800112491	visual scene
0.7800092862	tamil english
0.7799771398	german english
0.7799708194	acoustic word embeddings
0.7799468727	wasserstein
0.7799380198	word classes
0.7798218730	goal driven
0.7796077105	answer sentence selection
0.7795327048	domain mismatch
0.7795176162	text style transfer
0.7794784065	frobenius
0.7794709449	grounded conversations
0.7794629605	string similarity
0.7793643939	type theory
0.7793410057	word representation
0.7793234318	europe
0.7792720597	yelp
0.7792561153	quran
0.7792417928	human judgement
0.7791994830	continuous speech
0.7791832859	conversational question answering
0.7790784245	support vector
0.7790716515	hebrew
0.7790386938	traditional chinese
0.7790152221	feature importance
0.7788034688	video understanding
0.7787949269	graph structured
0.7787788707	word embedding spaces
0.7787580344	knowledge extraction
0.7787003264	persona based
0.7786323156	automatic summarization
0.7785507938	type driven
0.7785047687	multimodal representations
0.7784203009	multi head self attention
0.7784009228	computational cost
0.7783966338	drug reaction
0.7783813685	single document
0.7783196936	pre defined
0.7783016698	vector embeddings
0.7782370063	semantic fields
0.7781976811	nlp pipeline
0.7781122568	language models
0.7781026258	kazakh
0.7779109822	audiovisual speech
0.7778889916	question answering dataset
0.7778298631	sign language recognition
0.7777737166	canonical correlation
0.7776650123	text embedding
0.7776528973	speaker dependent
0.7776371919	lithuanian
0.7775955432	probabilistic models
0.7775138818	document similarity
0.7774928363	cross cultural
0.7774080638	latent representation
0.7773578979	discourse aware
0.7772678036	neural tts
0.7772632117	neural conversation models
0.7772463176	markov chain monte
0.7771576504	code generation
0.7771000630	linguistic features
0.7769631277	resource poor languages
0.7768297797	search space
0.7768010397	entity embeddings
0.7766062746	micro f1 score
0.7765957439	china
0.7764049502	billion words
0.7764023877	deep neural
0.7763283215	cortana
0.7761734943	indonesia
0.7761637446	graph attention networks
0.7761590422	naive bayes classifier
0.7761349489	trainable parameters
0.7760464620	sparql
0.7759550816	topic identification
0.7759233635	semantically meaningful
0.7758029049	query translation
0.7756869091	syntactic parsing
0.7756373310	multi dialect
0.7756169828	main contributions
0.7754990565	transformer base
0.7754643133	hotpotqa
0.7754587059	temporal logic
0.7754540985	retrieval based
0.7753462821	intra sentence
0.7751853285	gated graph
0.7751706123	information seeking conversations
0.7751331302	semi automated
0.7750763879	lexical semantic change
0.7749512058	problem solving
0.7749148807	structured data
0.7748428640	visual perception
0.7748333884	topic model
0.7747679518	romanian
0.7747646251	confidence scores
0.7747574645	mel frequency
0.7746991434	dialog context
0.7746297197	commons
0.7746074052	text classifiers
0.7746064566	explicit semantic analysis
0.7745874124	visual representations
0.7745586100	nearest neighbor search
0.7745305859	multi label classification
0.7744926049	knowledge selection
0.7744777672	sentence pairs
0.7744604252	short text classification
0.7743756343	annotation scheme
0.7743573889	image generation
0.7743135910	democratic
0.7743131938	precision recall
0.7741097505	code retrieval
0.7740483837	fixed dimensional
0.7740261643	knowledge enriched
0.7740256461	medical term
0.7739927116	intermediate task
0.7739785481	feed forward neural network
0.7738655580	text readability assessment
0.7737957931	unsupervised pretraining
0.7737906486	british
0.7737559666	tv
0.7736779651	utterance level
0.7736170396	relevance score
0.7734173893	cross modal retrieval
0.7733869109	presidents
0.7733409423	intelligent systems
0.7733395100	complex questions
0.7732783132	dialog act recognition
0.7731832132	automatic evaluation
0.7731381542	risk factors
0.7731215434	frame semantics
0.7729948055	formal language
0.7729707739	smith
0.7729532053	natural language descriptions
0.7729206414	entity pairs
0.7729189470	achieved remarkable
0.7728744401	target language
0.7728522013	mismatch problem
0.7728057594	linguistic knowledge
0.7727835293	hilbert
0.7727294610	easily accessible
0.7726957795	arabic text
0.7726794736	hierarchical structure
0.7726726186	query language
0.7726615459	topic mining
0.7726334565	brazilian
0.7726166232	neural sequence models
0.7724757370	findings suggest
0.7724198346	manually labelled
0.7723968740	state transition
0.7722709877	billion parameters
0.7722111544	linguistic regularities
0.7721878902	senseval
0.7721542258	embedding space
0.7721360596	long range dependencies
0.7720075457	academic articles
0.7720027975	compares favorably
0.7719941407	bengali language
0.7719832930	bi encoder
0.7719831821	ab
0.7719695862	text representation
0.7719132959	biomedical research
0.7719097198	africa
0.7717936489	sigma
0.7717891268	modeling units
0.7717801299	ctc attention
0.7717678372	domain expertise
0.7717589413	manual effort
0.7716971284	long text
0.7716889164	input perturbations
0.7716814114	memory augmented neural networks
0.7716050308	frame level
0.7715724493	crf based
0.7715554941	feature learning
0.7715481162	signal processing
0.7715393030	linguistic resources
0.7715316954	semi automatically
0.7714930338	aspect based sentiment
0.7714711104	phoneme based
0.7714579284	chinese social media
0.7713758124	task oriented dialogue systems
0.7713434536	noun noun
0.7713187346	egyptian
0.7713136168	manual evaluation
0.7712583045	simultaneous machine translation
0.7712223373	imdb
0.7712116154	diagnostic classifiers
0.7711773979	large vocabulary speech recognition
0.7711137058	medical text
0.7710082113	video qa
0.7709930263	youtube
0.7708824170	read speech
0.7708604732	unnatural language
0.7708485406	frame semantic parsing
0.7708480326	indus
0.7708248110	transformer encoder
0.7708010265	gaussian distributions
0.7705989975	heterogeneous graph
0.7705943468	synthetic parallel data
0.7705352183	syntactic features
0.7704742988	sanskrit
0.7704444149	controlled natural languages
0.7704430389	parameter efficient
0.7703601328	latin
0.7702356365	target speaker
0.7701999780	tls
0.7701941500	mandarin speech
0.7701859664	triplet loss
0.7700699722	word orders
0.7698045620	layered attention
0.7698028786	segment level
0.7697786147	norwegian
0.7697183257	speaker adaptive training
0.7695567904	spoken word
0.7695477556	high level
0.7695318768	speech translation
0.7694624783	document representation
0.7693599609	human brain
0.7693478046	cue words
0.7692700014	short message
0.7692033013	reference free
0.7689873916	anchor word
0.7689840681	health related
0.7688604673	nlp models
0.7687795059	greek
0.7687513064	coecke
0.7687140807	multiple instance
0.7686928249	visual dialogue
0.7686722689	dialogue act classification
0.7686612516	text retrieval
0.7686342340	shannon
0.7685744020	graph neural network
0.7685227671	temporal dependency
0.7685132938	transformation based learning
0.7684977916	small data
0.7684791234	temporal relation extraction
0.7684697552	sentence generation
0.7684581711	open source library
0.7684415961	story understanding
0.7683778483	lexical items
0.7683159744	tale
0.7682709877	lexical substitution
0.7681419423	fi
0.7681317144	relational semantics
0.7680933475	graph based
0.7679740975	bioasq
0.7679399794	template based
0.7679006207	malay
0.7678966568	memory consumption
0.7678919356	domain ontology
0.7677972156	multi pass
0.7677741374	language variety
0.7677558302	audio features
0.7677336500	categorical compositional
0.7677150853	clustering algorithm
0.7677139362	theoretic framework
0.7677036168	lrl
0.7676167087	sequence discriminative training
0.7676121983	long term dependencies
0.7675748114	long sequences
0.7675628982	bidirectional long short term
0.7674257090	improving robustness
0.7673594242	keyword based
0.7672598117	online community
0.7672444098	chamber
0.7671937682	english japanese
0.7671903515	congressional
0.7671707981	word clustering
0.7671545316	evaluation benchmark
0.7671536776	pooling operation
0.7671533743	low quality
0.7671228857	previously reported
0.7671152747	korea
0.7670531540	propaganda techniques in news articles
0.7670528685	subword modeling
0.7668484038	document embeddings
0.7666921403	french english
0.7666328145	embedding vector
0.7666142323	semantic shift
0.7666049019	code mixed data
0.7665420769	pretrained transformer
0.7664624431	transition based dependency parsing
0.7663587452	adjoining grammar
0.7662599314	interviewing
0.7662559936	distilling knowledge
0.7662399927	nepali
0.7661199042	contextual word embeddings
0.7660770314	semantic lexicons
0.7660327518	annotation projection
0.7659554199	subword embeddings
0.7659404692	jointly trained
0.7659116281	resource languages
0.7658028924	regression model
0.7657959694	evidence based
0.7657897127	attention head
0.7657891764	gender biases
0.7657822126	confidence measure
0.7657783342	robust parsing
0.7656953431	learning rate
0.7655886947	earth
0.7655671425	linguistic phenomena
0.7655623404	experimental study
0.7655431847	copa
0.7654472863	structured pruning
0.7653576373	labelled data
0.7652935454	biocreative
0.7652372991	earthquake
0.7652228333	frequent words
0.7651966910	bi directional attention
0.7651963937	plain text
0.7650908464	main reason
0.7650818862	graph attention network
0.7649761873	social media data
0.7648946531	span based
0.7648670603	syntactic trees
0.7648380257	ubuntu
0.7648210859	partially observed
0.7647927879	wsj0
0.7645470729	knowledge graph embeddings
0.7644725173	natural language queries
0.7644307881	string matching
0.7644251334	low level
0.7644235935	typological features
0.7643859876	deep convolutional
0.7643678910	sentence encoder
0.7643370051	level information
0.7643037781	centre
0.7642578570	unaligned data
0.7642430219	iq
0.7641299129	sentence scoring
0.7641272544	strong baselines
0.7640874696	network embedding
0.7640865500	multi30k
0.7640386707	commission
0.7638723145	kitchen
0.7637551518	speech recognition engine
0.7637063154	web documents
0.7636383237	semi supervised learning
0.7635085483	language conditioned
0.7634074492	argument quality
0.7633049216	social media sites
0.7632320200	topic distributions
0.7632080479	lambek
0.7632009308	hebrew bible
0.7631664738	asia
0.7631577293	alexa
0.7630846139	tales
0.7630052459	pairwise ranking
0.7629686191	linguistic units
0.7629533363	marriage
0.7629414965	hand labeled
0.7628928816	graph embedding
0.7628560039	span representations
0.7628292537	satellite
0.7626385124	scientific discourse
0.7625973937	fourier
0.7623936866	tree based
0.7623521778	uncertainty sampling
0.7623402901	cnn lstm
0.7623157638	syllable networks
0.7622510915	slavic
0.7622460710	multitask training
0.7622366864	dialogue policy learning
0.7622068757	adam
0.7621937733	semantic concepts
0.7621823825	communication game
0.7621127192	context vectors
0.7620685116	semitic
0.7619336237	atis
0.7619084460	xml
0.7618718995	semantic representation
0.7618649960	long texts
0.7618431956	semantically coherent
0.7615965437	roget's
0.7615614952	recommendation systems
0.7615241482	generative models
0.7614865878	feature set
0.7614118752	converted speech
0.7614050881	retrieval models
0.7613227534	english vietnamese
0.7612293546	wikisql
0.7611719261	network analysis
0.7611447642	multi hop qa
0.7611239040	selection method
0.7610935069	mt evaluation
0.7610595134	vision language
0.7609935622	correlation analysis
0.7609864714	non negative matrix factorization
0.7609771605	natural logic
0.7608270344	poisson
0.7608108392	joint learning
0.7607684774	feature space
0.7607644724	web application
0.7606665656	data annotation
0.7606511539	multi turn dialogue
0.7606439764	human machine
0.7606036171	aspect based
0.7605668482	brown
0.7605433701	implicit discourse relation classification
0.7604770320	concept normalization
0.7603416071	formal languages
0.7603380813	natural language semantics
0.7603143795	knowledge sharing
0.7602582872	unseen environments
0.7602174747	average f1 score
0.7602005378	listen
0.7601448177	statistical analysis
0.7601362855	feed forward networks
0.7600734305	levenshtein
0.7600320359	dop
0.7598553772	human gaze
0.7598477668	annotation graphs
0.7598422317	technical documents
0.7597675052	emotional responses
0.7597061032	knowledge graph construction
0.7596840783	extrinsic evaluation
0.7596811060	czech
0.7596655421	answer spans
0.7596169566	english czech
0.7595667583	intent recognition
0.7595659690	machine learning approaches
0.7595570639	ted
0.7595366767	narrative text
0.7595225219	personal attributes
0.7594149826	sense tagging
0.7592817698	visual commonsense reasoning
0.7592438544	drastically reduce
0.7591143468	unsupervised text style transfer
0.7589011536	chess
0.7586903829	twitter messages
0.7586684695	word boundaries
0.7586535025	stanford sentiment
0.7586503429	e commerce
0.7585918058	question similarity
0.7585873433	open data
0.7585340310	linguistic data
0.7584833728	graph structure
0.7583929443	traffic related
0.7583909670	statistical mt
0.7583651137	embedding layers
0.7583261777	fisher
0.7583056769	legal domain
0.7582428336	rasa
0.7582222363	referendum
0.7582222363	ridge
0.7582160120	word lattices
0.7582091472	statistical parametric speech
0.7581750662	labs
0.7581342544	bahasa
0.7581085437	cartesian
0.7579466799	human machine interaction
0.7579431120	human judgment
0.7579233373	user queries
0.7578840847	dialogue act recognition
0.7577150415	count based
0.7576203614	quotation
0.7576043264	generally applicable
0.7575938531	recognizing textual
0.7575799013	code switched text
0.7575318897	scholar
0.7571609001	chinese text
0.7571551747	dna
0.7571411306	pidgin
0.7571056366	yahoo
0.7570834178	cmu
0.7570538067	handcrafted features
0.7570435393	highly correlated
0.7570069838	baidu
0.7570026385	domain dependent
0.7569602639	clevr
0.7568467603	kurdish
0.7568086842	pre trained
0.7567599673	neural abstractive summarization
0.7566472974	social media content
0.7565996584	news corpus
0.7565914137	algerian
0.7565655195	answer retrieval
0.7565058365	content based
0.7564222935	topic classification
0.7563153973	cross linguistic
0.7563018800	sentence length
0.7562309032	reasoning skills
0.7561745886	spoken words
0.7561490796	feature vector
0.7560606044	indic
0.7559734714	simlex
0.7559538493	weibo
0.7559430083	asr performance
0.7558630814	matching network
0.7557763019	bidirectional rnn
0.7556339372	automatic question generation
0.7556257468	eastern
0.7556180030	document structure
0.7556162960	key insights
0.7554843443	microsoft
0.7552981354	alzheimer's
0.7552239248	open domain dialog systems
0.7552226030	frequency distributions
0.7551913909	oil
0.7551333438	single step
0.7551270108	ci
0.7549998306	k nearest neighbors
0.7549537677	multimodal machine translation
0.7549459700	roc
0.7549304360	human authored
0.7549256843	multilingual representations
0.7548949494	mean reciprocal rank
0.7548616464	linguistic cues
0.7548398874	multiple modalities
0.7548239318	reward learning
0.7547737517	semantic space
0.7547454725	propaganda technique
0.7547212059	attention networks
0.7546702876	attention network
0.7545762176	contextual representations
0.7544946004	validation set
0.7544864123	deep rl
0.7543857994	f1 points
0.7542728696	western
0.7542377249	evaluation metric
0.7541910541	ablation study
0.7540802296	future research directions
0.7540276694	timit
0.7540257552	feature structures
0.7539425927	scientific documents
0.7539337167	natural images
0.7539179001	long text generation
0.7539102553	iemocap
0.7538632275	sentence functions
0.7537840407	mmi
0.7537677882	pre trained transformers
0.7537225025	air
0.7537083013	attribute extraction
0.7536440652	probing tasks
0.7536235356	target dependent
0.7535392172	great progress
0.7535167263	manually constructed
0.7534969061	text corpus
0.7534166550	myanmar
0.7533477185	gpu
0.7533387672	activity detection
0.7533121451	cd
0.7532769718	semantic relevance
0.7532262373	frequency distribution
0.7531908139	empirical evidence
0.7531014018	mikolov
0.7530108854	pretrained language model
0.7529764018	supreme
0.7529212456	dialogue dataset
0.7528494845	figurative language
0.7528384626	dynamic memory
0.7526177269	ibm
0.7525892176	internal states
0.7525178547	seed dictionary
0.7524661259	liu
0.7524471745	chinese english
0.7524339447	wmt14 english
0.7524326614	transformer encoders
0.7523370352	sequential data
0.7522608277	speech signal
0.7522292370	multiwoz
0.7521083990	lexical information
0.7519371072	gutenberg
0.7519368732	fan
0.7519247072	visual attention
0.7518831405	multilingual machine translation
0.7518680872	high performance
0.7518651902	offenseval
0.7517049220	child directed
0.7516978453	equilibrium
0.7516150203	consistently outperforms
0.7515476746	parsing algorithm
0.7515476199	kl
0.7515444725	nlg systems
0.7515275267	duc
0.7514370476	frequency inverse document frequency
0.7514199127	graph attention
0.7514166550	gibbs
0.7514124211	gumbel
0.7513857599	japanese
0.7513781980	knowledge guided
0.7513695957	goal oriented dialogue systems
0.7513643269	latent variable model
0.7513142084	complex question answering
0.7513045621	language technologies
0.7512773596	ct
0.7512022795	recent successes
0.7510102958	identity based
0.7510036732	future works
0.7509280910	speech technology
0.7509065267	error rate reduction
0.7508876260	jointly optimize
0.7508848828	chen
0.7508572230	dependency based
0.7508219086	rouge
0.7508172395	joint goal accuracy
0.7507297143	free form
0.7507069970	lstm model
0.7506794448	turn level
0.7506687946	conceptual knowledge
0.7506086108	aspect level sentiment
0.7504719108	nigerian
0.7504594619	vector representations
0.7504527984	automatically extracted
0.7503635124	msr
0.7502855233	machine learning techniques
0.7500977575	extraction task
0.7500941512	information theory
0.7500482217	universal sentence representations
0.7500229344	arbitrary length
0.7500031735	unsupervised word
0.7499490268	character aware
0.7498703636	semantic representations
0.7498554147	low frequency words
0.7498099349	acl
0.7497889033	tac
0.7497319240	crawler
0.7497319240	administrative
0.7497316522	deep learning based
0.7497164636	latent structure
0.7497098519	small amounts
0.7496500800	neural conversation model
0.7495533546	text understanding
0.7494805315	variational attention
0.7494494670	contextually relevant
0.7494444999	previous works
0.7493443820	point wise
0.7493230202	phrase embeddings
0.7493183441	cuneiform
0.7492383065	fb15k
0.7491142604	discourse level
0.7490466101	java
0.7490438943	relative improvement
0.7490190712	tv shows
0.7489674990	ccg
0.7489162902	sigmorphon
0.7488748431	neural semantic parser
0.7488652114	tdnn
0.7488030240	goal oriented dialogue
0.7487819937	entity aware
0.7487122002	conll
0.7485047806	transformer architecture
0.7484902453	random initialization
0.7484581751	attempto
0.7484136308	laser
0.7483472260	iwslt
0.7483417756	hamming
0.7483360040	bbc
0.7483328487	unicode
0.7483289820	context modeling
0.7483010464	von
0.7482571166	sre
0.7482349060	mixture models
0.7482287760	word meaning
0.7482251622	language documentation
0.7481492877	conversation context
0.7481255198	user's query
0.7481230578	elastic weight
0.7480976389	consistently outperform
0.7480885353	beam search decoder
0.7479783724	multilingual neural machine translation
0.7479720143	abstractive dialogue
0.7479610778	domain adapted
0.7479519439	larger context
0.7478423560	twitter sentiment analysis
0.7478143076	great potential
0.7477404266	radial
0.7477206074	regularization techniques
0.7477193407	reference summaries
0.7477133436	gaussian distribution
0.7476428169	data streams
0.7475357439	hits
0.7474955361	likert
0.7474712890	semantic shifts
0.7474514993	deep networks
0.7474042664	unsupervised adaptation
0.7473372564	knowledge management
0.7473362882	distributed vector representations
0.7472771017	ary relation extraction
0.7471994517	aspect specific
0.7471444274	bridging anaphora
0.7471301546	syntax tree
0.7471220091	nist
0.7470995964	librispeech
0.7470320521	long range dependency
0.7470002439	frequency domain
0.7468963448	neural abstractive
0.7468514005	temporal structure
0.7468253670	relation detection
0.7468107620	feature vectors
0.7467878121	similar languages
0.7467752343	multilingual models
0.7467256604	language model pretraining
0.7467191395	sample sizes
0.7467174354	xlm
0.7466603281	biomedical corpora
0.7466502182	paraphrase corpus
0.7466311063	lexical selection
0.7465839782	conditional random
0.7465189193	question type
0.7465172087	indonesian
0.7465071943	speaker aware
0.7463632452	urdu
0.7462911081	autoregressive translation
0.7462722543	spoken language translation
0.7462502189	american
0.7462026850	labeled data
0.7461469959	task oriented dialog systems
0.7461342143	textual documents
0.7461115965	instance selection
0.7460081942	switchboard
0.7459869227	text fragments
0.7459570667	hard attention
0.7459481861	department
0.7459067431	sampling algorithms
0.7458945957	np
0.7458803393	news translation
0.7458503762	dependency length
0.7457995022	trec
0.7457176980	neural ranking
0.7456861711	fever
0.7456590244	document embedding
0.7456507935	ensemble methods
0.7456075650	zipf
0.7456017350	word analogy task
0.7455660701	complex sentences
0.7454589698	symbolic knowledge
0.7453934251	transformer architectures
0.7453689011	wmt
0.7453685158	apple
0.7453413386	state machine
0.7451683048	lower layers
0.7451240963	accurately predict
0.7450017777	lexicon based
0.7449838247	question aware
0.7449748760	human judgments
0.7448781450	ensemble model
0.7448630844	text analysis
0.7448422287	communication channel
0.7448334628	san
0.7448106533	solar
0.7448085402	mt output
0.7447828105	lambek grishin calculus
0.7446612882	ood
0.7446521047	autoregressive topic
0.7446437302	entity detection
0.7446293850	autoregressive model
0.7445382202	sql
0.7445036290	italian
0.7444840925	code mixed social media text
0.7444664011	product description
0.7444463154	glue
0.7443647531	wsj
0.7443095242	winograd
0.7441954484	character embeddings
0.7441664863	bangla
0.7441436775	syntax trees
0.7441252786	medical report
0.7440857786	mt systems
0.7440004309	social media text
0.7438675964	auc
0.7438551350	pas
0.7438105754	text collections
0.7438042491	differentiable neural
0.7437560708	speech features
0.7436883450	encoder decoder architecture
0.7436807998	arabic english
0.7436713055	deep generative models
0.7436016910	facebook
0.7435840849	path based
0.7435005404	topic specific
0.7434928810	unified medical language
0.7434567777	hungarian
0.7434402100	citizen
0.7434393392	unstructured text
0.7434203448	pretrained embeddings
0.7433162679	romance
0.7433110091	neural models
0.7432948952	new york times
0.7432882732	relevance matching
0.7432684863	ppi
0.7432567867	neural crf
0.7432548836	unlabelled data
0.7432325871	dutch
0.7432049128	wmt14
0.7431997145	multi paragraph
0.7431779235	competitive baselines
0.7431643896	knowledge graph reasoning
0.7431250004	information access
0.7430820730	deep voice
0.7430222909	narrative texts
0.7429985276	translation task
0.7429843788	coco
0.7429211018	multi encoder
0.7429021993	multi hop reading comprehension
0.7428854262	mfcc
0.7428737188	unified framework
0.7428682505	tamil
0.7428529652	average f1
0.7428380467	hidden representation
0.7428284807	human level
0.7428194501	spatial attention
0.7428048866	online discussion
0.7426861267	error rate reductions
0.7425918242	consistently improves
0.7424683607	entity mention
0.7424493287	crf model
0.7424410524	entity retrieval
0.7424229340	hand crafted features
0.7424189840	text complexity
0.7423089755	semantic analysis
0.7423081255	speech recognition systems
0.7422709758	python
0.7421198733	cc
0.7420055122	real user
0.7419711336	semantic segmentation
0.7419148369	mean squared error
0.7418599744	english language
0.7417763830	amazon
0.7417516569	li
0.7417312519	king
0.7417020540	multi head attention mechanism
0.7416323120	transformer networks
0.7415579282	chime
0.7414502348	cl
0.7413391054	loyalty
0.7413285684	hindsight
0.7412431287	ray
0.7411889528	question classification
0.7411417557	parallel data
0.7410484472	echo
0.7410384740	neo
0.7408800168	human reading
0.7408362961	ptb
0.7408171046	human feedback
0.7407954760	language resources
0.7407195803	empirical study
0.7407193019	atlas
0.7406575661	qg
0.7405959615	aishell
0.7405051709	limited supervision
0.7404597756	computational complexity
0.7403611134	higher level
0.7403270528	annotated corpora
0.7403239608	local context
0.7401089338	zhang
0.7400869995	xlnet
0.7400150313	da
0.7399183943	foster research
0.7398709131	invariant training
0.7398006315	news summarization
0.7397829368	noisy speech
0.7397413436	finnish
0.7397366094	highest score
0.7396266195	query based
0.7395974225	semantic properties
0.7395801614	multi domain dialogue
0.7395650972	kg embedding
0.7394896093	text document
0.7394689450	general domain
0.7394422462	data analysis
0.7394378309	unsupervised machine translation
0.7394339736	dimensional vector spaces
0.7394187205	invariant representations
0.7394073699	sentiment scores
0.7393823124	term frequency inverse document
0.7393539330	speaker embeddings
0.7393493316	remarkable success
0.7393315226	conversational speech recognition
0.7391824762	pdtb
0.7391277580	topological features
0.7391093308	salient sentences
0.7390946508	slightly lower
0.7389970791	snli
0.7389726801	multi instance
0.7389409358	target speaker's
0.7389224679	memory based
0.7389212646	lf
0.7388513591	ontonotes
0.7388010322	america
0.7387453755	long range correlations
0.7387303256	parameter tuning
0.7387129646	encoder outputs
0.7386700212	aurora
0.7386099420	user interaction
0.7385916654	bleu scores
0.7384017045	multi instance learning
0.7383320628	gaussian
0.7381868262	statistical parser
0.7381547249	biomedical information extraction
0.7381141948	reference sets
0.7381057404	sentence meaning
0.7380050642	natural languages
0.7379970609	auxiliary loss
0.7379576539	coreference annotation
0.7378682134	graphical model
0.7378514923	word count
0.7378185694	answer candidates
0.7377756332	sentiment classifier
0.7377224991	pan
0.7376888831	kit
0.7376798676	generated captions
0.7376781799	x ray
0.7376769007	neural sequence labeling
0.7376681875	sparse transformer
0.7376569769	eu
0.7376426651	syllable based
0.7376365673	citation sentiment
0.7376030746	feature based
0.7375031982	yang
0.7374240026	answering questions
0.7374115453	lium
0.7373763202	long distance dependencies
0.7373132836	sequence learning
0.7373064140	traditional machine learning
0.7372973187	relative error reduction
0.7372965797	portuguese
0.7372813795	dialogue history
0.7369468543	elmo
0.7369296199	conversational models
0.7369241507	boolean
0.7368696705	flat ner
0.7368140051	xl
0.7367878415	media posts
0.7366783289	implicit discourse
0.7366595580	joint modeling
0.7366311839	optical character
0.7366293189	evidence retrieval
0.7364586637	latent variable models
0.7364372556	language dependent
0.7364355004	global context
0.7364027025	concept based
0.7363899074	supporting evidence
0.7363155602	human agent
0.7362612853	contextual word representations
0.7361983235	mean average precision
0.7361819404	hierarchical clustering
0.7361293769	nat
0.7360969375	dialogue modeling
0.7360297124	gan based
0.7360169917	sc
0.7360028422	english chinese
0.7358436878	learning environment
0.7357534739	elderly
0.7357033600	brown corpus
0.7356609650	active memory
0.7356523533	morphological forms
0.7356231992	semantic change detection
0.7355466070	ood detection
0.7353586089	aspect based opinion
0.7353262095	consistent gains
0.7352410775	text coherence
0.7352189561	vector space models
0.7352050211	wmt18
0.7351751043	text adventure
0.7351352737	human evaluation
0.7351225804	icd
0.7351096256	card
0.7349513237	aspect category sentiment analysis
0.7349221470	document grounded
0.7348879661	bea
0.7348596484	originally developed
0.7348227932	linear discriminant
0.7348052009	multi turn response selection
0.7347334805	interestingly
0.7346887013	neural sequence
0.7346776740	binary classification
0.7342843101	joint embedding
0.7342807849	multi class classification
0.7341945742	feature engineered
0.7341659956	fully automated
0.7341279577	text documents
0.7341211213	hi en
0.7340419129	downstream tasks
0.7340287142	high resourced
0.7339718927	pseudo parallel
0.7339651639	w nut
0.7339294244	entity names
0.7339065129	similarity based
0.7339060802	peking
0.7338508690	semantically annotated
0.7337985300	rnn architectures
0.7337823626	voynich
0.7337247080	knowledge discovery
0.7336503073	title generation
0.7335780333	mixed language
0.7335491025	watch
0.7335372557	lstm rnns
0.7335288037	india
0.7335207092	english russian
0.7335008149	chronic
0.7334545499	intermediate representation
0.7333970506	targeted sentiment
0.7333949514	negative sentiment
0.7333708683	social interactions
0.7333237049	voice synthesis
0.7332844858	tweet level
0.7332132065	french
0.7331557365	swedish
0.7331158225	feature mapping
0.7331154556	neural variational
0.7330705865	basque
0.7330465537	human judgements
0.7330324224	quality control
0.7329870431	discourse context
0.7328761327	zero shot
0.7328158552	bond
0.7328050027	text representations
0.7326897528	community question
0.7326562726	semantic lexicon
0.7326240456	blacklivesmatter
0.7326240456	f_
0.7326005566	xtag
0.7325967848	dependency information
0.7325659337	phrase based statistical machine translation
0.7325078522	dialog agents
0.7324825723	spanish
0.7324795190	physics
0.7323951615	ehr
0.7323580814	prosodic features
0.7323352482	roberta
0.7322930509	governance
0.7322896248	language inference
0.7322314957	chinese machine reading comprehension
0.7321671070	news comments
0.7320910489	english
0.7320192619	dependency graphs
0.7320033546	grishin
0.7317801417	roman
0.7317438151	sequence alignment
0.7317195624	attracted increasing
0.7314752774	ctc model
0.7313953551	decoding algorithm
0.7313382467	commonsense question answering
0.7312594981	transformer layers
0.7312529230	supervised relation extraction
0.7312194214	automatically generated
0.7310305687	direct speech
0.7310028330	abstractive summary
0.7310021077	attempto controlled
0.7309244447	low resource settings
0.7308985757	deep transformers
0.7308838630	korean
0.7308488204	harmonic
0.7308107371	institutes
0.7307606496	pre processing
0.7306041727	binary codes
0.7305957273	prior works
0.7305803979	distributed word
0.7305747128	similarity metrics
0.7303960612	hierarchical attention network
0.7303268320	word boundary
0.7302757071	cough
0.7302679607	sequence matching
0.7302366773	dialogue models
0.7302065654	contextual embedding
0.7301951804	unstructured data
0.7300109974	natural language parsing
0.7299679494	abstractive text
0.7299533463	hidden representations
0.7299227992	f_1
0.7298122634	ai agents
0.7297953858	hierarchical decoding
0.7297801778	embedding model
0.7297623570	semantic fidelity
0.7297570827	ensemble method
0.7297559118	comparable corpus
0.7297369814	federal
0.7296886206	penn
0.7296033733	classical machine learning
0.7295534366	corpus statistics
0.7295437655	acoustic tokens
0.7294182492	user preference
0.7293459696	em
0.7293196370	pearson
0.7293113362	dnn models
0.7292639441	historical documents
0.7292364582	lda topic
0.7292068836	red
0.7292028189	gaussian mixture model
0.7291812112	multi turn conversations
0.7291467605	received increasing attention
0.7290848363	language production
0.7290025449	neural response generation
0.7289958447	action prediction
0.7289514638	seq2seq model
0.7289311423	unsupervised clustering
0.7288752908	encoder decoder models
0.7288224849	bi directional recurrent
0.7287940187	linguistic complexity
0.7287395468	interactive learning
0.7287301596	wikipedia based
0.7286693605	chi
0.7286553059	semantic structures
0.7286437523	open domain conversation
0.7286344450	visual semantic
0.7285829270	slot types
0.7285531840	mtl
0.7285380065	clinical concepts
0.7284582977	constrained decoding
0.7284182691	svm
0.7283693944	conneau
0.7282042878	gaussian mixture models
0.7281396637	critically important
0.7280839529	multiple documents
0.7279908400	short answer
0.7278924111	openai
0.7278508129	kingdom
0.7278502518	iterated
0.7277658785	hans
0.7277260284	sequence models
0.7276419848	word alignments
0.7275990798	similar questions
0.7275757781	optimization problem
0.7275440412	german
0.7275296840	search logs
0.7275027886	president
0.7274023986	arm
0.7272992626	awd
0.7272751864	abstractive sentence
0.7271422016	word clusters
0.7271014173	text entailment
0.7269449028	turkish
0.7269238187	scale free
0.7269191781	sentence summarization
0.7268972817	aligned corpora
0.7268952350	wind
0.7268822192	semantic similarity measures
0.7268639600	gated recurrent neural
0.7268248839	syntactic dependency
0.7268095621	multi label text classification
0.7267285138	github
0.7266706678	social norms
0.7266364288	polysynthetic
0.7265841578	semantic knowledge
0.7265630090	question answer pair
0.7265526077	multi word
0.7264979693	mining techniques
0.7264437993	pr
0.7264059605	small scale
0.7263601292	machine translation systems
0.7263589710	compression rate
0.7263135575	relation aware
0.7262729041	candidate answers
0.7262610538	semantically related
0.7262526877	hmm
0.7261618242	semantic graph
0.7261584926	eer
0.7261165946	written language
0.7260151011	persian language
0.7260126151	injecting
0.7259470733	wikipedia
0.7259422306	word pairs
0.7259369929	variational auto
0.7259129234	text chunking
0.7258941821	task specific
0.7258612658	bank
0.7257690123	scan
0.7257108942	kb
0.7256353554	biased language
0.7255539771	ssl
0.7254567732	computational efficiency
0.7253319133	performance degradation
0.7253239441	conversational machine comprehension
0.7252529414	multi span
0.7251704289	conversation structure
0.7250245061	protein interaction
0.7250216374	network architecture
0.7249935749	chemistry
0.7249214515	data sets
0.7248447052	jointly optimized
0.7248361022	personalized dialogue
0.7248132237	meta data
0.7247936194	topic quality
0.7247799005	u net
0.7247463432	symbolic representation
0.7246837696	speech representations
0.7246466233	word adjacency
0.7245820524	trf
0.7245045823	dataset biases
0.7244625543	srl
0.7244347699	entity matching
0.7244217878	place names
0.7242357514	qualitative analysis
0.7241918773	sota
0.7241387238	browser
0.7241304148	projection based
0.7241208353	lstms
0.7240788736	mit
0.7240783867	social media messages
0.7240445515	adversarial networks
0.7240428972	twitter dataset
0.7239211464	dependency lengths
0.7238684353	word substitution
0.7238203385	statistical language models
0.7237965203	markov
0.7237548605	relative word error rate
0.7237397865	coronavirus
0.7235785136	unsupervised neural machine translation
0.7235573502	kbp
0.7235450172	blue
0.7235361228	distributional models
0.7235291915	lexicon grammar
0.7235081283	lower dimensional
0.7234496113	language instructions
0.7233752557	alt
0.7233519555	trek
0.7233154735	north
0.7232534547	efficient implementation
0.7231828720	lstm networks
0.7231583579	transition based parsers
0.7230889327	rc
0.7230616100	gang
0.7230565878	reward functions
0.7230512019	markup
0.7229591813	memory cell
0.7229318944	cohen's
0.7228989278	semantic frames
0.7228740932	stochastic model
0.7228378531	east
0.7227635078	code search
0.7227405339	amr
0.7227351645	convolutional layer
0.7227232832	gru
0.7226769274	vector regression
0.7226561302	recent years
0.7226301361	initial results
0.7226016500	vae
0.7225993201	distilled
0.7225270652	word forms
0.7225026690	speech transcription
0.7224858540	processing tools
0.7224340436	star
0.7224038712	golden
0.7224024054	complex sentence
0.7223871754	unsupervised morphological
0.7223091547	surface features
0.7222910784	sne
0.7222699852	label space
0.7222391080	direct access
0.7221474496	textual data
0.7221397777	function words
0.7221370696	african
0.7220557744	preliminary results
0.7219943138	lstm based
0.7219919248	crf
0.7219481037	software tools
0.7219311463	significant drop
0.7218728889	jointly optimizing
0.7218457344	hand written
0.7217057552	wordnet
0.7216799230	dark
0.7216653159	gpt
0.7216613173	language proficiency
0.7215824045	chinese
0.7215376555	deep learning models
0.7214223403	mandelbrot
0.7214203567	biomedical entity
0.7214145249	coherent story
0.7214037675	evolutionary
0.7213473800	multi reference
0.7212733643	temporal dynamics
0.7212670231	image classification
0.7212634531	wiki
0.7212490936	asr systems
0.7212414334	spin
0.7212186490	aware attention
0.7211624013	propositional
0.7210956855	language variation
0.7210304572	learning algorithms
0.7210248086	high resource
0.7209954120	single gpu
0.7209663274	song
0.7209279371	ctc
0.7208557577	cord
0.7208417067	gan
0.7208270479	bert based
0.7207896934	small datasets
0.7207796928	health information
0.7207790003	ontology learning
0.7206811305	comprehend
0.7206417659	background noise
0.7206280596	t sne
0.7206083602	dense representations
0.7206061346	hip
0.7205451120	tts
0.7205297842	reproducible research
0.7204605397	empirical analysis
0.7204291204	arabic sentiment analysis
0.7204165049	transition based parsing
0.7203568755	bengali
0.7203523931	st
0.7203507501	indian
0.7202458953	response quality
0.7202311867	cooperative
0.7202070367	nlu
0.7201858574	unlabelled speech
0.7201316430	structured knowledge
0.7200395312	polysemous word
0.7200317394	technical terms
0.7199686729	software quality
0.7199526471	type aware
0.7198339262	recently introduced
0.7197735557	speech corpus
0.7197673744	humanities
0.7197389074	relation identification
0.7197265796	iterative back translation
0.7196382258	text length
0.7196092007	visual semantic embeddings
0.7195721411	ace
0.7195627866	user level
0.7195124175	multilayer
0.7193884269	unlabeled documents
0.7193480962	understudy
0.7193394247	wikitext
0.7193318763	hindi
0.7193098921	investigative
0.7192672162	stanford
0.7192054838	definition extraction
0.7191900694	sleep
0.7190860235	convolution layers
0.7190328938	cnn daily mail dataset
0.7190306284	e2e
0.7190189922	global features
0.7189464346	multi sense word embeddings
0.7189280647	movie ticket
0.7189187191	smt
0.7188020713	speaker characteristics
0.7187168840	matching model
0.7186621600	wing
0.7186362073	energy function
0.7186278197	contextual information
0.7186209151	automatically identifying
0.7185888200	public sentiment
0.7185806261	poor performance
0.7185715734	question generator
0.7185248978	pharmaceutical
0.7185248978	imagery
0.7185215552	dynamic context
0.7185000113	text preprocessing
0.7184809556	extracting keyphrases
0.7184771886	influenza
0.7184555513	fake review
0.7184301033	outer
0.7184272146	relational data
0.7183828588	cross lingual transfer learning
0.7182637371	state level
0.7181611637	sina
0.7181311727	cer
0.7181139286	fiction
0.7180608711	scientific writing
0.7179877588	greek language
0.7179797851	manually crafted
0.7179326066	warning
0.7179053753	agency
0.7178879785	oov
0.7178220835	current state
0.7177861707	gradient based
0.7177809733	neural relation extraction
0.7177384461	kg
0.7177160445	dialogue model
0.7176812892	micro level
0.7176393307	word piece
0.7176276740	genetics
0.7176276740	systemic
0.7176276740	ecological
0.7175498690	translational
0.7175474625	ir
0.7175158381	ter
0.7174737676	health conditions
0.7174570314	heritage
0.7174473493	generative modeling
0.7174073999	human ratings
0.7173950919	low dimensional vector
0.7172957245	syntactic analysis
0.7172633885	knowledge based
0.7171936923	bible
0.7171891771	text classifier
0.7171887451	visual concepts
0.7171127290	direct supervision
0.7170780215	billing
0.7170780215	hiding
0.7170625230	lifelong
0.7170615383	user input
0.7169929836	consistent improvements
0.7169823730	squad
0.7169805149	der
0.7169784157	multidimensional
0.7169751446	semantic annotations
0.7168619168	dm
0.7168415467	bilingual text
0.7167824861	dailymail
0.7166471032	automatically discovered
0.7166376947	dirichlet
0.7165094566	word vector spaces
0.7164473638	probabilistic topic models
0.7164232604	word embedding models
0.7163754454	previous studies
0.7163482219	node representations
0.7163467181	dnn
0.7163112504	event arguments
0.7163077455	inference speed
0.7162731978	data sparsity issue
0.7162645054	text readability
0.7162480532	domain invariant features
0.7161513990	forensic
0.7161129801	baby
0.7160520147	extremely low
0.7160464111	urban
0.7160042894	response generator
0.7159437470	nli
0.7159320259	human annotators
0.7159272845	single turn
0.7158568646	glottal
0.7158466581	tail entities
0.7158284509	chinese ner
0.7158067168	training objectives
0.7157360421	dialog model
0.7157109989	neural coherence
0.7156806382	text level
0.7156532487	marco
0.7156493137	spoken captions
0.7156292615	vanilla transformer
0.7155890069	metoo
0.7155845607	hyperbolic
0.7155814755	periodic
0.7155677511	pretrained word embeddings
0.7155577887	knowing
0.7155362418	substantially outperforms
0.7155108007	ai
0.7153539561	single layer
0.7153096874	social context
0.7152807796	clinical texts
0.7152492409	flight
0.7152492409	fuel
0.7152492409	planners
0.7152492409	constitution
0.7152470226	cyberbullying
0.7152278154	latent vector
0.7151897894	word translation
0.7151732530	polish
0.7150961992	transformation based
0.7150724294	sparseness problem
0.7150499665	temporal information
0.7150408655	prosodic information
0.7150396652	open domain conversational
0.7150024100	walk
0.7149504186	ad hoc retrieval
0.7148869513	mosei
0.7148869513	giza
0.7148766393	success rates
0.7148268841	bert large
0.7146666381	multinomial
0.7146658736	wait
0.7146422262	worker
0.7146064250	vietnamese
0.7145901458	drug interaction
0.7145503739	phonetic transcriptions
0.7145335115	initiative
0.7145315360	nist chinese
0.7145186446	deep learning architectures
0.7145184624	gas
0.7144927707	wnut
0.7144748394	statistical language
0.7144591115	sequence training
0.7144464036	cs
0.7144426679	summary generation
0.7143860057	glove
0.7143762605	rnn language models
0.7143468667	regularization method
0.7142743178	opinions expressed
0.7142287342	information gain
0.7141332987	generated poems
0.7141238322	born
0.7141202931	russian
0.7140694623	sentential context
0.7140670323	main ideas
0.7140287127	researchers working
0.7140196621	sorbian
0.7140189679	concept space
0.7140148467	dependency syntax
0.7138706650	data analytics
0.7138691693	description generation
0.7138601511	vinyals
0.7138062583	sequence length
0.7137951413	language development
0.7137355845	joint probability
0.7137264481	plausibility
0.7136906462	intermediate representations
0.7136696052	word similarities
0.7136675134	mrc tasks
0.7135718741	pre trained transformer
0.7135013550	higher quality
0.7133824774	differential
0.7133781716	machine translation evaluation
0.7133598724	source target
0.7133168745	humanitarian
0.7132932527	word problems
0.7132504405	few shot
0.7131749883	lms
0.7131107000	cellular
0.7129965415	slu
0.7129384148	linear classifier
0.7129367552	evidence extraction
0.7129301149	distant supervised
0.7129247543	main findings
0.7128956196	common knowledge
0.7128547940	arc
0.7127787032	ethics
0.7127467923	football
0.7127281929	open domain dialogue generation
0.7127132798	significantly outperform
0.7126941880	inference network
0.7126811069	grapheme based
0.7126634961	vqa
0.7126504417	dialog system technology
0.7125345360	nlg
0.7125340239	global constraints
0.7124350751	event mentions
0.7124349438	long short
0.7124347190	probabilistic generative model
0.7124284165	human involvement
0.7123927714	american sign
0.7123570507	recent studies
0.7123313678	explanation generation
0.7122602708	graphics
0.7122203979	news events
0.7121569983	language navigation
0.7121495977	network transducer
0.7120622082	medical terms
0.7119764273	semeval
0.7118988018	greatly reduces
0.7118978969	interpersonal
0.7118234758	semantic memory
0.7117991173	answering dataset
0.7117952270	swift
0.7117617710	lexical databases
0.7117576039	studio
0.7117406658	tagging tasks
0.7116555547	court
0.7116541618	fully connected layer
0.7115917136	proto
0.7115358355	swiss
0.7114834389	context free languages
0.7114634608	f1
0.7113984134	dietary
0.7113412955	query terms
0.7113242302	timeline
0.7113220045	bayes
0.7112504185	spoken queries
0.7112297179	scientific texts
0.7112290010	commonsense knowledge graphs
0.7111819219	back propagation
0.7111645681	relational information
0.7111333807	seed set
0.7110589617	automatic metrics
0.7110120640	wer
0.7109734653	tasking
0.7109734653	shen
0.7109734653	coping
0.7109687496	data oriented
0.7109622602	lstm
0.7109518817	non projective
0.7109138674	matter
0.7108847438	home
0.7108371563	factorized
0.7108259880	nlp pipelines
0.7108239036	intrinsic evaluation
0.7108017443	nut
0.7106036241	distant languages
0.7104188336	generating coherent
0.7103691282	lm
0.7103035176	advertising
0.7102645185	covid 19 pandemic
0.7102389692	speech production
0.7102133789	cp
0.7101355779	square
0.7100716611	french language
0.7100690245	position embeddings
0.7100238519	bandit
0.7098582697	modulated
0.7098375846	restoration
0.7098183689	precision rate
0.7097348614	interpolation
0.7096993860	weight consolidation
0.7096352981	resource speech challenge
0.7096270774	achieved impressive
0.7096179377	perceptron
0.7095888508	dialog dataset
0.7095148215	fantasy
0.7094999742	low resource language
0.7094674714	fitness
0.7094628105	bilingual word
0.7094516644	cross document event
0.7093905465	image recognition
0.7093099253	similarity matrix
0.7092890359	world wide
0.7092449019	dialogue context
0.7092045407	syntax based
0.7091590283	consortium
0.7091243139	lottery
0.7091013501	data efficiency
0.7090321308	chinese language
0.7089906208	discourse units
0.7089711448	bert
0.7089531255	glass
0.7089202764	lazy
0.7089107890	single model
0.7088893293	kappa
0.7088759826	network parameters
0.7088626071	flu
0.7088584695	resource setting
0.7085666492	presidential
0.7084816864	eventually
0.7084568770	windows
0.7084448545	manager
0.7084396232	controlled text
0.7084133641	portal
0.7083922856	zone
0.7083058524	fair
0.7082389368	event related
0.7081961313	extrinsic evaluations
0.7081822884	dependency relation
0.7081671193	neural text generation
0.7081665544	jointly learning
0.7081234133	product search
0.7080672703	neural attention
0.7079901230	episodic
0.7079869048	phrase tables
0.7079757482	pi
0.7079647949	extractive methods
0.7078962429	expressive speech
0.7078473604	reddit
0.7078149107	supertagging
0.7078046373	topic distribution
0.7077909782	cluster based
0.7077626908	bidirectional recurrent neural network
0.7076280523	ranking loss
0.7073907910	instance based
0.7073797717	twitter sentiment classification
0.7073230535	joint training
0.7072832099	dependency relations
0.7072772662	multimodal translation task
0.7072383099	visual representation
0.7072176562	aggregator
0.7071816576	data intensive
0.7071404648	contraction
0.7071404648	literacy
0.7071404648	survival
0.7070933803	restaurant
0.7070906136	lda
0.7070773069	highway
0.7070769481	embedding based
0.7070250812	visual questions
0.7069993853	medical conversations
0.7069362450	conditional generation
0.7069195410	political events
0.7069106106	code switched data
0.7069103862	south
0.7068916760	nlp applications
0.7068507452	arabic morphological
0.7067832304	interaction data
0.7067222447	vision based
0.7066245230	commonsense knowledge bases
0.7065396168	discrete latent
0.7065198021	pm
0.7065040960	autonomous agents
0.7064591596	generative lexicon
0.7064188546	persian
0.7063902439	arabic
0.7063039045	search algorithm
0.7062184441	additive
0.7060358318	data programming
0.7059637634	label distribution
0.7059554034	achieved great success
0.7059278262	text embeddings
0.7059111278	continual
0.7059059916	cnn
0.7058513042	local dependencies
0.7057894799	masked language
0.7057501111	institute
0.7057406517	bleu
0.7057326826	entity level
0.7056308812	european
0.7056156160	spoken dialog systems
0.7055165144	jobs
0.7054106869	rumour
0.7053733379	semantic similarities
0.7052351548	phylogenetic
0.7051995143	unifying
0.7051705430	laboratory
0.7051418121	computer assisted
0.7051359651	multiple passages
0.7050943324	exemplar
0.7050912134	rl
0.7050264112	jensen
0.7050123428	context window
0.7048675892	mrc
0.7047954640	convolutional architecture
0.7047932733	automatic evaluation metric
0.7047161648	rnn
0.7046675576	telugu
0.7046411318	basic questions
0.7046185624	recurrent layers
0.7045944479	probabilistic topic
0.7044598043	va
0.7044419360	closure
0.7044101607	word choice
0.7043168487	assisted
0.7042640042	therapy
0.7041318740	google
0.7041015016	labeled training data
0.7040939278	dag
0.7040914649	input string
0.7040469772	computational linguistic
0.7040407380	crf layer
0.7040264112	aryan
0.7040174538	multi step reasoning
0.7039854139	tensor based
0.7039766839	ranker
0.7039656034	deep speaker
0.7039529912	generating synthetic
0.7039360910	deep recurrent neural networks
0.7039325516	semantic relation
0.7038715566	lessons
0.7037399925	opinion extraction
0.7037293227	agglomerative
0.7037008054	sim
0.7036161767	dynamical
0.7036150125	elastic
0.7035973294	discrete symbols
0.7035685528	carlo
0.7035575228	personal information
0.7035499759	construction method
0.7034391783	snips
0.7033953148	phrase translation
0.7033574329	language resource
0.7033355269	document frequency
0.7033117525	appraisal
0.7032976417	clean speech
0.7032711884	seed words
0.7032646605	comparative study
0.7032510615	deep learning techniques
0.7031580873	dating
0.7031373103	vaswani
0.7031371986	turing
0.7031317212	lexical similarity
0.7030978940	white
0.7030699515	sign language translation
0.7029415983	craft
0.7029232217	quantized
0.7028908676	pre train
0.7028394944	sketch
0.7028294051	text corpora
0.7028206431	lstm network
0.7027859059	phrase representations
0.7027346104	smart
0.7026727802	object descriptions
0.7026592724	fact descriptions
0.7026175565	editor
0.7025575666	war
0.7025334132	thousand words
0.7024967853	key value memory
0.7024870704	discussion topics
0.7024762517	adversarial text generation
0.7024685528	indo
0.7024644028	embedding learning
0.7024634792	restaurant domain
0.7024440348	paraphrastic
0.7023978869	internet
0.7023599644	computation cost
0.7023461612	named entity extraction
0.7023439680	visualizing
0.7022629022	monitor
0.7022094559	syntactic ambiguity
0.7022028735	impairment
0.7021704043	specially
0.7021594703	jointly learns
0.7021300368	helpfulness
0.7020957131	memes
0.7020523109	conversation modeling
0.7020428735	image based
0.7020403884	multi domain dialogue state tracking
0.7020394329	fnc
0.7018933940	prover
0.7018623825	excellent performance
0.7017847920	dream
0.7017795010	key components
0.7017657473	neural semantic parsing
0.7017407984	dependency path
0.7016645693	occurrence statistics
0.7016509083	emergency
0.7015888922	random sampling
0.7015417599	self attentional
0.7015197972	abductive
0.7015015118	k nearest
0.7014768237	transformer network
0.7014438226	dynamic attention
0.7014362430	wh
0.7013929069	formal grammars
0.7013713311	migration
0.7013379843	great importance
0.7013099592	widely explored
0.7013041962	kullback
0.7012966533	linguistic analysis
0.7012554414	profiling
0.7012156160	bilstm
0.7011934387	automatically extracting
0.7011528291	n gram
0.7010951465	reference set
0.7010886130	dimensional vectors
0.7010774249	reasoning abilities
0.7010525133	hinglish
0.7010480798	bayesian
0.7010458632	mandarin
0.7009890697	entanglement
0.7009738284	leap
0.7009323160	siamese
0.7008635882	nlu models
0.7008138706	monetary
0.7007798101	mosi
0.7007481343	pico
0.7007084947	predefined categories
0.7006861294	zipf's
0.7006806046	local features
0.7006231058	deep convolutional neural
0.7006161414	surface level
0.7005998239	poster
0.7005646977	adventure
0.7005600163	trafficking
0.7005375326	slightly higher
0.7005289398	temporal graph
0.7004952506	rhythm
0.7004746428	trading
0.7004538182	lstm cnn
0.7004332146	ndcg
0.7004267139	self organizing
0.7004222343	credit
0.7004090403	continuum
0.7002270674	acoustic features
0.7001725036	categorizing
0.7001542239	fine grained entity type
0.7001395811	characterisation
0.7001395811	determinants
0.7001063416	informatics
0.7000399674	missing facts
0.7000060515	fr
0.6999919846	dialog corpus
0.6999881453	challenge set
0.6999634931	unsupervised dependency parsing
0.6999407370	open research
0.6998612640	defeasible
0.6998612640	particle
0.6997892705	medical literature
0.6997531042	semantic type
0.6997283825	discourse information
0.6997051106	third party
0.6996998228	alignment methods
0.6996670651	answering systems
0.6996337676	url
0.6995672391	insertion based
0.6995356702	lexical analysis
0.6994612251	rights
0.6994234610	devlin
0.6994043236	extended version
0.6993806606	biology
0.6993671606	cloze
0.6993597445	radford
0.6993434146	global information
0.6993276675	word error
0.6993043238	fast growing
0.6992185780	tutorial
0.6992080474	semantic correspondence
0.6991954180	consistent improvement
0.6991950444	pretrained models
0.6990293476	human generated
0.6990214409	low resource scenarios
0.6989863376	iso
0.6989539401	en
0.6989288946	room
0.6989191663	bidirectional decoding
0.6989028109	discern
0.6988291017	major issues
0.6988210793	sparse representations
0.6988046037	mathrm
0.6987130587	diacritic
0.6987069503	experimental results
0.6986525160	key point
0.6986303350	research area
0.6986207197	sampling based
0.6985354085	real news
0.6984709772	minimalist
0.6984707985	high frequency
0.6984178350	mel
0.6983742756	code switching asr
0.6983648525	multi hop question answering
0.6983439550	rhyme
0.6983389864	reinflection
0.6983273825	liu et al
0.6983233297	bert embeddings
0.6982707471	discriminative models
0.6982320398	forests
0.6981921979	macro averaged f1 score
0.6981656697	house
0.6981276327	arts
0.6980790820	building block
0.6979765622	te
0.6979633073	quotient
0.6979614284	cue
0.6979432595	aspect terms
0.6979399999	change detection
0.6979320473	neural based
0.6979215244	bidirectional language model
0.6979159636	entity summarization
0.6979056682	relational facts
0.6978610073	extreme
0.6978298320	couples
0.6978051829	food
0.6978010130	k means
0.6977691010	automatic metric
0.6977434951	reinforcement learning based
0.6977283897	knowledge base question answering
0.6976890328	diet
0.6976355053	document understanding
0.6975697372	feature maps
0.6975393465	morphological tags
0.6974917392	data efficient
0.6974621073	high accuracy
0.6974040033	human communication
0.6973884972	muse
0.6973161347	significant progress
0.6972960085	instant
0.6972660146	cross task
0.6972100513	times fewer
0.6971994014	word distributions
0.6971903237	learned representations
0.6971229940	linguistic theory
0.6970947027	hungry
0.6970755789	end users
0.6969852401	transformer language models
0.6969014618	bed
0.6968760723	unsupervised parsing
0.6968756952	span level
0.6968645724	wave
0.6967662217	domain adversarial
0.6967569594	importantly
0.6966764086	generated summaries
0.6965615275	topic segmentation
0.6965575792	research articles
0.6965523727	le
0.6965187125	spoken term
0.6965063458	online conversations
0.6964840701	parallel sentence
0.6964797527	output layer
0.6964786252	quantitative evaluation
0.6964715532	deep models
0.6964665675	pain
0.6964576893	urdu language
0.6963756828	neural coreference
0.6963702574	class based
0.6963212639	twitter
0.6963152599	pcfg
0.6962273884	text data
0.6961508350	regularized
0.6961027446	neural autoregressive
0.6960972160	dimensional space
0.6960893765	multi sense embeddings
0.6960871827	improving neural
0.6960836490	translation quality estimation
0.6960287417	drawing
0.6960238318	substantially improves
0.6960215886	criminal
0.6959938752	protein interactions
0.6959668404	capital
0.6959643467	structured learning
0.6959350686	los
0.6959202139	pragmatics
0.6958654797	irony
0.6958509174	nets
0.6957558625	first order logic
0.6957343628	rank based
0.6957063341	neural question generation
0.6956942994	sequencing
0.6955864574	credibility
0.6955787249	limited labeled data
0.6955516229	dialogue understanding
0.6954959692	weather
0.6954847094	cyber
0.6954738147	multimodal data
0.6954588483	entropy based
0.6954008094	artificial data
0.6953158738	outstanding performance
0.6953003230	sheds light
0.6952968200	anti
0.6952958538	vq
0.6952419452	gay
0.6952340061	automatic essay
0.6952234975	ner
0.6952163584	target vocabulary
0.6952067890	enterprise
0.6952020997	vinyals et al
0.6951312540	replication
0.6950162644	artist
0.6950055365	model robustness
0.6949908286	logics
0.6949795195	key points
0.6949520554	significantly improves
0.6949433826	medieval
0.6949423955	closely related languages
0.6949302141	suicide
0.6949091335	university
0.6948990827	noisy data
0.6948966334	maximization
0.6948059725	fusing
0.6947763708	explanation based
0.6947744994	self harm
0.6947241492	web data
0.6947157209	rich resource
0.6946902522	look ahead
0.6946881847	hierarchical reinforcement learning
0.6946557609	defect
0.6946198143	figure
0.6945861992	reasoning capabilities
0.6945147565	realm
0.6944739840	theorem
0.6944557774	distantly supervised relation
0.6943768825	internet users
0.6943357913	routing
0.6943329877	opera
0.6943165830	declaration
0.6942984602	asr
0.6942939775	tweet classification
0.6942927241	nuanced
0.6942820793	feature sets
0.6942408474	mt
0.6942332455	standard arabic
0.6940792808	relative wer
0.6940512863	writing systems
0.6940219153	search algorithms
0.6939825226	mind
0.6939609729	target domain
0.6939465756	conversational data
0.6937231334	transduction
0.6936992439	written english
0.6936652422	cross lingual ability
0.6936616413	stone
0.6936616413	universities
0.6935284129	decomposing
0.6935247599	internal representations
0.6935118671	living
0.6934518354	mimic
0.6934101526	straight
0.6933700751	chapter
0.6933584415	favor
0.6932935604	chinese english translation
0.6932658049	relationship detection
0.6932375184	app
0.6932242812	universal language
0.6932175461	bot
0.6931992920	normal speech
0.6931923365	word sequence
0.6931322208	cross language information retrieval
0.6931014824	entailment relations
0.6930517566	bayesian network
0.6930196065	differentiation
0.6930196065	organic
0.6929793274	semantic network
0.6929090900	press
0.6928583225	administration
0.6928127917	language game
0.6927925094	em algorithm
0.6927867687	seq
0.6927677380	albert
0.6927380153	community detection
0.6927250718	segmenting
0.6927226395	mutation
0.6927170706	data filtering
0.6927017152	sequent
0.6926807148	optimality
0.6926436900	bullet
0.6926276316	answer questions
0.6926102060	joint model
0.6926004143	workshop
0.6925018743	dialogue processing
0.6924945441	generative pre training
0.6924791986	participating systems
0.6924767371	analyzer
0.6924660411	gate
0.6924543874	ms
0.6924494591	ddi
0.6922920937	structural information
0.6922250088	rm
0.6921666986	retriever
0.6921658920	relation facts
0.6921573594	rough
0.6921498204	qa
0.6920764485	grapheme to phoneme conversion
0.6920519188	hidden layers
0.6920446916	film
0.6919523433	committee
0.6917994644	lfg
0.6917358509	pseudo label
0.6916635269	single corpus
0.6916580121	dictionary based
0.6916519682	syntactic patterns
0.6916508252	user requests
0.6914400367	instructional
0.6913584051	summarisation
0.6912365572	reverse
0.6912070559	automatic short answer
0.6911177609	gf
0.6910802199	k nearest neighbor
0.6910079101	cov
0.6910009454	external lm
0.6909799170	amr graph
0.6909711551	domain sensitive
0.6908729671	mnli
0.6907552732	extinct
0.6907552732	tension
0.6907552732	adjustments
0.6907552732	waves
0.6907521215	lexical functional
0.6907238825	logic based
0.6906993189	biomedical knowledge
0.6906735855	mediated communication
0.6906682436	dinner
0.6906673098	trans
0.6906638493	genetic
0.6906398508	peer
0.6906157351	rdf
0.6905731857	extractive document
0.6905503464	folk
0.6905237573	information loss
0.6904801922	highly imbalanced
0.6904705168	road
0.6904348471	bionlp
0.6904243300	human post
0.6904147933	fourth
0.6904034884	medical information
0.6903664348	water
0.6903664348	infant
0.6903550660	mscoco
0.6903165332	language representation
0.6902938612	raw text
0.6902874830	false information
0.6902143825	model behavior
0.6901162304	devlin et al
0.6901063029	insertion
0.6900130547	search query
0.6899723500	ann
0.6899093062	oov words
0.6898899311	entity pair
0.6898001715	instance learning
0.6897972699	autoencoders
0.6897554466	nmt
0.6897453300	medical data
0.6897182593	disentangled
0.6896826594	lower latency
0.6896419829	low resourced languages
0.6896113506	fst
0.6896088310	curse
0.6895375194	physician
0.6895188265	limited data
0.6895173749	aware training
0.6894822638	reproducibility
0.6893332192	directly optimizes
0.6892707453	case reports
0.6892456373	ita
0.6892387613	onset
0.6892180620	research fields
0.6891221917	clef
0.6891189484	adversarial text
0.6891047054	learnability
0.6890497876	communication systems
0.6890250565	seq2seq models
0.6890240943	human ai
0.6890139326	semantic coherence
0.6889985661	cross lingual word embeddings
0.6889805298	review corpus
0.6889625379	quality diversity
0.6889138475	small perturbations
0.6889072301	goodness
0.6889050464	mr
0.6888613506	cls
0.6888608226	kurdish language
0.6888177686	vlsp
0.6888066299	cross dataset
0.6887936057	accordance
0.6887846220	concept embeddings
0.6887813023	output length
0.6887389298	plagiarism
0.6886640487	human written
0.6886445577	open datasets
0.6886349825	significantly outperforms
0.6886159894	hospital
0.6886122418	syntactic information
0.6886086352	cross lingual embeddings
0.6885979668	subseteq
0.6885979668	heat
0.6885689928	secret
0.6884526633	pathology
0.6884373082	recent works
0.6884161036	rational
0.6884048640	word lists
0.6883853591	qa pairs
0.6883840325	document graph
0.6883265269	storytelling
0.6883121467	text passage
0.6883118051	meta information
0.6882854431	language pairs
0.6882844641	archives
0.6882519057	blogging
0.6882218306	language model adaptation
0.6882217085	amplitude
0.6882098914	attention flow
0.6882059546	nyt
0.6880915921	discriminative training
0.6880889717	cascade
0.6880267055	low diversity
0.6880194493	highly accurate
0.6880180620	dialog structure
0.6879884681	topic space
0.6879510296	tensor
0.6878963334	translation direction
0.6878522693	xlm r
0.6878312792	wmt16
0.6877814407	tf
0.6877331140	ic
0.6877173642	genre classification
0.6877155967	semantic distance
0.6876647922	adversarially
0.6876497172	neural machine translation systems
0.6876163598	sql generation
0.6875909872	sports
0.6875563206	segmentation method
0.6875504581	xnli
0.6875504581	esl
0.6875169340	million tweets
0.6875026534	man
0.6874497253	spotting
0.6874048754	conference
0.6873352751	long context
0.6873100568	overflow
0.6872767654	multilingual speech
0.6872669438	bi text
0.6872641429	criticism
0.6872506279	experimental evaluations
0.6871883181	google's
0.6871635819	downstream applications
0.6871436721	keyboard
0.6870940964	lp
0.6870806090	l1
0.6869990074	basic unit
0.6869379358	cross framework
0.6868809224	data sources
0.6868499957	overlapping speech
0.6867919548	key issues
0.6865486029	adaptation techniques
0.6865371231	dialogue states
0.6865082498	twitter user
0.6864734751	text augmentation
0.6864706864	lexical knowledge base
0.6864621365	talk
0.6864135924	suites
0.6863546814	neural architecture
0.6863517073	powered
0.6863404134	deaf
0.6862232135	knowledge grounded dialogue
0.6861803209	propaganda
0.6861479545	inner product
0.6860657130	academic paper
0.6859984333	cloud based
0.6859386819	worst
0.6859085762	desirable properties
0.6858526603	ernie
0.6858502709	siri
0.6857765181	communication protocol
0.6857662295	representation theory
0.6857171613	linguistic resource
0.6856925531	mart
0.6856925531	ta
0.6856608976	hierarchical topic
0.6856473929	indonesian text
0.6856472379	norm
0.6856065598	unseen entities
0.6855340311	orthogonal
0.6855043319	simulated environment
0.6854790824	rnn based
0.6854302273	radford et al
0.6854254538	icd 10
0.6853475177	sex
0.6853369202	human human
0.6853271346	bridging
0.6853189693	attend
0.6852046507	dense vectors
0.6851969008	aed
0.6851646903	aware neural
0.6851075630	feature rich
0.6848897854	prerequisite
0.6848762923	ad
0.6848747638	maximum mutual
0.6848479335	word class
0.6848143744	clickbait
0.6847792808	pretrained language
0.6847520362	multiword
0.6847336552	image description generation
0.6847132037	online health
0.6846655260	adr
0.6846566696	unpaired data
0.6845802251	convex
0.6845328348	sentence aligned
0.6845258098	information aggregation
0.6844856961	genome
0.6844766207	mix
0.6843976253	diarisation
0.6843349161	nlp
0.6843320526	code switching speech
0.6841639206	multiple domains
0.6841637456	previously proposed
0.6841166277	reproducing
0.6840981758	newly proposed
0.6840436496	lens
0.6840372087	humanoid
0.6840343722	mwe
0.6839676890	posterior distributions
0.6839012479	lie
0.6838804082	efficient training
0.6838480287	parliament
0.6838339313	sentiment information
0.6837641601	logical semantics
0.6837342494	task adaptive
0.6836749850	metaphor
0.6836607055	efficient neural
0.6836255730	fine tune bert
0.6836126647	jointly modeling
0.6835484930	classify sentences
0.6835421502	highly competitive
0.6834982192	achieved impressive results
0.6834363946	discriminating
0.6833723036	self attentive
0.6833424208	language complexity
0.6833413665	recognition systems
0.6833321582	alice
0.6832398238	word2vec
0.6832345772	assistive
0.6831568602	downstream nlp tasks
0.6831472639	code mixed tweets
0.6831359034	semantic relationships
0.6831199985	analysis reveals
0.6830906214	chime 5
0.6830769267	expectation
0.6830615015	deep transfer learning
0.6830118597	cuni
0.6830110530	ill formed
0.6830088640	ranking based
0.6829984197	snippet
0.6829667125	information retrieval systems
0.6829420825	turk
0.6829118247	sentence vectors
0.6828585283	dr
0.6828533463	learning capabilities
0.6828316519	~ \ citep
0.6827446659	sd
0.6827052105	distilling
0.6826999232	unl
0.6826952640	quality metrics
0.6826565783	cca
0.6826125116	semantic classes
0.6825695130	mikolov et al
0.6824980621	cross corpus
0.6824966568	improving generalization
0.6824714813	unaligned
0.6824658627	data privacy
0.6824652436	pros and cons
0.6824569376	snr
0.6824364561	curve
0.6824146116	drop
0.6824015720	siamese network
0.6823757246	hidden units
0.6822603635	eos
0.6822579411	mrr
0.6822522605	continuous speech recognition
0.6821380327	sentence relation
0.6821278328	sequence modelling
0.6820840915	multimodal language
0.6820559231	linguistic style
0.6820553479	purely data driven
0.6820506617	learning agents
0.6820482759	hotel
0.6820434490	prevention
0.6820212065	union
0.6820203414	tm
0.6819619530	suffix
0.6819328526	query document
0.6819114869	bibliographic
0.6819054521	duality
0.6819054521	cepstrum
0.6818006543	tree search
0.6817820950	type information
0.6817277068	cfg
0.6817059546	uas
0.6817007107	double
0.6816887655	tcm
0.6816887655	ke
0.6816887655	olac
0.6816689928	exact inference
0.6814578414	rnnlm
0.6814529808	prize competition
0.6813943505	token embeddings
0.6813801614	stem
0.6813644635	enriching
0.6813437617	equipment
0.6813127308	tagging model
0.6812957946	parsing strategies
0.6812664501	answer set
0.6812261087	noise robustness
0.6812113806	inheritance
0.6812067328	spoken dialogues
0.6811969095	stemming
0.6811793693	speech detection
0.6811096184	lower level
0.6811096123	candidate generation
0.6810490454	flows
0.6810378344	scale invariant
0.6810006556	multimodal translation
0.6809675980	multi target
0.6808533869	max
0.6807564099	alignment method
0.6807396007	semantic consistency
0.6807257379	mark
0.6807178084	data compression
0.6806645488	television
0.6806459870	breast
0.6805761709	stratification
0.6805223640	high risk
0.6805185862	animated
0.6805185862	currency
0.6805185862	typographical
0.6803993323	deft
0.6803773975	relaxation
0.6803729856	abuse
0.6802888019	summarization systems
0.6802828701	chinese named entity recognition
0.6802637455	text clustering
0.6802631797	pos
0.6802145057	assistant
0.6802126694	expert users
0.6801915526	distributional information
0.6801801085	multi domain task oriented
0.6801662495	nlp tasks
0.6801161724	noisy conditions
0.6801012229	extensive experiments
0.6800451941	semantic relation classification
0.6800042932	recently gained
0.6799662749	mass
0.6799377919	chest
0.6799293045	ud
0.6799243936	schools
0.6799023663	main steps
0.6798862527	concept level
0.6798712523	short range
0.6798468604	delta
0.6798122882	dynamic knowledge
0.6797760567	image text matching
0.6797388052	unseen domains
0.6796875559	sgns
0.6796697693	textvqa
0.6796697693	brazil
0.6796663572	break
0.6796422820	considerable improvement
0.6796305441	greatly reduce
0.6796131257	freedom
0.6795670192	forest
0.6795485039	prototyping
0.6795190443	unique characteristics
0.6794610919	lower perplexity
0.6793952739	actor
0.6793665899	abx
0.6793191514	convolution layer
0.6792307036	tag
0.6791456251	mathematics
0.6790670293	syntax semantics
0.6790375384	lr
0.6790329315	graph structures
0.6790267979	covid
0.6790220087	morpho
0.6790193868	lexico
0.6790094080	etm
0.6790094080	vcr
0.6790094080	fs
0.6790032552	encoding method
0.6789769846	hybrid approaches
0.6789566951	polyphonic
0.6789464669	mu
0.6788756380	manual annotation
0.6788483421	rnn model
0.6787801052	word lattice
0.6787545907	lexical constraints
0.6786923709	word distribution
0.6786626076	machine learned
0.6786138000	callhome
0.6785545467	tabular
0.6785336307	observing
0.6785252724	west
0.6785126243	cnl
0.6784560115	lexical sample
0.6784450642	softmax
0.6784421859	ocr
0.6784177674	wall
0.6784073760	discrete representations
0.6783364651	hybrid asr
0.6783357698	audio segments
0.6783312596	adem
0.6783312596	aida
0.6782811291	jointly trains
0.6782378572	fine tuned bert
0.6782311019	mgb
0.6782298503	attention models
0.6780918305	autonomous
0.6780557149	diagnostic dataset
0.6780464084	tolerance
0.6780464084	inequality
0.6780435962	crawl
0.6780103580	transformer
0.6779941591	children's
0.6779658325	learning robust
0.6778342800	similar question
0.6778262519	battery
0.6778001050	olr
0.6777933784	aware transformer
0.6777752427	replay
0.6777721880	text generation models
0.6777621007	clinical nlp
0.6777473951	chit
0.6777262177	related terms
0.6777146399	automatic segmentation
0.6777046704	faithful
0.6776704456	machine generated
0.6776502398	acyclic
0.6776145746	human intervention
0.6776091337	ground truth labels
0.6775656681	ds
0.6775603877	pca
0.6775462967	neural language model
0.6775421876	latent representations
0.6774625850	memory based learning
0.6774163614	la
0.6773823115	scoring functions
0.6773814236	situational
0.6773667972	divide and conquer
0.6773590574	character sequence
0.6773134905	sv
0.6772581856	synthetic corpus
0.6772496995	parameter space
0.6772321009	instagram
0.6772299032	basic units
0.6772270997	neural topic modeling
0.6772086119	audio signal
0.6771630321	ccf
0.6771604689	sars cov 2
0.6771244195	cts
0.6771068815	handwriting
0.6771061213	neural conversational
0.6769983866	language usage
0.6769573754	cross lingual alignment
0.6768871933	signal to noise ratio
0.6768750946	data quality
0.6768302273	shen et al
0.6768118951	jointly predicting
0.6768103444	conll 2003
0.6768060844	cqa
0.6767990999	computational power
0.6767034081	informative covid 19 english tweets
0.6766661513	gdpr
0.6766351004	locality sensitive
0.6766319028	darts
0.6766319028	sgnmt
0.6766319028	spmrl
0.6766319028	sumo
0.6766319028	ascii
0.6766089158	language structure
0.6764804552	elm
0.6764804552	nssi
0.6764804552	qed
0.6764804552	nvsm
0.6764804552	hqa
0.6764804552	ctb
0.6764804552	lcfrs
0.6764804552	hd
0.6764804552	nips
0.6764381120	training set
0.6764296070	alphabet
0.6763929052	modern nlp
0.6763868177	transformer model
0.6763101556	quantification
0.6762912599	relation extractor
0.6762653672	ipa
0.6762302273	peters et al
0.6761767579	dissertation
0.6761467000	engineered features
0.6761127756	sense definition
0.6760690732	mean opinion score
0.6760671720	r package
0.6760110424	embedding vectors
0.6759928525	sentiwordnet
0.6759675378	europarl
0.6759011123	multilingual embeddings
0.6758962997	discourse processing
0.6758936968	rnng
0.6758207298	text comprehension
0.6757814981	grounded language
0.6757808991	semantic vector
0.6757583719	motion
0.6757406988	international
0.6757242107	recently proposed
0.6756989776	multimodal machine
0.6756888446	ss
0.6756888446	cdr
0.6756888446	ltag
0.6756778564	textual inference
0.6756669885	lambada
0.6756310049	ws
0.6756017881	dialogue response
0.6755152184	differentiable
0.6754886268	global attention
0.6754836849	sphere
0.6754685624	audiovisual
0.6754528672	rsa
0.6754494284	grammar based
0.6754362668	chemical
0.6754156788	nations
0.6753929562	pay attention
0.6753685416	college
0.6752765840	attention model
0.6752661098	stochastic
0.6752138704	analogies
0.6751767759	hkust
0.6751767759	mrp
0.6751767759	icu
0.6751282929	textual similarity
0.6751228353	diabetes
0.6750897247	pagerank
0.6750582557	prize
0.6750251777	tc
0.6750224165	conneau et al
0.6750138407	multimedia
0.6749976386	lmf
0.6749821640	hand crafted rules
0.6749590985	correct answers
0.6748330612	mitigating
0.6748323455	talker
0.6747574290	multimodal interaction
0.6747506225	sentence ranking
0.6747167107	sst
0.6747077588	connectionist
0.6746939060	discourse representation
0.6746557606	common grounding
0.6746408866	handwritten
0.6746395167	wizard
0.6746363433	farsi
0.6746316401	computational costs
0.6745986274	relational database
0.6745717842	based sentiment analysis
0.6745440507	future context
0.6745394380	revised
0.6744735239	chinese word
0.6743899520	compositional models
0.6743775569	blanc
0.6743775569	qald
0.6743740164	ir tasks
0.6743693288	virtual
0.6743686894	pointer
0.6743554521	receptive
0.6743382939	med
0.6743347354	dc
0.6742952528	ui
0.6742952528	lra
0.6742948819	vector based
0.6742545458	cold
0.6742524984	logical structure
0.6742456803	eac
0.6742444078	truthfulness
0.6742409892	query understanding
0.6742202219	iqa
0.6742086399	unsupervised alignment
0.6741975716	dbpedia
0.6741693865	alignment based
0.6741686217	slot filling task
0.6741679546	null
0.6741625762	stanford natural
0.6741622054	notably
0.6741608703	unstructured documents
0.6741518784	neural parsers
0.6741303193	training paradigm
0.6740849151	point scale
0.6740672311	relevant passages
0.6740514029	gt
0.6740514029	rbmt
0.6739978046	fuzzy
0.6739691706	generalization power
0.6739493810	long term dependency
0.6739397666	question answering datasets
0.6739230580	zipfian
0.6738577497	embedding features
0.6738454423	hierarchical text classification
0.6737180695	awe
0.6736324428	common sense knowledge
0.6736321579	nmf
0.6735619195	aae
0.6735615139	fusion network
0.6735199240	nl
0.6734953825	standard lda
0.6734699086	api
0.6734637478	attention gated
0.6734535189	messaging
0.6734271119	central component
0.6734251731	labeled examples
0.6733517363	stop
0.6733484281	gist
0.6733453699	nn
0.6733307126	language learning
0.6732888446	ri
0.6731997315	mooc
0.6731687802	data augmentation techniques
0.6731615990	medline
0.6731542330	blstm
0.6731074686	transition based parser
0.6731040770	simple question answering
0.6730420476	lexical complexity
0.6729944879	encoder decoder framework
0.6729468566	register
0.6729326942	interesting findings
0.6728532390	bleu point
0.6728112573	multimodal transformer
0.6727805644	transductive
0.6727388165	toxicity
0.6727220961	drift
0.6727165568	sememe
0.6727025206	round
0.6726628058	cam
0.6726577414	latent semantic
0.6726294907	fast
0.6726217983	recurrent attention
0.6725871420	contact
0.6725659515	tvqa
0.6725628864	comprehension task
0.6725567944	esim
0.6725170085	entire document
0.6725125441	exam
0.6724510091	vc
0.6724359142	toefl
0.6724359142	ltl
0.6724349627	mapreduce
0.6723905376	wake
0.6723550542	latex
0.6723426033	commonsense validation
0.6723181395	open domain questions
0.6722860744	dealt
0.6722491489	agent learns
0.6722352782	stratified
0.6722243576	combinatory
0.6721225425	conll 2005
0.6720727554	cbow
0.6720421586	ulmfit
0.6719371409	logistic
0.6719293606	idf
0.6719223882	user ratings
0.6717942337	multimodal learning
0.6717633123	recurrent neural network based
0.6717495968	ptsd
0.6717495968	drt
0.6717435264	reader's
0.6717435264	chaining
0.6717382171	conll 2012
0.6717003081	meteor
0.6716838092	prepositional
0.6716778852	normalized
0.6716533951	eda
0.6716533951	cda
0.6716383790	deep transition
0.6715984250	responding
0.6715358200	idiomatic
0.6715147590	multi turn conversation
0.6714870569	lc
0.6714540789	exclusion
0.6714402898	tp
0.6714402898	nell
0.6714336551	cm
0.6714249921	scholarly
0.6713928448	textual information
0.6713727405	lt
0.6713465367	meta learning framework
0.6713219148	svd
0.6711827002	digital world
0.6711605805	pg
0.6711605805	nfh
0.6711385606	vaswani et al
0.6710808865	user utterances
0.6710630326	predictive model
0.6710573695	tei
0.6710068311	scraping
0.6709785440	comet
0.6709785440	ram
0.6709519898	backward
0.6709314018	supervised machine learning
0.6708395743	police
0.6708337696	aurora 4
0.6708265608	radio
0.6708024866	organizing
0.6707419504	fake news challenge
0.6707328342	small footprint keyword
0.6707115084	transe
0.6707057714	tacotron based
0.6706634406	question driven
0.6706208697	structured output
0.6706157251	target guided
0.6705740341	lime
0.6705236671	wmt english
0.6705232184	aspect information
0.6704981253	image grounded
0.6704851901	existing vqa
0.6704417728	action recognition
0.6703956222	dqn
0.6703907028	sbert
0.6703907028	dstc
0.6703854159	sentence rewriting
0.6703800020	brief survey
0.6703759324	dblp
0.6703759324	ims
0.6703759324	mm
0.6703729978	separating
0.6703724704	neural language modeling
0.6703288079	pl
0.6703236710	caption dataset
0.6703042612	tape
0.6703035651	personal data
0.6703034650	chains
0.6702807811	student answers
0.6702492294	corner parsing
0.6701621062	fol
0.6701621062	cyk
0.6701605317	bart
0.6701581356	evaluation measures
0.6701196067	transferable
0.6701038430	vernacular
0.6700884663	kbqa
0.6700638074	pmi
0.6700026761	image search
0.6699985803	tacred
0.6699666628	semeval 2013
0.6699622097	spanish language
0.6699554920	dialog evaluation
0.6699165925	user's goal
0.6698837045	human human conversations
0.6698561196	voice activity
0.6698484152	range context
0.6698411343	rnns
0.6698247381	rest
0.6697551060	oie
0.6697056851	word embedding space
0.6696953246	significant performance gains
0.6696725844	hierarchical model
0.6696457379	annotated data
0.6696241311	commonsense question
0.6696168178	crises
0.6695839185	camera
0.6695683782	spa
0.6695629374	semantic relationship
0.6695326842	controlling
0.6695285123	neural network based
0.6695250451	tackling
0.6695129999	multiwoz 2.0
0.6694967527	hin
0.6694932984	variational
0.6694928525	gigaword
0.6694692765	asp
0.6694343811	character n grams
0.6694040496	dsm
0.6693753978	noisy sentences
0.6693346703	continuous representations
0.6693057343	graph neural
0.6692067517	unlike previous
0.6692030272	vulnerability
0.6691882319	net
0.6691819568	amharic
0.6691629172	domain aware
0.6691591634	selectional
0.6691589560	mccws
0.6691589560	hasoc
0.6691589560	qr
0.6691589560	drs
0.6691589560	aat
0.6691589560	aqm
0.6691589560	acm
0.6691589560	gleu
0.6691589560	ake
0.6691589560	bic
0.6691589560	drg
0.6691468488	similarity function
0.6691427153	wikitext 103
0.6691016294	specaugment
0.6691016294	webnlg
0.6690824193	sat
0.6690730673	multilingual data
0.6690184766	subsequently
0.6690172855	previous attempts
0.6689763751	natural speech
0.6689150316	video frames
0.6689007311	unmt
0.6687961433	universal
0.6687476990	dstc2
0.6687465763	convolution based
0.6687347721	non parametric
0.6686560124	auditory
0.6686526759	member
0.6686505377	external resources
0.6686369912	ol
0.6686369912	cg
0.6685980904	softmax function
0.6685507951	ts
0.6685089471	mathbb
0.6684880347	comparable performance
0.6684760105	summary pairs
0.6684721759	clp
0.6684305367	coupled
0.6684207142	plm
0.6684207142	dsa
0.6684207142	clm
0.6683754338	neural ranking models
0.6683560648	disagreement
0.6683317348	related languages
0.6683145017	semantic textual
0.6682917383	multilingual speech recognition
0.6682595601	algebraic
0.6682439624	grammar rules
0.6682238917	bad
0.6682090551	quantum
0.6681842512	cancer
0.6680904002	visual commonsense
0.6680807428	molecular
0.6680766791	gmm
0.6680501480	quora
0.6680479880	neural language generation
0.6680281387	social media texts
0.6680141201	diffusion
0.6679976325	regression
0.6679271646	simple models
0.6679078471	fingerspelling
0.6678901447	ovis
0.6678901447	lsi
0.6678892130	rst
0.6678502132	gpus
0.6678256660	jnlpba
0.6678131592	vocabulary sizes
0.6677937377	recently attracted
0.6677870743	generating diverse
0.6677803628	speak
0.6677643623	dual
0.6677447997	paired data
0.6677311756	low resource neural machine translation
0.6677114308	ft
0.6677114308	td
0.6677114308	eend
0.6676946323	gained significant
0.6675812502	electronic medical
0.6675781148	character embedding
0.6675537474	dependency paths
0.6675367438	image understanding
0.6675326081	dialogue turns
0.6674862484	conversation models
0.6674671464	wild
0.6674346675	lid
0.6673769088	rumor
0.6673430407	loan
0.6673369402	dst
0.6673168335	computer mediated
0.6673160666	accident
0.6673006420	entity embedding
0.6672225463	ape
0.6671988076	interval
0.6671717235	annealing
0.6671545189	hashing
0.6671414613	pro
0.6671214583	million words
0.6670995978	rouge l
0.6670917583	victim
0.6670367878	synthetic training
0.6670138881	bm25
0.6669718001	ubm
0.6669718001	fsmn
0.6669543806	indirect
0.6669167460	owl
0.6669024192	slam
0.6668952715	front end
0.6668745978	bi
0.6668592414	stacking
0.6667985671	human annotated
0.6667760226	irt
0.6666905527	conflict
0.6666785955	residual
0.6666729513	external language model
0.6666652558	research efforts
0.6666029594	meta
0.6665848197	research areas
0.6665639528	modulation
0.6665516479	locality
0.6665328025	enrichment
0.6665253665	test clean
0.6665209426	chime 4
0.6665184375	fed
0.6665088196	linguistic factors
0.6664877269	hg
0.6664877269	jfleg
0.6664693798	black
0.6664667508	spatial information
0.6664288915	eeg
0.6664288915	gp
0.6663934185	visualization techniques
0.6663728930	switch
0.6663248296	gb
0.6663248296	rmn
0.6663000678	determination
0.6662888604	dependency graph
0.6662095988	text descriptions
0.6661734704	conclusion
0.6661541780	grid
0.6661328797	object detection
0.6661314274	bilinear
0.6661231789	generalizing
0.6661093559	dg
0.6661093559	esa
0.6661077070	faq
0.6660789500	vowel
0.6660267802	strong baseline
0.6660227959	sort
0.6660093417	mbr
0.6660093417	mb
0.6660093417	raml
0.6659889537	mac
0.6659776907	computing power
0.6659393414	dtw
0.6659304334	conduct extensive
0.6659046789	discharge
0.6658815064	parsing algorithms
0.6658814558	clinical research
0.6658727867	person
0.6658691839	factuality
0.6658464534	timeml
0.6658380387	entity relation
0.6658093819	cord 19
0.6658084271	confidence score
0.6658078107	portability
0.6657714547	unsupervised neural
0.6657459222	statistical features
0.6657237446	orthographic information
0.6656753467	knowledge gap
0.6656721115	results suggest
0.6656703961	attention maps
0.6656306866	optimal solution
0.6656157500	clwe
0.6656157500	ime
0.6656157500	cad
0.6656157500	zsl
0.6656157500	ict
0.6656157442	recently emerged
0.6655825357	sequence labeling tasks
0.6655712220	mpi
0.6654876145	electronic
0.6654830474	encoder network
0.6654334415	stacked
0.6654268840	national
0.6654002966	image region
0.6653846092	arabizi
0.6653819702	location information
0.6653792165	temporal attention
0.6653252705	robustly
0.6652999606	quantifying
0.6652909648	av
0.6652761160	equation
0.6652712257	graph augmented
0.6652553402	syst \ `
0.6652546917	nlp systems
0.6652331949	related fields
0.6651804668	directly optimize
0.6650897337	character sequences
0.6650481737	manually created
0.6650318306	sector
0.6650271656	fusion
0.6650198344	vad
0.6649319861	ner models
0.6648867907	rotowire
0.6648301939	user embeddings
0.6646943337	web based
0.6646154380	text style
0.6646101747	nce
0.6645950210	sms
0.6645871141	kr
0.6645864694	qe
0.6645857240	e mail
0.6645800788	explicitly modeling
0.6645446611	learning distributed representations
0.6645410339	merge
0.6645323504	corpus annotated
0.6644911598	k means clustering
0.6644741980	pacrr
0.6644589248	centrality
0.6644384327	ald
0.6644032427	distance measure
0.6643989487	dementiabank
0.6643989487	tagalog
0.6643989487	squad2.0
0.6643663182	party
0.6643542097	parallel training data
0.6643487389	speech emotion
0.6643367663	phonetic structures
0.6643099759	hpsg
0.6642904551	examining
0.6642894898	radiology
0.6642750870	icd 9
0.6642269252	fb
0.6641300769	sustainable
0.6641300769	quiz
0.6641300769	labour
0.6641300769	circuit
0.6641300769	loops
0.6641187420	aligning
0.6641108889	mitigating gender
0.6641083134	insurance
0.6640697083	significant improvements
0.6640558126	relation embedding
0.6640276406	multi hop question
0.6639940979	feature selection methods
0.6639791865	base completion
0.6639571182	bipartite
0.6639263857	circle
0.6638909325	attitude
0.6638851076	center
0.6638275714	multiwoz 2.1
0.6638191389	event types
0.6637871847	heaps
0.6637609246	folksodriven
0.6637298306	retention
0.6637298306	monoidal
0.6637289377	conll 2009
0.6637087853	neural approaches
0.6635696484	nested
0.6635499049	substantial progress
0.6634853676	reuters
0.6634550973	rmse
0.6634535294	infilling
0.6633973948	hybrid
0.6633742943	stay
0.6633204398	rte
0.6632884708	reply
0.6632842483	lexical level
0.6632829278	sentiment detection
0.6632752648	mask
0.6632171764	robust speech
0.6631824117	benchmark datasets
0.6631681245	umls
0.6629387810	tl
0.6629224708	linguistic variation
0.6628871094	ea
0.6628832109	news detection
0.6628772706	winning
0.6628448027	significantly improve
0.6628311777	duc 2004
0.6627079838	scdv
0.6626608904	lstm language models
0.6626436843	mean square
0.6626416273	characterizing
0.6626403977	learning strategy
0.6626307176	ace 2005
0.6625522664	supervised attention
0.6625457873	mmr
0.6624774843	phonemic
0.6623828826	scientific research
0.6623297914	covariance
0.6622566441	empirical observations
0.6622176115	state representation
0.6622168260	deep learning methods
0.6622151550	inter class
0.6621826730	chart
0.6621691894	conversation generation
0.6621679850	surprisingly effective
0.6621649680	grounded image
0.6621542330	bpe
0.6620888345	neural architectures
0.6620699514	triplet
0.6620663652	creaky
0.6620585744	spanglish
0.6620158023	memory efficient
0.6619881201	wmt17
0.6619182669	supervised training
0.6619072256	low rank matrix
0.6618790470	dyna q
0.6618724305	spell
0.6618667049	biomedical relation
0.6618434827	nre
0.6618214647	bootstrapping
0.6617892661	probing
0.6617710763	asr hypotheses
0.6617369651	completeness
0.6617073426	russian language
0.6617048356	ros
0.6616893137	severity
0.6616762689	f measure
0.6616556444	lsc
0.6616371940	deductive
0.6616303867	language pair
0.6616208459	elg
0.6615853756	analysing
0.6615662789	decoding speed
0.6615506833	widely applied
0.6615505573	triple classification
0.6614873032	check
0.6614496049	election
0.6614148757	comprehensive review
0.6613923316	plane
0.6613884102	matching models
0.6613530068	social media platform
0.6613522386	input representations
0.6612652123	schema challenge
0.6612616954	distilbert
0.6611818771	visual processing
0.6611763009	zero shot cross lingual transfer
0.6611620750	wat
0.6611601367	structuring
0.6610914257	lu
0.6610810869	task success
0.6610620670	relative reduction
0.6610096756	act
0.6609984329	sentence modeling
0.6609971735	dialectal
0.6609770908	open domain question
0.6609656500	fault
0.6608900814	empirical results
0.6608823960	hand engineered features
0.6608650480	violent
0.6608596287	statistical approach
0.6608527044	gcn
0.6608466886	cpc
0.6608466886	cner
0.6608002107	geographic information
0.6607905561	matlab
0.6607811280	hierarchical classification
0.6607545985	deep convolutional neural networks
0.6607508833	hdp
0.6607377249	ast
0.6607377249	vsm
0.6607368168	xtreme
0.6607368168	alfred
0.6606964150	robot
0.6606817162	semantic structure
0.6606598505	rhetorical
0.6606567303	moses
0.6605658503	freebase
0.6605607030	multiplication
0.6605582827	inflection
0.6605447913	label embeddings
0.6604664513	counterfactual
0.6604420349	sum
0.6604230551	innovative
0.6603982610	engineer
0.6603757441	tdnn f
0.6603583250	domain classification
0.6603284890	bayes classifier
0.6603282546	collective
0.6603252333	science
0.6603175026	apis
0.6603028363	regularization effect
0.6602252722	ucca
0.6601929793	wikitablequestions
0.6601389745	speech representation
0.6601303769	information sharing
0.6600508151	electra
0.6600041124	machine translations
0.6599490191	evidence sentences
0.6599413636	lexical overlap
0.6598649265	adaptation problem
0.6598592272	yago
0.6598063840	apps
0.6598025392	gnn
0.6597284238	class labels
0.6597163299	medical knowledge
0.6597031214	automated speech
0.6596170041	wsc
0.6595852410	statistical parsing
0.6595451735	learning framework
0.6595408176	oscar
0.6595222194	lattice structure
0.6594815756	workflow
0.6594116065	sds
0.6593974854	extracting entities
0.6593941859	pooling layer
0.6593819102	description length
0.6593553626	sentence level semantics
0.6593461683	imputer
0.6593461683	signwriting
0.6593461683	dlgnet
0.6593461683	spanbert
0.6593461683	nlis
0.6593461683	babyai
0.6593461683	mctest
0.6593461683	liang
0.6593461683	magicoder
0.6593461683	bibliometrics
0.6593461683	lagrangian
0.6593461683	montagovian
0.6593461683	congress
0.6592649253	conversational model
0.6591780823	multilingual translation
0.6591669499	similarly
0.6591518810	generalization capabilities
0.6591471296	recovery
0.6591355941	conceptnet
0.6591249411	aspect level
0.6591190946	classification tasks
0.6591121234	nns
0.6590927880	medical images
0.6590643445	sqa
0.6590307208	spatio
0.6590236298	motivation
0.6589891325	natural language processing techniques
0.6589703707	linguistic patterns
0.6589480243	pushdown
0.6589306725	conspiracy
0.6589241614	locally
0.6589108022	interview
0.6589009790	language bias
0.6588660634	statistical model
0.6588180203	inverse
0.6587299606	gene
0.6587256431	commodity
0.6587162392	tense
0.6587134883	adversarial
0.6587058803	attract
0.6587022005	mild
0.6586612349	aes
0.6586404926	framenet
0.6586187124	na
0.6586046724	curation
0.6586041911	chart based
0.6585882440	upper
0.6585096376	summarization evaluation
0.6585039072	generated responses
0.6584368162	tcn
0.6584252930	relevant knowledge
0.6584042968	multi view learning
0.6583961939	domain general
0.6583917578	nlc
0.6583684118	concept embedding
0.6583577105	information content
0.6583478371	dynamic
0.6583125747	attention fusion
0.6583081954	chatbot
0.6582820993	statistical language model
0.6582032718	recognition algorithm
0.6581991058	intelligence
0.6581822982	structure information
0.6581451329	armed
0.6581451329	regulation
0.6581088319	free grammars
0.6580758752	dialog response
0.6580579493	semantic dependency
0.6579650141	fastspeech
0.6579612364	sensing
0.6579612364	steady
0.6579591435	effort required
0.6579386576	sandhi
0.6579384098	entity classification
0.6579382184	plda
0.6579378434	lstm layers
0.6579279084	mlqa
0.6579279084	www
0.6578814475	asymmetric
0.6578621186	worlds
0.6578343009	sparse attention
0.6578115313	generating high quality
0.6577846165	multi30k dataset
0.6577233604	grammatical framework
0.6577186207	sr
0.6576815798	encryption
0.6576316182	neurons
0.6576159422	sincnet
0.6575920645	open information
0.6575836580	adjective
0.6575832453	imitation
0.6575772084	nrm
0.6575451256	march
0.6575273467	cpu
0.6575249977	human annotation
0.6575159879	topic embedding
0.6574545013	lab
0.6574484227	text based
0.6573659046	multi task learning framework
0.6573252316	universal sentence
0.6572570129	optical
0.6572520933	qa dataset
0.6572438343	multilingual neural machine
0.6572132778	wisdom
0.6571900131	listening
0.6571757880	css
0.6571417109	kalm
0.6571398975	english corpus
0.6571224935	data exploration
0.6571222091	zhang et al
0.6571173383	wizard of oz
0.6571112107	gap
0.6571070287	principal
0.6570751076	substance
0.6570670171	las
0.6570655000	inductive
0.6570040704	learning curve
0.6568707136	odds
0.6568674772	pdf
0.6568415348	forget
0.6568303618	lst
0.6568303618	ppdb
0.6568083927	query generation
0.6567861545	crimes
0.6567536501	modal interactions
0.6567215810	isa
0.6566254724	encoder decoder based
0.6565754833	estonian
0.6565754833	imagenet
0.6565506248	assembly
0.6565306148	natural language questions
0.6565110980	coecke et al
0.6565039039	subtitles
0.6564916561	cascading
0.6564810127	scitail
0.6564708085	translation errors
0.6564683390	kge
0.6564419166	neural network architectures
0.6564238169	surrounding words
0.6564143642	reference based
0.6564116209	hand designed
0.6564020252	hypernymy
0.6563621972	information seeking conversation
0.6563151075	cle
0.6563151075	ntcir
0.6563151075	muc
0.6562895272	keyphrase
0.6562688759	greatly improved
0.6562318257	coda
0.6562186207	ilp
0.6562064949	human labeled
0.6561948239	articulatory
0.6561612606	rgb
0.6561280264	boosting
0.6561042375	product related
0.6560385386	eurovoc
0.6560365513	ce
0.6560152740	unsupervised nmt
0.6560019823	acute
0.6559959813	beings
0.6559893053	xel
0.6559893053	mci
0.6559775416	code switched speech
0.6559391272	flickr8k
0.6558821741	hearing
0.6558508622	graph generation
0.6557887053	bli
0.6557441590	diverse responses
0.6557359713	structural similarity
0.6556809565	doctor
0.6556599172	scheduled
0.6556001482	depression
0.6555682209	large scale pretraining
0.6555583956	source codes
0.6555488353	wikiqa
0.6555488353	quac
0.6555354686	ale
0.6555187988	lexicalized tree
0.6554976200	reference summary
0.6554213640	annotated speech
0.6554128787	priority
0.6553364267	retrieval performance
0.6553312480	mle
0.6553184561	byte
0.6552993879	recursive
0.6552955071	language specific
0.6552466005	balancing
0.6552419925	dynamic topic
0.6552208486	student model
0.6552186207	avsd
0.6551950836	anchor
0.6551579439	cloud
0.6551563454	skip gram model
0.6551015552	standard english
0.6550850104	du
0.6550508614	embedding models
0.6550313609	dysarthric
0.6550301518	rnnlms
0.6550269918	human level performance
0.6550161526	spam
0.6550155664	super
0.6550051908	pre trained language models
0.6550017025	anomaly
0.6549712213	opinion targets
0.6549634513	beta
0.6548664702	unsupervised data
0.6548064628	classification problems
0.6547788278	translation systems
0.6547680572	mining
0.6547222377	deception
0.6547175898	multilingual
0.6547025288	statistical methods
0.6546365608	visual speech
0.6545955028	absa
0.6545933884	image synthesis
0.6545626080	transformers
0.6545277903	squad 1.1
0.6544973752	historical text
0.6544773107	exponential
0.6544269006	text modeling
0.6544220299	blank
0.6544022769	graph based semi supervised
0.6543994121	multimodal attention
0.6543752730	controlled experiments
0.6543104551	pursuit
0.6541671423	amie
0.6541671423	nlpcc
0.6541671423	vatex
0.6541671423	mnist
0.6541083635	bidaf
0.6540709939	sampling strategies
0.6540478350	topic based
0.6540277647	ml
0.6539878692	social platforms
0.6539063548	nsp
0.6539063548	mcqa
0.6539063548	ddeo
0.6539063548	acd
0.6538790840	kbs
0.6538702803	coefficient
0.6538344483	copynet
0.6538344483	prophetnet
0.6538173573	global representation
0.6537325239	offenseval 2020
0.6536902182	event representations
0.6536870572	united
0.6536739836	high performing
0.6536554846	ka
0.6536369670	stack
0.6535561628	resolving
0.6535518819	key idea
0.6535180961	map
0.6535103826	decomposition
0.6535031969	distributional representations
0.6534992922	model transfer
0.6534889362	traffic
0.6534813435	clinical concept
0.6534614369	tip
0.6534476503	cvae
0.6534331511	lvcsr
0.6534326148	mission
0.6534326148	sigmoid
0.6533927471	lexrank
0.6533530497	teacher model
0.6532428138	summarizing
0.6532385677	protection
0.6531766821	simplequestions
0.6530890528	ross
0.6530784859	mds
0.6530455335	information conveyed
0.6530184740	syntax
0.6530167324	neighborhood
0.6529848065	ensemble models
0.6529268982	wsd
0.6529250920	sg
0.6529245474	clinical data
0.6529189466	eae
0.6529189466	dnc
0.6529189466	olid
0.6529189466	stl
0.6529189466	krl
0.6529189466	dcg
0.6529189466	liwc
0.6529189466	mp
0.6529065856	hierarchical deep
0.6528971228	chunking
0.6528703140	syntactic knowledge
0.6528635699	robust asr
0.6528451499	far field
0.6528256181	multiple sources
0.6528131375	datr
0.6528131375	uit
0.6527993223	residual learning
0.6527587536	high capacity
0.6527550649	analogy
0.6527395019	discrepancy
0.6527304901	significant improvement
0.6527173850	multilingual word embeddings
0.6527078730	contrastive
0.6526741293	convolutional attention
0.6526739412	analysis tool
0.6526464668	detailed analyses
0.6525958615	ssr
0.6525897493	similarity tasks
0.6525868642	future research
0.6525822490	human emotions
0.6525805425	textual networks
0.6525596017	slt
0.6525142040	memory usage
0.6525039830	f1 measure
0.6524683003	word similarity datasets
0.6524326908	english tweets
0.6523812593	tu
0.6523497050	local attention
0.6523475014	language detection
0.6523460885	mlp
0.6523110842	debiasing
0.6522773669	adversarial texts
0.6522701557	key words
0.6522635640	linear models
0.6522327542	neural language
0.6521999153	phrase extraction
0.6521952119	task completion dialogue
0.6521577382	luis
0.6521368453	critically
0.6521043092	head words
0.6520238718	cbt
0.6520189307	open class
0.6520070304	tunisian
0.6520070304	voxceleb
0.6519727649	news domain
0.6519547578	babel
0.6519496034	negative impact
0.6519492755	january
0.6519307065	auto
0.6519230748	sf
0.6518903066	evaluation methodology
0.6518825728	lxmert
0.6518825728	vgg
0.6518808500	tpr
0.6518808500	sdr
0.6518566240	medical texts
0.6518185607	cherokee
0.6518112443	textual description
0.6517548082	low resource language pairs
0.6517496329	adversarial domain adaptation
0.6517388062	compression
0.6517368612	single label
0.6517054611	absolute f1
0.6516281678	semantically rich
0.6516139782	rnn encoder decoder
0.6516134831	discocat
0.6515767115	relu
0.6515639397	lstm models
0.6515233092	multimodal context
0.6515104655	footnote
0.6515081863	earley
0.6514800198	opennmt
0.6514623288	dart
0.6514580043	moral
0.6514488901	equivalence
0.6513845509	thinking
0.6513639376	kernel based
0.6513391643	landscape
0.6512680530	bilstms
0.6512599509	emotionpush
0.6512376356	word matching
0.6512184913	dialogue evaluation
0.6512184056	stackoverflow
0.6511232901	training sets
0.6511086911	high dimensionality
0.6510904267	crime
0.6510794053	speech retrieval
0.6510638402	manipulating
0.6510025486	bidirectional recurrent neural networks
0.6509751498	vote
0.6509096423	foundation
0.6508809511	exploring
0.6508625762	ultrasound
0.6508372082	linear maps
0.6508219808	conditional
0.6508191495	beginning
0.6508005906	gec
0.6507470557	event data
0.6507354262	demo
0.6507187488	paws
0.6507078704	explaining
0.6505397974	cutting
0.6505059597	neural image
0.6504717963	extractive question
0.6504601560	mlm
0.6504427626	fixed vocabulary
0.6504305495	transport
0.6504103384	transducer
0.6504081863	kaggle
0.6503811910	quality evaluation
0.6503144637	boost performance
0.6502612983	event prediction
0.6502052838	generic responses
0.6501894082	statistical learning
0.6501743182	recurrent neural network language model
0.6501722370	mechanical
0.6501697913	multimodal embeddings
0.6501388762	candidate set
0.6501380780	tradition
0.6500844317	main contribution
0.6500773001	acoustic data
0.6500701858	meta classifier
0.6500238093	cn
0.6499930310	grounding
0.6499347436	sequential information
0.6499223411	nli datasets
0.6499061444	semantic embedding
0.6498953261	sadrzadeh
0.6498508533	n grams
0.6498149025	playing
0.6498000097	integer linear
0.6495618335	source language
0.6495282368	heuristic based
0.6494619873	curriculum
0.6494148023	dataset construction
0.6493444233	xgboost
0.6493359901	organizational
0.6493257476	squad 2.0
0.6493164284	chr
0.6493164284	lisa
0.6493018848	masked
0.6492945404	theta
0.6492738603	health data
0.6492711715	dnns
0.6492622804	computer science
0.6491868562	target entities
0.6491812686	mag
0.6491349094	wmt20
0.6490925655	localization
0.6490504332	key factor
0.6490117654	empirical evaluation
0.6490004089	late
0.6489716249	de en
0.6489688695	automatic text
0.6489543828	generation framework
0.6489100453	guided
0.6488506809	yang et al
0.6488184056	oxford
0.6487928591	md
0.6487818846	pie
0.6487465904	soft labels
0.6487402378	graph representations
0.6487274951	responsible
0.6487110736	data augmentation technique
0.6486993728	word translations
0.6486344220	sign
0.6485562516	audio samples
0.6484827200	injection
0.6484681594	learning objective
0.6484526054	msa
0.6484508655	dl
0.6484497006	macro averaged f1
0.6484482508	bertscore
0.6484482508	wiktionary
0.6484482508	faqs
0.6484282267	grus
0.6483926158	analogy tasks
0.6483202445	consonant
0.6483180325	enhancing
0.6483135267	catch
0.6482675946	promising results
0.6482530387	spider
0.6482402510	computational approaches
0.6482112359	pcfgs
0.6482030244	ltp
0.6481783501	human curated
0.6481371555	squared
0.6480979147	cws
0.6480921294	mediqa
0.6480645019	large volume
0.6480611823	diachronic
0.6480591607	default
0.6480591607	adopting
0.6479571177	syntactic annotations
0.6479221583	dimensional vector
0.6478626755	dt
0.6478038381	ewc
0.6477920688	chat
0.6477645706	adaptive
0.6477514453	machine learning approach
0.6477099432	production systems
0.6477017155	subword information
0.6476853878	emotionx
0.6476853878	allen
0.6476853878	hadoop
0.6476853878	november
0.6476853878	twitter's
0.6476853878	vilbert
0.6476853878	rdf2vec
0.6476853878	meddra
0.6476853878	theano
0.6476853878	lucene
0.6476853878	reco
0.6476853878	ace05
0.6476853878	activitynet
0.6476853878	setswana
0.6476853878	medmentions
0.6476853878	senteval
0.6476853878	bahdanau
0.6476853878	geoquery
0.6476853878	pearson's
0.6476853878	metamap
0.6476853878	august
0.6476853878	pakistan
0.6476519337	semantic features
0.6476387987	offensive
0.6476014243	neural topic model
0.6475549211	examination
0.6475410615	hou
0.6475281902	pt
0.6474974165	chen et al
0.6474840752	twitter corpus
0.6474560192	~ \ cite
0.6474157616	text encoding
0.6473937452	unimorph
0.6473937452	densenet
0.6473937452	babelnet
0.6473160933	conditional probability
0.6472813007	mrs
0.6472613869	text genres
0.6472418207	fresh
0.6472310446	emr
0.6472196655	std
0.6471958992	sorani
0.6471926172	probabilistic context free
0.6471865971	excellent results
0.6471290193	ontological
0.6471169707	document vectors
0.6470785014	tempeval
0.6470682913	ate
0.6470468590	unlike
0.6470217705	voice
0.6470146498	neural dialogue generation
0.6470024899	online social media
0.6469753003	strict
0.6469525308	dependency distance
0.6469419671	kws
0.6469079906	web document
0.6468981694	perl
0.6468884348	vocal
0.6468679928	lsa
0.6468536506	wmt19
0.6468417983	parsing model
0.6468364736	patent
0.6468065457	translation quality
0.6467731100	wolof
0.6467592800	nar
0.6466953334	gans
0.6466759290	aware
0.6466613132	web
0.6465968944	thesaurus
0.6465809920	summarization techniques
0.6465713648	politics
0.6465701191	grn
0.6465701191	scd
0.6465701191	sbvr
0.6465701191	dbn
0.6465115138	sequence level training
0.6465061149	relational learning
0.6463990051	language representations
0.6463325221	clir
0.6463196319	pivot
0.6462777478	ra
0.6462669530	socioeconomic
0.6462614351	interdisciplinary
0.6462543356	empirical studies
0.6462331332	health domain
0.6461671070	facilitate future research
0.6461376231	model ensemble
0.6461152457	successfully applied
0.6460939379	sa
0.6460666455	zerospeech
0.6460666455	jaccard
0.6460261924	kgc
0.6460041552	ll
0.6460012828	pa
0.6459816890	relation types
0.6459754891	mnmt
0.6459623727	gnns
0.6459521239	heart
0.6459450467	favour
0.6459450467	mildly
0.6459348473	reg
0.6458814188	structural properties
0.6458799384	speech quality
0.6458795307	mos
0.6458784314	ontosensenet
0.6458534729	image matching
0.6458492802	factorization
0.6458465720	slp
0.6458465720	pn
0.6458465720	ldc
0.6458465720	enas
0.6458465720	spss
0.6458131855	mt quality
0.6457605125	hierarchical transformer
0.6457515750	fine tuning bert
0.6457488104	word structure
0.6457101779	simultaneous
0.6456685539	context agnostic
0.6456653909	eye
0.6456154307	segmental
0.6455696906	middle
0.6454844852	verbmobil
0.6454665884	deriving
0.6454511386	context response
0.6453949230	personalization
0.6453654474	aqg
0.6453654474	msc
0.6453544297	unification
0.6453513522	convolution
0.6453480534	asvspoof
0.6453480534	cpus
0.6453454781	video question
0.6453358743	encyclopedia
0.6453110703	ac
0.6452910780	dp
0.6452603956	significantly improved
0.6452347999	incremental
0.6452026147	discourse relation classification
0.6451930316	solid
0.6451916363	recently
0.6451779020	expert annotated
0.6451717190	webquestions
0.6451666021	unexpected
0.6451609942	rescore
0.6451479023	nps
0.6451090076	sequence classification
0.6450566593	triviaqa
0.6450271620	referring
0.6450256593	absolute increase
0.6449643091	bert's
0.6449119264	decoding strategies
0.6449113486	question retrieval
0.6449074776	interactive
0.6447341364	learning objectives
0.6447020053	safe
0.6446579104	collaborative
0.6446335408	woz
0.6445955385	grouping
0.6445768377	newspaper
0.6445667707	cycle
0.6445521689	resource rich languages
0.6445445003	qanet
0.6445445003	eventkg
0.6445445003	fusionnet
0.6445445003	trecqa
0.6445445003	grover
0.6445203523	maximum likelihood training
0.6444999789	flickr
0.6444815929	wikidata
0.6444364206	generation systems
0.6444170162	open domain qa
0.6443646911	active
0.6443531769	cosine
0.6443332733	summarization
0.6443184165	kgs
0.6442668533	long short term memory network
0.6442553591	wfst
0.6442419512	benchmark dataset
0.6441927191	interspeech
0.6441685509	discovering
0.6441664133	human subjects
0.6441658488	newly developed
0.6440763349	tfidf
0.6440476306	launch
0.6440476139	query answering
0.6440402795	fact extraction
0.6440357458	masking
0.6440122693	standing
0.6439875206	feature representation
0.6439737091	word graph
0.6439697084	speech representation learning
0.6439625474	unsupervised summarization
0.6439589577	teacher models
0.6439202635	alter
0.6438782530	multilingual image
0.6438599020	street
0.6438398068	bhojpuri
0.6438398068	june
0.6438398068	watson
0.6438398068	clark
0.6438398068	dyck
0.6438398068	cantonese
0.6438233045	code switching speech recognition
0.6438229442	mwes
0.6438184814	sockeye
0.6437917150	sick
0.6437699693	cnns
0.6437011877	preferential
0.6437011877	fractal
0.6436393377	intervention
0.6436186673	marker
0.6435860653	vl
0.6435775383	sequential labeling
0.6435585835	semantic resources
0.6435474184	emotional states
0.6434796954	docred
0.6434722610	topic extraction
0.6434637409	light
0.6434558339	adjoining
0.6433804405	hurting
0.6432904389	reading comprehension dataset
0.6432294010	spain
0.6432294010	f0.5
0.6431485236	relation embeddings
0.6431307024	unstructured textual
0.6431221585	syntactic parsers
0.6430843915	domain transfer
0.6430648620	t5
0.6430311149	japanese sentences
0.6429854469	democracy
0.6429813302	iterative
0.6429124762	diminishing
0.6428812185	textual representations
0.6428343859	generated word
0.6427866262	answer sentence
0.6427716034	attention based encoder decoder
0.6427645127	fofe
0.6427633562	apc
0.6427633562	crnn
0.6427364662	asr output
0.6427072195	target word
0.6426981376	back translated
0.6426962176	bidirectional attention
0.6426798926	experimentally
0.6426740387	superior performance
0.6426300666	input tokens
0.6426010287	authorship
0.6425325947	lexicalized
0.6425260180	faithfulness
0.6424868472	target token
0.6424588130	larger scale
0.6424518581	nemo
0.6424360642	sgd
0.6424302902	training algorithm
0.6423538991	ctc loss
0.6423126058	sample size
0.6422225374	neighbor
0.6421665246	grounded
0.6421556395	non autoregressive
0.6421339123	copy
0.6421308500	ssa
0.6421308500	acg
0.6421308500	stft
0.6421182703	el
0.6421026321	neural network language models
0.6420501043	dependency parse
0.6420475503	cfls
0.6419837797	long tail entities
0.6419779947	compositional
0.6419413394	low dimensional space
0.6419010237	trait
0.6418917929	longest
0.6418389889	independence
0.6418298893	nas
0.6418113544	side effects
0.6417038333	uda
0.6416606412	bidirectional encoder representations from transformers
0.6416605448	sentence processing
0.6416101709	performance gains
0.6415825598	contextualized language
0.6415802946	ne
0.6415000832	trade
0.6414204979	ae
0.6413988811	ap
0.6413565338	uyghur
0.6413221724	gcns
0.6412373918	covid 19
0.6412143725	noun
0.6412142627	sp
0.6412016657	f score
0.6411894506	clinical domain
0.6411155670	similar images
0.6411120038	ar
0.6410921231	scene aware
0.6410659923	table
0.6410643910	relation network
0.6410368666	based tool
0.6410315283	corpus based
0.6409913012	generator
0.6409777076	digital
0.6409663235	user provided
0.6409365034	explainable
0.6409068680	semantic dependencies
0.6408865327	target languages
0.6408565338	boltzmann
0.6408565338	manipuri
0.6407994191	simon
0.6407480759	blog
0.6407242774	written texts
0.6407170783	character level neural
0.6406444013	tongue
0.6406300404	word tokens
0.6406039581	inside outside
0.6405887229	icon
0.6405368184	technique classification
0.6405171152	sentiment transfer
0.6404875386	svms
0.6404875386	a2w
0.6404798798	document level relation extraction
0.6404576088	bar
0.6404544768	reader
0.6403710017	weak
0.6403418285	ncbi
0.6403041125	information structure
0.6403004586	al
0.6402918191	qualitative evaluation
0.6402872387	near perfect
0.6402737920	vln
0.6402356080	sparsity issue
0.6402218847	generalized
0.6402113829	multi domain dialogue state
0.6402019396	speaker information
0.6401979225	weak labels
0.6401612927	morfessor
0.6401414042	structural features
0.6401260839	networking
0.6401220145	frame parsing
0.6401152881	ending
0.6401042531	model interpretability
0.6400982079	language priors
0.6400812659	weighted
0.6400753959	commonsenseqa
0.6400753959	voxceleb1
0.6400753959	checkthat
0.6400753959	propbank
0.6400753959	duluth
0.6400753959	france
0.6400753959	wikipedia's
0.6400342348	attention distributions
0.6400110086	generative
0.6399842893	bn
0.6399729588	nonparametric
0.6399524263	discontinuous
0.6398850684	big
0.6398712170	edited
0.6398662395	logical rules
0.6398446013	phobert
0.6398406219	inducing
0.6398330849	annotator
0.6398271079	ami
0.6398270127	mse
0.6398070742	multinli
0.6397642604	bea 2019
0.6397579707	augmenting
0.6397260875	wordnet based
0.6397076211	introduction
0.6397060835	directional
0.6396690690	bat
0.6396518907	neural image captioning
0.6396390429	feed forward neural
0.6396042881	argument structures
0.6395788333	evaluation set
0.6395709137	explicit semantic
0.6394989588	pre processing step
0.6394663015	sentiment corpus
0.6394475997	sas
0.6394236822	user utterance
0.6393480402	enterprises
0.6393319363	smiles
0.6392961532	high resolution
0.6392850863	training data
0.6392849549	ma
0.6392537493	bwes
0.6392337026	infersent
0.6392337026	alime
0.6392337026	searchqa
0.6392337026	textrank
0.6392337026	amazon's
0.6392337026	rnn's
0.6392337026	markovian
0.6392263701	hr
0.6392010385	infinite
0.6392008520	veracity
0.6391987024	ln
0.6391870314	publishing
0.6391755484	safety
0.6391749576	attentive
0.6391306213	allocation
0.6391248896	student
0.6390942294	emotion
0.6390423424	scene text
0.6390203447	nlp literature
0.6390126660	detection
0.6389968107	sparc
0.6389892556	natural language processing tools
0.6389166418	suit
0.6389115940	book
0.6388605396	multilingual corpus
0.6388602335	correspondence
0.6388498803	coqa
0.6388338614	exact match accuracy
0.6388042718	computational resources
0.6387841213	teacher network
0.6386908424	statistical models
0.6386785256	seq2seq
0.6386138319	articulation
0.6386009431	calm
0.6385648413	neural network models
0.6385610349	car
0.6385338146	social media mining
0.6385009524	surprisingly
0.6385003773	contextualized
0.6384820459	ontology
0.6384715249	mocha
0.6384708222	dot
0.6384385303	search strategies
0.6384329481	treebank
0.6384321659	streaming data
0.6384122357	qa systems
0.6383507880	html
0.6383461370	corpus creation
0.6383258197	emnlp
0.6382614934	layered
0.6382353437	hmms
0.6382349963	variable number
0.6382322367	lightweight
0.6381893594	system's performance
0.6381849152	pre trained word embeddings
0.6381495705	leveraging
0.6381303761	liar
0.6381160934	natural language sentences
0.6380864165	talking
0.6380799084	cost effective
0.6380742644	finance
0.6380145242	cider
0.6379989138	seq to seq
0.6379955251	learning distributed
0.6379268267	bing
0.6379121200	crs
0.6378852423	ehrs
0.6378668334	ned
0.6378161300	wmd
0.6377600323	disco
0.6377491141	open relation
0.6377309778	assignment
0.6377270312	songs
0.6377192233	dialogue learning
0.6377111031	movie
0.6376639024	reading comprehension models
0.6376050189	mdl
0.6375716863	chain
0.6375700481	predicting user
0.6375624407	child
0.6375390027	originated
0.6375211376	benchmarking
0.6375142337	dialogue based
0.6375121903	event temporal
0.6374984352	mask based
0.6374898923	real data
0.6374812233	document level context
0.6374703363	wikihop
0.6374263905	refinement
0.6374227549	asv
0.6373908817	premise
0.6373750263	composing
0.6373701496	happy
0.6373439683	mc
0.6372871693	topic word
0.6372783558	deep recurrent neural
0.6372463642	conll 2014
0.6372278777	previous approaches
0.6371963671	logic
0.6371665747	english code mixed
0.6371464960	ot
0.6371020472	human computer conversation
0.6370667172	spectrum
0.6370450207	deep learning framework
0.6370189727	integer
0.6369862543	kernel
0.6369401186	tree
0.6368826350	vaes
0.6368749891	journal
0.6368246185	projecting
0.6368171809	gn
0.6368063091	aided
0.6367906822	nooj
0.6367820447	entailment
0.6367716321	review rating
0.6367710874	cr
0.6367128480	capsule
0.6367101429	controllable
0.6366844547	steer
0.6366820320	overload
0.6366704984	squared error
0.6366337215	england
0.6366337215	frisian
0.6366337215	icelandic
0.6366337215	corenlp
0.6366337215	linkedin
0.6366337215	montague
0.6366337215	february
0.6366248955	screen
0.6365994738	sit
0.6365994738	ctp
0.6365994738	swb
0.6365889540	prototype
0.6365823913	reasoning module
0.6365730560	affine
0.6365712865	how2
0.6365712865	verbnet
0.6365712865	sentimix
0.6365712865	hownet
0.6365712865	vardial
0.6365712865	newsqa
0.6365712865	lstm's
0.6365622540	distill knowledge
0.6365496242	prime
0.6365485157	intelligent
0.6365057936	assessing
0.6364668087	terminology
0.6364607391	udpipe
0.6364607391	coq
0.6363534452	connectives
0.6363513018	input feature
0.6363480887	detecting
0.6363406471	user representation
0.6363303707	intention
0.6362984179	linking
0.6362969029	visual feature
0.6362953826	linguistic expressions
0.6362849601	conversational question
0.6362620362	complex reasoning
0.6362528006	openie
0.6362373603	ceiling
0.6361861384	i2b2
0.6361700398	investigating
0.6361602907	alarm
0.6361556956	representing text
0.6361480480	space complexity
0.6361472059	modality specific
0.6361425872	spice
0.6361165461	target tokens
0.6361085087	superglue
0.6361085087	winnow
0.6361085087	emformer
0.6361085087	sinkhorn
0.6361085087	magahi
0.6361085087	wn18rr
0.6361085087	dailydialog
0.6361085087	familia
0.6361085087	opensubtitles
0.6361085087	procrustes
0.6361085087	movieqa
0.6361085087	july
0.6361085087	berlin
0.6361085087	hessian
0.6361085087	hearst
0.6361085087	december
0.6361085087	textworld
0.6361085087	vietnam
0.6360775699	few shot relation classification
0.6359684712	stability
0.6358976401	ga
0.6358974304	output layers
0.6358910957	permutation
0.6358486497	post evaluation
0.6358242493	machine learning methods
0.6358049966	single sentence
0.6357752366	institution
0.6357408326	external knowledge sources
0.6357286697	rumoureval
0.6357286697	iwslt14
0.6357286697	openbookqa
0.6357286697	tripadvisor
0.6357028276	open ie
0.6357015611	surveillance
0.6356847181	semantic dependency parsing
0.6356832045	vector space representations
0.6356822853	argumentation
0.6356081680	salience
0.6356046392	lmtc
0.6356046392	tmpca
0.6356046392	csa
0.6356046392	asa
0.6356046392	ib
0.6356046392	pdp
0.6356046392	ctr
0.6356046392	plp
0.6356046392	som
0.6356046392	msvd
0.6355847549	sentence classification tasks
0.6355804811	ai research
0.6355793578	ed
0.6355769097	cast
0.6355432932	symmetric
0.6355402482	single sense
0.6355402396	neural qa
0.6355088983	visdial
0.6354761010	competitive performance
0.6354650279	monotonic
0.6354437274	email
0.6354436579	selective
0.6354143386	large language models
0.6354081434	execution accuracy
0.6353838578	qbe
0.6353639456	maximum
0.6353501251	cross lingual ner
0.6353424968	remote
0.6353290049	mmt
0.6353194978	keyword matching
0.6353181038	semeval 2015
0.6353041808	desired properties
0.6353033419	statistic
0.6352914553	common words
0.6352899985	span
0.6352827967	nlm
0.6351406622	silver
0.6351380632	algebra
0.6351305219	word similarity tasks
0.6351250452	legal
0.6351209963	disaster
0.6351209017	sparsemap
0.6351209017	lambek's
0.6351031527	outbreak
0.6350903109	minimum description
0.6350849909	title
0.6349959076	whatsapp
0.6349724611	declarative
0.6349174833	adi
0.6348884498	substantially outperform
0.6348821078	il
0.6348694563	hierarchical
0.6348319710	acceptance
0.6348209671	ptlms
0.6348209671	openqa
0.6347902932	automated
0.6347808288	breaking
0.6347763097	mini
0.6347112929	text to speech
0.6347084947	tiny
0.6346834398	sl
0.6346796814	biomedical
0.6346652331	sw
0.6346566620	duplicate
0.6346510917	video data
0.6346395073	fairness
0.6346360872	negation
0.6346084537	cs speech
0.6345926450	raw data
0.6345485694	transparent
0.6345385129	autoregressive decoding
0.6345273829	maml
0.6345098840	text to speech synthesis
0.6344988236	long range context
0.6344958004	crfs
0.6344897198	distance
0.6344790408	biobert
0.6343881002	confined
0.6343881002	exclusivity
0.6343462633	en de
0.6343347462	social interaction
0.6342136831	hierarchical dirichlet
0.6342119811	conditioned
0.6341881576	mae
0.6341857732	syllable
0.6341806323	significant gains
0.6341600497	smallest
0.6341516434	das
0.6341289640	policy based
0.6340814395	article pairs
0.6340646947	explainability
0.6340570744	semantic networks
0.6340075634	multi component
0.6340037889	response diversity
0.6339631910	fixed length vector
0.6339628390	binary classification task
0.6339592123	subspace
0.6339380222	iot
0.6339380222	erps
0.6339380222	tlms
0.6339357837	online speech recognition
0.6338891494	seg
0.6338701290	multitask
0.6338579369	dynet
0.6338579369	urls
0.6338579369	prince
0.6338482595	grain
0.6337196740	asl
0.6337150879	tensorflow
0.6337085678	narrativeqa
0.6337085678	drss
0.6336914953	considerable improvements
0.6336699306	fraud
0.6336675351	tacotron2
0.6336493186	intent detection and slot filling
0.6336404506	bert models
0.6335862990	empirically
0.6335000844	self attention
0.6334958306	roberta large
0.6334861359	fly
0.6334792555	lexical knowledge
0.6334773224	incorporating syntactic
0.6333812324	artificial
0.6333807289	medical dialogue
0.6333207644	campaign
0.6333164565	density
0.6332955108	erc
0.6332955108	nq
0.6332955108	stc
0.6332811192	parallel text
0.6332804146	math word
0.6332368227	adjacency
0.6332110619	fine tuning stage
0.6331893620	language family
0.6331579050	stance
0.6331578604	typed feature
0.6331282882	tasnet
0.6331088702	non monotonic
0.6331031295	association
0.6330750631	simulator
0.6329981319	neuro
0.6329954679	sans
0.6329221756	parsing
0.6328031929	visual
0.6327930602	ctc models
0.6327680932	emergent
0.6327454125	wise attention
0.6327322514	decoder states
0.6326452582	graph
0.6326235883	reaction
0.6326101341	ig
0.6326099285	teaching
0.6325950728	bow
0.6325724441	rat
0.6325662866	cas
0.6325647932	mat
0.6325482218	cite
0.6325441127	shaping
0.6325395073	timely
0.6325391697	significantly reduce
0.6325197334	scalable
0.6325149005	vi
0.6324827480	broadcast
0.6324422637	atis dataset
0.6324290038	visually grounded language
0.6324247903	binding
0.6324081253	graph representation
0.6323631314	word overlap
0.6323526345	cohesion
0.6323382956	self supervised
0.6322969204	attentional
0.6322893537	engine
0.6322870087	search based
0.6322580372	neuroscience
0.6322558456	analysis tools
0.6322345619	document generation
0.6322116532	online learning
0.6321974066	inflection generation
0.6321235531	situated
0.6321115159	kbc
0.6321111713	physical systems
0.6321058720	compact models
0.6321016850	graphical models
0.6320869216	sts
0.6320409964	prolog
0.6320401485	medical questions
0.6320333679	rewriting
0.6319915860	tone
0.6319668207	cognitive processes
0.6319660105	deep
0.6319357783	abstract
0.6319186877	input method
0.6318737255	compositional language
0.6318714179	transliteration
0.6318578763	colbert
0.6318578763	october
0.6318578763	grosz
0.6318536470	corpus filtering
0.6318392934	paper proposes
0.6318295255	automl
0.6318093718	toxic
0.6317029789	virus
0.6317008006	suicidal
0.6316922429	probabilistic latent
0.6316551804	graph parsing
0.6316531784	nc
0.6316517592	procedural
0.6316254044	l2
0.6316167822	matrix based
0.6315894239	npi
0.6315609324	inference networks
0.6315323560	centers
0.6314981141	argument components
0.6314654028	similar words
0.6314618128	autoencoder
0.6313659788	censorship
0.6313035780	surface
0.6312846976	challenge datasets
0.6312593741	deep architectures
0.6312175612	concept learning
0.6312075015	nnlms
0.6311937211	asc
0.6311683237	constrained
0.6311456975	sememe knowledge
0.6311198577	brain
0.6310960359	headings
0.6310960359	vertical
0.6310950420	robust
0.6309898596	mobile
0.6309828899	joint
0.6309774306	multimodal dialogue
0.6309741866	lexical relations
0.6309707007	reading
0.6309551689	large text
0.6309123571	minimum
0.6308993110	visual understanding
0.6308972035	pu
0.6308836926	competence
0.6308602916	probabilistic
0.6308353480	denoising
0.6308033272	dialog
0.6307811685	model driven
0.6307694368	verbal
0.6307488503	micro
0.6307366678	specifically
0.6306667098	neural dialogue models
0.6306473321	borrowed
0.6306273121	neural modules
0.6306158098	warping
0.6306089506	mwps
0.6306089506	convnets
0.6304595359	nodes represent
0.6304440347	calculating
0.6304387419	generation order
0.6303642980	oral
0.6302864888	stackexchange
0.6302707922	meaning preserving
0.6302251790	plms
0.6302200956	cqs
0.6301921899	foundations
0.6301656286	recipes
0.6301360972	promise
0.6301326146	weighted graph
0.6300833357	transformer language model
0.6300449481	dense
0.6300389026	multilingual offensive language
0.6299879341	voting
0.6299834923	exhaustive
0.6299450360	cross lingual word
0.6299229363	dstc7
0.6299123866	learning techniques
0.6298377611	passing
0.6298167264	hopfield
0.6297834011	humor
0.6297538790	heterogeneous sources
0.6297401640	extracting
0.6297195190	understanding
0.6296992751	lg
0.6296642453	forgetting
0.6296528679	audio data
0.6296450364	search method
0.6296394366	exploiting
0.6296348335	batch
0.6295921865	text segments
0.6295138690	analytics
0.6295114623	semantic embeddings
0.6294901712	densely
0.6294879894	experimental
0.6294599014	ee
0.6294322758	memn2n
0.6293576311	continuation
0.6292804013	achieve competitive results
0.6292522062	convnet
0.6292224331	claim detection
0.6291575637	flair
0.6291115118	machine learning based
0.6291061683	highly scalable
0.6290995856	source text
0.6290484907	analogical
0.6290093718	substitution
0.6290043901	large documents
0.6290023750	ban
0.6289814618	general linguistic
0.6289812637	interactive attention
0.6289580270	competitive results
0.6289443737	average accuracy
0.6289166971	si
0.6289078661	encouraging results
0.6288776205	consumer
0.6288125607	news
0.6287860341	performance gain
0.6287692016	mathematical model
0.6287498592	ensemble approach
0.6287485912	surrogate
0.6287418910	gating
0.6287343113	recent neural
0.6287179184	extraction tool
0.6287078427	sentimental
0.6286874338	captioning
0.6286405783	victim model
0.6286227733	correct answer
0.6286153641	cae
0.6285912853	unsupervised
0.6285891581	public domain
0.6285585729	government
0.6285501591	covid 19 open research dataset
0.6285500848	bioner
0.6285374711	drl
0.6285021317	data scientists
0.6284754848	compact
0.6284500772	node embeddings
0.6284187129	ensemble
0.6283700590	supremacist
0.6283655589	meme
0.6283418670	han
0.6283384395	transaction
0.6283339476	laws
0.6283176132	summarization methods
0.6282862449	training strategies
0.6282770177	past years
0.6282587107	induction
0.6282467499	semantic models
0.6282379926	abstractive
0.6282222587	neural embeddings
0.6281977857	formalization
0.6281929642	multi attention
0.6281862941	training samples
0.6281840289	live
0.6281778273	relational
0.6281735777	speech corpora
0.6281712762	nlidbs
0.6281402131	reservation
0.6281402131	chair
0.6281402131	differentially
0.6281402131	greatest
0.6281263904	novels
0.6281091193	simple recurrent
0.6281003310	test questions
0.6280820535	representational
0.6280257495	video grounded
0.6280207321	ugc
0.6280207321	clstm
0.6280207321	vqg
0.6280207321	rcnn
0.6280149574	sexism
0.6279946449	analysis shows
0.6279876195	trac
0.6279757453	generating
0.6278868526	neural machine translation models
0.6278687356	multimodal
0.6277811797	based qa
0.6277639368	oracle
0.6277279125	treelstm
0.6276339326	context size
0.6276089613	bit
0.6275883123	sparse data
0.6275868258	character models
0.6275430356	ai systems
0.6275266032	tod
0.6275086278	multiple choice question
0.6274958411	neural summarization
0.6274741561	gram
0.6274465946	element
0.6274388955	intellectual
0.6273241615	grammar error
0.6273081145	propagation
0.6272982162	wsi
0.6272823871	model's performance
0.6272217927	categorial
0.6271867567	trending
0.6271816576	plan
0.6271706029	scientific
0.6271679508	modality
0.6271269983	vqa dataset
0.6271061773	speaker specific
0.6270967893	cf
0.6270669047	selection model
0.6270389351	efficient algorithms
0.6270299110	statistically significant improvement
0.6270153126	ca
0.6270048188	competitive result
0.6269894542	bidirectional
0.6269822725	back translation
0.6269801026	pp
0.6269268438	propara
0.6269067699	nnlm
0.6269006002	holy
0.6268808995	wavelet
0.6268450444	10 fold cross validation
0.6268319894	remarkably
0.6268028866	nb
0.6267134878	splitting
0.6267043007	reputation
0.6266877528	gated
0.6266858189	indo european language
0.6266650511	model size
0.6266566569	entropy
0.6266543450	rs
0.6266410247	challenge
0.6266341634	relating
0.6266264093	specification
0.6266060872	significantly reduced
0.6265946385	interference
0.6265836773	personalized
0.6265832446	translationese
0.6265018328	multimodal features
0.6264970601	proficiency
0.6264966160	s2s
0.6264894884	orders of magnitude faster
0.6264734691	slang
0.6264192253	segmentation criteria
0.6264096906	english penn
0.6264004437	unstructured texts
0.6263763727	robust speech recognition
0.6263447696	natural text
0.6263380975	spot
0.6263244649	mined
0.6262942799	layernorm
0.6262942799	pns
0.6262705646	smooth
0.6262668267	lastly
0.6262289540	judge
0.6262206846	evaluating
0.6262000684	resnet
0.6261971042	supervised learning approach
0.6261915288	dan
0.6261836326	dialect
0.6260593925	charts
0.6260279789	programmer
0.6260036602	practical applications
0.6260012149	information sources
0.6259982840	search
0.6259481731	dataset creation
0.6259217967	language representation models
0.6258136390	transportation
0.6257923700	blanks
0.6257903987	frame based
0.6257651210	cv
0.6257270373	recognizing
0.6257262241	inclusion
0.6256718296	controlled natural
0.6255960045	commonsense
0.6255674913	rnn t
0.6255019261	provision
0.6254685188	sparse
0.6254588269	complex question
0.6254542266	off line
0.6254422446	informed
0.6254022438	significantly outperformed
0.6253665328	wnut 2020
0.6253578370	retrieval based dialogue
0.6253504704	model based
0.6253357019	discriminative model
0.6252982162	ser
0.6252509387	heterogeneous data
0.6252439074	lit
0.6251894259	adapting
0.6251827002	image sentence
0.6250894023	news reports
0.6250353233	travel
0.6250318032	soft
0.6250208511	latent features
0.6250082860	dialog data
0.6249856106	bilingual
0.6249534668	user embedding
0.6249473655	rl based
0.6249237322	efficient
0.6249093718	command
0.6248163424	lemma
0.6247069875	adaptable
0.6246965444	machine generated text
0.6246408000	navigation
0.6246308240	transformer based language models
0.6245946509	cwi
0.6245946509	lrp
0.6245946509	cmc
0.6245701194	annotating
0.6245496890	cognitive
0.6245483703	removal
0.6245233942	typology
0.6245146377	translation model
0.6245032879	experimental results demonstrate
0.6244848179	integrating
0.6244805686	preliminary experiments
0.6244481177	english speaking
0.6244138547	equations
0.6244021158	output embedding
0.6243962069	truly low resource
0.6243645077	conclusions
0.6242929544	word2vec model
0.6242915413	library
0.6242099985	dd
0.6242036543	networks
0.6242014902	language encoders
0.6241921469	user comments
0.6241673396	mapping
0.6241299161	hybrid models
0.6241178182	movement
0.6240738037	level sentiment analysis
0.6239977002	partial input
0.6239384882	ancient
0.6238800266	language styles
0.6238764830	data distributions
0.6238472966	text ranking
0.6238184041	attribute transfer
0.6238072528	european language
0.6237009039	mimic iii dataset
0.6236469046	poi
0.6235432074	identification
0.6235379390	enhanced
0.6235119403	motor
0.6234475569	probability model
0.6234204547	personality
0.6233915873	multimodal approach
0.6233550460	long short term memory recurrent
0.6233299545	qrnns
0.6233153502	external information
0.6232923143	data structure
0.6232628266	nle
0.6232444994	q
0.6232311667	processing steps
0.6232273035	unpaired
0.6232138539	birth
0.6232127736	rank
0.6232047088	resolution
0.6231966844	automatically constructed
0.6231721451	inference model
0.6231663227	probabilistic model
0.6231507300	multimodal information
0.6230967080	new york
0.6230943684	medical
0.6230800878	xai
0.6230800878	hrl
0.6230800878	amt
0.6230787065	score prediction
0.6230619278	inference algorithm
0.6230549810	informativeness
0.6229392982	generation network
0.6228980813	dnn acoustic
0.6228737888	lyrics
0.6228492579	english arabic
0.6227714768	aa
0.6227586090	male and female
0.6227452340	instruction following
0.6227213991	linguistic rules
0.6227007789	training examples
0.6226030413	driven
0.6225947287	lattices
0.6225858188	neg
0.6225819564	dialogue summarization
0.6225741883	measuring semantic
0.6225107836	awareness
0.6224768778	dat
0.6224566173	cnls
0.6224329591	pronoun
0.6224325490	ge2e
0.6224325490	awes
0.6224325490	prlms
0.6223440184	probing task
0.6223080262	noise contrastive
0.6222900993	greatly improves
0.6222794083	lite
0.6222414985	sdp
0.6221474239	input token
0.6220680293	encoder decoder architectures
0.6220206638	pbsmt
0.6219950021	bipolar
0.6219339891	multiple relations
0.6219006698	mg
0.6218814872	streaming
0.6218285573	vision language tasks
0.6218186215	source domain
0.6218103903	object recognition
0.6217797258	affective
0.6217701412	automatic
0.6217255757	compositional distributional models
0.6217241374	subword
0.6217156148	stanza
0.6217156148	facebook's
0.6217156148	conv
0.6216901440	atomic
0.6215906969	track
0.6215684105	cis
0.6215517325	qas
0.6215478895	rna
0.6215452197	image captioning models
0.6215328421	islands
0.6215093718	imbalanced
0.6214047460	major components
0.6213037806	linguistic theories
0.6212798544	tracing
0.6212641071	intensity
0.6211990882	treebanks
0.6211085588	w2v
0.6211037702	retrieval
0.6211019277	journals
0.6210864582	holistic
0.6210584861	exploratory
0.6210403555	structured information
0.6210247755	simple modification
0.6210141995	translation pairs
0.6210046454	semantic retrieval
0.6209927367	mixed
0.6209623381	deep generative model
0.6209521150	bp
0.6209466779	performance improvement
0.6209340663	discovery
0.6208472557	pre existing
0.6208411001	valuable information
0.6208179527	srs
0.6208179527	nlqa
0.6207865917	product information
0.6207791270	small vocabulary
0.6207263959	top down
0.6207184434	answer quality
0.6207117044	unix
0.6206856907	limited size
0.6206589765	relational similarity
0.6206285969	index
0.6205521672	entire sentence
0.6205455832	knowledge embedding
0.6205385630	love
0.6205320110	theory
0.6205156567	topology
0.6205014807	manual labeling
0.6204961953	computer aided
0.6204544520	cnn based
0.6204337882	cap
0.6203902131	concentrates
0.6203902131	myriad
0.6203883143	compose
0.6203761587	flat
0.6203611003	ner datasets
0.6203428160	vivo
0.6203263495	f0
0.6203014738	su
0.6202938156	supervision signals
0.6202889087	world assumption
0.6202720635	notes
0.6202715464	development set
0.6202628923	autoregressive neural machine translation
0.6202514173	analyzing
0.6202202084	lattice
0.6201860153	tiger
0.6201260190	endangered
0.6201208876	deep convolutional neural network
0.6200782953	maxent
0.6200773630	semeval 2016
0.6200632801	test cases
0.6200595085	artificial neural
0.6200525355	entity tagging
0.6200351290	rf
0.6200324002	recent research
0.6200231977	debates
0.6199680341	forcing
0.6199337714	grammars
0.6199171355	background
0.6198924398	diverse topics
0.6198103048	receiver
0.6197973116	submitted systems
0.6197818732	economy
0.6197418816	write
0.6197092162	stress
0.6197045843	conceptual
0.6197022321	empirical success
0.6196870925	alleviated
0.6196275536	portable
0.6196222128	highly relevant
0.6195837496	recurrent layer
0.6195607213	response theory
0.6195578927	ebl
0.6195578927	bci
0.6195578927	nel
0.6195578927	amn
0.6195578927	utd
0.6195578927	pbmt
0.6195578927	isr
0.6194905952	training speed
0.6194649447	er
0.6194602836	neural baselines
0.6194465951	adversarial network
0.6193986110	classification algorithms
0.6193530902	drug
0.6192440821	spirit
0.6192440821	uniqueness
0.6192440821	confronted
0.6192212747	controllable text
0.6191999418	diagram
0.6191959295	grading
0.6191926397	supervised data
0.6191420813	transmission
0.6191348691	literary
0.6190862592	tweet sentiment
0.6190263283	fb15k 237
0.6190240194	infodemic
0.6190206191	cascades
0.6190124557	general public
0.6189937373	image caption pairs
0.6189934773	suggestion
0.6189616891	minimization
0.6189466016	governed
0.6189466016	hampered
0.6189164116	stylometry
0.6188697754	constraint
0.6188402502	unsupervised representation learning
0.6188183017	talks
0.6187763287	machine learning algorithms
0.6187652987	claim generation
0.6187600957	augmentation
0.6187495364	acc
0.6187306293	latent
0.6187166695	unsupervised text style
0.6186956823	attribution
0.6186873827	flow based
0.6186824131	re ranking
0.6186721364	mtn
0.6186606028	domain adaptation methods
0.6186597243	statistical parametric
0.6186571229	debias
0.6186441594	feedback
0.6185987518	gain insight
0.6185680152	computability
0.6185590975	data distribution
0.6185086233	generation tasks
0.6185031891	error free
0.6184919831	z
0.6184894898	factual
0.6184653073	neural generation
0.6184291805	pun
0.6184256534	twin
0.6183853800	lsh
0.6183815745	sec
0.6183773583	model adaptation
0.6183357008	shallow
0.6183355498	taking
0.6182934598	comparing
0.6182603565	automatic image
0.6182503100	acoustics
0.6181915225	confusion2vec
0.6181915225	dissecting
0.6181915225	personalizing
0.6180939285	code summarization
0.6180869565	april
0.6180303332	economics
0.6179865463	argument
0.6179063314	forum
0.6179003353	language understanding evaluation
0.6178927603	mfccs
0.6178707368	fsl
0.6178694830	generating text
0.6178270642	poses challenges
0.6176967293	open challenge
0.6176925078	teacher
0.6176651103	bag
0.6176314383	inferring
0.6175942288	key contribution
0.6175811223	zero resource
0.6175751112	mlms
0.6175751112	wers
0.6175745952	small world
0.6175372569	kd
0.6175205659	dsl
0.6175129965	nil
0.6175085707	applying deep
0.6175027742	newsroom
0.6174000001	directed speech
0.6173983951	computers
0.6173432067	coupling
0.6172834185	prune
0.6172685697	forecasting
0.6172259596	transducers
0.6172151792	charge
0.6171767987	cbr
0.6171699628	hat
0.6171666182	system technology challenges
0.6170212599	generating questions
0.6170203375	espnet
0.6170151327	putting
0.6170018142	local information
0.6169912047	fnc 1
0.6168777948	selecting relevant
0.6166814251	memory slots
0.6166339346	pre trained models
0.6165815740	quantization
0.6165786946	report generation
0.6165682196	seq2seq based
0.6165315241	neural agents
0.6164962650	temperature
0.6164653307	fl
0.6164653307	dfsmn
0.6164653307	mcts
0.6164653307	asag
0.6164653307	fda
0.6164653307	kld
0.6164653307	qac
0.6164653307	sae
0.6164653307	wals
0.6164653307	gif
0.6163875962	quantity
0.6163459794	enhanced multi
0.6162922739	reasoner
0.6162852588	global
0.6162571655	predicting
0.6161999662	conversation systems
0.6161951732	federated
0.6161898299	evaluation benchmarks
0.6161670340	unintended
0.6161627169	augmented neural networks
0.6161573312	semeval 2014
0.6161339639	comment
0.6161110898	vec
0.6160653823	knowledge learned
0.6160474827	law
0.6160433296	ebm
0.6160433296	ccc
0.6160433296	dca
0.6160433296	weat
0.6160424826	numeracy
0.6160424826	interlocutor
0.6160077995	large corpora
0.6159816413	thirdly
0.6158990173	functional
0.6158853226	previously generated
0.6158747943	neural word embeddings
0.6158747162	visual linguistic
0.6158709094	reflexive
0.6158632127	root
0.6158507915	ordered
0.6158463771	sentence understanding
0.6158411678	fully exploit
0.6157712059	high probability
0.6157361517	osn
0.6157361517	ece
0.6157257332	imagination
0.6157064880	tens
0.6156282652	sentiment datasets
0.6156025605	qc
0.6155996612	causality
0.6155922837	information search
0.6155782874	mil
0.6155109138	video
0.6154581681	continuity
0.6154529224	mono
0.6154294517	pipelined
0.6154193271	linguistic context
0.6154115353	woman
0.6154017948	rnn architecture
0.6153886728	empirically validate
0.6153818319	rethinking
0.6153252832	intelligible
0.6153206508	measuring
0.6153045850	english speakers
0.6152943111	longitudinal
0.6152815868	influential
0.6152785554	automated evaluation
0.6152208798	conduct extensive experiments
0.6151628333	selection process
0.6151603929	recognition challenge
0.6151355880	context independent
0.6151318607	entity information
0.6151153555	co occurrence
0.6150486029	modular
0.6150248013	transition based dependency
0.6150245946	mirroring
0.6150245946	celebrity
0.6150245946	laugh
0.6150004624	phonology
0.6149944315	section
0.6149873582	concludes
0.6149764820	prevalence
0.6149686041	benchmarking datasets
0.6149547866	j
0.6149534989	customized
0.6149470771	mitigation
0.6149230986	regularization
0.6149147884	gold
0.6148762437	scibert
0.6148399681	phi
0.6148238824	synthesis
0.6148216607	indicative
0.6148183060	vector machines
0.6148106735	contour
0.6148064684	shown promising results
0.6147944273	novelty
0.6147940255	story
0.6147926929	contextual
0.6147117764	dependency structure
0.6146301214	conversely
0.6145175810	task guided
0.6145118837	evaluators
0.6145007309	pinyin
0.6144695671	large memory
0.6144520953	parameter size
0.6144332710	primary task
0.6144166821	mesh
0.6144003677	deep contextualized word
0.6143907913	natural language processing research
0.6143841273	deterministic
0.6143584204	code
0.6143449425	naming
0.6143005703	decision process
0.6142330525	mutual
0.6142322725	consensus
0.6142053254	intervals
0.6142021157	current
0.6141737000	learning
0.6141145231	structured query
0.6141032338	human translations
0.6140878264	user study
0.6140864907	aws
0.6140833893	similarity datasets
0.6140498786	revise
0.6140449343	language translation
0.6140088817	recent attempts
0.6139486416	bert pre training
0.6139217456	booking
0.6139084445	amrs
0.6139035977	cognition
0.6138475761	exposed
0.6137910071	based clustering
0.6137684358	failure
0.6137260970	transfer methods
0.6136962417	weakly supervised learning
0.6136848463	polarized
0.6136647648	neural embedding
0.6136642764	clarifying
0.6136348693	automatic alignment
0.6135779561	attention based neural
0.6135667999	event
0.6135296279	modeling
0.6135232423	ids
0.6135161315	mi
0.6135115470	correlational
0.6135115470	hypertext
0.6135040126	mixed domain
0.6134843271	indicator
0.6134769047	low resources
0.6134714932	morpheme
0.6134668454	concept
0.6134117704	overview
0.6134078832	polish language
0.6133420536	distancing
0.6133269286	morphology
0.6133140708	translated texts
0.6132971373	cds
0.6132962495	improving
0.6132566257	sequence based
0.6132519917	template
0.6132071149	se
0.6131827877	text dependent
0.6131612695	post training
0.6131463626	vsms
0.6131245426	simlex 999
0.6130297806	annotated resources
0.6129761022	statistical machine translation systems
0.6129754978	significantly lower
0.6129608849	vanishing
0.6129465085	question answering tasks
0.6129433004	based feature
0.6129111887	wang
0.6129111887	fortunately
0.6129021321	knowledge contained
0.6128933216	regression based
0.6128895156	computational semantics
0.6128860795	mood
0.6128800875	overnight
0.6128546128	task independent
0.6128518911	bake
0.6128399677	anaphora
0.6127917689	generation model
0.6127838939	paying
0.6127799350	langue
0.6127275459	augmented
0.6126187350	sarcasm
0.6125721086	modern
0.6125019106	hub5
0.6124913451	package
0.6124650228	moving
0.6124215074	cloning
0.6124215074	deviations
0.6123609072	ontonotes 5.0
0.6123441986	token representations
0.6123154173	ppl
0.6123081350	shallow semantic
0.6123076551	generated stories
0.6122477932	latent code
0.6122361298	manual annotations
0.6122259027	trie
0.6120989853	higher layers
0.6120648562	spearman
0.6120553871	graded
0.6120116907	highly optimized
0.6120050024	non native speakers
0.6119830711	topic inference
0.6119630802	_1
0.6119595741	gradient
0.6119171941	query aware
0.6119102529	regression models
0.6118683638	anthology
0.6118590605	character level language modeling
0.6118439614	popular benchmarks
0.6118410215	realization
0.6117922181	bounding
0.6116761825	nmt systems
0.6116710642	human behavior
0.6116675345	vist
0.6116675345	edl
0.6116675345	dmn
0.6116437730	color
0.6116119423	durian
0.6116119423	c4.5
0.6115617040	waveform
0.6115363459	distributional
0.6115028945	proxy
0.6114871275	formulating
0.6113002055	web content
0.6112605417	fsts
0.6112360853	unseen words
0.6111947431	proven effective
0.6111307658	industrial
0.6111230029	wikitext 2
0.6111128964	nes
0.6111064028	deep network
0.6109788051	financial
0.6109716889	window
0.6108993247	cat
0.6108835155	descriptive
0.6108599297	rewrite
0.6107776313	abstraction
0.6107020894	cats
0.6106914563	attributed
0.6106391332	english french translation
0.6106089634	data size
0.6105571487	experimental setup
0.6105405008	aggregation
0.6105353739	speech transformer
0.6104469238	separable
0.6104044094	specificity
0.6103889459	medicine
0.6103836933	aishell 1
0.6103562319	simple sentences
0.6103196062	ve
0.6102909822	nat models
0.6102600481	user interactions
0.6102356276	recurrent models
0.6101580047	subtask b
0.6101512806	remains challenging
0.6101391067	environmental
0.6101378152	db
0.6100516468	citation
0.6100183585	sentence representation learning
0.6100169850	extractor
0.6099947753	versatile
0.6099946386	qualitatively
0.6099909246	company
0.6099741793	learning based
0.6099651378	granular
0.6099560162	region
0.6099361705	linear
0.6099321916	involvement
0.6098293194	generated summary
0.6098272106	globe
0.6097381685	hu
0.6096683163	source sentence
0.6096552739	asr errors
0.6096520399	sense
0.6096181713	similarity analysis
0.6095835650	comparative
0.6095808948	ddis
0.6095714569	recently published
0.6095633789	sad
0.6095508821	della
0.6095451570	tbl
0.6095376059	contextual word embedding
0.6094728179	questions requiring
0.6094589688	multi language
0.6094579351	inter dependencies
0.6094562326	distance based
0.6094022247	expanding
0.6093813373	word embedding based
0.6093397511	dialogue
0.6092578265	automatically detecting
0.6092349815	acting
0.6092348631	consolidation
0.6092049042	hop reasoning
0.6091968491	output space
0.6091715055	based conversation
0.6091612242	chinese corpus
0.6091528785	evaluation sets
0.6091450247	tap
0.6091281494	semantic categories
0.6091110134	spark
0.6091027319	evolution
0.6091016449	weakly
0.6090953522	normalize
0.6090899902	flaws
0.6090867071	corpus analysis
0.6090015523	convolutional
0.6089290980	patient
0.6089240585	disjoint
0.6089160042	g2p
0.6089114233	multilingual language models
0.6088659276	extensive
0.6088494993	tracking
0.6088345478	modelling
0.6088309005	aspect categories
0.6087953528	labelling
0.6087932630	load
0.6087797394	integrating knowledge
0.6087557418	learner
0.6087503503	network
0.6087239685	crowd
0.6087229040	letter
0.6086734917	uni
0.6086492396	deep feature
0.6086052148	ensemble based
0.6085903823	core
0.6085881054	philosophical
0.6085875513	jointly learned
0.6085774268	reciprocal
0.6085710761	security
0.6085314001	achieves superior performance
0.6085268962	trip
0.6085268962	standardize
0.6085201156	single word
0.6085195236	_
0.6084730992	notation
0.6084670587	text recognition
0.6084145128	comprehension
0.6084107913	entailed
0.6083683345	conversational
0.6083566564	million parameters
0.6083566500	contextual knowledge
0.6083432295	dictionary
0.6083273446	chatbots
0.6082915329	sense annotated
0.6082866206	domain adversarial training
0.6082033370	elicit
0.6081785968	kaldi
0.6081542864	read
0.6081353105	training signal
0.6080663053	captioning models
0.6080515146	common practice
0.6080418330	typological
0.6079788064	recognise
0.6079767159	suppress
0.6078931358	multiple knowledge sources
0.6078697968	early
0.6078433661	early modern
0.6077815272	breadth
0.6077471846	negative results
0.6077417982	swda
0.6077298686	joint inference
0.6077055351	attention
0.6076943222	word order information
0.6076907801	corpus size
0.6076808961	hand annotated
0.6076404018	clinicalbert
0.6076195051	cross lingual document
0.6075982703	language understanding tasks
0.6075804674	code mixed text
0.6075609681	learning paradigm
0.6075369235	co occurring
0.6075155360	autoregressive
0.6074794605	cyclegan
0.6074098048	combining
0.6073474955	ages
0.6072940874	large batch
0.6072564891	recurrent
0.6072430911	medical entities
0.6071736434	tempo
0.6071489581	human effort
0.6070917584	embedding words
0.6070909501	paragraph
0.6070779444	ro
0.6070686018	marketing
0.6070444645	gpt 2
0.6070285586	natural language query
0.6070115158	critical issue
0.6069810841	viability
0.6069608275	substantially improve
0.6069341323	high resource languages
0.6068971433	relevance
0.6068813508	evaluation shows
0.6068684904	evaluation criteria
0.6068639328	array
0.6068570605	code mixed social media
0.6068361435	reconstruction
0.6068314968	crowdsourcing
0.6067944994	poetry
0.6067636149	automatic evaluation metrics
0.6067373941	agree
0.6066568482	neural
0.6066377541	baseline approach
0.6065958277	originate
0.6065888769	summary quality
0.6065684763	evidence supporting
0.6065675236	semeval 2020
0.6065028698	under resourced languages
0.6064231555	network structure
0.6063700359	german text
0.6063365286	regularizer
0.6063091509	recent literature
0.6062880905	bilingual corpus
0.6062642029	mnb
0.6062382065	real world applications
0.6062373545	slm
0.6062373545	mdp
0.6062329833	multiple context free
0.6062181192	transformer based architecture
0.6062147483	improved performance
0.6061132451	bert language model
0.6061000677	fully utilize
0.6060211580	large pretrained
0.6059926077	lexical diversity
0.6059592343	captions
0.6059406182	question answering task
0.6059359946	clustering
0.6059191974	lang
0.6058568911	supervised methods
0.6058564773	education
0.6058295239	subword based
0.6056902362	neural dependency parsing
0.6056883994	non native
0.6056615115	swap
0.6056190459	gold labels
0.6055840259	models outperform
0.6055627919	cnn dailymail dataset
0.6055435441	graph reasoning
0.6054959175	summarization models
0.6054837224	temporal
0.6054767996	comprehension datasets
0.6054316884	significantly reduces
0.6053465202	deep learning systems
0.6053266384	fish
0.6052609593	embedding layer
0.6052471098	ref
0.6052294777	source languages
0.6052059287	ladder
0.6051698762	crisis
0.6051164689	robotic
0.6050071913	o
0.6049868047	norms
0.6049652514	frequency based
0.6049533316	principle
0.6049508350	ubiquity
0.6049461674	explicit discourse
0.6049430417	low resource scenario
0.6049423774	attribute
0.6048755332	explanation
0.6048201993	large margins
0.6048114850	pyramid
0.6047941511	paraphrase
0.6047840799	agnostic
0.6047568879	plot
0.6047556300	co occurrences
0.6047455751	dot product attention
0.6047128448	mentioned entities
0.6047083318	fusion networks
0.6045897756	emoji
0.6045261027	cross lingual entity linking
0.6045161628	lexicalization
0.6045082574	pretrain
0.6044885305	acsa
0.6044885305	bt
0.6044770787	dense vector
0.6043950892	answering
0.6043784043	wikihow
0.6043602173	toy
0.6042851350	discourse segments
0.6042755823	gpt2
0.6042532216	extending
0.6042346875	derivational
0.6042247440	billion word
0.6042190883	qualitative analyses
0.6041513034	phenotyping
0.6041332200	joint representation
0.6041187414	avoiding
0.6041130033	stroke
0.6040590441	d
0.6040434291	analysis methods
0.6040389004	causal
0.6040144144	continuous vector space
0.6040003368	n ary
0.6039971575	takes into account
0.6039793278	real users
0.6039018963	failing
0.6038928740	merit
0.6038913728	clickbaits
0.6038730884	negative reviews
0.6038606927	failures
0.6038361376	unsupervised semantic
0.6038045998	academia
0.6037621562	external knowledge base
0.6037499205	relation
0.6037358102	lowest
0.6037247531	normalization
0.6037045127	offline
0.6036689586	kernels
0.6036594694	question answering models
0.6036135066	patient representations
0.6035973141	document level nmt
0.6035628513	generalizable
0.6035472663	note
0.6035430193	spectral
0.6035406846	accounting
0.6035107364	android
0.6033950257	pitch
0.6033169977	fitting
0.6033096905	wet
0.6033074928	magnitude faster
0.6032934538	taking into account
0.6032729411	pointwise
0.6032578104	multi domain dataset
0.6032357052	distillation
0.6032050204	manually designed
0.6031926942	math
0.6031393560	neural generative
0.6031373387	turn based
0.6030958694	target object
0.6030630195	boxes
0.6030309506	grammatical rules
0.6029230523	keras
0.6029158460	wordpieces
0.6028723912	extensively evaluate
0.6028476657	random field
0.6028195843	analogy questions
0.6027836512	outlier
0.6027772752	friends
0.6027769534	smatch
0.6027709291	euphonic
0.6027114220	substantially higher
0.6027031175	conversational dialogue
0.6026952510	transformation
0.6026655199	article describes
0.6026489681	delete
0.6026456105	clean
0.6026174816	orders of magnitude
0.6025846941	matching based
0.6025678650	decision making process
0.6025501679	engineering
0.6025170074	unlabeled corpora
0.6025142924	classification
0.6024952143	shortest
0.6024926052	system technology challenge
0.6024469653	wordnets
0.6024423035	cfgs
0.6024423035	edus
0.6024199133	pit
0.6024129132	ccs
0.6023743277	calculus
0.6023733152	latent factors
0.6023313456	semantically relevant
0.6022912554	bidirectional long
0.6022680778	generation
0.6022369464	output units
0.6022225446	dimensionality
0.6022091673	phrase based machine translation
0.6022052059	neural cross lingual
0.6021823382	controlled
0.6021592382	hierarchical latent
0.6021326591	relation extraction task
0.6021316970	natural language texts
0.6020848611	fall
0.6020827799	anns
0.6020697788	bert model
0.6020617248	ngram
0.6020589605	annotation framework
0.6020519787	chime 3
0.6020496885	ethnic
0.6020368718	epsilon
0.6020324548	intersection
0.6020162749	clinical
0.6020121367	supersenses
0.6019007053	spectral features
0.6018853609	psd
0.6018760262	pytorch
0.6018622774	scaling
0.6018211067	symbolic reasoning
0.6018122789	ins
0.6017994833	l2r
0.6017756713	mispronunciation
0.6017756713	repurposing
0.6017756713	mastering
0.6017639057	multilingual offensive
0.6017463619	cleaner
0.6017356961	sparsification
0.6017356961	trolling
0.6017356961	suppression
0.6017356961	pseudocode
0.6017356961	outlook
0.6017356961	transgender
0.6017356961	extrapolation
0.6017356961	lossless
0.6017356961	disfluent
0.6017356961	rhythms
0.6017356961	epidemiological
0.6017356961	spiking
0.6017356961	confounds
0.6017356961	steganography
0.6017356961	deciphering
0.6017356961	normalised
0.6017356961	hyperlinks
0.6017356961	reflections
0.6016601238	knowledge resources
0.6016148649	positive
0.6016076101	adverse
0.6015689311	estimation
0.6015634524	tagging accuracy
0.6015330886	deep q learning
0.6015300130	concentrated
0.6014938130	diarization
0.6014896507	activation
0.6014806673	label information
0.6014722174	agencies
0.6014188698	bottleneck
0.6014121049	ahead
0.6013853346	crawled
0.6013761743	forced
0.6013552468	topic related
0.6012363833	rotate
0.6012111371	automated text
0.6011853640	human performance
0.6011667063	gradient based optimization
0.6011627280	expansion
0.6011037742	character
0.6010145208	extracted features
0.6010142878	correspondences
0.6010101213	high degree
0.6009922534	violence
0.6009810832	self organization
0.6009171238	candidate sentence
0.6008426090	information fusion
0.6008063271	sexual
0.6008033867	translator
0.6006808899	toponym
0.6006808899	affordance
0.6006771902	fewrel
0.6006362030	building
0.6005848611	massively
0.6005557764	instantiations
0.6005557764	pros
0.6005557764	2mix
0.6005446034	complex word
0.6004859746	jointly learn
0.6004767674	analogous
0.6003931981	dstc8
0.6003881225	chinese discourse
0.6003556321	formality
0.6003448814	semi supervised training
0.6003072659	extraction
0.6002565505	adaptation
0.6002381460	cost efficient
0.6002344296	thought
0.6002338659	lexicon
0.6001937554	affinity
0.6001817624	service
0.6001481269	dsms
0.6001318625	a2c
0.6001298212	structured
0.6001024540	rapid
0.6000360633	neural topic models
0.6000305696	biographical
0.6000187145	translating natural language
0.5999576452	contrasts
0.5998808148	fully data driven
0.5998744680	doctors
0.5998577364	selection
0.5998440086	processing systems
0.5998228199	additional training data
0.5997881235	state space
0.5997785470	entail
0.5997378962	g
0.5996775822	beam
0.5996556650	information flows
0.5996170404	news dataset
0.5996115775	ideal
0.5996061337	semantic level
0.5995823010	predictive performance
0.5995101382	annotation task
0.5994740099	roll
0.5994602507	extractive
0.5994454672	coattention
0.5994444558	position embedding
0.5994321213	approx
0.5994248606	spoofing
0.5994140684	structure grammar
0.5993995755	ambiguity
0.5993859622	ticket
0.5993808990	dialog agent
0.5993612854	dialogue corpora
0.5993379255	word types
0.5993238437	creative
0.5993110657	karaka
0.5992697072	semantically correct
0.5992624422	epistemic
0.5992526830	bigru
0.5992385217	strong supervision
0.5991967880	unlike traditional
0.5991545546	intra
0.5991503860	existing approaches
0.5991011197	recent
0.5990988434	comparative experiments
0.5990833030	enforcing
0.5990305053	participation
0.5990081967	emotional
0.5989780032	bias detection
0.5989222582	identifiable
0.5989142200	disease
0.5988959937	proxies
0.5988868330	enriched
0.5988347963	class label
0.5988205202	large scale multi domain
0.5988146345	structured semantic
0.5987865813	phonetics
0.5987257357	multivariate
0.5987199927	competing approaches
0.5987036612	act recognition
0.5986319446	approximate
0.5986141790	musical
0.5985720763	topical
0.5985474804	localizing
0.5984984468	behavioral
0.5984373742	wic
0.5984326482	message
0.5983584746	preserving
0.5983573091	bagging
0.5983573091	likewise
0.5983362132	speech segments
0.5983302587	majority
0.5982922828	wrong
0.5982518901	giant
0.5982474160	parallelize
0.5982231272	sufficiently large
0.5982162379	multiview
0.5982162379	aging
0.5982162379	windowing
0.5982035640	early stages
0.5981040679	word sequences
0.5980643247	graphical
0.5980494400	translation shared task
0.5980351694	navigating
0.5980219373	fictional
0.5980103392	spontaneous
0.5979431424	reliance
0.5979210796	singular
0.5978828803	gen
0.5978485393	related tasks
0.5977977145	head
0.5977532893	idiom
0.5977264981	candidate entities
0.5977235078	accountability
0.5977064680	neural dialogue
0.5976547787	mimicking
0.5975780371	followup
0.5975768845	stemmer
0.5975747700	file
0.5975676707	low resource machine translation
0.5975579801	resource settings
0.5975547703	twitter posts
0.5974873381	trigger
0.5974765060	average length
0.5974702174	assert
0.5974326027	term set
0.5973302099	encoder decoder networks
0.5972864802	multilingual text
0.5972732463	historical texts
0.5972223669	adrs
0.5971801358	controversy
0.5970912874	reinforcement
0.5970877838	questions posed
0.5970795097	r2r
0.5970595979	frame
0.5970541933	spatial
0.5970315141	full fledged
0.5970213884	target words
0.5970141676	basically
0.5969963607	da classification
0.5969611679	plsa
0.5969182045	floating
0.5969182045	resolvers
0.5969177975	marginal
0.5968917144	language semantics
0.5968683224	embedding
0.5968521953	syntactic
0.5968476216	prediction
0.5967755722	concurrent
0.5967537724	self reported
0.5967492634	protocols
0.5967405233	knowledgeable
0.5967273925	mrls
0.5966983434	l
0.5966955533	graphs
0.5966850939	morphological information
0.5966841191	alternatively
0.5966488822	qa model
0.5966444691	journalism
0.5966400416	parallel text corpora
0.5966164600	optimizing
0.5966136361	unsupervised abstractive
0.5965274914	open
0.5964564397	glosses
0.5964349994	historically
0.5964349994	lemmatizer
0.5963924978	healthcare
0.5963756210	reference
0.5963640810	tell
0.5963554722	analogy task
0.5963526076	hardness
0.5963526076	19th
0.5963344099	compound words
0.5962628175	city
0.5962433307	spelling
0.5962052929	sequence discriminative
0.5961650467	multi
0.5961488722	predictive models
0.5961451531	fine grained sentiment
0.5961291411	quantifier
0.5961157897	hands
0.5960908168	conditional masked
0.5960349994	gradual
0.5960328807	char
0.5960188781	qa datasets
0.5959918244	multistream
0.5959918244	memoization
0.5959918244	dubbing
0.5959918244	multihop
0.5959918244	hunting
0.5959918244	gapping
0.5959918244	institutional
0.5959918244	counterfactually
0.5959918244	depthwise
0.5959918244	identifiability
0.5959918244	adposition
0.5959918244	compressive
0.5959918244	younger
0.5959918244	lagging
0.5959918244	prohibition
0.5959918244	knowledgebase
0.5959918244	hedges
0.5959918244	reactive
0.5959918244	piecewise
0.5959899196	classifying
0.5959830680	par
0.5959662363	visualization
0.5959326119	attacking
0.5959326119	obfuscation
0.5959106710	rationale
0.5958585380	infusing
0.5958585380	gisting
0.5958585380	factorial
0.5958040903	pivots
0.5957743329	user information
0.5957546424	sample space
0.5957119990	disentangling
0.5956834194	identifying
0.5956794306	poems
0.5956392168	greedy
0.5956368648	data preparation
0.5956141676	decay
0.5956068591	one billion word
0.5955896488	similarity task
0.5955132506	satire
0.5954582487	multiple languages
0.5954451651	simple data augmentation
0.5954351230	interaction
0.5953707995	fuse
0.5953697449	hate
0.5953521941	benchmark corpus
0.5952851131	observer
0.5952768547	larger corpora
0.5952760873	sentiment
0.5952714299	puns
0.5952425043	qa models
0.5952340720	anticipate
0.5952330796	human speech
0.5952167141	memorize
0.5952022675	geometry
0.5951897480	answer
0.5951555134	spoken
0.5951431538	animal
0.5951379272	calibration
0.5951372883	accurate
0.5951160979	dispute
0.5950274356	lan
0.5949839548	recommendation
0.5949589835	multi objective
0.5949582881	asr error
0.5949143863	wwm
0.5949100453	textual corpora
0.5948761248	multifractal
0.5948659672	altogether
0.5948646185	manifold
0.5948387364	developer
0.5948185852	r
0.5947734295	originally proposed
0.5947246971	entrainment
0.5947037300	esperanto
0.5946856493	type prediction
0.5946554019	discourse
0.5946378527	assist users
0.5946069531	translators
0.5945570461	deep learning algorithms
0.5945555622	short utterances
0.5945489051	predicting missing
0.5945180505	automata
0.5945179810	prosody
0.5944811940	cross lingual sentiment
0.5944659429	spoke
0.5944207967	significant performance improvement
0.5943692851	achieves competitive results
0.5943642892	embodied
0.5943143992	essay
0.5943047794	ranking
0.5942696011	young
0.5942669170	input sequence
0.5942533719	negation words
0.5942099543	identity
0.5942063215	hierarchical models
0.5941980691	speech applications
0.5941909892	news data
0.5941808544	data sparsity
0.5941758818	image features
0.5941536566	pre trained transformer models
0.5941459516	examinations
0.5941377465	news corpora
0.5940957081	rationales
0.5940753706	continuous
0.5939806457	relies heavily
0.5939561779	relevant documents
0.5939056132	ready
0.5938701220	rap
0.5938437998	compound
0.5938414566	revolution
0.5938253198	priors
0.5937941607	implications
0.5937818351	context vector
0.5937542752	zeros
0.5937542752	debugging
0.5937542752	empowered
0.5937542752	explorer
0.5937321568	laughter
0.5937192176	news story
0.5935816419	ng
0.5935561926	previous researches
0.5935497471	semantic
0.5935444513	parallel training
0.5935300415	proposed method
0.5934424136	unseen data
0.5934393496	improved
0.5934140521	sememes
0.5934140521	conferences
0.5934018647	insensitive
0.5933717765	minimizing
0.5933502232	keeping
0.5933026916	accent
0.5932849019	clms
0.5932677970	evaluation
0.5932644725	machines
0.5932561324	character n gram
0.5932317480	speech understanding
0.5932136540	product
0.5931607726	trainable
0.5931218062	ratio
0.5931186513	eval
0.5931097981	significance
0.5930922780	newsgroups
0.5930180079	polyglot
0.5930158902	sequential
0.5929827595	client
0.5929727751	teach
0.5929576210	semantics
0.5929385815	entity
0.5929248933	balanced dataset
0.5929206778	abusive
0.5929171410	begins
0.5929139280	birds
0.5929110010	specific rules
0.5928990862	computer vision
0.5928821322	neural translation
0.5928795542	attention based neural machine
0.5928640683	experimentation
0.5928533845	confusion
0.5927759408	protests
0.5927203303	psychology
0.5927170732	colour
0.5927153096	review dataset
0.5926719632	trees
0.5926632349	neuron
0.5926632349	speeding
0.5926632349	relaxed
0.5926461441	sampling
0.5926202151	podcast
0.5926052746	pointing
0.5925364564	ethereum
0.5925187187	benefited
0.5925034321	catalog
0.5924393377	discrimination
0.5924100299	intelligibility
0.5924011879	assessment
0.5923873227	word selection
0.5923707449	detect fake news
0.5923647447	multilingual training
0.5923547686	stimulate
0.5923547686	spent
0.5923201152	exploration
0.5922775291	crossing
0.5922176945	manual feature engineering
0.5922125089	lexical
0.5921983867	harnessing
0.5921308632	indistinguishable
0.5921250223	foster
0.5921043245	subjective evaluation
0.5920755927	parliamentary
0.5920500408	needing
0.5920245196	decoding
0.5920242197	partition
0.5920214879	avg
0.5920003457	door
0.5919883349	recent efforts
0.5919563565	excitation
0.5919563565	proposition
0.5919393810	source context
0.5919095423	worse
0.5918276479	transfer
0.5918199292	forward
0.5918045727	photo
0.5918018047	combinatorial
0.5917988937	dominated
0.5917980124	justification
0.5917322698	significantly higher
0.5917294840	reuse
0.5917224430	dearth
0.5917162046	cache
0.5917118276	preliminary evaluation
0.5916917169	reading comprehension systems
0.5916514666	diverse domains
0.5916079263	neural response
0.5915990734	vector operations
0.5915930304	stationary
0.5915930304	immune
0.5915930304	proprietary
0.5915872196	group
0.5915545991	projection
0.5915481657	commutative
0.5915432587	ideation
0.5914338491	scrambling
0.5914218275	lstm language model
0.5914213915	hub
0.5914138236	person's
0.5913158926	reward
0.5913128180	extremely difficult
0.5912953001	source tokens
0.5912888482	archive
0.5912681523	coreference
0.5912657097	phrase pairs
0.5912601277	ranking model
0.5912448025	anonymization
0.5912448025	interlingual
0.5912418152	human cognition
0.5912225527	ams
0.5912077246	planning
0.5912056193	computing
0.5911582793	doc2vec
0.5911029864	visual content
0.5911009609	collocation
0.5910780855	indication
0.5910506841	protect
0.5910483948	generated question answer pairs
0.5910384656	nucleus
0.5909968067	tanh
0.5909968067	dice
0.5909949836	educational
0.5909899368	neural entity
0.5909670974	sentence level attention
0.5909539061	courses
0.5908808136	extremely challenging
0.5908651132	asymmetry
0.5908226924	language directions
0.5907815005	fidelity
0.5907692855	common space
0.5907304117	annotation
0.5906534113	stylized
0.5906447804	facets
0.5905893430	regularizing
0.5905893430	monotonicity
0.5905533614	nlp problems
0.5905087357	prejudice
0.5905087357	nominals
0.5904869348	exemplified
0.5904719584	response candidates
0.5904563565	recursion
0.5904319318	tickets
0.5904319318	captioner
0.5904319318	curious
0.5904319318	traveling
0.5904319318	directory
0.5904319318	unfolding
0.5904319318	verbalization
0.5904124291	product attributes
0.5903724666	green
0.5903146582	corner
0.5902499626	y
0.5902441389	infrastructure
0.5902108459	highly effective
0.5902075995	toolkit
0.5901980475	nationality
0.5901893430	interleaved
0.5901819246	species
0.5901740900	automating
0.5901721770	word phrase
0.5901572600	retrofitting
0.5901462192	context based
0.5901386992	existence
0.5901297436	accented
0.5901090316	lexical item
0.5900928233	board
0.5900927874	source sentences
0.5900857156	accumulation
0.5900610838	death
0.5900564510	training efficiency
0.5900563565	aggressive
0.5900443211	input features
0.5900349036	large pre trained language models
0.5900312152	brand
0.5899333070	lyrical
0.5899333070	briefs
0.5899289834	pharmacovigilance
0.5899288088	level prediction
0.5899265095	tracking challenge
0.5898824743	poisoning
0.5898558775	merging
0.5898210266	phrase
0.5898038042	m
0.5898005052	verification
0.5897558970	multilingual asr
0.5897389344	final layer
0.5897386299	sequence data
0.5897336480	hi
0.5896667029	high efficiency
0.5896556691	translation models
0.5896502638	dozens
0.5896502638	cater
0.5896160030	linguistic information
0.5896088652	large scale language models
0.5895374299	semi supervised methods
0.5895214592	official test
0.5895188306	domestic
0.5894457373	pilot study
0.5894277994	pe
0.5893756730	provenance
0.5893043668	manipulation
0.5893009506	linked
0.5892341073	uncovering
0.5892013395	traditional
0.5891290825	plug
0.5891033523	proportions
0.5890690620	online social network
0.5890125743	nonetheless
0.5889304043	neural network acoustic models
0.5889275184	defense
0.5888856813	attentional encoder
0.5888788559	documentation
0.5888715355	label bias
0.5887848250	uniform
0.5887676602	today
0.5887528805	forums
0.5887262343	cross
0.5886071391	vulnerable
0.5885886238	recognition rate
0.5884786232	sent
0.5884462320	decompositional
0.5884346136	delay
0.5884194819	unified model
0.5884083546	tagging
0.5883875655	biomedical articles
0.5883198229	distill
0.5883045375	sentence level representations
0.5882816717	attention weight
0.5882582685	semantic vector space
0.5881845794	greatly improve
0.5881351575	time delay neural network
0.5881156961	readmission
0.5881156961	synthesizer
0.5881152404	performance evaluation
0.5881066919	taxonomy
0.5880387784	crucially
0.5880326990	alignment
0.5880174062	single token
0.5880138055	corpus
0.5879556770	amortized
0.5879340500	preliminary study
0.5878952329	composition
0.5877793804	unified medical language system
0.5876819003	shi
0.5876785057	focused multi document
0.5876741069	cross domain sentiment
0.5876703277	observational
0.5876543557	revisiting
0.5876404016	sequence generation models
0.5876237573	centric
0.5875853809	generative model
0.5875745685	asr models
0.5875669052	unseen speakers
0.5874669832	human activity
0.5874636782	vision and language navigation
0.5874632667	harassment
0.5874490896	feature representations
0.5873784365	innovation
0.5873756452	rephrase
0.5873740795	mem
0.5873345570	tunes
0.5873342041	timexes
0.5873298917	empirical
0.5873275544	rnn models
0.5873191302	modal
0.5873090203	test accuracy
0.5872712576	comparable results
0.5872463460	evidenced
0.5872413925	machine learning classifiers
0.5872392807	sift
0.5872206717	limited resources
0.5871490000	aiming
0.5871215748	message level
0.5870709481	binary
0.5870617345	clue
0.5870610045	managing
0.5870555010	philosophy
0.5870452130	dramatically improves
0.5870205700	psychological
0.5869938227	human evaluations
0.5869636894	shot
0.5869180722	canonical
0.5869135955	multichannel
0.5868990314	image analysis
0.5868741263	margin based
0.5868698858	affordances
0.5868466632	supertags
0.5867791729	wav2letter
0.5867791729	extraversion
0.5867791729	frontend
0.5867791729	text8
0.5867791729	reformulating
0.5867791729	grasping
0.5867791729	multistage
0.5867791729	paraphraser
0.5867791729	scaffolds
0.5867791729	naively
0.5867675575	grammar
0.5867455518	generation process
0.5867038769	deduction
0.5866505744	prototypes
0.5865929659	classic
0.5865903640	practicality
0.5865634888	reusing
0.5865634102	dog
0.5865106698	lda based
0.5864653263	formal framework
0.5864077957	statistical approaches
0.5863324472	keyword
0.5863143967	tweet
0.5863032091	outperforms strong baselines
0.5863003097	statistical nlp
0.5862658054	construction
0.5862636945	tail
0.5862412256	aggression
0.5862323125	hyper
0.5862269083	substantial improvements
0.5862025894	visual speech recognition
0.5861784945	pull
0.5861383785	center embedding
0.5861329616	opacity
0.5861329616	teaches
0.5861329616	valency
0.5861160238	typing
0.5861043376	received little attention
0.5860587871	explorations
0.5860534532	post
0.5860460003	aligner
0.5860299803	choosing
0.5859910650	decade
0.5859857252	funny
0.5859684870	reasoning
0.5859672307	language based
0.5859627168	transcribing
0.5859621961	men
0.5859595198	u
0.5859506986	relation learning
0.5859212068	counter
0.5859022035	ads
0.5858456403	objective evaluation
0.5858281035	parallel
0.5858035437	synopses
0.5858035437	disparity
0.5858035437	depicted
0.5858017168	boundary
0.5857816132	di
0.5857353766	similarity
0.5857351521	grammatically
0.5857227482	limited domain
0.5857179996	regular language
0.5857148687	walks
0.5856763679	hide
0.5855859180	attending
0.5855462464	speech generation
0.5855432425	economic
0.5855159092	student network
0.5854939556	imagine
0.5854101302	language structures
0.5853716641	socially
0.5853716641	phenotype
0.5853716641	underspecified
0.5853574452	pinpoint
0.5853303864	grapheme to phoneme
0.5852907794	estimator
0.5852635089	accuracy improvement
0.5852077275	human languages
0.5851423591	automatic speech recognition systems
0.5851141695	multi task deep
0.5850941824	experimental comparison
0.5850904130	document level neural machine
0.5850803522	neighbourhood
0.5850803522	integrative
0.5850288615	linguistic structures
0.5849943821	conversation model
0.5849782305	bidirectional recurrent neural
0.5849549242	document level neural machine translation
0.5848947792	disfluency
0.5848828952	normalisation
0.5848745865	acronym
0.5848697910	belief
0.5848641632	compress
0.5848322892	threat
0.5848194752	open problems
0.5848174907	zh
0.5847591387	rating
0.5847396658	tied
0.5847061551	state transducers
0.5846994585	incorporating
0.5846878850	aggregate
0.5846818839	comprehensive survey
0.5846693008	team
0.5846528350	guidance
0.5845541395	multimodal representation
0.5845329114	debate
0.5845217038	high impact
0.5844712054	fused
0.5844569920	w
0.5844556894	colloquial
0.5844403353	online platforms
0.5844190287	mimics
0.5843804048	dilated
0.5843592683	requests
0.5843522639	effective learning
0.5842833169	text input
0.5842550532	average score
0.5842478926	manually defined
0.5842414092	decoupled
0.5842330624	geometric
0.5842119948	count
0.5841102102	target domains
0.5840560612	constituent
0.5840540718	markets
0.5840540718	terminal
0.5840513097	min
0.5840513097	systematicity
0.5840513097	stepwise
0.5840513097	imprecise
0.5840513097	decompositions
0.5840513097	combating
0.5840513097	chats
0.5840415901	lasso
0.5840406114	keyphrases
0.5840341902	anonymized
0.5839848821	relevant context
0.5839341277	unsupervised machine
0.5839330829	contextualized language models
0.5839096802	low resource conditions
0.5839034496	acceleration
0.5839034496	drives
0.5838853564	research project
0.5838571027	librispeech corpus
0.5838040397	target sequence
0.5838017933	concept pairs
0.5838011460	simplified
0.5837842506	existing works
0.5837739823	sides
0.5837371006	speech translation systems
0.5836771760	low dimensional vectors
0.5836742029	supervision data
0.5836563052	twitter users
0.5836561943	layout
0.5836363043	meets
0.5836053507	augmentation methods
0.5835809317	linguistic constraints
0.5835630081	fragment
0.5835339257	bilstm based
0.5835018633	language generator
0.5834745326	makers
0.5834745326	transitive
0.5834497280	disasters
0.5834465737	developmental
0.5834352432	cross entropy training
0.5834036200	supertag
0.5834036200	epidemiology
0.5834036200	n2c2
0.5832882446	word mapping
0.5832830313	e
0.5832799089	segment
0.5832200545	relation extraction methods
0.5832007990	gloss
0.5831715262	stream
0.5831186178	training objective
0.5830004270	deceptive
0.5829651751	context specific
0.5828856339	language embeddings
0.5828720293	viewpoint
0.5828226563	generated response
0.5827796921	script
0.5827685441	method outperformed
0.5827543980	judicial
0.5827381895	inference models
0.5827233395	deep multimodal
0.5827172036	sensitive
0.5826793420	multiscale
0.5826738646	leaf
0.5826686145	words tend
0.5826659146	skeleton
0.5826639567	semeval 2017
0.5826514951	correction
0.5825640310	refuted
0.5825163244	descent
0.5824987063	individual's
0.5824987063	neighbour
0.5824670070	anew
0.5824292977	experimental result
0.5823970369	synthesizers
0.5823774639	temporal context
0.5823700681	specific information
0.5823392691	record
0.5823053029	machine
0.5822962776	synsets
0.5822666670	research direction
0.5822665851	morphological
0.5822194371	annotated training data
0.5822025275	softmax loss
0.5821871514	troll
0.5821669364	result shows
0.5821634669	approximating
0.5821324057	gauge
0.5821324057	fastest
0.5821304335	advancement
0.5820455254	source sequence
0.5820320653	online
0.5820314065	relevance based
0.5820049788	banking
0.5820049788	reflection
0.5820049788	adapters
0.5819311718	coherent text
0.5819244201	cnn models
0.5819183147	health
0.5819172304	orthography
0.5818914121	downstream nlp
0.5818755533	comprehension ability
0.5818687162	compilation
0.5818666605	acquisition
0.5818417089	let
0.5818240945	human perception
0.5818191749	speaker verification systems
0.5818147708	learning word representations
0.5817529575	bayesian models
0.5817443026	distinguishing
0.5817086559	medical image
0.5816839058	intersectional
0.5816839058	dispersion
0.5816395104	extensible
0.5816252754	public dataset
0.5815938104	multimodal inputs
0.5815464171	dementia
0.5815348154	accuracy rate
0.5815291486	isolated
0.5814698752	perfect
0.5814657772	single source
0.5814589550	multiwoz dataset
0.5814134511	large scale multi
0.5814057245	natural language description
0.5813981856	traditional media
0.5813859389	empirical evaluations
0.5813643139	focused
0.5813471947	automation
0.5813331685	rnn language model
0.5813191645	don't
0.5813079740	passive
0.5812783652	minute
0.5812637013	listener
0.5812579372	efficient learning
0.5812521804	tracker
0.5811788645	reading comprehension task
0.5811653987	analysis
0.5811489958	anxiety
0.5811154775	es
0.5811141679	tags
0.5810630745	pay more attention
0.5810412531	glyph
0.5810412531	commenting
0.5809991440	clarification
0.5809925214	contrary
0.5809333637	multilingual sentence
0.5809237890	linearization
0.5809109594	regular languages
0.5808793045	best practices
0.5808733589	strategic
0.5808016325	convolution networks
0.5807539296	timit dataset
0.5807438216	joy
0.5807341595	merits
0.5807341595	concentrate
0.5807249891	legislation
0.5806504904	model's prediction
0.5806349456	explained
0.5806231324	delivery
0.5805940955	intake
0.5805940955	countermeasures
0.5805940955	reflective
0.5805940955	modularized
0.5805880615	tips
0.5805862963	visual information
0.5805736033	simple heuristics
0.5805583705	prepositions
0.5805583705	happiness
0.5805578462	proactive
0.5805578462	lipreading
0.5805412531	inquiry
0.5805033885	conventionally
0.5805033885	tourism
0.5804734859	quasi
0.5804734859	constructive
0.5804387027	ie
0.5804304688	converter
0.5804180335	current approaches
0.5803991995	embedding methods
0.5803686878	offense
0.5803253219	neural methods
0.5803190528	subjected
0.5803190528	akin
0.5803114508	t
0.5803089314	separation
0.5802854387	review
0.5802608977	hash
0.5802568225	socialbot
0.5802568225	stickers
0.5802568225	breakdown
0.5802568225	answerer
0.5802163834	diversified
0.5802154947	static word embeddings
0.5802086221	language networks
0.5801415439	translation
0.5800636606	generation algorithm
0.5800326647	functional grammar
0.5799861358	conform
0.5799806479	user
0.5799705383	targeting
0.5798495279	significantly faster
0.5798450188	recurrent neural network architectures
0.5798367951	referential
0.5797777148	private
0.5796891991	topic
0.5796863049	decoupling
0.5796863049	mixup
0.5796451419	subsets
0.5796401361	domain specific corpora
0.5796263048	diacritization
0.5796263048	diagnosing
0.5796223165	ideally
0.5796143939	stabilize
0.5796143939	document's
0.5796143939	unaware
0.5795987374	balanced
0.5795817920	dialogue turn
0.5794801451	human language
0.5794592903	image
0.5794499072	ngrams
0.5794395246	contextualised
0.5794395246	polarization
0.5794395246	tri
0.5794395246	accelerated
0.5794395246	replicated
0.5794271968	impressive results
0.5793845567	oracles
0.5793701611	comparative evaluation
0.5793640580	persona
0.5793475566	multiple input
0.5793420788	sequence tasks
0.5792877848	contest
0.5792349076	x
0.5792348641	game
0.5792024768	domain corpus
0.5791320564	anchored
0.5791141160	highly efficient
0.5791047227	unit
0.5790519672	research field
0.5790347826	simulation
0.5790279619	emphasis
0.5790089382	structural
0.5790047192	asset
0.5789861358	tract
0.5789666183	rescoring
0.5789259990	browsing
0.5789192188	experimental settings
0.5789050548	shot settings
0.5788358554	text content
0.5788158019	foreign
0.5787876511	natural
0.5787774957	human experts
0.5787613262	moderation
0.5787452051	restricted
0.5786761347	preparation
0.5786580694	language engineering
0.5786529019	political
0.5786523790	tweets
0.5786298878	corpus level
0.5786102178	job
0.5786073892	reside
0.5785858085	reporting
0.5785829933	pseudo
0.5785677857	naive
0.5785619512	benefiting
0.5784668397	memory
0.5784663453	position information
0.5784526423	typed
0.5784187990	gradient based methods
0.5784159205	race
0.5784082923	ablation experiments
0.5783966346	natural language question
0.5783863049	performers
0.5783863049	revisited
0.5783799723	twitter sentiment
0.5783781351	labeled samples
0.5783750872	outcome
0.5783545438	entropic
0.5783228717	generating sentences
0.5783105464	biomedical entities
0.5782945002	knowledge graph based
0.5782850079	detection methods
0.5782830629	scenes
0.5782434292	answering forums
0.5782408535	pre training and fine tuning
0.5782320745	averaging
0.5782061074	predefined set
0.5781935860	scaled
0.5781882449	annual
0.5781863361	facet
0.5781781982	textual content
0.5781759732	contextually
0.5781742274	diacritics
0.5781157389	unified
0.5781078864	analyst
0.5780877737	deliberation
0.5780877737	gamma
0.5780818919	communications
0.5779966521	dialogue tasks
0.5779732827	mixed speech
0.5779558928	obesity
0.5779413274	computational
0.5779398219	hearer
0.5778947121	delayed
0.5778940123	visual scene aware dialog
0.5778926360	retrospective
0.5778926360	implicature
0.5778926360	auditing
0.5778359636	grade
0.5778306543	generalisation
0.5778271246	attention module
0.5778270847	long short term memory recurrent neural
0.5778259307	inspection
0.5778169560	opinion
0.5778065174	vqa model
0.5777918524	word representation models
0.5777881783	chinese nlp
0.5777520187	important role
0.5777421947	pragmatic
0.5777368227	retrieving relevant
0.5777302816	magnitude
0.5777089354	media
0.5776971151	accelerating
0.5776673157	annotation effort
0.5776073510	sizable improvements
0.5775742012	dialogues
0.5775637334	biaffine
0.5775376722	fighting
0.5775035092	held out test set
0.5775024309	generating natural language
0.5774832479	winner
0.5773991223	sparse features
0.5773517366	action
0.5772999553	completion task
0.5772655006	neural text
0.5772615653	monaural
0.5772615653	personalised
0.5772615653	reverberant
0.5772493717	finally
0.5772424062	compositionality
0.5771560534	orientation
0.5771468045	f
0.5771441629	integrated
0.5771306619	paradox
0.5771306619	fooling
0.5771306619	wordpiece
0.5771306619	landmark
0.5771176162	bio
0.5771148255	output sequence
0.5770994167	transfer approach
0.5770800659	pooled
0.5770800659	graphemic
0.5770467504	sparsely
0.5770467504	geospatial
0.5770467504	decipherment
0.5769962012	sliding
0.5769844373	multiplex
0.5769844373	doubly
0.5769844373	recognising
0.5769149564	dependency
0.5768976413	application specific
0.5768948803	specific domains
0.5768868972	preposition
0.5768599953	decoder
0.5768412487	self supervision
0.5768348321	optimization based
0.5768213683	single domain
0.5767640809	learning algorithm
0.5767404488	fledged
0.5767404488	rater
0.5767315213	les
0.5767209610	layer perceptron
0.5765785762	closest
0.5765701037	discrete data
0.5765568845	generator network
0.5765506945	recurrent neural network language models
0.5765401694	family
0.5765323890	syntactic tree
0.5764776142	crossword
0.5764674928	faceted
0.5764673969	hierarchical multi task
0.5764502538	switched
0.5764426608	single task
0.5764368304	countering
0.5763775852	machine translation models
0.5763752799	textual
0.5763664210	manuscript
0.5763569906	ideology
0.5763523240	counterfactuals
0.5763523240	skim
0.5763431634	word type
0.5763414021	historical data
0.5763315057	additionally
0.5763102890	transformer based models
0.5762406189	conversation
0.5762128611	sciences
0.5761843676	earnings
0.5761843676	anticipation
0.5761843676	rerank
0.5761843676	eliciting
0.5760776172	fast inference
0.5760583784	fine grained sentiment analysis
0.5760245436	biological
0.5759954873	matching
0.5759882812	suite
0.5759504685	implicit relations
0.5759156813	paucity
0.5758910689	semantic phenomena
0.5758899806	state transducer
0.5758805135	task's
0.5758710806	semantic information
0.5758550253	question
0.5758489806	linear classifiers
0.5758426604	multiple sentences
0.5758395071	visual attributes
0.5758308926	complex structures
0.5757999130	medication
0.5757926263	key factors
0.5757863854	term
0.5757373127	domain generalization
0.5757124074	virality
0.5757034725	disambiguation
0.5756804085	communicating
0.5756737232	spherical
0.5756737232	inception
0.5756737232	displacement
0.5756169531	concretely
0.5755563336	models learn
0.5755453477	logical
0.5755451403	negative transfer
0.5755298559	constituency
0.5754807850	model achieves
0.5754680435	lm based
0.5754519908	ultra
0.5754388420	bidirectional encoder representations
0.5754368906	ensembles
0.5754254029	data samples
0.5754156813	posteriori
0.5754080295	project
0.5753707852	comprehensive
0.5753364606	contract
0.5753359293	tonal
0.5753265574	semantics based
0.5753159123	semi supervised learning approach
0.5752988579	synchronic
0.5752408192	opinion terms
0.5752149896	fact extraction and verification
0.5752074253	manifested
0.5751744256	marc
0.5751585447	curiosity
0.5751575848	adapter
0.5751520808	querying
0.5751245109	diff
0.5751166272	studying
0.5750964036	perhaps
0.5749340087	complex structure
0.5748662342	predictive accuracy
0.5748501338	parser
0.5748365646	predominant
0.5748365646	penalize
0.5748125629	recognition performance
0.5747728429	box
0.5747418125	deeper
0.5747149908	language recognition
0.5747106757	fairseq
0.5746804085	gaze
0.5746740638	sketches
0.5746740638	endogenous
0.5746409945	task2
0.5746296405	network representation
0.5746278862	pruning
0.5746100608	roberta based
0.5745844688	automatically detect
0.5745841871	knn
0.5745438091	collapse
0.5745146708	web 2.0
0.5744873015	nous
0.5744437247	learners
0.5743903899	narration
0.5743902343	theme
0.5743487141	large text corpora
0.5743416216	kernelized
0.5743416216	unexpectedly
0.5743206532	based framework
0.5742896303	auxiliary
0.5742667294	x vector
0.5742504551	real world scenarios
0.5742490368	vector
0.5741984657	twitter conversations
0.5741853773	disentanglement
0.5741853773	negotiation
0.5741760730	fame
0.5741622157	h
0.5741284998	proving
0.5741175010	intent
0.5740987879	isolate
0.5740902757	hint
0.5740677736	resume
0.5740471794	pseudo data
0.5740229205	coco dataset
0.5740148469	semantic accuracy
0.5740036615	semi
0.5739884632	ary relation
0.5739859847	outbreaks
0.5739796400	main result
0.5739564843	remember
0.5738788207	perturbation
0.5738744113	vehicles
0.5738744113	telling
0.5738144976	hatred
0.5738050242	neighbours
0.5738050242	programme
0.5737873015	island
0.5737793174	reinforced
0.5737533609	temporal classification
0.5737515038	discrimination task
0.5737310472	reference translations
0.5737259561	human annotations
0.5737094678	hierarchical representations
0.5737011364	network embeddings
0.5736699145	visual world
0.5736650255	figurative
0.5736596609	composite
0.5736308886	future studies
0.5736238932	fact based
0.5735603275	other's
0.5735372191	neural network transducer
0.5734900069	zero pronoun
0.5734639279	trust
0.5734502482	weighting
0.5734328107	word embedding algorithms
0.5734260479	standards
0.5734259660	concatenative
0.5734259660	sublanguage
0.5734215188	guess
0.5734134363	semeval 2010 task 8
0.5734076726	triple
0.5733591295	prosodic
0.5733569907	aspect
0.5733419207	save
0.5733396256	quaternion
0.5733396256	decomposable
0.5733396256	infused
0.5733396256	profit
0.5733236081	minimal
0.5732625188	local
0.5732172733	text samples
0.5732131684	comprehending
0.5732131684	sparsemax
0.5732131684	films
0.5732131684	bytes
0.5732131684	tie
0.5732131684	pragmatically
0.5731857132	hoping
0.5731640576	vision
0.5730106988	input words
0.5730074220	negative training
0.5730062922	bimodal
0.5729985845	compiling
0.5729985845	phonotactic
0.5729644189	cohort
0.5729644189	stylometric
0.5729644189	pivoting
0.5729644189	verse
0.5729644189	occupation
0.5728764451	trial
0.5728667279	attribute value
0.5728475065	addressing
0.5727707312	checker
0.5727627188	solver
0.5727565162	emotion cause
0.5727262661	tying
0.5727262661	classroom
0.5727262661	organisation
0.5727262661	anonymity
0.5727262661	tokenizer
0.5727241509	edge
0.5727119600	renewed
0.5726957910	online content
0.5726861269	symptom
0.5726113276	vmf
0.5725802288	classification accuracy
0.5725585616	generating natural
0.5723572868	accounted
0.5722923093	arithmetic
0.5722907391	ontologies
0.5722865986	end to end trainable
0.5722706190	numeral
0.5722706190	unwritten
0.5722706190	silent
0.5722706190	accommodation
0.5722671908	recognition
0.5722380468	distantly
0.5722079080	lemmatization
0.5722034249	dialogue policies
0.5722025142	triage
0.5722025142	recommending
0.5721893935	simplification
0.5721840973	leaderboards
0.5721840973	tropes
0.5721840973	resilient
0.5721840973	textbook
0.5721840973	conjugation
0.5721840973	debunking
0.5721840973	shortcuts
0.5721840973	ideological
0.5721840973	justifying
0.5721840973	household
0.5721840973	romanization
0.5721840973	proximal
0.5721840973	blending
0.5721840973	perceptrons
0.5721840973	morph
0.5721840973	roadmap
0.5721840973	appendix
0.5721840973	secure
0.5721649586	similarity scores
0.5721447259	grapheme
0.5721444975	simile
0.5721264791	carryover
0.5721186372	clinical named entity
0.5721080141	common phenomenon
0.5721027186	clip
0.5720454505	eat
0.5720403464	proposal
0.5720399362	output distributions
0.5720274072	discriminative
0.5720148660	evolving
0.5719821374	blame
0.5719597605	impressive performance
0.5719486786	shareable
0.5719486786	flood
0.5719486786	harvesting
0.5719486786	genomic
0.5719486786	indigenous
0.5719422649	dilemma
0.5719422649	hyperpartisan
0.5719422649	biography
0.5719422649	skipgram
0.5719422649	tailoring
0.5719001335	server
0.5718990879	prediction based
0.5718482005	perception
0.5718135155	oriented
0.5717362428	hierarchical memory
0.5717359847	degrading
0.5717359847	ranges
0.5717060358	conventional hybrid
0.5716955668	vehicle
0.5716692490	exchange
0.5716125152	experimental evaluation
0.5715717839	weight
0.5715180548	hit
0.5714749549	editorial
0.5714749549	verifiability
0.5714749549	lexicalist
0.5714743563	pre trained language model
0.5714631008	relative improvements
0.5714372917	refined
0.5713726055	contextual text
0.5713504790	longstanding
0.5713281724	neural machine translation model
0.5712973349	babi tasks
0.5712912978	sentiment treebank
0.5712450087	distant
0.5711389937	textual evidence
0.5711259039	predicate
0.5711098753	v1
0.5710969088	hidden
0.5710663399	quest
0.5710663399	multihead
0.5710663399	multiparty
0.5710595398	indexing
0.5710351816	content words
0.5710249738	crowds
0.5709850866	endowing
0.5709850866	jump
0.5709850866	office
0.5709850866	invertible
0.5709850866	unveiling
0.5709845445	inspired
0.5709654226	independently trained
0.5709614994	equal
0.5709236820	selector
0.5708816773	candidate answer
0.5708331791	visual input
0.5707970827	multilevel
0.5707774455	agenda
0.5707721881	frequency
0.5707548365	pretrained
0.5707413017	intends
0.5707311103	limit
0.5707273500	valued
0.5707249313	time series
0.5706855207	language specific bert
0.5706389868	availability
0.5706025200	pattern
0.5705762706	data driven approach
0.5705665869	implicit
0.5705490668	coordinated
0.5705260471	verbs
0.5705240897	reproducible
0.5704981297	solving
0.5704758826	geoparsing
0.5704588576	sociolinguistics
0.5704554578	speech samples
0.5703966734	centering
0.5703665984	optimal
0.5703060386	8 bit
0.5702823032	relational graph
0.5702502909	block
0.5702418350	linguistic differences
0.5702364504	based data augmentation
0.5702208778	agent's
0.5701596609	associative
0.5701044713	persuasion
0.5700749972	simple questions
0.5700702896	converting
0.5699929682	meaning
0.5699287947	device
0.5699085876	propensity
0.5698938908	syllabification
0.5698764124	page
0.5698736798	numerical
0.5698261788	industrial applications
0.5698197341	initial experiments
0.5698060262	supply
0.5697756353	code switching data
0.5697706150	positive effects
0.5697699886	convolutions
0.5697696534	disagree
0.5697668632	world
0.5697417188	augmentation technique
0.5697048564	directed
0.5696537949	exit
0.5696453035	phrase based statistical machine
0.5696436203	research topics
0.5696265681	ellipsis
0.5696258772	space model
0.5695624421	natural language modeling
0.5695352565	e commerce websites
0.5694986499	data driven models
0.5694955711	natural language question answering
0.5694934571	acts
0.5694916031	punctuation
0.5694855446	pick
0.5694711470	tolerant
0.5694293921	beam search algorithm
0.5694264350	arousal
0.5694164374	mod
0.5693831165	reasoning steps
0.5693768761	supported
0.5693737195	streams
0.5693569642	generation models
0.5693560375	bottom up
0.5692961194	mention level
0.5692774164	terminological
0.5692573185	edit
0.5692309562	barrier
0.5691953876	coded
0.5691383654	categorization
0.5691272438	segmentation
0.5691220237	heads
0.5691152359	meeting
0.5690586608	model treats
0.5690377730	document describes
0.5690090094	pretraining
0.5689644783	targeted
0.5689345107	don't know
0.5688786195	positive rate
0.5688491089	plenty
0.5688491089	denoted
0.5688432418	additional features
0.5688416868	authoring
0.5688026513	looked
0.5687744254	intensities
0.5687704379	pass
0.5687570928	existing methods
0.5687442915	commentary
0.5687159726	feature embedding
0.5686874183	supposed
0.5686591393	syntactic rules
0.5686261472	approaching
0.5686096277	open source implementation
0.5685804959	type labels
0.5685695719	precise
0.5685473913	preference
0.5685445600	dataset bias
0.5684726459	switching
0.5684186975	whisper
0.5684186975	anchoring
0.5684007417	narrative
0.5683820435	ranking algorithm
0.5683793454	contextual language models
0.5683730243	instantiation
0.5683541075	language grounded
0.5683365278	context
0.5683204881	general data
0.5682991652	research community
0.5682774164	maximal
0.5682583073	recent progress
0.5682411583	ordinal
0.5682181080	existing
0.5682084187	mail
0.5681925515	distributed training
0.5681919560	co occurrence statistics
0.5681372765	cooking
0.5681257082	quantitatively and qualitatively
0.5681201776	unstructured clinical
0.5681035871	flow
0.5680976397	historical
0.5680792115	cybersecurity
0.5680690382	temporal analysis
0.5680639142	professional
0.5680324619	romanized
0.5680186994	pace
0.5679985025	categorical
0.5679746012	loop
0.5679740007	repair
0.5679731037	restriction
0.5679276298	exposure
0.5679263515	projections
0.5678841073	confounders
0.5678119716	fusion method
0.5677579442	adversarial loss
0.5677574654	technically
0.5677574654	regimen
0.5677574654	van
0.5677520790	softmax based
0.5677316631	normalizing
0.5677253006	distant speech
0.5677020563	pages
0.5677011986	neural word
0.5676617740	irrespective
0.5676177230	general architecture
0.5675548190	20th
0.5675408712	word learning
0.5674608981	higher accuracy
0.5674505132	coder
0.5674456875	exact
0.5674436102	unseen test
0.5673886121	catastrophic
0.5673828903	phonetic features
0.5673817897	large scale knowledge graph
0.5673511424	quantitative
0.5673207804	go
0.5673173068	subtitle
0.5672982797	conversion
0.5672578876	grammatical
0.5672409866	positional
0.5672118233	conversation data
0.5672066878	support set
0.5671192545	nowadays
0.5671183770	classical
0.5671143799	positives
0.5671104058	backtranslation
0.5671104058	colors
0.5671104058	entertainment
0.5670625671	model outperforms
0.5670601377	learning approach
0.5670517690	dropped
0.5670435679	focusing
0.5670323231	verb
0.5669371361	mathematical
0.5668968839	stereotypes
0.5668807209	asynchronous
0.5668499823	informal language
0.5667820207	contextualization
0.5667728290	proportion
0.5667667062	cross lingual knowledge
0.5667262047	blackbox
0.5667243584	log linear models
0.5667112512	object
0.5666848132	relation type
0.5666464841	introducing
0.5665903028	input sentence
0.5665605741	training instances
0.5665603912	thumbs
0.5665517162	scene
0.5665497195	text span
0.5665353657	training strategy
0.5665247516	textual sources
0.5665056004	image datasets
0.5664374341	performance boost
0.5664048644	voices
0.5663426733	rationalization
0.5663426733	corruption
0.5663426733	depressed
0.5663426733	tutoring
0.5663426733	generalised
0.5663426733	interlingua
0.5663426733	assembling
0.5663426733	metonymy
0.5662998366	today's
0.5662894921	review text
0.5662833724	narratives
0.5662399163	fine grained named entity
0.5662343760	monoids
0.5662343760	supervisor
0.5662343760	codemixed
0.5662343760	intervene
0.5662343760	panels
0.5662343760	histograms
0.5662343760	bioinformatics
0.5662343760	descriptor
0.5662343760	embedders
0.5662343760	screenplays
0.5662343760	prioritized
0.5662343760	taxi
0.5662343760	outdoor
0.5662343760	profanity
0.5662343760	stylistically
0.5662343760	nonmonotonic
0.5662343760	overwriting
0.5662343760	bilateral
0.5662343760	thirty
0.5662343760	dialectology
0.5662343760	disparities
0.5662343760	interrelationship
0.5662343760	prioritizing
0.5662343760	chord
0.5662343760	hiring
0.5662343760	backing
0.5662343760	infectious
0.5662343760	tunable
0.5662343760	horizon
0.5662343760	inadequacy
0.5662343760	determiner
0.5662343760	timescale
0.5662343760	henceforth
0.5662343760	perturb
0.5662343760	aspectual
0.5662343760	phonotactics
0.5662343760	separators
0.5662294828	high dimensional space
0.5662105395	heterogeneous
0.5662040080	coefficients
0.5661989940	shown promising
0.5661882236	vae based
0.5661822848	neural network approaches
0.5661419982	binary relation
0.5661016021	native languages
0.5660947452	positive examples
0.5660841222	interpretable
0.5660729342	theoretic
0.5660723897	table to text generation
0.5660413295	bringing
0.5660413295	plural
0.5660250549	microblog
0.5659976676	positive or negative
0.5659625607	single channel speech
0.5659593103	gesture
0.5659157422	speech
0.5659002823	data structures
0.5658707633	genre
0.5658707443	longer texts
0.5658432188	lexically
0.5657823045	response matching
0.5657026397	empathy
0.5657023723	era
0.5656877334	performance drops
0.5656846351	rapid growth
0.5656575261	schizophrenia
0.5656575261	preconditions
0.5656575261	speculation
0.5656575261	corporate
0.5656575261	omissions
0.5656575261	batched
0.5656575261	audit
0.5656575261	populating
0.5656575261	tactical
0.5656575261	historic
0.5656428449	attached
0.5656428449	consisted
0.5656253311	original text
0.5656160721	scoring
0.5655823700	homology
0.5655823700	universals
0.5655631674	tourist
0.5655196898	transferability
0.5654438430	word embedding methods
0.5654081046	tricks
0.5654081046	filler
0.5653924067	distributed
0.5653861144	decoder layers
0.5653741067	microblogs
0.5653412543	propagate
0.5653230044	fit
0.5653076387	maximizing
0.5652944894	substitutions
0.5652395912	unified medical
0.5652222049	filtering
0.5651965331	supervision
0.5651729316	candidate selection
0.5650839757	key challenges
0.5650603089	public datasets
0.5650569064	model comparison
0.5649929736	master
0.5649690197	grouped
0.5649206552	number
0.5649192536	weighted f1 score
0.5649162177	task based
0.5648924800	unannotated corpus
0.5648891708	text mining methods
0.5648807158	predictability
0.5648774041	nlp research
0.5648348348	character level models
0.5648335137	simultaneous machine
0.5648141414	posterior
0.5647945725	trolls
0.5647754713	summarization dataset
0.5647671914	dangerous
0.5647469094	ago
0.5647469094	localize
0.5647275901	summary
0.5647077728	stock
0.5646936248	pre trained embeddings
0.5646742758	slot value pairs
0.5646709273	grounded speech
0.5646408463	speech frames
0.5646355662	objective functions
0.5646266842	theory of mind
0.5646171343	ultimately
0.5645427336	regularization term
0.5645233379	collecting
0.5644566079	results reported
0.5644554164	open ended questions
0.5644478350	plug and play
0.5644381160	drug interactions
0.5644362816	coordinate
0.5644078249	modularity
0.5644078249	divergences
0.5643522057	fix
0.5643383328	preliminary
0.5642885694	speech recognition errors
0.5642150202	exams
0.5641939287	syntactic semantic
0.5641880564	nonlinear
0.5641791886	separately trained
0.5641573834	qa tasks
0.5641334639	word representation learning
0.5640784157	progressive
0.5640775927	annotation cost
0.5640705521	shifting
0.5640680311	phone
0.5640566724	relevant information
0.5640528778	output distribution
0.5640269433	native
0.5640114024	power
0.5639607482	channel model
0.5639597610	deep transfer
0.5639331758	personal
0.5638765880	multiplicative
0.5638336084	lecture
0.5637685230	frames
0.5637646927	measurement
0.5637485304	worthiness
0.5637094127	composition function
0.5636612897	oz
0.5635937767	translating
0.5635871964	replacement
0.5635657805	depth
0.5635435566	informative responses
0.5635307101	experiments
0.5635275082	indian social media
0.5634834423	alphabets
0.5634820083	k
0.5634725429	class specific
0.5634448891	abstractive methods
0.5634445151	text
0.5633663346	learning mechanism
0.5633602049	\ url https
0.5633423188	token based
0.5633419135	calling
0.5633275309	pathway
0.5632793222	invariance
0.5632768528	average
0.5632763913	low resource setting
0.5632449537	information systems
0.5632402935	deep cnn
0.5632157644	inflectional
0.5631914395	semeval 2018 task 7
0.5631911751	topic vectors
0.5631863146	language phenomena
0.5631373926	guessing
0.5631339694	optimization
0.5630745253	semeval 2019 task 6
0.5630720048	neural lms
0.5630612897	multitude
0.5630558948	speaker
0.5630470561	reactions
0.5630452826	fundamental step
0.5630127857	convolutional recurrent
0.5629942879	presentation
0.5629926932	unbiased
0.5629834121	split
0.5629770303	formal
0.5629518999	motivated features
0.5629112933	demonstration
0.5629080154	filling
0.5628964542	ring
0.5628668937	paper argues
0.5627997213	specific language models
0.5627741859	final model
0.5627248961	rule based approach
0.5627204067	fool
0.5626796929	polynomial
0.5626727163	supplement
0.5626409746	lstm crf model
0.5626202213	correlation with human judgments
0.5626195903	conversational text
0.5625686067	submodular
0.5625686067	credible
0.5625271322	textual features
0.5625196268	finer
0.5625074805	mortality
0.5625019837	statistical
0.5624806572	knowledge source
0.5624694316	deep neural models
0.5624665454	n best hypotheses
0.5624591659	body
0.5624263973	crowdsourced
0.5623777036	nlp techniques
0.5623551884	results confirm
0.5623493303	audio
0.5623010572	base model
0.5622720157	smt systems
0.5622273831	priming
0.5622273831	hubs
0.5621859496	encoder
0.5620902369	music
0.5620163047	designing
0.5619476619	supervised setting
0.5619458166	implicit knowledge
0.5619457031	firstly
0.5619100092	training methods
0.5618771820	synonym
0.5618376363	crossings
0.5618212018	attraction
0.5618212018	fingerprints
0.5618042731	localized
0.5618042731	cooperation
0.5618042731	encyclopedic
0.5617639989	recognizers
0.5617571295	customize
0.5617488614	prior distribution
0.5617362733	prediction models
0.5616962961	expressive
0.5616133284	informal
0.5615938672	organization
0.5615711494	phonetic
0.5615393475	translate
0.5614243882	user groups
0.5614185362	compressing
0.5613913047	constructing
0.5613793540	paramount
0.5613793452	movements
0.5613410607	multiple tasks
0.5613112897	ary
0.5612314725	previous
0.5612061685	state
0.5611597514	empathetic
0.5611571921	specific features
0.5611476086	overcoming
0.5611437702	decoder generates
0.5611320141	taxonomies
0.5611247957	network models
0.5611030009	shortage
0.5610576387	audience
0.5610135532	voice based
0.5609945381	quantitative evaluations
0.5609940958	word form
0.5609470770	simplifying
0.5609153696	p
0.5609151003	morphological annotation
0.5609103149	systems submitted
0.5608913086	smoothing
0.5608483747	supervision signal
0.5608179992	shopping
0.5607983463	extracting information
0.5607968436	user generated data
0.5607957506	intuitively
0.5607569268	matters
0.5607063964	learning approaches
0.5606880045	attention based lstm
0.5606845241	meet
0.5606823053	great challenge
0.5606726082	fasttext
0.5606723526	data augmentation method
0.5606361547	learnable
0.5606100688	tailed
0.5605682649	art
0.5605550499	image processing
0.5605160347	life
0.5605025785	invariant
0.5604996220	evaluation setup
0.5604781039	pure
0.5604560613	estimating
0.5604269895	graph embedding methods
0.5604077157	simple heuristic
0.5604070184	meanwhile
0.5604025652	shaped
0.5603898621	acoustic signal
0.5603634635	keywords
0.5603413340	adjusting
0.5603413340	conceptualization
0.5603213169	synthesizing
0.5603077813	text generation tasks
0.5602981548	classification task
0.5602775003	generating responses
0.5602177128	choice questions
0.5601926205	deep neural network models
0.5601887818	supervised classifiers
0.5601787540	large scale datasets
0.5601320141	connecting
0.5600846226	health issues
0.5600121998	list
0.5599626790	open domain dialog
0.5599522657	human expert
0.5599225066	executable
0.5599147996	learning based approach
0.5599000385	promising directions
0.5598684799	informative summaries
0.5598577875	special cases
0.5598417045	sufficient training data
0.5598322196	c
0.5598283118	technical
0.5598167758	input utterance
0.5597928383	cyclic
0.5597928383	recovering
0.5597666907	leader
0.5597417292	multiple nlp tasks
0.5596900046	diagnosis
0.5596859506	massive
0.5596467507	stopping
0.5596170327	visually
0.5594603442	supervised approaches
0.5594380775	real life applications
0.5594157532	pronunciation
0.5593915111	compounds
0.5593589347	syntactic complexity
0.5593477377	box attacks
0.5593465209	editing
0.5593299169	characterization
0.5593195638	automatically learning
0.5593112897	handful
0.5592937003	plans
0.5592833778	simple
0.5591574666	heuristic rules
0.5591345001	word identification
0.5589815633	orthographic
0.5589562259	text spans
0.5589525195	downloaded
0.5589402337	protected
0.5589402337	beamforming
0.5589402337	customization
0.5589402337	interpreter
0.5589061004	global structure
0.5589024847	subjectivity
0.5588565991	creativity
0.5588240196	diagnostic
0.5587642913	similarity score
0.5587291830	calibrated
0.5586966942	software
0.5586785576	spatiotemporal
0.5586785576	vertex
0.5586687035	depressive
0.5586538430	polarities
0.5585542476	conversations
0.5585538733	geolocation
0.5585240928	refine
0.5584985258	dev
0.5584789834	autoregressive transformer
0.5584641404	labeling
0.5584520776	goal oriented dialog systems
0.5584291426	modes
0.5584214830	loading
0.5584214830	disambiguator
0.5584214830	attractors
0.5583892495	embedding algorithms
0.5583823687	uncertainty
0.5583802220	unwanted
0.5583626428	text image
0.5583533933	discriminant
0.5583472700	long sentences
0.5583322084	stimulus
0.5583259748	programmes
0.5583259748	stereotyping
0.5583259748	conjunctive
0.5583259748	playback
0.5583259748	embodiment
0.5583259748	conditionals
0.5583259748	syllogistic
0.5583259748	streamable
0.5583259748	hatespeech
0.5583259748	cyclical
0.5583259748	seventh
0.5583259748	rhyming
0.5583259748	abnormality
0.5583259748	defending
0.5583259748	scoped
0.5583259748	imputation
0.5583259748	prescription
0.5583259748	thirteen
0.5583259748	queen
0.5583259748	artistic
0.5583259748	summarise
0.5583259748	nonsensical
0.5583259748	neighbourhoods
0.5583259748	entertaining
0.5583259748	lightly
0.5583259748	multifaceted
0.5583259748	quoted
0.5583259748	deceive
0.5583259748	contingent
0.5583259748	alert
0.5583259748	restoring
0.5583259748	anytime
0.5582857685	competitive baseline
0.5582825120	management
0.5582695323	plethora
0.5582672134	subsequence
0.5582665207	subjective evaluations
0.5582281184	similes
0.5582144332	interpretability
0.5581744101	vocabulary speech recognition
0.5581414286	re ranker
0.5581303097	proceedings
0.5581273182	written form
0.5581126294	perceive
0.5580806679	scoring model
0.5580535777	prototypical
0.5580373797	gru based
0.5580283803	empirical study shows
0.5579819775	french corpus
0.5579474090	geo
0.5579344770	neural discourse
0.5578822392	equally important
0.5578609110	critique
0.5578566837	noisy user generated
0.5578384926	responders
0.5578384926	lexicography
0.5577799321	neural features
0.5577151558	mixing
0.5577090866	learning latent
0.5576932639	license
0.5576886239	search results
0.5576177574	single sentences
0.5575906763	perceived
0.5575664567	technology
0.5574910715	utilities
0.5574796509	hypernym
0.5574796509	abbreviation
0.5574731603	text passages
0.5574399801	clustering based
0.5574121732	versatility
0.5573755950	adding
0.5573608941	accurately identify
0.5573504424	neural network architecture
0.5573136557	pathways
0.5572996928	trigram
0.5572967637	paid
0.5572796707	answer prediction
0.5572671556	dialects
0.5572417211	rule
0.5571956589	survey
0.5571512639	frozen
0.5571482028	language independent features
0.5571426086	parent
0.5571162682	sexist
0.5570736353	acoustic conditions
0.5570728420	word counts
0.5570230963	linguistic representations
0.5570039874	summarizer
0.5569771975	static
0.5569726868	linguistic
0.5569658632	validation
0.5569529451	videos
0.5568632962	low resource machine
0.5568608003	comprehension questions
0.5567915456	selection methods
0.5567779526	suitability
0.5567656052	plain
0.5567481659	classifier based
0.5567349000	bilingual data
0.5566788890	document
0.5566459592	clustering algorithms
0.5565987914	re
0.5565281002	transfer based
0.5564855901	multiple meanings
0.5564826031	subjective
0.5564768764	syntactic tasks
0.5564624264	multilingual context
0.5564312901	model surpasses
0.5564193804	similarity computation
0.5564002691	supervised
0.5563450056	discourse tree
0.5563177408	hindered
0.5562079018	pipelines
0.5562007122	synchronous
0.5561654241	encoder layers
0.5561642649	factored
0.5561569447	face to face
0.5561525275	origin
0.5561331876	originating
0.5561203496	cross lingual information retrieval
0.5561160243	achieving competitive results
0.5561133930	legal information
0.5561050016	mechanistic
0.5561050016	texture
0.5561050016	dysarthria
0.5561050016	opposites
0.5561050016	memorable
0.5560687646	mixture of experts
0.5559816711	metric based
0.5559398711	machine learning method
0.5558845456	truncation
0.5558845456	typo
0.5558845456	specialisation
0.5558845456	contextualizing
0.5558845456	pathological
0.5558845456	distress
0.5558845456	singer
0.5558845456	injury
0.5558845456	homographs
0.5558845456	aesthetic
0.5558845456	spammers
0.5558845456	presupposition
0.5558845456	blocking
0.5558845456	faithfully
0.5558845456	consultation
0.5558845456	collocational
0.5558845456	calculi
0.5558845456	slice
0.5558845456	ultrametric
0.5558719590	suggestions
0.5558620995	commitment
0.5558620995	retail
0.5558376538	denotations
0.5558372153	embedding technique
0.5558058820	paired image
0.5557948135	semeval 2010
0.5557872348	autoregressive neural
0.5557849991	bi directional long short term
0.5557825891	sentence extraction
0.5557679219	real world text
0.5557426436	test data
0.5554826641	experimental results showed
0.5554543132	reinforce
0.5554276819	essence
0.5554141167	recently developed
0.5554081364	# metoo
0.5553970614	generalization
0.5552861517	initially
0.5552397666	repairs
0.5552381717	searching
0.5552198348	document collection
0.5552082773	books
0.5552060891	adaptation data
0.5552026584	acoustic
0.5551358660	label set
0.5551350417	training distribution
0.5551313006	determining
0.5550857057	case
0.5550784849	user generated text
0.5550590935	reading times
0.5550549235	succinct
0.5550549235	pronominal
0.5550389815	supervised classification
0.5550073924	guideline
0.5550073924	correlating
0.5550073924	parody
0.5550073924	presuppositions
0.5550073924	multilingualism
0.5550073924	intonational
0.5550073924	compromised
0.5550073924	phishing
0.5550073924	sharp
0.5550073924	indications
0.5550073924	rewarding
0.5550073924	radicals
0.5549621922	bigram
0.5549173353	v2
0.5548765361	pixel
0.5548523212	regularization technique
0.5548111668	completion
0.5547383150	vectorization
0.5547383150	judging
0.5547383150	psychometric
0.5547383150	dyadic
0.5547383150	headed
0.5547383150	trustworthy
0.5547318336	embeddings
0.5546934021	experimenting
0.5546893615	robustness
0.5546649251	answerability
0.5546642305	knowledge graph embedding models
0.5546314985	history
0.5546275393	linked open
0.5546137323	tasked
0.5545853988	methods outperform
0.5545508609	probe
0.5545353930	bayesian approach
0.5544773835	engaged
0.5544613852	geographic
0.5543342104	unsupervised document
0.5542977069	discriminative features
0.5542615353	traditionally
0.5542495434	based question answering
0.5542385233	text quality
0.5541680977	skew
0.5541680977	unlexicalized
0.5541680977	wanted
0.5541566270	sequence generation tasks
0.5541423276	guiding
0.5541353075	main components
0.5541257825	autoencoding
0.5541086656	random
0.5540862431	driving
0.5540457689	regressive
0.5540186604	compounding
0.5540186604	restricting
0.5540186604	subcategorization
0.5540164186	location
0.5539637847	results reveal
0.5539565420	classification model
0.5539467052	tagger
0.5539446193	avenues
0.5539347622	correlated
0.5539179417	type
0.5538807185	fine tuning pre trained
0.5538554943	conjunction
0.5538523854	scientific knowledge
0.5538468618	surprising
0.5537864225	qualitatively and quantitatively
0.5537836305	polite
0.5537782805	natural language text
0.5537282426	negative
0.5536863669	consequently
0.5536651910	achieves comparable
0.5536606607	elections
0.5536175976	specialization
0.5535940292	nlg tasks
0.5535463020	sensitive information
0.5535222699	transfer learning approach
0.5534651704	knowledge representations
0.5534393524	extractive summarization models
0.5533477578	multi turn response
0.5533403541	bidirectional transformer
0.5533335122	length
0.5533241876	paraphrased
0.5532226837	n
0.5531695015	cascaded
0.5530831428	evaluator
0.5530264795	performance improvements
0.5530246985	based text classification
0.5530243097	transparency
0.5530061116	bert representations
0.5530029814	nlu tasks
0.5529186432	co
0.5529140044	partial
0.5528848998	performance drop
0.5528672444	application
0.5528595811	agreement based
0.5528071818	model obtains
0.5527678922	source domains
0.5527548428	target task
0.5527397666	bags
0.5527397666	impaired
0.5527066803	collaboration
0.5525852249	nmt models
0.5525831428	pushing
0.5525831428	clinically
0.5525831428	reconstructing
0.5525069617	comprehension tasks
0.5524813507	academic
0.5523950638	labeled instances
0.5523927651	variance
0.5523498856	neural attention model
0.5523471923	english texts
0.5523354549	dull
0.5523161191	richness
0.5522758216	disorders
0.5522586249	requirement
0.5522558489	sampling rate
0.5522459533	algebras
0.5522102836	experiment shows
0.5522016227	encoding scheme
0.5521000601	nmt model
0.5520988579	modern english
0.5520638787	noise model
0.5520608334	category
0.5520141526	calls
0.5519968971	instance
0.5519459493	entire sequence
0.5519274887	treating
0.5519272352	neural translation models
0.5518955828	spoken dialogue system
0.5518770225	crosslingual
0.5518605353	continuous bag of words
0.5518406693	english sentences
0.5518201600	mediated
0.5518065185	sponsored
0.5517717538	video content
0.5517319681	applying
0.5517133621	melody
0.5516924535	main idea
0.5516512262	adverbs
0.5515588259	redundancy
0.5514973462	pressure
0.5514876322	visual cues
0.5514818809	workers
0.5514816947	machine translation approaches
0.5514762263	research explores
0.5514748211	training procedure
0.5514516447	participate
0.5514375514	seed
0.5513959222	source words
0.5513929057	scalar
0.5513827980	coherent topics
0.5513471762	uncertainty based
0.5513166397	relatedness
0.5512923475	visual objects
0.5512884886	pivotal
0.5512884886	advocate
0.5512483343	scheduling
0.5512402794	pronouns
0.5512295181	indoor
0.5512293052	encoder representations
0.5512136327	neutral
0.5511821678	authority
0.5511721509	pursue
0.5511008538	radical
0.5510266076	response
0.5509929313	comparatives
0.5509929313	causally
0.5509929313	occupational
0.5509929313	loose
0.5509929313	compliance
0.5509929313	metaphoric
0.5509929313	impression
0.5509929313	hear
0.5509929313	degeneration
0.5509929313	denoise
0.5509929313	picking
0.5509496063	protest
0.5509454563	synthesize speech
0.5509070714	transcribed data
0.5508716321	natural language interfaces to databases
0.5507777332	sequence model
0.5507689731	senseval 2
0.5507169965	model development
0.5506800780	monolingual text
0.5506684312	inference
0.5506333569	input audio
0.5506093095	headline
0.5505984990	promoting
0.5505830436	mechanics
0.5505031053	cross lingual learning
0.5504938726	learning word embeddings
0.5504853083	parity
0.5504820806	host
0.5504291378	incompleteness
0.5503700447	n best list
0.5503671020	end user
0.5503600087	overlapping
0.5503322787	main objective
0.5502930820	non canonical
0.5502596958	appeared
0.5502531438	visualizations
0.5502324812	cultural
0.5501146570	extensive experimental
0.5500057895	un
0.5499779865	readability
0.5499534546	transcriber
0.5498445413	copying
0.5498170450	legislative
0.5498170450	subtraction
0.5498147574	match
0.5497228911	dis
0.5496998776	positive sentiment
0.5496730947	mixture
0.5496461456	achieves competitive performance
0.5496398451	english word
0.5495582770	model design
0.5495402807	free
0.5495146841	annotated examples
0.5495093880	neural network language model
0.5495091541	effectively utilize
0.5494951790	synthetically
0.5494701630	blind
0.5494366191	incomplete
0.5494097589	diagnose
0.5493742770	rewards
0.5493345978	employing
0.5492988284	fusion methods
0.5492904417	metadata
0.5492646175	modern standard
0.5492579929	possibilities
0.5492381406	shelf
0.5492371532	agent
0.5492216872	basic
0.5491995460	retrieval task
0.5491725458	cross lingual classification
0.5491465027	interpreting
0.5491136441	strong results
0.5491029772	relationship
0.5490003193	context information
0.5489997719	data sparsity problem
0.5489336706	co occur
0.5488873022	feature
0.5488603523	unnatural
0.5488508694	noticeable
0.5488229028	referent
0.5488229028	politicians
0.5488229028	analytic
0.5488228279	significant margin
0.5488202923	illness
0.5488134572	correlation
0.5487933484	deep neural network architecture
0.5487909501	bag of words
0.5487465550	based evaluation
0.5487306543	user studies
0.5486889093	trick
0.5486764347	speeches
0.5486332402	public data sets
0.5486252656	token level labels
0.5486118365	discussion
0.5485862111	machine learning models
0.5485698205	gendered
0.5485637483	mining tools
0.5485337662	old
0.5485215546	metaphors
0.5485122252	unsupervised pre training
0.5484810361	automatically annotated
0.5484788590	morphological complexity
0.5484757254	speaking
0.5484227951	tutorials
0.5484227951	literals
0.5484227951	programmers
0.5483571234	degree
0.5483567906	next
0.5483545561	predictive
0.5483403225	consistency
0.5483306441	second order
0.5483215302	language guided
0.5482857710	rouge score
0.5482592517	susceptible
0.5482592517	ultimate
0.5482090241	additional supervision
0.5481919653	surge
0.5481885252	theoretical framework
0.5481684197	dereverberation
0.5481684197	poetic
0.5481684197	diversification
0.5481647027	providers
0.5481592987	valence
0.5481110309	thoughts
0.5481059507	ambiguous words
0.5480931783	results highlight
0.5480099217	recent approaches
0.5480036287	belongs
0.5480018068	scarce data
0.5479884935	down stream
0.5479001171	attack
0.5478883679	target entity
0.5478653814	demographic
0.5478261900	representation model
0.5478044443	models struggle
0.5477640827	trajectory
0.5477640827	unbounded
0.5477640827	persistent
0.5477411242	end to end speech synthesis
0.5477314915	entailment task
0.5477022913	knowledge
0.5476930003	headlines
0.5476122170	sentiment label
0.5475957169	end task
0.5475870575	self play
0.5475750162	word context
0.5475641338	sentiment analysis dataset
0.5475553192	review summarization
0.5475489698	finetuned
0.5475239791	chinese english and english german
0.5474519652	real world datasets
0.5474065149	elimination
0.5474000498	debating
0.5474000498	hopping
0.5474000498	antisocial
0.5474000498	workplace
0.5474000498	deteriorate
0.5474000498	psychotherapy
0.5474000498	captioned
0.5474000498	smoothness
0.5474000498	journey
0.5474000498	entailments
0.5474000498	lifestyle
0.5474000498	icons
0.5474000498	regularisation
0.5474000498	transformational
0.5474000498	styled
0.5474000498	quantifiable
0.5474000498	decidability
0.5474000498	sociological
0.5474000498	prospective
0.5474000498	inspecting
0.5474000498	recurrences
0.5474000498	amplification
0.5474000498	triphone
0.5474000498	fourteen
0.5474000498	splitter
0.5474000498	discontiguous
0.5474000498	disclosures
0.5474000498	eligibility
0.5474000498	distributionally
0.5473279818	pooling
0.5472730113	neural relation
0.5472026802	schemas
0.5471685115	training corpora
0.5471663482	partitioning
0.5471663482	inventories
0.5471663482	antonyms
0.5471345631	neural extractive
0.5471295600	state action
0.5470762540	finding
0.5470518355	volume
0.5470461037	document level sentiment analysis
0.5470248472	patient conversations
0.5469865550	ternary
0.5469181656	linker
0.5469181656	conformity
0.5469181656	joke
0.5469181656	playlist
0.5469157036	framing
0.5469082572	speaker similarity
0.5469022970	potentials
0.5468963920	macro
0.5468786526	alpha
0.5468601880	precisely
0.5468398417	emojis
0.5468270913	short
0.5467919074	multimodal neural machine
0.5467870724	paraphrasing
0.5467388189	under resourced
0.5467250166	perspective
0.5467248850	achieved promising
0.5467049383	generation based
0.5466922602	realize
0.5466885809	imitate
0.5466885053	cnn model
0.5466689872	winners
0.5466643029	offs
0.5465929831	denotation
0.5465929831	boosted
0.5465929831	simulating
0.5465743943	filter
0.5465728862	computational models
0.5465409607	relation representations
0.5465123635	transcription
0.5464955840	manual
0.5464840358	spectrogram
0.5464792091	variable
0.5464375500	enhancement
0.5464063849	conquer
0.5463952849	informational
0.5463199146	instruction
0.5462384796	decision
0.5462362296	circumvent
0.5462258285	individual words
0.5461554745	acquiring
0.5461090293	rc models
0.5460811755	method outperforms
0.5460716399	classification models
0.5460671721	banks
0.5460671721	neighborhoods
0.5460574231	label
0.5460319988	gated convolutional
0.5459982437	social
0.5459900706	natural language processing systems
0.5459687915	memory mechanism
0.5459660534	independent content
0.5459518386	searches
0.5458627547	domain detection
0.5458309524	information exchange
0.5458091772	position
0.5457830227	prefix
0.5457711670	abundance
0.5457710444	based tagger
0.5457557419	cities
0.5457477552	alike
0.5457259094	regex
0.5457254117	successor
0.5456960604	polar
0.5456830429	librispeech test
0.5456450692	billions
0.5456174401	pre trained word
0.5456173828	mobile applications
0.5455813449	did
0.5455352344	clarity
0.5455127519	compromising
0.5454912764	semantic meaning
0.5453879916	normal
0.5453773907	reduplication
0.5453486805	multiple references
0.5452552729	relevant parts
0.5451451260	transformer based architectures
0.5451120330	ensembling
0.5451065030	directives
0.5450893982	news text
0.5450883871	formulaic
0.5450883871	definitional
0.5450741587	type level
0.5450139778	extended
0.5449943427	chance
0.5449765794	detector
0.5449742912	reordering
0.5449603189	development data
0.5449476170	significantly boost
0.5449360173	assistant systems
0.5448952772	biased
0.5448739326	losing
0.5448576237	suffering
0.5447697805	proportional
0.5447415644	effective representations
0.5447069053	language tasks
0.5446880954	propositions
0.5446859867	violation
0.5446297317	text independent
0.5445879716	straight through
0.5445842982	classification problem
0.5445756230	draft
0.5445637657	small size
0.5445409190	word information
0.5445346288	libraries
0.5444880795	automatic classification
0.5444630924	dialog datasets
0.5444434917	billion
0.5444032668	fold
0.5443344630	aids
0.5442067878	multiple styles
0.5441979173	humorous
0.5441329721	community
0.5441088162	sense level
0.5440901603	popularity
0.5440792846	past
0.5440451197	promotion
0.5440340446	high level features
0.5440030743	target sentence
0.5439953747	undirected
0.5439733227	lean
0.5439500335	careful analysis
0.5439282634	network's
0.5438095637	say
0.5437837585	forward and backward
0.5437791770	literal
0.5437740662	character representation
0.5437642462	last
0.5437426463	heavily rely
0.5437426463	rely heavily
0.5437340200	world's
0.5436933728	semeval 2019
0.5436916623	isomorphism
0.5436836455	database
0.5436534944	programming
0.5436435587	prediction tasks
0.5436254621	explicit supervision
0.5435914174	comparative analysis
0.5434785245	optimisation
0.5434785245	advancing
0.5434614522	document summary
0.5434599886	prediction problem
0.5434567273	microphone
0.5434547893	pertaining
0.5434542676	promising performance
0.5434515797	mitigated
0.5434215008	loss
0.5434156668	drug name
0.5434088261	semantic word
0.5433826935	program
0.5433698682	motifs
0.5433534326	level sentiment classification
0.5433443551	return
0.5433268615	interacting
0.5433028081	integration
0.5432944039	slot
0.5432931290	composition functions
0.5432767919	formant
0.5432767919	smoothed
0.5432767919	traversal
0.5432283178	author's
0.5432169155	globally
0.5431696463	unsupervised style
0.5431661790	improving neural machine translation
0.5431607928	controller
0.5431560722	autonomy
0.5431560722	multimodality
0.5431560722	stopword
0.5431560722	documenting
0.5431560722	stressed
0.5431560722	scoping
0.5431560722	certified
0.5431560722	customizable
0.5431560722	attackers
0.5431560722	enforcement
0.5431560722	nondeterministic
0.5431560722	addressee
0.5431560722	converging
0.5431560722	isotropic
0.5431560722	instructor
0.5431560722	possessive
0.5431560722	momentum
0.5431560722	visualising
0.5431560722	parsimonious
0.5431560722	decipher
0.5431560722	poet
0.5431560722	toolbox
0.5431560722	conception
0.5431253243	induced
0.5430831486	previous research
0.5430824084	faster
0.5430497253	leakage
0.5430265307	supervised tasks
0.5430008157	academia and industry
0.5429556557	deep model
0.5429073295	factoid
0.5428943288	unrestricted
0.5428812060	arrive
0.5428732256	alternating
0.5428676115	seq2seq learning
0.5428348697	noisy
0.5428186704	profiles
0.5427710409	phonological
0.5427436147	utilizing
0.5427360872	semantically
0.5427038801	significant performance improvements
0.5426940109	fight
0.5426940109	moderators
0.5426940109	introductory
0.5426940109	intentional
0.5426940109	intensional
0.5426940109	probabilistically
0.5426940109	passenger
0.5426940109	casting
0.5426940109	synergistic
0.5426940109	counseling
0.5426940109	questioning
0.5426940109	freezing
0.5426940109	misconceptions
0.5426940109	lexicographic
0.5426696749	preserved
0.5426187827	translation tasks
0.5426157230	deep learning approach
0.5425860189	hyperparameter
0.5425626720	pregroup
0.5425164882	achieved great
0.5424838095	sequential order
0.5424432900	protein
0.5424411663	class
0.5424277550	training stage
0.5423979856	attacker
0.5422550419	lexical units
0.5422403566	predictive modeling
0.5422306366	school
0.5422139754	learned embeddings
0.5421810008	based approaches
0.5421773856	generative process
0.5421361379	concise
0.5421255815	dependent
0.5421149402	reformulation
0.5420584115	data sparseness
0.5420443187	intentions
0.5419519469	graph completion
0.5419222997	big challenge
0.5419132480	previously published results
0.5419034377	triggered
0.5419009275	code mixed social
0.5418938177	relative word error
0.5418910919	recognizer
0.5418271381	lectures
0.5418271381	envelope
0.5418271381	increment
0.5418271381	letting
0.5418271381	referencing
0.5418084723	practitioners
0.5417632849	cuisines
0.5417591192	conjunctions
0.5417532717	semantic types
0.5417243157	simulated
0.5416784983	business
0.5416556950	source representations
0.5416542049	lip
0.5416462040	synthetic parallel
0.5416424173	usual
0.5416224262	output labels
0.5416174513	engines
0.5416146406	modifiers
0.5416082942	send
0.5415862092	speech data
0.5415646523	sounding
0.5415278919	intriguing
0.5415069459	tau
0.5414995145	sentiment based
0.5414698654	algorithmic
0.5414590073	labeled documents
0.5414585822	elementary
0.5414523664	inflected
0.5414291950	empirical findings
0.5414106070	impact
0.5413950086	ordinary
0.5413735220	derivation
0.5413439810	overfit
0.5412843218	retrieval systems
0.5412782814	pre trained lms
0.5412473819	partner
0.5412352200	modeling approach
0.5412275690	distinction
0.5412105314	react
0.5411961979	transforming
0.5411950166	stand alone
0.5411566004	thesis
0.5411342053	secondary
0.5411290959	recipe
0.5411016151	paper describes
0.5410919456	data driven approaches
0.5410883798	claims
0.5410789911	price
0.5410085831	involves identifying
0.5409566729	wmt 2014
0.5409288265	position paper
0.5408614516	dissemination
0.5406176911	gpt 3
0.5405633464	broadly
0.5405439847	stars
0.5404853189	deep learning architecture
0.5404204641	interpretation
0.5403852091	expressiveness
0.5403433880	multi choice reading
0.5403069746	high quality datasets
0.5402553615	sequential model
0.5402390666	fine grained classification
0.5401713842	abstractive summarization models
0.5401349627	recent deep learning
0.5401189739	counterhate
0.5401189739	skin
0.5401189739	underspecification
0.5401189739	nonparallel
0.5401189739	inductively
0.5401189739	dataflow
0.5401189739	backwards
0.5401189739	employee
0.5401189739	transitional
0.5401189739	newcomers
0.5401189739	marginalization
0.5401189739	sociocultural
0.5401189739	rhythmic
0.5401189739	acted
0.5401189739	traditions
0.5401189739	statically
0.5401189739	eighteen
0.5401189739	disability
0.5401189739	extents
0.5401189739	delimited
0.5401189739	monolingually
0.5401189739	appearances
0.5401189739	successively
0.5401189739	exceedingly
0.5401189739	parameterizations
0.5401189739	disabilities
0.5401189739	collaborations
0.5401189739	velocity
0.5401189739	summarising
0.5401189739	snapshot
0.5401189739	skipped
0.5401189739	intertextuality
0.5401189739	regulate
0.5401189739	untagged
0.5401189739	crowdsource
0.5401189739	trellis
0.5401189739	intuitionistic
0.5401189739	sixth
0.5401189739	sublanguages
0.5401189739	workbench
0.5401189739	prospect
0.5401189739	inanimate
0.5401189739	nuisance
0.5401189739	conversions
0.5401189739	commits
0.5401189739	suspicious
0.5401189739	postprocessing
0.5401189739	leaks
0.5401189739	organisations
0.5401189739	visitors
0.5401189739	tablets
0.5401189739	noting
0.5401189739	earthquakes
0.5401189739	witnessing
0.5401189739	emulating
0.5401189739	invention
0.5401189739	scholarship
0.5401189739	equivalences
0.5401189739	lobbying
0.5401189739	bottle
0.5401189739	phrased
0.5400865957	satirical
0.5400836189	prescriptions
0.5400836189	licensing
0.5400836189	disconnected
0.5400836189	hallucinations
0.5400836189	hostility
0.5400836189	spike
0.5400836189	blends
0.5400836189	coreferences
0.5400836189	keystroke
0.5400836189	fonts
0.5400836189	finish
0.5400836189	convention
0.5400836189	randomization
0.5400836189	geotagged
0.5400836189	densities
0.5400836189	inferencing
0.5400836189	regulations
0.5400836189	angry
0.5400836189	voiced
0.5400836189	port
0.5400822899	incident
0.5400822899	contracts
0.5400822899	frontier
0.5400822899	exposing
0.5400822899	emotionally
0.5400822899	differentiating
0.5400584804	wmt 2016
0.5400532139	definition
0.5400516071	clause
0.5400042365	efficient inference
0.5399787274	healthy
0.5399181722	sentiment analysis tasks
0.5399070059	conducting
0.5398997694	singleton
0.5398843843	rnn decoder
0.5398466223	priori
0.5398145393	biggest
0.5397464040	discriminator
0.5397187734	multilingual dataset
0.5396545999	generated texts
0.5396280489	sense induction
0.5396208397	cs data
0.5395938684	discrete
0.5394863439	refining
0.5394374280	component analysis
0.5394026368	design decisions
0.5393879372	indexed
0.5393870132	directions for future research
0.5393815992	evaluations
0.5393642209	english text
0.5393618922	training material
0.5393576582	personas
0.5393576582	identifier
0.5393471253	similar contexts
0.5393436325	traditional methods
0.5393379577	encoder decoder model
0.5393291824	model capacity
0.5393269224	term memory network
0.5393168360	explanations
0.5393082747	though
0.5392517376	ran
0.5391853903	cluster
0.5391781929	convincing
0.5391781929	expressivity
0.5391781929	axis
0.5391781929	revisions
0.5391781929	determiners
0.5391762126	singing
0.5391407632	text extraction
0.5391065860	interventions
0.5390763101	inspire
0.5390728448	monolingual models
0.5390086714	user questions
0.5390039265	language text
0.5390019806	recordings
0.5389737231	picture
0.5389190170	unlabeled speech
0.5389153710	attracted significant
0.5388730015	acoustic unit
0.5388703772	feasibility
0.5387168807	timing
0.5387104843	proofs
0.5386973190	instability
0.5386732753	central
0.5386695701	compiler
0.5386534715	records
0.5386469986	logographic
0.5386385387	friendly
0.5385976465	devoted
0.5385734956	grammatical structure
0.5385426463	lexical based
0.5385338987	word character
0.5385333215	unsupervised domain
0.5384956818	climate
0.5384863439	acceptability
0.5384807665	energy
0.5384785733	set size
0.5384612596	align
0.5384504775	an exploratory study
0.5383972625	dialogue datasets
0.5383552071	future
0.5383538635	pair
0.5383311790	aligned data
0.5383192874	conventional
0.5382959976	end to end differentiable
0.5382948487	range dependencies
0.5382933399	digital text
0.5382754883	correcting
0.5382649170	german english translation
0.5382071919	probes
0.5381170059	human biases
0.5381149424	detailed analysis
0.5380653512	query
0.5380520993	abstract concepts
0.5379767066	negative or neutral
0.5379757021	graph construction
0.5379751108	application scenarios
0.5379585639	resourced language
0.5379071117	games
0.5377770609	performance gap
0.5377759848	irrelevant information
0.5377580909	advice
0.5377211317	transformed
0.5376282387	zero shot cross lingual
0.5375460162	volumes
0.5375184197	human dialogues
0.5374442423	lyric
0.5374012991	identities
0.5373879637	multiclass
0.5373879637	robotics
0.5373873021	concrete
0.5373006808	simple unsupervised
0.5372999397	materials
0.5372745885	rooted
0.5372143226	attend and spell
0.5372139223	debiased
0.5372139223	atoms
0.5372139223	wins
0.5372139223	curricula
0.5372139223	told
0.5371603278	profile
0.5371419164	distributed language
0.5371351064	encoder layer
0.5371337956	deploying
0.5370719910	based approach
0.5370533505	recast
0.5369611241	mrc models
0.5369345538	unsupervised manner
0.5369340794	me
0.5368768486	eager
0.5368548976	experiments showed
0.5368351846	unify
0.5368236257	fulfillment
0.5368236257	reconstructor
0.5368236257	casing
0.5368236257	emphatic
0.5368236257	geometries
0.5368032449	inference efficiency
0.5367888117	annotated datasets
0.5367171231	diachronic corpora
0.5366747132	soccer
0.5366448107	previously studied
0.5366296783	adversaries
0.5364777771	encoder decoder network
0.5364678227	removing
0.5364600504	beat
0.5364464276	third
0.5364143339	saliency
0.5363852886	sentiment analysis of code mixed
0.5363521008	seeds
0.5363337084	microblogging
0.5363191662	wait k
0.5362992503	sparsity problem
0.5362485150	flexible
0.5362455243	think
0.5362430762	easy
0.5362298128	avenues for future
0.5362025782	tens of thousands
0.5361918846	evaluation tasks
0.5361730526	rich feature
0.5361725070	achieve comparable performance
0.5361617148	casual
0.5361250139	# blacklivesmatter
0.5360506625	ubiquitous
0.5360413205	hierarchical recurrent neural network
0.5360144192	multi task learning approach
0.5359570784	inflated
0.5359570784	plugin
0.5359542085	diagrams
0.5359462533	hop
0.5359428281	summarization task
0.5358788423	neural encoder decoder
0.5358353740	topological
0.5358232775	bound
0.5358171708	detailed comparison
0.5358019605	scope
0.5357954313	word discrimination
0.5357837475	human attention
0.5357716260	medical question
0.5357423933	interrogative
0.5357342029	active area of research
0.5356871925	coordination
0.5356618590	computer
0.5356266317	roles
0.5356145410	convert
0.5355727819	expense
0.5355682546	text classification tasks
0.5355318538	regional
0.5354955312	pairwise
0.5354784909	empirically demonstrate
0.5354752794	achieve promising results
0.5354628144	f scores
0.5354585750	shown great
0.5354574508	noisy parallel
0.5353616990	special attention
0.5353527893	trained word embeddings
0.5353364721	reference corpus
0.5353297386	intact
0.5353297386	scripted
0.5353297386	recurrently
0.5353297386	routine
0.5353297386	rotation
0.5353297386	vagueness
0.5353297386	connotation
0.5353297386	deletions
0.5353297386	unacceptable
0.5353297386	substrings
0.5353297386	horizontal
0.5353297386	voicing
0.5353297386	linearity
0.5353297386	landmarks
0.5353297386	rho
0.5353297386	sensorimotor
0.5353203940	mental
0.5352946259	room to room
0.5352810109	dependencies
0.5352396328	multiple times
0.5352200259	background information
0.5351712683	continuous word representations
0.5351561211	off policy
0.5351373113	nonverbal
0.5351165612	order
0.5350290062	model's ability
0.5349656144	transfer learning based
0.5349392683	additional training
0.5349392212	phonetically
0.5349392212	preparing
0.5349392212	confounding
0.5349392212	contradictory
0.5348728029	text description
0.5348396291	s
0.5348256062	scripts
0.5347845810	denoised
0.5347845810	propagandistic
0.5347845810	misuse
0.5347845810	ironic
0.5347845810	authenticity
0.5347845810	emitting
0.5347845810	forecasts
0.5347845810	lasting
0.5347845810	optimising
0.5347845810	misses
0.5347845810	phonologically
0.5347845810	representatives
0.5347845810	substring
0.5347845810	strokes
0.5347845810	unresolved
0.5347845810	fulfilling
0.5347845810	polymorphic
0.5347845810	parsable
0.5347845810	checkpoint
0.5347845810	perceptually
0.5347845810	neck
0.5347845810	untapped
0.5347845810	sided
0.5347845810	condensed
0.5347845810	generalisability
0.5347845810	articulated
0.5347845810	favorite
0.5347845810	discoveries
0.5347845810	leaning
0.5347845810	rural
0.5347845810	marginalizing
0.5347845810	desires
0.5347845810	hyponyms
0.5347845810	planned
0.5347845810	orientations
0.5347845810	biologically
0.5347845810	caveats
0.5347845810	routes
0.5347845810	departure
0.5347845810	axiom
0.5347845810	homogeneity
0.5347798344	id
0.5347650447	improving performance
0.5347632777	significant advantages
0.5347275177	phrase based statistical
0.5346620109	interpretive
0.5346574744	assist
0.5346517271	essential information
0.5346509120	opinion mining and sentiment analysis
0.5346224048	helping
0.5346016733	recruiters
0.5346016733	computerized
0.5346016733	reinforcing
0.5346016733	rationality
0.5346016733	budgets
0.5346016733	synchronized
0.5346016733	teacher's
0.5346016733	agreements
0.5346016733	delexicalized
0.5346016733	conversing
0.5346016733	tutor
0.5346016733	ethnicity
0.5346016733	illustration
0.5346016733	useless
0.5346016733	neologisms
0.5346016733	fluctuation
0.5346016733	termhood
0.5346016733	lexemes
0.5345466786	meta analysis
0.5345395184	monitoring
0.5345226342	whispered
0.5345028723	belong
0.5344777319	word
0.5344773484	idioms
0.5344076419	edu
0.5343909676	strongest
0.5343909676	combat
0.5343788369	interface
0.5343472170	outlets
0.5343403587	version
0.5343244471	conversational datasets
0.5343033659	mis
0.5342980215	syntactic relations
0.5342958539	bitext
0.5342654007	topic information
0.5342429523	constituting
0.5342383532	sound
0.5341489292	win
0.5341123278	retrieval methods
0.5340859541	matrix
0.5340567405	data points
0.5340175354	question types
0.5339966324	selected sentences
0.5339474054	silence
0.5339474054	informativity
0.5339193109	rumors
0.5338900178	comprehensive experiments
0.5338838385	experiments demonstrate
0.5338626616	paragraphs
0.5338252504	data model
0.5338141871	bootstrap
0.5338085003	concatenation
0.5338042753	english words
0.5337871176	thresholding
0.5337871176	referenced
0.5337723818	dominance
0.5337674394	gram lms
0.5337576163	dialogue contexts
0.5337549290	lingual transfer
0.5337467881	real time
0.5337339093	motivated
0.5337005979	overall
0.5336902651	english speech
0.5336561848	linguistic input
0.5336523708	visual analysis
0.5335870460	simplest
0.5335323295	latest
0.5335037144	parallelism
0.5334839701	tails
0.5334623562	current methods
0.5334079974	societal
0.5333273980	joint entity and relation extraction
0.5333129328	working
0.5333070304	automatic translation
0.5332441000	replacing
0.5332383768	revealing
0.5332289592	target
0.5332165922	duration
0.5331661755	prepare
0.5330767741	significant differences
0.5330743653	highlighting
0.5330689389	multiple aspects
0.5330624234	pruned
0.5330624234	synonymy
0.5330261411	matching score
0.5330047658	ive bayes
0.5329645526	approach outperforms
0.5329330224	resource
0.5329193260	skills
0.5328637954	projected
0.5328072696	retrieve
0.5327277970	resultant
0.5327192125	language transfer
0.5327077950	recruitment
0.5326945458	writing
0.5326677792	real word
0.5326653568	open source python
0.5326265808	conversational telephone
0.5326145534	dropout
0.5325989647	recognition accuracy
0.5325986167	vocabulary
0.5325367561	scientists
0.5324323400	long
0.5324078301	syntactic representations
0.5323721455	formants
0.5323640406	treatment
0.5323334148	maintenance
0.5323118079	connective
0.5323118079	omission
0.5322973166	public
0.5322877521	creation
0.5322800763	product or service
0.5322644434	internal representation
0.5322091011	reasoning tasks
0.5321897236	relevant sentences
0.5321540823	achieve remarkable
0.5320975663	listeners
0.5320757655	considerable attention
0.5320617986	challenges
0.5319440891	extrinsic tasks
0.5318987123	attentional neural
0.5318839900	language modeling tasks
0.5318604183	human conversations
0.5318566314	sourcing
0.5318286921	click
0.5318162805	graph network
0.5317725500	imposed
0.5317216627	assuming
0.5317066061	unary
0.5317066061	irregularity
0.5317066061	brands
0.5317066061	cooccurrence
0.5317066061	pitfalls
0.5317066061	syllabic
0.5316723391	deletion
0.5316674926	asking
0.5316536709	surprise
0.5316496872	statement
0.5316195861	target sentences
0.5315948368	author
0.5315928012	syntactically and semantically
0.5315853918	handling
0.5315689715	increasingly popular
0.5315689090	de
0.5315298985	country
0.5315100665	student models
0.5315067687	topic modeling approaches
0.5314735705	prevalent
0.5314720735	pretrained bert
0.5314418551	combining multiple
0.5314373253	shared parameters
0.5314279777	dynamics
0.5314208147	significantly outperforming
0.5313860432	amenable
0.5313074214	mention
0.5312982816	entity coreference
0.5312845980	feature level
0.5312688353	deep learning model
0.5312501355	vis
0.5312289810	memories
0.5311924898	hyponymy
0.5311862993	word embedding vectors
0.5311769491	standardized
0.5311690091	multiple datasets
0.5311609105	level features
0.5311314091	waste
0.5311063388	received significant
0.5311012933	model utilizes
0.5310830684	lost
0.5310829409	predictors
0.5310520956	associate
0.5310248002	arxiv
0.5309732405	backend
0.5309732405	triggering
0.5309732405	multilabel
0.5309732405	serial
0.5309732405	employment
0.5309732405	attaining
0.5309732405	multilinguality
0.5309732405	artifact
0.5309540679	future researchers
0.5309539476	module learns
0.5309299277	instructions
0.5308536792	path
0.5308085058	traits
0.5308073136	reference translation
0.5307869012	pressing
0.5307869012	asymptotic
0.5307649364	unsupervised methods
0.5307556943	natural language processing tasks
0.5306794850	reviews
0.5306593880	generation quality
0.5305584907	alternate
0.5305229396	unlike existing
0.5305099678	extension
0.5304938948	acoustic model training
0.5304900436	reason
0.5304480470	psycholinguistic
0.5304004079	symbolic
0.5303442857	interacts
0.5303278928	sampling algorithm
0.5302365343	style
0.5301994313	visual textual
0.5301935508	proliferation
0.5301812412	vqa datasets
0.5301635446	comparable data
0.5301536994	unsupervised cross lingual
0.5301446741	few shot learning
0.5300756403	negative effects
0.5300484978	counselors
0.5300484978	opioid
0.5300484978	spontaneity
0.5300484978	prerequisites
0.5300347042	stylistic features
0.5299305306	demonic
0.5299002425	tables
0.5298535719	augmentation approach
0.5298450878	crowdworkers
0.5298450878	predictable
0.5298450878	collocations
0.5298160818	speech based
0.5298100569	preprocessing
0.5298064481	hybrid architecture
0.5297676246	adaption
0.5297676246	stacks
0.5297676246	configurable
0.5297676246	anchors
0.5297676246	bootstrapped
0.5297676246	fixing
0.5297676246	derivative
0.5297676246	inverted
0.5296932032	context embeddings
0.5296924898	signatures
0.5296924898	naturalistic
0.5296924898	acronyms
0.5296924898	happened
0.5296773209	converse
0.5296689346	text segment
0.5296456665	word based
0.5296407313	baseline methods
0.5296314949	tokenization
0.5296313982	automatically extract
0.5296188933	health related information
0.5295938671	proposals
0.5295280092	end to end
0.5294986373	interaction extraction
0.5294890880	attentions
0.5294672566	perceptual
0.5294671829	synthetic
0.5294091812	persuasive
0.5293822330	relation extraction models
0.5293277249	defining
0.5293239357	hallucination
0.5293213968	biomedical question
0.5293017139	inner
0.5292779325	single words
0.5292514552	proposed approach
0.5291968058	feedforward
0.5291423191	instead
0.5291298502	unable
0.5291135472	acquires
0.5290735827	feeds
0.5290290615	co occurrence matrix
0.5290229999	respond
0.5289581993	difference
0.5289253569	edition
0.5288859502	sentence
0.5288466814	human
0.5288192321	trusted
0.5287894133	training corpus
0.5287734953	factual information
0.5287067914	para
0.5287012301	b
0.5286580519	generated questions
0.5286412370	assessments
0.5285631533	based applications
0.5285394557	demand
0.5285337001	argument extraction
0.5285141069	sentential
0.5285007129	raters
0.5284927306	advanced
0.5284927194	inference procedure
0.5284830136	view
0.5284800658	unlike conventional
0.5284590198	instantiated
0.5284317582	traditional approaches
0.5283820767	pre trained bert
0.5283636553	run
0.5283413992	string
0.5283298814	physical
0.5282883064	naturalness
0.5282866371	domain expert
0.5282845939	stand
0.5282189378	do
0.5281779377	semeval 2016 task
0.5280948725	ranking approach
0.5280761409	unlabeled text
0.5280760770	lstm architecture
0.5280313897	effects
0.5280203794	attends
0.5279893829	dictionaries
0.5279842696	interplay
0.5278828180	processing
0.5278820205	co reference resolution
0.5278752650	going
0.5278625957	second
0.5277938671	edits
0.5277726761	teachers
0.5277674883	iteration
0.5277636603	theoretical analysis
0.5277511136	stopwords
0.5276797948	isomorphic
0.5276797948	entangled
0.5276179377	implementing
0.5276131610	real sentences
0.5276000085	opposed
0.5275820403	lingual
0.5275750752	text alignment
0.5275715883	generative network
0.5275177978	untranscribed
0.5275027615	holes
0.5274423205	emotions
0.5274315849	situation
0.5274193914	token
0.5273933335	insightful
0.5273933335	sensory
0.5273933335	discrepancies
0.5273933335	productive
0.5273818006	restaurants
0.5273785900	advancements
0.5273764946	money
0.5273659627	parallelizable
0.5273659627	chatting
0.5273659627	sorting
0.5273601419	approach achieves
0.5273444533	recent advancements
0.5273078169	input texts
0.5272554704	varying degrees
0.5272530918	image content
0.5272340597	assistants
0.5272321148	absolute accuracy
0.5271292642	sizable
0.5271279177	movies
0.5271193948	compromise
0.5271193948	dictation
0.5271193948	cohesive
0.5271193948	tradeoffs
0.5271193948	labelers
0.5271193948	interdependence
0.5271193948	textbooks
0.5270684005	truth
0.5270668342	specifications
0.5270137347	learning bilingual
0.5269885961	language interface
0.5269826033	women
0.5269461971	worth
0.5268993303	gazetteers
0.5268931678	results obtained
0.5268755638	dramatic
0.5268555866	final output
0.5268529742	end to end speech recognition
0.5268446422	biographies
0.5268446422	extremist
0.5268366324	deviation
0.5268135417	initial
0.5267759016	neural network based models
0.5267130733	related words
0.5266944897	unsupervised translation
0.5266748294	abstracts
0.5266563319	baseline model
0.5266198125	strength
0.5265882151	semantic meanings
0.5265850427	precision
0.5265657146	recent success
0.5265395705	development and test sets
0.5264532977	i
0.5263833072	hoc
0.5263701328	tagset
0.5263576644	description
0.5262759988	basic idea
0.5262757736	practically
0.5262263195	crafted features
0.5262037174	key
0.5261420203	visual language
0.5261304894	randomized
0.5261218676	semantic clustering
0.5261163652	e2e model
0.5261049944	considerations
0.5261001195	method achieves
0.5260514390	replace
0.5260411216	layer lstm
0.5260212831	chunk
0.5259961350	dimension
0.5259080307	category information
0.5258866658	owing
0.5258831346	semi supervised approach
0.5258817563	although
0.5258738263	diverse questions
0.5258517543	distractor
0.5258222515	neighbors
0.5258119209	improves performance
0.5258079751	labor
0.5258067912	bridge
0.5257935720	multi sense word
0.5257905499	slu model
0.5257587229	text structure
0.5257538185	gain
0.5257479890	reasoning model
0.5257245617	phase b
0.5256894266	traditional systems
0.5256838561	extensive experimental results
0.5256832015	sheds
0.5256827578	achieve competitive performance
0.5256820757	variation
0.5256497946	correct word
0.5256245932	labeled datasets
0.5255896419	robots
0.5255762947	human readers
0.5255689655	hierarchical structures
0.5255645299	here
0.5255455131	provide empirical evidence
0.5255071971	amr to text generation
0.5254729375	data to text generation
0.5254411373	sparseness
0.5253951597	product features
0.5253935390	extensive experimental evaluation
0.5253695058	cell
0.5253549198	computation
0.5253370691	rumours
0.5253357630	inferential
0.5253317605	language model pre training
0.5253304728	gaps
0.5252128747	oversampling
0.5252128747	electoral
0.5252128747	une
0.5252128747	psychiatric
0.5252128747	playlists
0.5252128747	paralinguistic
0.5252128747	semiring
0.5252096613	periods
0.5251517330	portions
0.5251429949	future development
0.5251366425	paths
0.5251326797	news content
0.5251252885	chinese dataset
0.5250218942	model predictions
0.5250000655	word problem
0.5249221428	look
0.5248960723	pre
0.5248497322	1st
0.5248152784	expert
0.5248096670	divergent
0.5247999480	connection
0.5247038316	finetuning
0.5246812245	language
0.5246640770	3rd
0.5246336057	character level nmt
0.5246016715	distillation method
0.5245948367	decoding methods
0.5245793315	semeval 2020 task 11
0.5245377845	text inputs
0.5245150233	systematic evaluation
0.5244363898	disambiguating
0.5244065204	parents
0.5244065204	exogenous
0.5243416022	task specific knowledge
0.5243080226	set programming
0.5242012698	snli dataset
0.5241612599	fulfill
0.5241336975	ethical
0.5241240861	complaint
0.5241164092	encoders
0.5241080585	initialization
0.5240951915	rigorous
0.5240598302	comprised
0.5240550077	distractors
0.5240550077	eliminating
0.5240533139	arguably
0.5240341145	uncertain
0.5240330184	target speech
0.5240013537	lot
0.5239620472	diseases
0.5239276930	representation
0.5238866658	fraction
0.5238534881	specific word
0.5238332892	ask
0.5237654265	linguistics
0.5237140500	aimed
0.5236319349	implication
0.5236284453	token by token
0.5236014597	paper presents
0.5235851068	once
0.5235400369	authored
0.5235215949	standard metrics
0.5235043539	labeled dataset
0.5234748702	independent features
0.5234712486	domain specific knowledge
0.5234704350	\ cite
0.5234455656	trends
0.5234391705	language features
0.5234376346	based machine translation
0.5234143615	network architectures
0.5232946954	vocabularies
0.5232829776	actionable
0.5232591261	controversial
0.5231993693	additional information
0.5231949441	bracketed
0.5231908985	yor \ ` ub \
0.5231875442	data driven methods
0.5231730413	embedding representations
0.5231199230	embedded
0.5230755632	discussing
0.5229635941	fire
0.5229564270	experimental analysis
0.5229419816	inside
0.5229158389	mfcc features
0.5229044635	deal
0.5228978076	appealing
0.5228760688	developing
0.5228341896	meta learning approach
0.5228282285	feed forward network
0.5227801692	meetings
0.5227745703	interviews
0.5227540906	long history
0.5227504407	exclusively
0.5227317470	unsegmented
0.5227317470	vague
0.5227317470	confusing
0.5227317470	cheap
0.5227147440	execution
0.5226881943	critic
0.5226821158	prior studies
0.5226795108	accompanied
0.5226240523	annotation process
0.5226150547	compensate
0.5225951877	defenses
0.5225951877	commit
0.5225951877	radiological
0.5225951877	illegal
0.5225951877	universe
0.5225951877	centralized
0.5225806447	centered
0.5225633451	memory requirements
0.5225507705	assistance
0.5225109549	inter
0.5225013385	automaton
0.5224829617	fact
0.5224757054	rich semantic
0.5224420416	topic modeling based
0.5224387587	succeed
0.5224312084	thresholds
0.5224312084	exclusive
0.5224312084	intricate
0.5224199748	training sentences
0.5223702034	glyphs
0.5223700139	model theoretic
0.5223544919	productivity
0.5223384752	improves translation quality
0.5223317321	unconditional
0.5223317321	memorization
0.5223317321	ties
0.5223067526	compile
0.5222957783	encouraged
0.5222264329	shape
0.5222173030	topically
0.5221782041	attention based neural network
0.5221542866	\ bf
0.5221299339	community based
0.5221174814	dialogue generation model
0.5221047735	specific domain
0.5220155901	complex queries
0.5219576756	fine grained control
0.5219560128	volatility
0.5219560128	managers
0.5219560128	adult
0.5219560128	contours
0.5219560128	capitalization
0.5219560128	assertion
0.5219444575	dataset comprising
0.5219358086	decoding process
0.5219237088	reality
0.5219228373	base noun
0.5219081057	assigning
0.5218275173	whilst
0.5218273928	complementary information
0.5217613566	am
0.5216115881	citations
0.5215958087	unsupervised acoustic
0.5215906795	held out test
0.5214878894	polarity
0.5214865700	proof of concept
0.5214563420	formula
0.5214535573	topic clustering
0.5214007456	temporally
0.5213703776	i.i.d
0.5213193153	recurrent model
0.5213060743	linearly
0.5212951820	grasp
0.5212881943	appearing
0.5212694233	render
0.5212352251	log linear model
0.5211985356	polysemy
0.5211396912	origins
0.5211328138	importance
0.5210758276	cumulative
0.5210758276	shortcut
0.5210758276	pedagogical
0.5210758276	coherency
0.5210758276	transactions
0.5210322634	14 english german
0.5209962534	taggers
0.5209630210	ever growing
0.5209466189	partially
0.5209243939	position encoding
0.5209151367	aid
0.5208795264	neural network model
0.5208760865	proposed technique
0.5208518857	inversion
0.5208465452	point
0.5208166233	monolingual and cross lingual
0.5208148069	ordering
0.5207730979	tractable
0.5207669866	trouble
0.5207669866	admission
0.5207669866	contextualize
0.5207669866	insert
0.5207669866	parallels
0.5207669866	trustworthiness
0.5207669866	behavioural
0.5207669866	incongruity
0.5207669866	rephrasing
0.5207669866	devising
0.5207669866	familiarity
0.5207669866	tendencies
0.5207669866	biomedicine
0.5207669866	disagreements
0.5207512770	based metrics
0.5207467483	conceptually
0.5207439312	reasoning process
0.5207215865	interpretable representations
0.5207037956	hence
0.5206798479	recommender
0.5206482211	highly variable
0.5206463648	word errors
0.5205998051	language inference tasks
0.5205633936	previous results
0.5205047580	unstructured
0.5204689888	none
0.5204566215	individual models
0.5204007456	supplementary
0.5203679617	stands
0.5203591105	refer
0.5203391159	hierarchical graph
0.5203329161	retrieval results
0.5202977822	challenges involved
0.5202857855	2nd
0.5202759824	phoneme conversion
0.5202365054	medications
0.5202365054	psycholinguistics
0.5202310751	human interpretable
0.5202173956	simple yet effective
0.5202146492	perceptions
0.5202092388	conditional sequence
0.5201979863	coherence
0.5201228351	based transfer
0.5201221950	objective
0.5201063371	output probabilities
0.5200623024	learning word
0.5200462014	manifolds
0.5200359755	language interfaces
0.5199967294	quick
0.5199662329	prior approaches
0.5199598588	enabling
0.5199473901	\ footnote
0.5199342059	adequacy
0.5199281684	world's languages
0.5198592075	effectively leverage
0.5198107529	incomplete data
0.5197803101	bandwidth
0.5197374556	classification methods
0.5197143503	drive
0.5196466912	graphemes
0.5195899812	evolved
0.5195845929	inexpensive
0.5195522062	commerce
0.5195493854	first
0.5194916130	symptoms
0.5194872829	data regime
0.5194647695	interactive visual
0.5194347713	experiments conducted
0.5194248774	marks
0.5193886238	achieve higher
0.5193640752	real world tasks
0.5193292932	topic level
0.5192934262	reranking
0.5192776710	recurrence
0.5192720677	annotated dataset
0.5192650707	sentence level classification
0.5192634487	tuning
0.5192298241	backbone
0.5191625172	decomposed
0.5190873182	questionnaire
0.5190554153	wsj corpus
0.5190545570	attack models
0.5190374594	intent prediction
0.5190182912	shortcoming
0.5189721132	text sources
0.5189507456	exercise
0.5188976962	painting
0.5188976962	alternately
0.5188976962	bug
0.5188976962	decentralized
0.5188976962	alternations
0.5188976962	unlimited
0.5188976962	subtitling
0.5188976962	thing
0.5188976962	integrity
0.5188976962	focal
0.5188976962	backed
0.5188976962	semiotic
0.5188976962	unusual
0.5188976962	assemble
0.5188976962	governing
0.5188976962	antonym
0.5188963831	diversity
0.5188097961	agreement
0.5188074308	shared
0.5187802984	graph networks
0.5187646333	granularities
0.5186812327	vector models
0.5186783732	encoding
0.5186433749	adversary
0.5186191617	decoding step
0.5185080182	culture
0.5184202511	cons
0.5184092939	empirically evaluate
0.5183900398	consistent
0.5183813561	exploitation
0.5183154257	risk
0.5183105976	an empirical study
0.5183026218	engaging
0.5182353466	counting
0.5182324714	regular
0.5182156767	peers
0.5182156068	doubt
0.5181804197	recurrent neural network models
0.5181783715	summarization datasets
0.5181433957	emotion categories
0.5181093005	3d scene
0.5180582141	attention modules
0.5180300278	score
0.5180040293	tests
0.5180023661	advantages and disadvantages
0.5179842738	asr model
0.5179407341	satisfaction
0.5178952219	dataset size
0.5178872033	generators
0.5178455464	extract answers
0.5178134437	spoken question
0.5178078366	potential applications
0.5177635980	self contained
0.5177205701	sequence to sequence
0.5177137462	argumentative
0.5177084789	decoder structure
0.5177034165	retrieval model
0.5176964680	response pairs
0.5176822118	yet
0.5176701609	source side
0.5175878685	creating
0.5175876867	nouns
0.5174884799	deep semantic
0.5174785901	traditional models
0.5174212319	change
0.5173947808	high inter
0.5173803753	specifically designed
0.5173472237	based acoustic
0.5173355905	adaptation method
0.5173326778	based methods
0.5173271718	tts systems
0.5173259445	hierarchical encoder
0.5172446095	human annotator
0.5172297962	focused multi
0.5171823885	challenging problem
0.5171667818	image representation
0.5171425871	automatic identification
0.5170826678	synonyms
0.5170442920	automated analysis
0.5170228956	reports
0.5169876328	automatically generate
0.5169310819	quantifiers
0.5169302140	autoencoder based
0.5169203379	ai based
0.5169114481	embedding matrix
0.5168595536	phones
0.5168445422	framed
0.5168430561	simulated data
0.5168326472	english news
0.5168136808	characteristic
0.5167197712	standard benchmarks
0.5166862442	unknown
0.5166534825	conduct experiments
0.5166153109	real world data
0.5166045500	online media
0.5165797961	user goals
0.5165560721	audio based
0.5165139733	categorization task
0.5164271949	relation classifier
0.5164008196	role
0.5163150730	context representations
0.5163082055	gram language model
0.5163055805	key information
0.5162968891	accuracy loss
0.5162611188	accuracy score
0.5162547518	single
0.5162544364	answering complex
0.5162485963	analytical
0.5162458995	$ \ theta
0.5161438724	sentiment word
0.5161394310	clinicians
0.5161394310	validating
0.5161394310	unambiguous
0.5161344035	market
0.5161209815	expert knowledge
0.5161073301	production
0.5160548551	evaluation method
0.5160018399	votes
0.5159549745	hierarchy
0.5159441146	automatically identify
0.5159215546	claim
0.5158741564	language expression
0.5158741453	tradeoff
0.5158323887	aware network
0.5158178813	arabic words
0.5158135573	domain
0.5157803716	artificially
0.5157508018	year period
0.5157495689	network based approach
0.5156963389	revision
0.5156918130	granularity
0.5156546055	accurately capture
0.5156113828	urgent
0.5156100354	chinese sentiment
0.5155817623	ranking problem
0.5155399885	bags of words
0.5153916117	language model based
0.5153840628	test
0.5153794093	external data
0.5153272282	degrees
0.5153056585	deep transformer
0.5153038412	fluctuations
0.5152954229	hashtag
0.5152770259	grounded dialogue
0.5152751261	jokes
0.5152751261	termination
0.5152751261	alternation
0.5152641500	english translation
0.5152485596	computational language
0.5152350891	superior results
0.5151506051	output sequences
0.5151028168	usability
0.5150820340	error
0.5150813092	\ textsc
0.5150667271	disciplinary
0.5150667271	physiological
0.5150408938	prior
0.5149944096	connected
0.5149898179	sub task b
0.5149440675	ablation
0.5149163490	semeval 2020 task 9
0.5149110970	discussions
0.5148992097	endings
0.5148953780	automatically generates
0.5148866832	times larger
0.5148530076	divergence
0.5148363035	generalise
0.5148109170	range
0.5147780495	patient's
0.5147721644	data extraction
0.5147620446	morphosyntactic
0.5147333477	document sets
0.5147240424	dax
0.5147166554	relief
0.5147166554	confusions
0.5147129266	persons
0.5146844497	experts
0.5146403442	noising
0.5146087587	blogs
0.5145798799	explicit
0.5145746898	expressive enough
0.5145721195	shift
0.5145338690	experiment results
0.5145303662	facial
0.5144444383	lda model
0.5144417834	attributions
0.5144417834	abstracting
0.5144417834	primal
0.5144340929	week
0.5144086360	push
0.5143678413	de identification
0.5143425314	row
0.5143240651	stories
0.5142974590	continuous vector representations
0.5142190221	unsupervised training
0.5141769292	adversarial attacks against
0.5141629971	favorably
0.5141523758	model outperformed
0.5141422319	integrated approach
0.5141410537	notable
0.5141203139	resource rich language
0.5141174471	deep lstm
0.5141032861	easily extended
0.5140758647	taxonomic
0.5140539724	sentiment words
0.5140487080	effectively captures
0.5140429239	machine translation quality
0.5139991514	input sequences
0.5139382497	population
0.5139141421	aware representations
0.5138803238	trade off
0.5138257803	verifying
0.5137849851	binomials
0.5137849851	dosage
0.5137849851	performant
0.5137849851	strategically
0.5137849851	likeliness
0.5137849851	attach
0.5137849851	pregnancy
0.5137849851	task1
0.5137849851	ensembled
0.5137849851	understandability
0.5137849851	monolithic
0.5137849851	batching
0.5137849851	idiosyncrasies
0.5137849851	multiply
0.5137849851	attachments
0.5137849851	untranslated
0.5137849851	objectively
0.5137849851	graphic
0.5137849851	career
0.5137849851	outlining
0.5137849851	blocked
0.5137849851	dreams
0.5137849851	unimportant
0.5137849851	malware
0.5137849851	novices
0.5137849851	deliberate
0.5137849851	joining
0.5137849851	mistaken
0.5137849851	defend
0.5137849851	narrowing
0.5137849851	impressions
0.5137849851	follower
0.5137849851	positivity
0.5137849851	affiliation
0.5137849851	positioning
0.5137849851	verbatim
0.5137849851	resampling
0.5137849851	confidences
0.5137849851	theorems
0.5137849851	headword
0.5137849851	anticipated
0.5137849851	satisfiability
0.5137849851	afford
0.5137849851	undeciphered
0.5137849851	niche
0.5137782724	spoken languages
0.5136851015	major challenges
0.5136546436	relevant responses
0.5136520676	mixtures
0.5135484984	target side
0.5135244098	parties
0.5135121047	weakness
0.5134944262	scientific community
0.5134484573	symbol
0.5134414534	request
0.5133949138	existing solutions
0.5133846874	discriminators
0.5133846874	screening
0.5133732525	hypernyms
0.5133550578	rich languages
0.5133087602	development
0.5132639062	data sizes
0.5132517505	checking
0.5132338195	processing tasks
0.5131795157	provide evidence
0.5130812831	promising solution
0.5130338510	compelling
0.5130338510	distinguished
0.5129792909	ongoing research
0.5127930019	definite
0.5127890081	minimally
0.5127776023	public discourse
0.5127442038	ever increasing
0.5127354606	leveraging external
0.5126652713	alignment information
0.5126639772	unequal
0.5126254876	integral
0.5126090768	context words
0.5125894110	extensive set of experiments
0.5124852333	hypothesis
0.5124549056	small
0.5124334771	effective
0.5124172554	unlabeled corpus
0.5124064513	alongside
0.5123522645	phenotypes
0.5123522645	timestep
0.5123522645	microphones
0.5123522645	emission
0.5123522645	lexeme
0.5123522645	radically
0.5123522645	subnetworks
0.5123522645	simulators
0.5123522645	expansions
0.5123522645	underrepresented
0.5123522645	ecosystem
0.5123522645	propagating
0.5123522645	recency
0.5123522645	progression
0.5123499846	nlp based
0.5123336930	clustering method
0.5122789529	moments
0.5122588132	current research
0.5122451758	syntactic parameters
0.5122243871	aggregating
0.5122197040	pictures
0.5121714072	chinese to english
0.5121694128	databases
0.5121402610	interfaces
0.5120954440	commercial
0.5120823113	substance use
0.5120478765	information captured
0.5119729797	minutes
0.5119662124	german speech
0.5118687379	reflected
0.5118649380	clustering results
0.5118617834	tuples
0.5117988187	contemporary
0.5117867116	technique called
0.5117565731	anger
0.5117530533	familiar
0.5116659333	linguistic characteristics
0.5116016812	demonstrations
0.5116016812	numeric
0.5115810726	rich
0.5115756232	end goal
0.5115682099	single language
0.5115186553	chinese translation
0.5115102876	successes
0.5115095530	trials
0.5114713947	unsupervised learning algorithm
0.5113614342	biomedical named
0.5113501307	feature spaces
0.5112481769	unsupervised approaches
0.5112034658	opportunities
0.5111906809	models perform
0.5111523785	disorder
0.5111440453	cope
0.5111352627	comparable accuracy
0.5111334816	comparison
0.5111118181	letters
0.5111005071	spaces
0.5110865539	antecedent
0.5110856349	put forward
0.5110609200	generating summaries
0.5110161460	lambda
0.5109612531	context encoder
0.5109148628	large scale language
0.5108986347	related information
0.5108755275	total
0.5108734470	hierarchies
0.5108714214	originality
0.5108714214	sur
0.5108714214	survive
0.5108393701	disinformation
0.5108357916	bias
0.5108123482	typical
0.5107780647	local feature
0.5107727852	divide
0.5107225214	clarify
0.5106281626	computational methods
0.5106264166	memory neural networks
0.5106228528	generic
0.5106032035	obstacle
0.5106016812	manuscripts
0.5105957215	top ranked
0.5105598280	multimodal dataset
0.5105379923	parsing accuracy
0.5105319837	language processing research
0.5105156504	worldwide
0.5104896603	guide
0.5104888869	svm based
0.5104276729	pilot
0.5103879390	experiments reveal
0.5103863106	acoustic input
0.5103786938	embedding based models
0.5103756174	model named
0.5103071473	vulnerabilities
0.5103014513	reference texts
0.5102416444	mappings
0.5102075539	calendar
0.5102016976	careful
0.5101976489	imaging
0.5101429598	distinguishes
0.5101429598	segmenter
0.5101384125	bases
0.5101140223	direct
0.5101037719	disfluencies
0.5101037719	imperfect
0.5101037719	successive
0.5100549591	annotated sentences
0.5100497868	training dataset
0.5100178229	results demonstrate
0.5099791319	hard negative
0.5099730933	network based
0.5099627973	this
0.5099564110	output spaces
0.5099330575	effectively capture
0.5098938809	automatic detection
0.5098813367	character level representations
0.5098368777	learning vector representations
0.5098217768	hop questions
0.5098151127	retain
0.5098068135	provide insights
0.5097864388	informative
0.5097711206	standard
0.5097439612	achieves comparable results
0.5097358625	target data
0.5097307276	opinionated
0.5097307276	technological
0.5097239892	downstream task
0.5096934415	positive impact
0.5096579293	entity discovery
0.5096566106	cognate
0.5096371066	nlp researchers
0.5096204961	subgraph
0.5096179482	parse
0.5096104844	literature
0.5095989139	complex network
0.5095931951	chinese english translation tasks
0.5095883547	normative
0.5095883547	raising
0.5095436175	previous methods
0.5095428811	classification method
0.5094830088	explosion
0.5094720621	fine grained analysis
0.5094629095	therefore
0.5094522158	gender
0.5094302776	moment
0.5094264998	act classification
0.5093989391	newswire
0.5093816345	median
0.5093600078	models generate
0.5093340630	sentiments
0.5093205845	introspection
0.5093205845	strengthening
0.5093205845	inscriptions
0.5093191397	phoneme
0.5092738583	classification layer
0.5092432132	heading
0.5092432132	advertisers
0.5092432132	disjunctions
0.5092432132	collapsed
0.5092432132	disputed
0.5092421017	synthesized
0.5092247949	signer
0.5092247949	selectivity
0.5091933537	cepstral
0.5090894697	skilled
0.5090894697	warrants
0.5090894697	tactics
0.5090894697	caching
0.5090894697	laptop
0.5090894697	username
0.5090894697	rivals
0.5090894697	viseme
0.5090894697	compensation
0.5090894697	accomplishing
0.5090894697	alias
0.5090894697	permutations
0.5090894697	functioning
0.5090894697	aiding
0.5090894697	automatique
0.5090894697	abnormalities
0.5090894697	exclude
0.5090894697	prominently
0.5090894697	paving
0.5090894697	authoritative
0.5090894697	manuals
0.5090894697	remarks
0.5090894697	internally
0.5090894697	trivia
0.5090894697	mouth
0.5090894697	producers
0.5090894697	composes
0.5090894697	carriers
0.5090894697	novice
0.5090894697	accidents
0.5090770960	counts
0.5090028452	control
0.5089907870	detection task
0.5089826398	returns
0.5089234340	existing resources
0.5089179414	based techniques
0.5087881827	related features
0.5087621948	self
0.5087550547	covered
0.5087532251	method yields
0.5087532044	modifier
0.5087310191	store
0.5087016976	narrow
0.5086711201	expose
0.5086263371	semeval 2020 task 12
0.5086016893	simpler models
0.5085783946	chemicals
0.5085783946	conditionally
0.5085783946	charges
0.5085783946	lighter
0.5085783946	personalities
0.5085783946	distortions
0.5085783946	arguing
0.5085783946	interleaving
0.5085783946	sixteen
0.5085783946	traversing
0.5085783946	optimizer
0.5085783946	repeat
0.5085783946	diversify
0.5085783946	disgust
0.5085783946	isizulu
0.5085783946	feedbacks
0.5085783946	creators
0.5085783946	tough
0.5085783946	clouds
0.5085783946	extrinsically
0.5085783946	student's
0.5085783946	scaffolding
0.5085783946	adverbial
0.5085783946	animacy
0.5085783946	imagined
0.5085783946	improper
0.5085783946	entering
0.5085783946	consonants
0.5085783946	continuations
0.5085783946	discipline
0.5085597726	teams participated
0.5085283430	training scheme
0.5085191773	monolingual
0.5084826379	english translation task
0.5084825124	conventional approaches
0.5084416753	phrase attachment
0.5084381685	room for improvement
0.5084270878	multi source neural
0.5084211201	lookup
0.5084107960	sampling strategy
0.5083899957	modest
0.5083520910	senses
0.5083171594	intensive
0.5082623791	low dimensional representations
0.5082506332	rouge metrics
0.5082384368	diverse
0.5082291096	collections
0.5082230809	modification
0.5082122548	presence
0.5081938995	female
0.5081593977	semantically related words
0.5081563904	trivial
0.5081387512	existing metrics
0.5081020159	alleviating
0.5081020159	associating
0.5081020159	accessing
0.5080987353	consistency loss
0.5080520959	emoticons
0.5080493616	contextual features
0.5080228455	combination
0.5080149756	quotes
0.5079923623	missing
0.5079923544	checkers
0.5079601946	chinese natural language
0.5079025546	results revealed
0.5078955528	special
0.5078771157	ehr data
0.5078600617	most
0.5078212238	strong evidence
0.5077796198	single vector
0.5077695932	input text
0.5077597868	efficient method
0.5077187825	syntactically
0.5076818557	similar language
0.5076701997	effectively improve
0.5076611736	user specific
0.5075778342	enormous
0.5075038844	speech input
0.5074962085	systematically compare
0.5074793887	content
0.5074140985	constant
0.5073904135	occurring words
0.5073613542	standard datasets
0.5073366780	converted
0.5072913232	approximations
0.5072849055	phase
0.5072297602	information extracted
0.5072033732	coherent
0.5071155568	deleted
0.5071155568	geometrical
0.5070962710	machine learning community
0.5070818096	existing studies
0.5070323238	retrieve relevant
0.5069584688	significant challenges
0.5069391434	argues
0.5069256101	children
0.5069216230	simple combination
0.5069147157	experiments confirm
0.5069021312	thematic
0.5068616968	specific knowledge
0.5068308917	language generation models
0.5068160220	approach yields
0.5068123212	nuances
0.5067951392	base
0.5066758870	signs
0.5066469733	journalists
0.5065831396	systematic
0.5065777510	results
0.5065584949	similarity search
0.5065254328	syntactic properties
0.5065190589	central role
0.5064803044	neural conversation
0.5064661307	squad dataset
0.5064545420	statements
0.5064450860	temporal relationships
0.5064056820	significantly worse
0.5063917784	non trivial
0.5063911773	selecting
0.5062025459	summarization model
0.5061749703	policy
0.5061502514	adapts
0.5061250097	scale
0.5061248962	topic analysis
0.5060852962	statistical properties
0.5059892755	justify
0.5059872077	consequence
0.5059778720	hardware
0.5059717366	directional lstm
0.5059375070	test sentences
0.5058694035	retrieving
0.5058689101	achieve high performance
0.5058670102	automatic speaker
0.5058246896	hashtags
0.5057638196	strong performance
0.5057488151	near optimal
0.5057278495	intend
0.5057278495	popularly
0.5057016188	aligned
0.5056903286	multilingual and cross lingual
0.5056760950	pretrained model
0.5056381900	protocol
0.5056233430	hot
0.5056214929	massive text
0.5054321765	encodings
0.5054190364	racial
0.5053690327	strengths and weaknesses
0.5053672749	consumers
0.5053658350	draws
0.5053358284	understand language
0.5053233290	enforce
0.5052410554	deep language models
0.5051971226	character level convolutional
0.5051850905	method obtains
0.5051621553	sentiment classifiers
0.5050849953	link
0.5050711975	paintings
0.5050711975	bugs
0.5050711975	wrongly
0.5050348381	challenging task
0.5049545592	dataset
0.5048956270	exemplars
0.5048842101	unsupervised neural machine
0.5048200213	outlines
0.5047941342	disambiguate
0.5047906019	resolve
0.5047412382	mean
0.5047132722	english to german
0.5047074655	facto
0.5046980500	complex
0.5046706304	entity recognition task
0.5046418406	essentially
0.5044830088	sacrificing
0.5044732935	male
0.5044606168	humour
0.5044015128	\ textbf
0.5043962132	recommendations
0.5043684991	earlier
0.5043513532	document network
0.5043353675	prediction task
0.5042913232	hateful
0.5042723922	competitively
0.5042696412	eliminate
0.5042668510	execute
0.5042104522	low data
0.5041595340	separate tasks
0.5041443387	based measures
0.5041314387	lead
0.5041160864	assisting
0.5040897208	skip
0.5040837256	width
0.5040775345	companies
0.5040745852	readable
0.5040607155	autoregressive sequence
0.5040319059	metric
0.5040007951	competencies
0.5039924892	sample
0.5039918535	submission
0.5039370002	chapters
0.5039362699	binary relations
0.5039000067	promising direction
0.5038880376	research topic
0.5038828215	weighted average
0.5038466985	extraction performance
0.5038047974	emergence
0.5037268106	source
0.5037170706	model
0.5037016785	term memory
0.5036947861	starting
0.5036903286	supervised and semi supervised
0.5036723492	modeling framework
0.5036564758	refers
0.5036539504	manages
0.5036176390	demographics
0.5035292801	focus
0.5034749171	quadratic
0.5034249716	deep neural network based
0.5034087155	thread
0.5033652360	grammatical knowledge
0.5033235382	speech content
0.5033043188	judgement
0.5033043188	checked
0.5032606897	embed
0.5032504042	key terms
0.5032040152	syntactic processing
0.5031775350	main approaches
0.5031651849	prediction accuracy
0.5031101582	acoustic to word
0.5030773857	referents
0.5030359838	language style
0.5029736742	generate high quality
0.5029470438	original meaning
0.5029280066	semantic feature
0.5028859004	contributing
0.5028642286	text snippets
0.5028498530	viable
0.5028428267	effect
0.5028411143	mainstream
0.5028333332	jointly train
0.5028286284	advertisement
0.5028286284	bracketing
0.5028286284	capsules
0.5028155150	attention based model
0.5027951288	arising
0.5027944208	adversarial multi task
0.5027913232	subwords
0.5026931234	multilingual transfer
0.5026648438	turning
0.5026648438	marking
0.5026648438	complaints
0.5026595815	representations learned
0.5026564758	consist
0.5026564758	absence
0.5026485898	interpolated
0.5026485898	transitivity
0.5026485898	innate
0.5026439915	lives
0.5025916018	based neural network
0.5025700689	codebook
0.5025700689	typos
0.5025700689	motivating
0.5025700689	printed
0.5025700689	centroid
0.5025700689	exceptions
0.5025231047	spectra
0.5025231047	closeness
0.5025045355	fundamentally
0.5024909412	grained
0.5024786441	hierarchical representation
0.5024258258	unanswerable
0.5024173452	identification tasks
0.5023936742	important sentences
0.5023648413	emotional content
0.5023549903	partly
0.5023372985	toward
0.5023129716	neural keyphrase
0.5022147510	recently shown
0.5021411867	helps improve
0.5021386531	million word
0.5021281889	minority
0.5021202750	non autoregressive neural machine translation
0.5021024273	cultures
0.5021024273	notice
0.5021024273	standardization
0.5020928242	generate fluent
0.5020636478	comparisons
0.5020547335	investigation
0.5020323947	treatments
0.5020306066	skill
0.5019854375	detailed information
0.5019444492	representation space
0.5018133079	affect
0.5017886688	effectively learn
0.5017392484	threshold
0.5017375568	negligible
0.5017358715	neural named entity
0.5017224509	rule based approaches
0.5017023051	existing taxonomy
0.5016659420	religious
0.5016659420	trackers
0.5016659420	spatially
0.5016659420	foundational
0.5016659420	closing
0.5016659420	dividing
0.5016659420	sociolinguistic
0.5016659420	formalized
0.5016659420	formalizing
0.5016647708	automatic document
0.5016605209	encoder decoder structure
0.5016338829	imitating
0.5016338829	seamless
0.5016338829	visualisation
0.5016338829	posing
0.5016338829	navigational
0.5016338829	signed
0.5016338829	writings
0.5016279948	collection
0.5016267746	summary sentences
0.5016189461	experiment
0.5015626535	publication
0.5015483382	$ \ leftrightarrow
0.5015460464	deficient
0.5015460464	conjuncts
0.5015460464	excluded
0.5014956045	diverse languages
0.5014574709	telephone
0.5014347990	recent papers
0.5014213339	restore
0.5014213339	brackets
0.5014213339	altering
0.5014213339	trigrams
0.5014213339	particles
0.5014184123	triggers
0.5014182676	word generation
0.5013790626	interactions
0.5013429734	fragments
0.5013209555	inject
0.5013075424	improve generalization
0.5012982535	speaker adaptive
0.5012423086	importance scores
0.5012304660	automatically learns
0.5012208185	results showed
0.5012157972	structured neural
0.5011926141	unannotated
0.5011909932	societies
0.5011765440	becoming increasingly
0.5011433909	plausible
0.5011388904	unlabeled
0.5011370382	strengthen
0.5011304437	inadequate
0.5011117720	nli model
0.5011088443	fake
0.5010872493	simplify
0.5010601596	achieve competitive
0.5010401697	recall
0.5009661441	vocoder
0.5009044117	driven learning
0.5008930581	language modeling objective
0.5008919627	leak
0.5008919627	revenue
0.5008919627	staged
0.5008919627	accumulating
0.5008919627	chief
0.5008919627	foreground
0.5008919627	mistake
0.5008919627	therapeutic
0.5008919627	catalogue
0.5008919627	retrained
0.5008919627	discretized
0.5008919627	officials
0.5008919627	reviewer
0.5008919627	connotations
0.5008919627	tandem
0.5008919627	adjectival
0.5008919627	hypothetical
0.5008919627	inclusive
0.5008919627	opposition
0.5008919627	indexes
0.5008919627	directionality
0.5008919627	checkpoints
0.5008919627	delexicalization
0.5008919627	implicatures
0.5008826599	standalone
0.5007785402	wav2vec
0.5007785402	misspelling
0.5007542888	decompose
0.5006944390	bots
0.5006744065	accomplished
0.5006735150	relative
0.5006516487	after
0.5006473901	$ f_1
0.5006176390	intonation
0.5005669548	multi label text
0.5005587596	operational
0.5005492501	word n grams
0.5005173314	similar documents
0.5003639895	passage
0.5003252437	religion
0.5003016609	practice
0.5002883179	supervised learning framework
0.5002582443	distribution
0.5002434926	reusable
0.5002188525	chinese texts
0.5001732919	coffee
0.5001603198	reinforcement learning method
0.5001051984	enhanced model
0.5000651770	learning architectures
0.5000059727	thorough
0.4999645298	retaining
0.4999604903	neural module
0.4999196261	extracting features
0.4999144592	representing words
0.4999128624	supervised models
0.4998320959	minor
0.4998237462	subsequences
0.4998064288	articles
0.4998021744	linguistic tasks
0.4997882897	blocks
0.4997667765	transformations
0.4996864438	breakthroughs
0.4996264521	running text
0.4996065935	harm
0.4996007249	structure
0.4995911757	english to french
0.4995862158	conventional methods
0.4994952628	excellent
0.4994840512	deep representations
0.4994768262	achieves competitive
0.4994757157	quotations
0.4994757157	occupations
0.4994571084	rich features
0.4994017558	references
0.4993938372	automatic and human evaluations
0.4993709764	consequences
0.4993230600	arcs
0.4992987068	begun
0.4992756416	human labeled data
0.4992558556	interested
0.4992438352	drawback
0.4992438352	offered
0.4992438352	margins
0.4992399049	competition
0.4992056923	attention based models
0.4992021334	essays
0.4991985956	parametric
0.4991880188	ingredient
0.4991880188	band
0.4991880188	lookahead
0.4991880188	maximally
0.4991880188	visible
0.4991569533	parallel computation
0.4991366063	inner workings
0.4991176259	de facto
0.4991168565	then
0.4991042220	compatible
0.4991001762	large scale dataset
0.4990891073	designers
0.4990877917	word2vec and glove
0.4990868691	innovations
0.4990556980	grow
0.4990468173	alterations
0.4990468173	dialogic
0.4990468173	miss
0.4990468173	mobility
0.4990468173	human's
0.4990468173	tailor
0.4990468173	gazetteer
0.4990468173	visibility
0.4990468173	puzzles
0.4990468173	mathematically
0.4990292924	speaker's
0.4990165808	thus
0.4989970416	fluent
0.4989441868	events
0.4989424614	probability
0.4989291935	likelihood
0.4989072169	organizers
0.4989061418	arguments
0.4988879584	linguistic descriptions
0.4988513929	provider
0.4988480784	large scale analysis
0.4988101845	accommodate
0.4987948730	output sentence
0.4987669437	readings
0.4987642993	serving
0.4987554218	returned
0.4987380746	representations
0.4987010089	nmt training
0.4986761870	evaluation task
0.4986523901	lstm encoder
0.4986431372	literature based
0.4986167719	speech challenge
0.4986058471	satisfy
0.4985980111	clips
0.4985826001	corrupted
0.4985779269	retrieval tasks
0.4985570887	pool
0.4985451498	hundreds of thousands
0.4985447455	human annotated data
0.4985422388	coarse to fine
0.4985022574	continue
0.4984719566	language dataset
0.4984649993	barriers
0.4984567186	smbr
0.4984549885	inference problem
0.4984501680	belonging
0.4984480524	sequence labeling models
0.4984261140	sales
0.4984261140	workload
0.4984261140	comparability
0.4984261140	cycles
0.4983725546	learning representations
0.4983620064	phrasal
0.4983501837	navigate
0.4983348426	spurious
0.4983290549	increasingly important
0.4982906849	adjectives
0.4982763065	complex natural language
0.4982587867	dialogue corpus
0.4982277724	syllables
0.4982240965	common
0.4981978166	politeness
0.4981401409	substantial improvement
0.4981201279	cells
0.4981025837	cross lingual information
0.4980864875	attended
0.4979989089	top
0.4979534826	model performs
0.4979180766	elicitation
0.4979180766	regulatory
0.4979180766	releases
0.4979180766	corrects
0.4979180766	rhetoric
0.4979180766	solvers
0.4979180766	cuisine
0.4979180766	paced
0.4979180766	interpolating
0.4979180766	negations
0.4979180766	broadcasts
0.4979180766	perplexities
0.4979180766	phrasing
0.4979180766	interconnected
0.4979180766	sees
0.4979180766	subordinate
0.4979180766	fifty
0.4978906713	linear combination
0.4978768481	complex tasks
0.4978715956	v
0.4978168471	published results
0.4977862485	scorer
0.4977862485	acoustically
0.4977862485	opposing
0.4977862485	stances
0.4977862485	empower
0.4977862485	meaningless
0.4977862485	postings
0.4977862485	degraded
0.4977862485	thesauri
0.4977853165	skipping
0.4977812684	context representation
0.4977612594	inconsistencies
0.4977447889	publications
0.4977297493	agents
0.4977283415	adequate
0.4976361501	qualities
0.4975898096	proof
0.4975811921	recurrent architectures
0.4975483382	$ \ mathit
0.4974964142	fine
0.4974876379	english german translation
0.4974771685	indispensable
0.4973660922	chinese social
0.4973656339	achieve comparable
0.4973109344	detailed
0.4972572742	modelled
0.4972249166	storyline
0.4972249166	racism
0.4972249166	reranker
0.4972249166	genders
0.4972249166	episodes
0.4972249166	abnormal
0.4972249166	rejection
0.4972249166	substitutes
0.4972249166	prospects
0.4971994795	semeval 2020 task 8
0.4971223130	decoding framework
0.4970881308	complexity
0.4970322002	pipeline based
0.4969507493	desire
0.4969452970	sequence encoder
0.4969333485	given
0.4969235532	document level relation
0.4969112849	nearby
0.4969056414	key value
0.4968021166	indicators
0.4967952699	joint distribution
0.4967675221	processing speed
0.4967647761	artists
0.4967647761	tagsets
0.4967629837	humans
0.4967291222	wide
0.4967228303	descriptors
0.4967136292	solved
0.4967064572	de identified
0.4966893135	outperforms competitive baselines
0.4966721813	baseline models
0.4966438352	overhead
0.4965967204	parts of speech
0.4965719557	influenced
0.4965627178	pay
0.4964192542	biomedical named entity
0.4963484867	hard
0.4963416682	inference stage
0.4962712469	model employs
0.4962458830	comments
0.4962429595	uncommon
0.4962399770	findings
0.4961817378	sticker
0.4961394595	joint entity
0.4960648759	learned attention
0.4960559073	sequential nature
0.4960406214	items
0.4960347484	set based
0.4960222012	privacy
0.4960149122	language input
0.4959999388	attack success
0.4959893302	subjects
0.4959860373	regression task
0.4959809532	verification task
0.4959658335	features improve
0.4959547470	intrinsic and extrinsic
0.4959411944	couple of years
0.4959374236	automatic processing
0.4958938502	start
0.4958919660	automatically construct
0.4958877558	right
0.4958607885	questions generated
0.4958236772	scale dataset
0.4958230268	part of speech tagging
0.4958165279	tagging and dependency parsing
0.4957952301	generated data
0.4957719925	concatenated
0.4957572504	non deterministic
0.4957379146	evidence
0.4957180670	medical experts
0.4957083346	chest x
0.4956864122	data
0.4956803212	level attention
0.4956214039	initialized
0.4956060539	performance metrics
0.4955957789	outstanding
0.4955738003	definitions
0.4955548494	ease
0.4955328715	environments
0.4955175108	footprint
0.4955172271	attachment
0.4955141607	model combines
0.4954524730	pre training tasks
0.4953891578	training
0.4953725136	important words
0.4953451526	training improves
0.4952909417	gaining
0.4952256517	relevant questions
0.4952083035	assessed
0.4951824440	products
0.4951736049	lexicons
0.4951710538	document level translation
0.4951290589	questions
0.4951115387	strong
0.4951105428	tuple
0.4951105428	constraining
0.4950796066	dialogue data
0.4950719063	unlike previous approaches
0.4950653728	intrinsic
0.4950643207	typologically
0.4950158284	achieves substantial improvements
0.4950027919	m bert
0.4949919319	proposed framework
0.4949916409	multiple word
0.4949863373	structured knowledge bases
0.4949352163	shown remarkable
0.4949199217	multilingual nlp
0.4949137605	additional data
0.4949136163	stems
0.4948644317	waiting
0.4948644317	anaphors
0.4948563471	information
0.4948475198	pre trained model
0.4948379488	explicitly model
0.4948008076	well formed
0.4947895330	quantitative metrics
0.4947474189	traditional nlp
0.4947297764	looking
0.4946992468	model learns
0.4946923242	rate reductions
0.4946650444	decoders
0.4946479268	anomalies
0.4946479268	staff
0.4946479268	purchasing
0.4946479268	contradicts
0.4946479268	departments
0.4946419847	viral
0.4945956750	temporal dependencies
0.4945855378	sequence labeling model
0.4945767691	slot value
0.4945553723	small sample
0.4945414840	uncover
0.4944987375	semantic content
0.4944850587	point of view
0.4944318436	order language
0.4944210819	distribute
0.4944210819	deleting
0.4944210819	keys
0.4944210819	pointers
0.4943690377	ir models
0.4943352347	bert encoder
0.4942777368	english data
0.4942425284	homophily
0.4942370803	elegant
0.4942364777	open problem
0.4942210513	unreliable
0.4941284470	support
0.4941276305	stylistic
0.4941002865	achieved promising results
0.4940706003	opposite
0.4940427357	termed
0.4940237807	transition
0.4940192861	topic words
0.4939914531	cost
0.4939811460	translated sentences
0.4939719794	maps
0.4939419040	necessity
0.4938999013	generation methods
0.4938928940	identifying and categorizing
0.4938781729	equipped
0.4938021813	impose
0.4937869139	perturbations
0.4937802069	misinformation
0.4937480168	unsupervised text
0.4937317516	deep learning approaches
0.4936786159	engineers
0.4936698138	proposed model
0.4936594335	pixels
0.4936460893	large data
0.4936121611	automatic acquisition
0.4935670742	experiments suggest
0.4934630874	high cost
0.4934587596	initialize
0.4933827110	pre specified
0.4933750315	babi
0.4933570141	semi structured data
0.4933395224	code and pre trained
0.4933157248	neural semantic
0.4932737329	textual network
0.4932590745	image to text
0.4932240760	caption
0.4931596772	easily applied
0.4931520205	fluency
0.4930546927	non commutative
0.4930183452	shared space
0.4929912990	pre training task
0.4929897946	attention distribution
0.4929802638	subgoals
0.4929780634	convolutional recurrent neural
0.4929404183	towards
0.4929226574	versa
0.4929167932	lots
0.4929148929	asian languages
0.4929054804	offering
0.4928993672	adaptation approaches
0.4928756257	rules
0.4928629824	judges
0.4928529692	summarization tasks
0.4927863302	transformer based encoder
0.4927772071	ner dataset
0.4927702756	prediction model
0.4927265464	low dimensional embedding
0.4927102029	noised
0.4926763134	papers
0.4926747694	consumption
0.4926458183	dependence
0.4926255987	ended
0.4926241467	distinctions
0.4926206965	nd
0.4926028937	benchmark
0.4925963517	brief
0.4925826887	embeddings outperform
0.4925481591	projective
0.4925113699	large proportion
0.4924921107	backpropagation
0.4924921107	deciding
0.4924827591	level dialogue
0.4923843101	evaluation scores
0.4923025076	non autoregressive machine translation
0.4922344365	existing datasets
0.4922310743	pipeline
0.4922065544	in cabin
0.4921909882	large scale corpora
0.4921841595	correctness
0.4921815898	rapid development
0.4921720570	de la
0.4921630569	schema
0.4921371081	led
0.4920653624	coding
0.4920549414	vector machine
0.4920488450	relates
0.4920480128	latent semantics
0.4920377721	sequence
0.4920084688	online communication
0.4919932154	guidelines
0.4918851000	baseline results
0.4918642812	conditional generative
0.4918487928	digits
0.4918453998	resource language pairs
0.4918257474	set
0.4918072558	step by step
0.4917672979	rnn based models
0.4917335152	text translation
0.4916161163	status
0.4916159358	probable
0.4915864129	capturing
0.4915764699	bitexts
0.4914384461	optimizes
0.4914285700	vocoders
0.4914285700	timelines
0.4914285700	multiplicity
0.4914285700	packages
0.4913969665	fail to generalize
0.4913576095	metrics
0.4913442252	parameter
0.4913008958	generalize to unseen
0.4913000201	end task oriented
0.4912867959	processing applications
0.4912453411	validation and explanation
0.4911800793	relevant tweets
0.4911590311	attention based nmt
0.4911499965	sounds
0.4910309754	slight
0.4910008963	branching
0.4909058283	extraction techniques
0.4908970572	article
0.4908678414	generalizability
0.4908438919	ner systems
0.4908217658	affecting
0.4908206450	basis
0.4907662390	ndcg @
0.4907157204	realized
0.4906383620	speech utterances
0.4906368650	terms
0.4906212710	emphasize
0.4906096503	cleaned
0.4906096503	truncated
0.4906096503	provably
0.4906096503	transliterated
0.4906096503	parallelized
0.4906096503	certainty
0.4906096503	crawling
0.4906082131	representation methods
0.4906010028	worthy
0.4905561981	a case study
0.4905425257	limited vocabulary
0.4905205845	places
0.4905128837	classification benchmarks
0.4904955067	associations
0.4904866708	quality
0.4904634789	universality
0.4904457913	lists
0.4904264254	iterations
0.4904263135	characters
0.4903520444	true
0.4903463990	regularize
0.4903419025	appropriateness
0.4903361905	logically
0.4903361905	concatenate
0.4903357978	evaluation datasets
0.4902905715	extraction methods
0.4902595263	occurring
0.4902392821	im
0.4902231457	generation task
0.4902192135	internal and external
0.4901883047	originally
0.4901659732	age
0.4901463996	absent
0.4901420795	branch
0.4901249696	animals
0.4900405423	journalistic
0.4900405423	algorithmically
0.4900405423	existed
0.4900405423	1m
0.4900360327	speech related
0.4899542638	mbart
0.4899542638	finitely
0.4899238007	utterance
0.4899071410	level graph
0.4898722416	utilization
0.4898692498	rigid
0.4898501328	titles
0.4898451826	model generates
0.4898066983	utility
0.4897665667	seeking
0.4897192552	communication
0.4897030946	experiences
0.4896959294	pretraining tasks
0.4896910864	research
0.4896051491	deals
0.4895680986	operating
0.4895672847	baseline systems
0.4895539229	imperative
0.4895525385	labelled
0.4894476440	stable
0.4894447443	bounds
0.4893795860	learning process
0.4893300677	name
0.4893182098	authorities
0.4892708938	item
0.4892701190	methodologies
0.4892567435	segmentation task
0.4892483597	differing
0.4892190431	weeks
0.4891854571	moreover
0.4891824985	calculation
0.4889685320	categorized
0.4889138846	unavailable
0.4888921107	tensors
0.4887955967	source word
0.4887616828	curated
0.4887602347	process models
0.4887362052	arabic texts
0.4886747337	standard evaluation
0.4886642830	japanese to english
0.4886404764	face
0.4886325303	units
0.4885044307	context aware neural machine
0.4884944981	training process
0.4884772880	input sentences
0.4884400612	inventory
0.4884026408	executed
0.4884023025	v1.0
0.4883878373	time delay
0.4883749250	digit
0.4883749250	metrical
0.4883712213	conditioning
0.4883499325	ambiguities
0.4883345322	biases
0.4882967023	model fine tuning
0.4882595967	members
0.4882483597	actors
0.4882483597	processor
0.4882062972	tracks
0.4881802410	texts
0.4881783247	arabic sentiment
0.4881026116	detection models
0.4880673657	what's
0.4880656689	threads
0.4880630647	extensive evaluations
0.4880516808	experimental evidence
0.4880432394	identifying relevant
0.4880413242	self attention heads
0.4880164087	baseline approaches
0.4880054091	approximately
0.4879786334	tts model
0.4879725432	logistic regression model
0.4879676498	acoustics to word
0.4879607546	commands
0.4879271370	evaluation procedure
0.4879161798	additional input
0.4878850950	structured language
0.4878165455	replaced
0.4877602569	replicate
0.4877499521	principles
0.4877244442	open challenges
0.4877092758	speech to text translation
0.4875746434	strong transformer
0.4875677988	patents
0.4875544116	encoder model
0.4875168751	achieve significant improvements
0.4875044498	competitive accuracy
0.4874688337	celebrities
0.4874343928	multilingual language
0.4873998065	computational tools
0.4873520282	parsers
0.4873474816	jointly
0.4873364493	produce high quality
0.4873201880	model achieved
0.4872859507	semantic aspects
0.4872788443	corresponds
0.4871825894	coming
0.4871732952	transfer learning models
0.4871049854	matching tasks
0.4871039422	\ texttt
0.4870976042	nodes and edges
0.4870850947	linguistically
0.4870220395	text classification models
0.4870200662	large quantities
0.4870008277	finite
0.4869813149	model architectures
0.4869610461	popular topic
0.4869328917	transferring
0.4869173138	based baseline
0.4868991888	burden
0.4868988668	spellings
0.4868682500	unexplored
0.4868626246	wmt 2020
0.4868399449	visualizes
0.4868399449	buy
0.4868399449	exchanged
0.4868399449	subclass
0.4868399449	instrument
0.4867343458	neural encoder decoder models
0.4867076542	modifying
0.4866367985	few shot text classification
0.4866214152	illustrated
0.4866184868	speech community
0.4865476975	expanded
0.4865243021	language pre training
0.4865221381	5 gram
0.4864782341	answer ranking
0.4864777844	low
0.4864401182	open source toolkit
0.4864142466	runs
0.4863712803	neural topic
0.4863266382	large pre trained
0.4863217907	executing
0.4863217907	businesses
0.4863217907	viewing
0.4863217907	ample
0.4863217907	totally
0.4863217907	pairing
0.4863217907	imposing
0.4862884789	large scale unlabeled
0.4862262029	adaptation methods
0.4861609243	inconsistent
0.4861504360	easy to implement
0.4860975061	src
0.4860759036	adaptive training
0.4860717526	model produces
0.4860374391	automated theorem
0.4859935444	research challenges
0.4859022925	transcripts
0.4858802959	language representation model
0.4858573551	detection performance
0.4858461725	generalization performance
0.4858140675	operation
0.4858133914	interrelated
0.4858133914	lift
0.4858133914	initializations
0.4858133914	externally
0.4858133914	scans
0.4858133914	clicks
0.4858133914	convenience
0.4858133914	blend
0.4858133914	analyzers
0.4858133914	subtree
0.4858133914	noteworthy
0.4858133914	complementarity
0.4858133914	realizations
0.4858133914	brevity
0.4858133914	precedence
0.4858133914	groupings
0.4858133914	politically
0.4858133914	concisely
0.4858133914	etymological
0.4858133914	richly
0.4858133914	sorts
0.4858133914	anomalous
0.4858133914	penalties
0.4858133914	citing
0.4857920788	shared embedding
0.4857904057	reducing
0.4857837740	self training
0.4857814593	judgements
0.4857739881	adaptive learning
0.4857549778	in
0.4857363443	second pass
0.4857342844	force
0.4857063989	agglutinative
0.4857048279	incorporation
0.4856611791	generation techniques
0.4855714239	salient information
0.4855593004	model yields
0.4855503926	devised
0.4855162809	languages
0.4855125263	emerging
0.4855058688	tagging task
0.4854805513	drops
0.4854805513	moderate
0.4854661110	deep generative
0.4854633357	percent
0.4854238885	study investigates
0.4854042888	similar sentences
0.4854015996	unsuitable
0.4853925340	notions
0.4853800297	model parameters
0.4853308203	transfer learning approaches
0.4853148639	access
0.4852951743	surpass
0.4852650241	reinforcement learning framework
0.4852533096	synthetic text
0.4851774710	subtask
0.4851711205	subset
0.4851693717	constrain
0.4851653492	pseudo training
0.4851471186	judged
0.4851161740	acoustic word
0.4850822502	uchi
0.4850822502	autoregressively
0.4850822502	lend
0.4850822502	aggregators
0.4850822502	engagingness
0.4850822502	timestamp
0.4850822502	generator's
0.4850822502	complicate
0.4850822502	factorize
0.4850822502	accommodates
0.4850822502	habits
0.4850822502	codebase
0.4850822502	harms
0.4850822502	incurred
0.4850822502	endowed
0.4850822502	routines
0.4850822502	border
0.4850822502	inquiries
0.4850822502	milestone
0.4850822502	misalignment
0.4850822502	latencies
0.4850822502	suspected
0.4850822502	imperceptible
0.4850822502	disseminated
0.4850822502	youth
0.4850822502	struggling
0.4850822502	instantaneous
0.4850822502	executions
0.4850822502	nonterminals
0.4850822502	walking
0.4850822502	imitates
0.4850822502	recalled
0.4850822502	accesses
0.4850822502	regularizations
0.4850822502	breakthrough
0.4850822502	bonus
0.4850822502	advocates
0.4850822502	pretty
0.4850822502	disclose
0.4850822502	visemes
0.4850822502	tease
0.4850822502	irregularities
0.4850822502	minorities
0.4850822502	fulfilled
0.4850822502	predetermined
0.4850822502	cutoff
0.4850822502	maximized
0.4850822502	atypical
0.4850822502	persuade
0.4850822502	spurred
0.4850822502	estimations
0.4850822502	clinician
0.4850822502	complemented
0.4850822502	revisits
0.4850822502	influx
0.4850822502	diversely
0.4850822502	variances
0.4850822502	inequalities
0.4850822502	economical
0.4850822502	sole
0.4850822502	conforming
0.4850822502	pioneering
0.4850822502	tractability
0.4850822502	licensed
0.4850822502	synchronously
0.4850822502	periodically
0.4850822502	goods
0.4850822502	wrote
0.4850822502	precomputed
0.4850822502	bypasses
0.4850822502	unsuccessful
0.4850822502	anaphor
0.4850822502	coincide
0.4850822502	physically
0.4850822502	lemmatized
0.4850822502	researching
0.4850822502	minimise
0.4850822502	sociology
0.4850822502	nonterminal
0.4850822502	overwhelmingly
0.4850822502	permitted
0.4850822502	attractor
0.4850822502	linguist
0.4850822502	16k
0.4850822502	spur
0.4850822502	inspires
0.4850822502	2018a
0.4850822502	abstracted
0.4850822502	spreads
0.4850822502	neighbouring
0.4850822502	confuse
0.4850822502	functionally
0.4850822502	appending
0.4850822502	interfering
0.4850822502	realistically
0.4850822502	underperforms
0.4850822502	switches
0.4850822502	executor
0.4850822502	sacrifice
0.4850822502	enormously
0.4850822502	reliant
0.4850822502	permissive
0.4850822502	emit
0.4850822502	team's
0.4850822502	occupy
0.4850822502	400k
0.4850822502	diminish
0.4850822502	conceived
0.4850822502	elaboration
0.4850822502	writer's
0.4850822502	imaginary
0.4850822502	dropping
0.4850822502	visualise
0.4850315551	sparsity
0.4850121912	discourse features
0.4850097857	inspiration
0.4850097857	disentangle
0.4849967254	tacotron 2
0.4849935581	classification loss
0.4849304409	morphological features
0.4849270444	furthermore
0.4849203681	peak
0.4849072012	human interactions
0.4848748506	contextual representation
0.4848700719	named
0.4848339134	accepted
0.4848269890	extractors
0.4848040997	outperforms existing
0.4847900839	descriptions
0.4847806964	data scarcity problem
0.4847730028	large gains
0.4847498734	global semantics
0.4847253564	couple
0.4847253564	word's
0.4847153555	increasing attention
0.4846642250	sharing
0.4846641471	expressions
0.4846215884	matching problem
0.4846029941	key role
0.4845840747	semantic processing
0.4845721584	posts
0.4845697985	faster decoding
0.4844994051	phrases
0.4844976493	based architectures
0.4844726223	specific aspect
0.4844411761	hits @
0.4844268322	matching algorithm
0.4843725147	hybrid neural network
0.4843192387	trained jointly
0.4843112453	unprecedented
0.4843037371	appearance
0.4842767863	understanding tasks
0.4842617492	word probabilities
0.4842341655	sub optimal
0.4842313948	achieves significant improvements
0.4841887517	further
0.4841647565	neural network approach
0.4841610465	standard lstm
0.4841526256	academic research
0.4841336301	aligns
0.4841281333	tremendous
0.4840941501	outperforms previous
0.4840753653	hybrid method
0.4840017516	improved results
0.4839681383	empirical results demonstrate
0.4839258144	language explanations
0.4838768073	expressing
0.4838599791	averaged
0.4838551820	rule based systems
0.4838489686	models trained
0.4838423516	significant performance
0.4838209555	train test
0.4838041686	reliable
0.4837446883	translation accuracy
0.4837319684	generality
0.4837166367	does
0.4836863739	perform worse
0.4836801582	separated
0.4836672172	traditional machine
0.4836553125	paper compares
0.4836446842	closed
0.4836277188	material
0.4836272310	syntactic cues
0.4835965200	multiple turns
0.4835709077	linguistic processing
0.4835667096	domain specific information
0.4834147953	report
0.4834068324	smaller models
0.4834064227	filterbanks
0.4834064227	formatting
0.4834021953	log
0.4833673961	multilingual setting
0.4832878481	regime
0.4832784997	words
0.4832645420	away
0.4832594640	locate
0.4832516938	semantic constraints
0.4832407147	german to english
0.4832048831	accurate prediction
0.4831781411	aware self attention
0.4831386958	competitive performance compared
0.4831353488	division
0.4831283583	platform
0.4831276762	language processing community
0.4830741334	exacerbated
0.4830741334	crafting
0.4830741334	memorizing
0.4830741334	retrain
0.4830741334	danger
0.4830741334	promises
0.4830741334	font
0.4830741334	decouples
0.4830741334	violations
0.4830741334	excessive
0.4830741334	till
0.4830741334	rewrites
0.4830741334	venues
0.4830741334	discriminatively
0.4830741334	stocks
0.4830741334	utilising
0.4830741334	emphasizing
0.4830741334	encounters
0.4830741334	article's
0.4830741334	elaborated
0.4830398702	generative neural
0.4829946942	alternatives
0.4829347734	end to end task oriented
0.4829293239	heuristic
0.4828950449	des
0.4828900706	goes
0.4828389200	memory module
0.4828165785	synthetic training data
0.4827362065	large volumes
0.4827140551	patients
0.4826117198	left
0.4825727997	enjoy
0.4825641015	document corpus
0.4824998450	zero
0.4824905245	explicitly capture
0.4824644374	dnn model
0.4824643295	semantic modeling
0.4824215514	evaluation methods
0.4823875953	detection tasks
0.4823558236	current datasets
0.4823433259	natural question
0.4823229357	translated
0.4822799919	dialogs
0.4822587788	target tasks
0.4822397200	model complexity
0.4822246649	domain adaptation techniques
0.4821573762	expression generation
0.4821559162	semantic parses
0.4821277461	clients
0.4821277461	proteins
0.4821277461	stereotypical
0.4821277461	amplified
0.4821277461	comprehensible
0.4821277461	decouple
0.4821277461	dissimilarity
0.4821277461	meaningfully
0.4821277461	followers
0.4821277461	questionnaires
0.4821277461	layouts
0.4821277461	desiderata
0.4821277461	displaying
0.4821277461	loses
0.4821277461	resolutions
0.4821277461	realizing
0.4821277461	listed
0.4821277461	metaphorical
0.4821277461	subtrees
0.4821277461	repetitions
0.4821277461	swapping
0.4821277461	justified
0.4821277461	axes
0.4821029867	4 gram
0.4819976111	require commonsense
0.4819927219	spectrograms
0.4819037130	removed
0.4818951843	simple neural
0.4818545153	$ \ sim
0.4818485789	reasonable performance
0.4818396672	inherent
0.4817376965	model evaluation
0.4816739246	practical
0.4816691462	ii
0.4815773690	vectors
0.4815754508	things
0.4814684740	communicative
0.4814633593	made great progress
0.4814449795	entity graph
0.4814352107	automate
0.4813554202	extrinsic
0.4812525704	recognition errors
0.4812253808	large improvements
0.4812250470	piece
0.4812193326	managed
0.4811981641	document set
0.4811919156	linguists
0.4811904304	likes
0.4811791629	relied
0.4811713610	automatic data
0.4811519880	extensions
0.4811195857	arises
0.4811120212	phrase based machine
0.4810789951	gradient methods
0.4810741042	iv
0.4810685523	premises
0.4810685523	observable
0.4810685523	adjusted
0.4810685523	interoperability
0.4810527571	interact
0.4809420633	\ citep
0.4809277318	renewed interest
0.4808776428	lines
0.4808457307	machine translation task
0.4808037737	receive
0.4807614982	neural model
0.4807481937	artifacts
0.4807471036	efficient algorithm
0.4807160212	previously developed
0.4806568155	generated output
0.4806373812	multi task training
0.4806171823	height
0.4806171823	centred
0.4806171823	observables
0.4806171823	cent
0.4806171823	subgroup
0.4806171823	assortative
0.4806171823	exponents
0.4806108364	almost exclusively
0.4805789765	linguistic unit
0.4805729420	model outputs
0.4805564609	accelerate
0.4805386341	readers
0.4805120284	hostile
0.4804545445	text datasets
0.4804436593	feeling
0.4804389034	attributes
0.4803996232	perplexity
0.4803645451	confidence
0.4801876902	expression
0.4801501032	queries
0.4800829575	naturally
0.4800697835	translated text
0.4800185684	weakly labeled
0.4800149449	context sensitive spelling
0.4800129871	manipulate
0.4799231049	future improvements
0.4798928940	encoders and decoders
0.4798806601	representing
0.4798785115	grams
0.4798682100	provide valuable
0.4798555498	simpler
0.4797913073	interpretation methods
0.4797882397	proves
0.4797841079	rate
0.4797384442	edges
0.4797037361	aware model
0.4796759916	adversarial multi
0.4796527532	viewed
0.4796440750	large scale knowledge graphs
0.4796372755	\ rm
0.4796232607	gathering
0.4795997647	approach improves
0.4795726963	while
0.4795641541	representation parsing
0.4794631522	speech to text
0.4794401647	newspapers
0.4794250960	acoustic feature
0.4793832933	domain specific language
0.4793360469	organize
0.4792903387	restrictions
0.4792490651	evaluation measure
0.4792312288	appeal
0.4792127119	small corpus
0.4791570439	calibrate
0.4791570439	unfaithful
0.4791570439	broaden
0.4791570439	assessors
0.4791570439	intensively
0.4791570439	warrant
0.4791570439	duplicates
0.4791570439	behaved
0.4791570439	operated
0.4791570439	entered
0.4791570439	strive
0.4791570439	duplication
0.4791570439	avenue
0.4791570439	snapshots
0.4791570439	distills
0.4791570439	gaming
0.4791570439	episode
0.4791570439	deployments
0.4791570439	1k
0.4791570439	tier
0.4791570439	savings
0.4791570439	utmost
0.4791570439	offsets
0.4791570439	uploaded
0.4791570439	angles
0.4791570439	stochastically
0.4791570439	optimizers
0.4791570439	untested
0.4791570439	standpoint
0.4791570439	timestamps
0.4791570439	incredible
0.4791570439	unadapted
0.4791570439	threefold
0.4791570439	elegantly
0.4791570439	homes
0.4791570439	timesteps
0.4791570439	damage
0.4791570439	impede
0.4791570439	synergy
0.4791570439	hurdle
0.4791570439	persuasiveness
0.4791570439	regularizes
0.4791570439	outputting
0.4791570439	concentrating
0.4791570439	doubling
0.4791570439	40,000
0.4791570439	subsumes
0.4791570439	networked
0.4791570439	accumulates
0.4791570439	acknowledged
0.4791570439	fulfills
0.4791570439	constrains
0.4791570439	cheaply
0.4791570439	shorthand
0.4791570439	sustained
0.4791570439	volunteers
0.4791570439	ambitious
0.4791570439	differentiated
0.4791570439	artefacts
0.4791570439	heterogenous
0.4791570439	exploding
0.4791570439	mismatching
0.4791570439	prefers
0.4791570439	affordable
0.4791570439	leaders
0.4791570439	coined
0.4791570439	exchanging
0.4791570439	delays
0.4791570439	mirror
0.4791570439	soundness
0.4791570439	preprocess
0.4791570439	facility
0.4791570439	evolutions
0.4791570439	deterioration
0.4791570439	impressively
0.4791570439	10m
0.4791570439	endow
0.4791570439	embody
0.4791570439	psycho
0.4791570439	abstain
0.4791570439	hinges
0.4791570439	archived
0.4791570439	began
0.4791570439	tenses
0.4791570439	surroundings
0.4791570439	lends
0.4791570439	manufacturers
0.4791570439	undertaken
0.4791570439	unavoidable
0.4791570439	geared
0.4791570439	reproduces
0.4791570439	accompany
0.4791570439	mandatory
0.4791570439	extant
0.4791570439	downward
0.4791570439	characterise
0.4791570439	overt
0.4791570439	homonymy
0.4791570439	instrumental
0.4791570439	missed
0.4791570439	debated
0.4791570439	speaks
0.4791570439	duplicated
0.4791345422	language processing tools
0.4791040296	downstream models
0.4790712349	relevant words
0.4790684608	question representation
0.4790420690	accept
0.4790420690	deficiency
0.4790420690	hints
0.4790257362	insight
0.4789532814	lstm neural
0.4789349033	approximation
0.4788989396	labeler
0.4788989396	uncovered
0.4788989396	diagrammatic
0.4788989396	drill
0.4788850519	task related
0.4788789841	neural turing
0.4788763855	theory based
0.4787953708	collaborate
0.4787571245	images
0.4787513115	attacks
0.4787298298	benchmark data sets
0.4786880410	resource conditions
0.4786740800	pseudo multi
0.4786048207	speech database
0.4785127946	based models
0.4783699836	pre trained word vectors
0.4783399552	simulations
0.4782890330	studies
0.4782822197	statistics
0.4782737759	binary classifiers
0.4782637748	prominent
0.4782056302	popular
0.4782042823	task
0.4782025151	attain
0.4781862145	kinds
0.4781851366	alignments
0.4781835770	regarded
0.4781655552	achieves superior
0.4780881226	order information
0.4780151485	cut
0.4779465016	century
0.4778985914	answer pairs
0.4778930914	percentage
0.4778910415	individual features
0.4778658011	syntax and semantics
0.4778399324	natural language processing models
0.4777970574	usage
0.4777242408	previously
0.4777222089	testing
0.4777063509	common approach
0.4776796234	collectively
0.4776720970	generation problem
0.4776519120	empirically investigate
0.4776033063	logs
0.4775937949	function
0.4775839791	negatives
0.4775723275	query languages
0.4774856926	extraction systems
0.4774783145	dataset demonstrate
0.4774757477	morphologically
0.4774203102	term memory networks
0.4773996075	indeed
0.4773716366	large
0.4773632535	tasks requiring
0.4773326683	asking questions
0.4772964852	diverse datasets
0.4772850785	cross language information
0.4772350185	predict
0.4771996001	embedding method
0.4771977202	target style
0.4771265586	tuned
0.4771031037	wmt 2019
0.4770908925	node
0.4770886732	classifier
0.4770185329	convolutional and recurrent neural networks
0.4769947707	sequence tagger
0.4769874407	morphemes
0.4769782971	parses
0.4769587121	english machine translation
0.4769244133	know
0.4769238257	$ \ rightarrow
0.4769038993	application domains
0.4768923903	machine learning model
0.4768695157	domain specific corpus
0.4768459739	method improves
0.4768238755	engage
0.4766965484	high dimensional data
0.4766949268	local structure
0.4766755801	tasks include
0.4766458124	million sentences
0.4766392057	semeval 2018 task
0.4766277335	generated sequence
0.4766239342	fundamental problems
0.4765990996	multiple models
0.4765763280	disambiguation task
0.4765407198	portion
0.4765162312	n best lists
0.4764874000	stage
0.4764793644	wants
0.4764422331	campaigns
0.4764307983	surprisal
0.4763691011	\ sc
0.4763265567	baseline method
0.4763180449	budget
0.4762885924	without sacrificing
0.4762796181	task performance
0.4762477341	text to sql
0.4762438325	handled
0.4762100694	apart
0.4761965324	substitute
0.4761965324	curves
0.4761510674	measure based
0.4761463282	editors
0.4761251942	end to end spoken language understanding
0.4761249536	language commands
0.4761173332	models
0.4760779322	cross lingual settings
0.4760711766	subsequent
0.4760062223	recommend
0.4759952374	unigram
0.4759755686	competing
0.4759621153	rule based machine
0.4759406725	model performance
0.4759388915	syntactic parse
0.4759372755	$ _1
0.4759058349	segmentation algorithm
0.4758862237	additional linguistic
0.4758813841	5k
0.4758813841	instructors
0.4758481574	activity
0.4758440429	bert based models
0.4758267423	final performance
0.4758010884	details
0.4757973380	principled
0.4757825894	conveyed
0.4757622931	channels
0.4757604872	implicitly learn
0.4757379235	text sequence
0.4757365662	attentive neural
0.4757208665	targets
0.4756997367	typically requires
0.4756653400	accomplish
0.4756526697	poem
0.4755924295	cross lingual image
0.4755212175	afterwards
0.4755172986	human translation
0.4755109514	mid
0.4755108213	pre training models
0.4754335759	self attention networks
0.4753686951	per
0.4753064187	compatibility
0.4752497722	shed light
0.4752456099	realistic
0.4750874755	corpora
0.4750355040	notoriously
0.4750235980	model distribution
0.4749881372	based neural
0.4749791419	differently
0.4749698387	repository
0.4749681116	polysemous
0.4749643692	domain specific embeddings
0.4749580677	pre trained encoder
0.4749418607	secondly
0.4749249951	anything
0.4749196930	divided
0.4748915700	paper investigates
0.4748204436	difficult to interpret
0.4748058866	establishing
0.4747444639	aforementioned
0.4746746321	attracted much attention
0.4746307131	unrelated
0.4746273539	agent communication
0.4746222283	crucial role
0.4745650771	mining tasks
0.4745374670	pre trained language
0.4744591136	retrieved documents
0.4744574978	bounded
0.4744411055	semeval 2017 task
0.4744409711	resulted
0.4744371578	understanding systems
0.4744348517	overlap
0.4743943349	word embeddings learned
0.4743231268	adjust
0.4742831393	learning problem
0.4742717383	results comparable
0.4742192158	says
0.4742109310	cues
0.4741979538	next utterance
0.4741717835	accompanying
0.4741545890	pre training method
0.4741414419	system's
0.4741154120	distinctive
0.4741154120	lexica
0.4741121624	target domain data
0.4740838602	interfaces to databases
0.4740583644	alignment quality
0.4740370403	trained models
0.4738965038	compiled
0.4738693388	specific context
0.4738438661	received much attention
0.4738406026	framework achieves
0.4738352682	operator
0.4738196552	term discovery
0.4737547958	conversational machine
0.4737378311	mutually
0.4737310656	model shows
0.4736914827	plots
0.4736717418	similar tasks
0.4736018815	resulting model
0.4735952258	space
0.4735820235	changing
0.4735763338	training framework
0.4735457100	achieve impressive
0.4734015882	cross lingual document classification
0.4734001422	study shows
0.4733739251	obvious
0.4733192056	word level representations
0.4733183017	services
0.4732426974	fear
0.4732358063	native english
0.4731802718	coarse
0.4731716009	income
0.4731716009	employees
0.4731716009	60k
0.4731716009	requested
0.4731698992	hundreds
0.4731567654	limited training data
0.4731238880	severe
0.4730929517	growth
0.4730708578	complementary
0.4730438477	parsing task
0.4730135476	governments
0.4730135476	optimally
0.4730135476	newer
0.4730135476	merges
0.4730135476	geography
0.4730135476	accepting
0.4730135476	distinctiveness
0.4730135476	reformulate
0.4730135476	webpages
0.4730135476	compliant
0.4730135476	misspelled
0.4730135476	arrays
0.4730135476	tones
0.4730135476	incompatible
0.4730135476	subsumption
0.4730070698	customer
0.4729832025	based seq2seq
0.4729580258	wikisql dataset
0.4729521479	lies
0.4729467902	dealing
0.4729383804	beyond
0.4729281014	confirmed
0.4729035315	examples
0.4728527230	ground
0.4728353428	classification performance
0.4728000216	joint representations
0.4726957671	level sentiment
0.4726246229	names
0.4726040196	augmentation method
0.4725968552	meta learning algorithm
0.4725625076	derived
0.4725507515	update
0.4725437684	longer term
0.4725179480	parsing results
0.4724637860	network structures
0.4724619977	augmentation framework
0.4724612635	cross lingual setting
0.4724272218	disambiguated
0.4724064556	lockdown
0.4724064556	borrow
0.4724056756	gathered
0.4723493942	pre training methods
0.4723464922	efficiency
0.4723245987	recommender system
0.4722981385	differentiate
0.4722811405	implicit semantic
0.4722673541	supervised learning methods
0.4722243301	user and item
0.4721339821	reviewers
0.4719544332	qualitative
0.4719540236	document level sentiment
0.4719521479	remedy
0.4719438619	node classification
0.4719375677	programs
0.4719261940	experimental results suggest
0.4719104986	proper
0.4718926408	highly structured
0.4718907460	variety
0.4717881132	interpreted
0.4717848018	classifiers
0.4717642224	proximity
0.4717538357	decode
0.4717261761	high level semantic
0.4717210755	joint models
0.4717045022	victims
0.4716893312	topics
0.4716220865	neural baseline
0.4715991580	great deal
0.4715646490	whereas
0.4715020867	generated question
0.4714722329	purpose
0.4714114071	preceding
0.4713972487	meanings
0.4713635115	participating
0.4713292827	numerals
0.4713292827	unordered
0.4713292827	dozen
0.4713292827	pauses
0.4713292827	depths
0.4713292827	offset
0.4713292827	genes
0.4713292827	outliers
0.4713292827	concreteness
0.4713292827	presentations
0.4713292827	optimised
0.4713292827	cognitively
0.4713292827	submit
0.4713292827	excluding
0.4713249516	concerned
0.4713226672	training and test sets
0.4712764712	unfortunately
0.4712546837	major
0.4712450405	genres
0.4712319583	information provided
0.4711918564	medical domain
0.4711758459	reduction
0.4711553105	general
0.4711202968	mentioned above
0.4710817985	cleaning
0.4710420633	\ mathcal
0.4710168773	convenient
0.4710105176	computer scientists
0.4710030868	compression methods
0.4709844033	concepts
0.4709619659	varieties
0.4709379696	corrections
0.4709379696	compressed
0.4709354197	summarization method
0.4709200763	objectives
0.4709157133	risks
0.4709027450	method called
0.4708003756	imbalance
0.4707938555	since
0.4707455628	isolation
0.4706659552	generally
0.4706366626	third place
0.4706241959	segmentations
0.4706241742	configurations
0.4706221939	following
0.4706187632	an empirical investigation
0.4705674587	player
0.4705265612	hours
0.4705234862	verified
0.4705232140	comparable
0.4704569303	modified version
0.4704568379	language parsing
0.4704274203	messages
0.4704250422	pieces
0.4704211410	drastically
0.4704098424	compositional model
0.4703854812	posed
0.4703438030	contained
0.4703242907	translation performance
0.4703054834	near
0.4702872294	unsupervised evaluation
0.4702581615	area under
0.4702506841	synthetic datasets
0.4702380051	relying
0.4702287944	neural network framework
0.4702225097	nearest
0.4701072566	librispeech dataset
0.4700827527	asked
0.4700422703	formalisms
0.4699778644	automatic post
0.4699537150	theoretical
0.4698633927	unconstrained
0.4698567773	free form text
0.4698442847	overlaps
0.4698442847	epidemic
0.4698442847	signature
0.4698432700	word embeddings trained
0.4697845884	impossible
0.4697752312	lately
0.4697644281	processing pipeline
0.4697614404	similar performance
0.4697563902	automatically learn
0.4697214829	vqa task
0.4697096235	assertions
0.4697020069	convolutional architectures
0.4696890119	model improves
0.4696417873	unsupervised relation
0.4696061723	large scale pre trained language
0.4695506064	based features
0.4695493353	featuring
0.4695493353	shifted
0.4694974930	simulate
0.4694903629	30k
0.4694103758	architecture
0.4693321178	entities
0.4693124083	compression techniques
0.4693043770	learning networks
0.4692993286	topic prediction
0.4692798681	automatic translations
0.4692289323	level representations
0.4691582557	describing
0.4691303783	contribution
0.4691129168	parsing tasks
0.4690795362	systems built
0.4690656581	local knowledge
0.4690504590	large vocabulary speech
0.4689895637	possibly
0.4688949379	tailored
0.4688409521	documents
0.4688374941	n gram overlap
0.4688056076	reliability
0.4687570603	generation capabilities
0.4687387074	based asr
0.4687381246	inferences
0.4687192361	my
0.4687187680	text format
0.4686426744	back off
0.4686272104	neural nlp
0.4686077415	classification techniques
0.4685777027	outcomes
0.4685712025	become increasingly popular
0.4685616349	benchmark tasks
0.4685319238	harmful
0.4685319238	trace
0.4685235399	ambiguous
0.4684955698	target text
0.4684843003	researchers
0.4684599317	multiple
0.4684442068	website
0.4684051101	experimentally demonstrate
0.4683694975	vanilla
0.4682668463	source document
0.4682355998	distances
0.4681583823	types
0.4681247284	research problems
0.4680728555	missing information
0.4680543147	approach obtains
0.4680379005	contrast
0.4680219990	time warping
0.4679973257	mrc datasets
0.4679798777	multilingual news
0.4679420633	\ rightarrow
0.4678581669	takes as input
0.4678536287	very deep convolutional
0.4678476998	currently
0.4678373117	human responses
0.4678291434	large transformer
0.4678104595	word level attention
0.4677612966	real world settings
0.4677499702	level
0.4677096235	structurally
0.4677096235	saving
0.4676534510	neural network based approaches
0.4676203317	remarkable performance
0.4675725631	let's
0.4675444519	malicious
0.4675431217	similar results
0.4675319238	infrequent
0.4675260954	hypergraph
0.4675260954	angle
0.4675257572	generation module
0.4674390336	datasets demonstrate
0.4674339723	learning multi
0.4674308779	ideas
0.4674241560	classification results
0.4673907576	based method
0.4673616510	conventional models
0.4673021427	translations
0.4672638104	time consuming
0.4672344059	interactively
0.4672274855	translation toolkit
0.4672083486	visualize
0.4671607715	transferring knowledge
0.4671562401	manual analysis
0.4671447313	improve performance
0.4670634896	intelligently
0.4670178808	groups
0.4669457456	encoder architecture
0.4668793691	thanks
0.4668727651	deep latent variable
0.4668570272	quantitative analysis
0.4668165528	correlates
0.4667961175	manage
0.4667869106	linear time
0.4667492980	nature
0.4667241594	claimed
0.4667105511	word level embeddings
0.4666977935	deliver
0.4666729929	neural representations
0.4666574152	long term goal
0.4666449022	higher correlation
0.4666426478	question detection
0.4666030251	dialog tasks
0.4665414278	fine grained information
0.4665001650	variable models
0.4664721142	course
0.4664694810	recover
0.4664683672	alternative approaches
0.4664556941	co reference
0.4664380453	abstractions
0.4664121082	previous models
0.4663903950	obtaining
0.4663808102	false
0.4662989805	high resource language
0.4661988744	presenting
0.4661742884	recognition task
0.4660255373	ner model
0.4659251627	entry
0.4659206504	servers
0.4659206504	struggles
0.4659206504	skewed
0.4659206504	compete
0.4659206504	hotels
0.4659206504	subsystems
0.4659206504	initializing
0.4659206504	conservative
0.4659206504	mere
0.4659206504	uncertainties
0.4659206504	speedups
0.4659206504	recovered
0.4659206504	paradigmatic
0.4659196091	gather
0.4658281324	bigrams
0.4657990863	lexical categories
0.4657779812	polynomial time
0.4657526340	supervised settings
0.4657442204	tuning stage
0.4657119517	important issue
0.4655325132	unsupervised representation
0.4655019347	observation
0.4654875499	iii
0.4654591803	preservation
0.4654241663	recommended
0.4654241663	cognates
0.4654180382	snippets
0.4653670323	pre training framework
0.4653393386	nevertheless
0.4653339178	review datasets
0.4652236532	automatic and human evaluation
0.4652030524	shifts
0.4651970658	less resourced
0.4651537583	parsing techniques
0.4651513498	learn
0.4651489314	supervised approach
0.4651416640	symmetry
0.4650979955	self supervised learning
0.4650531366	genuine
0.4650525944	paraphrases
0.4649887226	waveforms
0.4649492201	regions
0.4649233615	encountered
0.4649176350	modeling techniques
0.4648755401	intuition
0.4648657437	machine learning and deep learning
0.4648573671	posteriors
0.4648573671	subspaces
0.4648573671	sadness
0.4648573671	histories
0.4648295161	clean and noisy
0.4647756584	sub
0.4647237890	dynamically
0.4646920390	actions
0.4646707017	reconstruct
0.4646526152	validity
0.4646428245	limited labeled
0.4646048424	sequences
0.4645671694	institutions
0.4645671694	distortion
0.4645671694	disparate
0.4645413581	yes
0.4645274775	\ emph
0.4645029232	mismatch
0.4644718898	margin
0.4643735207	slots
0.4643563852	herein
0.4643326590	advances
0.4643234443	aspects
0.4642318765	question answering model
0.4640884547	shared latent
0.4640546548	segmentation errors
0.4639880776	word level features
0.4639696574	evaluation dataset
0.4639477442	search strategy
0.4639081180	large datasets
0.4639014100	sentiment analysis in twitter
0.4637965474	inability
0.4637629266	characterized
0.4636698976	imbalance problem
0.4636327684	human participants
0.4636084788	failed
0.4635278928	potential solution
0.4635139356	$ \ mathcal
0.4634981814	main
0.4634969851	self critical
0.4634883435	$ \ alpha
0.4634356236	natural language understanding tasks
0.4633874363	space models
0.4633034928	monolingual embeddings
0.4632990018	thousands
0.4632619713	model variants
0.4632618327	prediction network
0.4632417182	black box models
0.4632409863	current models
0.4632265983	numerous
0.4631538743	beside
0.4631413978	asr outputs
0.4631407669	annotations
0.4631381083	testset
0.4631381083	favourably
0.4631381083	distinguishable
0.4631381083	comply
0.4631381083	ruling
0.4631381083	infancy
0.4631381083	seminal
0.4631381083	underlies
0.4631381083	earliest
0.4631381083	violated
0.4631381083	v1.1
0.4631381083	absolutely
0.4631381083	inputting
0.4631381083	casts
0.4631381083	plagued
0.4631381083	underexplored
0.4631381083	stark
0.4631381083	v2.0
0.4631381083	3x
0.4631381083	definitive
0.4631381083	ported
0.4631381083	moved
0.4631381083	deaths
0.4631381083	denying
0.4631381083	motions
0.4631381083	professions
0.4631381083	inadvertently
0.4631381083	pools
0.4631381083	accelerates
0.4631381083	differentiates
0.4631381083	wearing
0.4631381083	reusability
0.4631381083	optimise
0.4631381083	responds
0.4631381083	finished
0.4631381083	observes
0.4631381083	handcrafting
0.4631381083	uninterpretable
0.4631381083	fueled
0.4631381083	parameterize
0.4631381083	flawed
0.4631381083	standardizing
0.4631381083	verbose
0.4631381083	aka
0.4631381083	contradicting
0.4631381083	disentangles
0.4631381083	contradict
0.4631381083	verses
0.4631381083	existent
0.4631381083	succeeds
0.4631381083	ehealth
0.4631381083	favoring
0.4631381083	subsampling
0.4631381083	consumed
0.4631381083	embodies
0.4631381083	linearized
0.4631381083	10th
0.4631381083	grades
0.4631381083	priorities
0.4631381083	formalise
0.4631381083	evoked
0.4631381083	categorizes
0.4631381083	optimistic
0.4631381083	shortening
0.4631381083	encompass
0.4631381083	minimized
0.4631381083	sensor
0.4631381083	showcasing
0.4631381083	selections
0.4631381083	deviate
0.4631381083	interdependencies
0.4631381083	disregarding
0.4631381083	consolidate
0.4631381083	resurgence
0.4631381083	webpage
0.4631381083	detectable
0.4631381083	syntagmatic
0.4631381083	uniformity
0.4631381083	literatures
0.4631381083	culturally
0.4631381083	parser's
0.4631381083	concordance
0.4631381083	annotates
0.4631381083	confused
0.4631381083	saturation
0.4631381083	supplemented
0.4631381083	agreed
0.4631295349	compresses
0.4631242413	testbed
0.4631148783	factor
0.4631079305	human computer
0.4630963003	pre trained language representation
0.4630650121	composed
0.4630275763	countries
0.4629954043	he
0.4629537889	rather
0.4628796025	detection accuracy
0.4628641864	continuing
0.4628641864	beating
0.4628641864	substructures
0.4628641864	justifications
0.4628641864	lag
0.4628641864	encounter
0.4628641864	apparently
0.4628641864	writer
0.4628641864	placement
0.4628641864	scanned
0.4628641864	shuffled
0.4628250141	ex
0.4627064713	order of magnitude
0.4626941587	clustered
0.4626877429	based algorithm
0.4626852550	condition
0.4626814522	panel
0.4626814522	explainers
0.4626814522	obfuscated
0.4626814522	discriminatory
0.4626814522	constants
0.4626814522	picked
0.4626814522	fluid
0.4626814128	zero resource speech
0.4626474325	prompt
0.4625960922	singular value
0.4624946630	nouns and verbs
0.4624678155	together
0.4624551068	simple algorithm
0.4624340895	first order
0.4624337352	hopefully
0.4624333977	heuristics
0.4624134032	textual context
0.4623483842	place
0.4623049711	line
0.4622989167	giza + +
0.4622617857	level of abstraction
0.4622081902	grained semantic
0.4621853257	adversarial domain
0.4621581585	structured attention
0.4621027258	categorize
0.4620908182	machine interaction
0.4620832642	inefficiency
0.4620832642	officially
0.4620832642	differentiability
0.4620832642	configured
0.4620832642	enjoyed
0.4620832642	registered
0.4620832642	manifests
0.4620832642	negativity
0.4620832642	cubic
0.4620832642	concatenates
0.4620832642	cohorts
0.4620832642	surveyed
0.4620832642	stops
0.4620832642	unveil
0.4620832642	depicts
0.4620832642	exercises
0.4620832642	poets
0.4620832642	enter
0.4620832642	refinements
0.4620832642	permitting
0.4620832642	manners
0.4620832642	males
0.4620832642	peculiarities
0.4620832642	arrival
0.4620832642	ameliorate
0.4620832642	17th
0.4620832642	censored
0.4620832642	cornerstone
0.4620832642	bypass
0.4620832642	adversely
0.4620832642	4x
0.4620832642	hindering
0.4620832642	radiologists
0.4620832642	hardest
0.4620832642	alters
0.4620832642	filterbank
0.4620832642	publishers
0.4620832642	misclassified
0.4620832642	postulate
0.4620832642	ecommerce
0.4620832642	bracket
0.4620832642	typicality
0.4620832642	combinatorially
0.4620832642	informing
0.4620832642	vectorized
0.4620832642	uneven
0.4620832642	nesting
0.4620832642	misclassification
0.4620832642	disadvantage
0.4620832642	reproduced
0.4620832642	bulk
0.4620832642	companion
0.4620832642	manifestations
0.4620832642	zones
0.4620832642	permit
0.4620832642	suffices
0.4620832642	8th
0.4620832642	uncovers
0.4620656029	seeing
0.4620554546	multimodal models
0.4619899574	spans
0.4619749050	relations
0.4619630071	she
0.4619465038	problematic
0.4618949805	proposed algorithm
0.4618869544	systems
0.4618726335	on wmt14 english german
0.4618513642	correspond
0.4617873951	predictor
0.4617873951	facilitating
0.4617703482	language processing tasks
0.4616974303	selectively
0.4616403459	domain specific word
0.4616298484	design
0.4616126993	embedding similarity
0.4616079377	coders
0.4616079377	saved
0.4616079377	2,000
0.4616079377	factually
0.4616079377	characterizations
0.4616079377	launched
0.4616079377	informs
0.4616079377	scraped
0.4616079377	coherently
0.4616079377	replicating
0.4616079377	exceptionally
0.4616079377	decent
0.4616079377	understandings
0.4616079377	consult
0.4616079377	conceptualize
0.4616079377	supportive
0.4616079377	sentence's
0.4616079377	executes
0.4616079377	tokenized
0.4616079377	adjusts
0.4616079377	clearer
0.4616079377	intentionally
0.4616079377	2x
0.4616079377	competitor
0.4616079377	compactly
0.4616079377	disregard
0.4616079377	separability
0.4616079377	substituting
0.4616079377	commonplace
0.4616079377	pressures
0.4616079377	sophistication
0.4616079377	coreferent
0.4616079377	suitably
0.4616079377	diagnosed
0.4616079377	prioritize
0.4616079377	assists
0.4616079377	year's
0.4616079377	geographically
0.4616079377	illustrative
0.4616079377	replying
0.4616079377	collaboratively
0.4616079377	specialist
0.4616079377	organised
0.4616079377	concluding
0.4616079377	occasionally
0.4616079377	flag
0.4616079377	20,000
0.4616079377	rewritten
0.4616079377	pursued
0.4616079377	parametrization
0.4616079377	transmit
0.4616079377	intense
0.4616079377	discursive
0.4616079377	compressions
0.4616079377	abbreviated
0.4616079377	contributors
0.4616079377	plugged
0.4616079377	pointed
0.4616079377	chose
0.4616079377	divides
0.4615997840	schedule
0.4615794108	mentions
0.4615585265	treats
0.4615257196	chinese machine reading
0.4615220328	parameterized
0.4615106725	empirical data
0.4614712841	codes
0.4614650110	relax
0.4614425828	if
0.4613773947	implementation
0.4613686892	distributional representation
0.4613355095	question answering system
0.4613115499	mode
0.4613012626	form
0.4612586085	conducted experiments
0.4612316242	open question
0.4611913818	synset
0.4611217297	combinations
0.4609920984	fill
0.4609913153	state tracking challenge
0.4609411589	operates
0.4609366663	diagnoses
0.4609366663	approximated
0.4609224958	multiple senses
0.4609203388	models require
0.4608600450	perfect accuracy
0.4608508723	nmt architecture
0.4608051390	varies
0.4607922202	aware sentence
0.4607597788	practical application
0.4607475206	compositions
0.4606571802	network language models
0.4606439797	prominence
0.4606439797	columns
0.4606439797	competitions
0.4606439797	complementing
0.4606439797	idiosyncratic
0.4606439797	continued
0.4606439797	orderings
0.4606439797	formulae
0.4606439797	prose
0.4606439797	contradiction
0.4606439797	smartphones
0.4606350193	lemmas
0.4606246591	multiple ways
0.4605840092	stakeholders
0.4605840092	6th
0.4605840092	investment
0.4605813325	extract information
0.4605447510	summaries produced
0.4605157246	noise
0.4604946630	products and services
0.4604434260	natural language inference task
0.4603297852	recognition evaluation
0.4602903121	criterion
0.4602724239	improve translation quality
0.4602557195	branches
0.4601801806	environment
0.4601635952	features
0.4601439327	c + +
0.4601209238	extracted information
0.4601135974	translation evaluation
0.4600892523	communities
0.4600468306	clustering techniques
0.4599649006	difficult problem
0.4599601604	besides
0.4599272849	natural language tasks
0.4598906623	effective solution
0.4598873995	detection approach
0.4598483747	image information
0.4598133572	sentence context
0.4597249035	textual representation
0.4596715239	scoring method
0.4596558541	operations
0.4595343308	level transformer
0.4594467052	factors
0.4594387433	estimators
0.4594387433	incidents
0.4594297029	specific topic
0.4594181241	underline
0.4593714392	non negative matrix
0.4593681681	overlapped
0.4593279963	semantics and pragmatics
0.4592597594	english and french
0.4592576977	one shot
0.4592544589	socio
0.4592544589	mismatched
0.4592514183	forecast
0.4592358919	emails
0.4592139262	domain data
0.4592060599	network layers
0.4592015773	complexities
0.4591325699	informative covid 19 english
0.4590728575	visual modalities
0.4590069906	english datasets
0.4589469346	proposed methods
0.4589412039	adjuncts
0.4588672517	existing models
0.4588660437	label prediction
0.4588599927	trained bert
0.4588117063	sentiment score
0.4587896778	neural sentence
0.4587201363	structured model
0.4587177621	learning multilingual
0.4587005616	applications
0.4586680711	coherence models
0.4586308412	sentiment labels
0.4586229589	model captures
0.4586113213	mining applications
0.4585679595	templates
0.4585522396	learning joint
0.4585465627	actively
0.4585465627	spreading
0.4585465627	primitive
0.4585424588	unsupervised speech
0.4584458732	constraints
0.4584210363	outperforms strong
0.4584022838	cross lingual embedding
0.4583603349	predicted
0.4583176403	significantly increases
0.4583014167	study
0.4582640872	formation
0.4582485449	abstractive model
0.4582461827	similarities and differences
0.4582408823	linkage
0.4582408823	browse
0.4582403483	unsupervised mt
0.4582252176	paradigm
0.4582185838	experimental data
0.4582181263	structured representation
0.4581998841	lesser
0.4581998841	parallelization
0.4581998841	merged
0.4581998841	adaptations
0.4581998841	manipulated
0.4581998841	conflicting
0.4581870283	observations
0.4581675710	parameter settings
0.4581585549	supervision for relation extraction
0.4581223943	nodes
0.4580755501	people's
0.4580699361	defects
0.4580699361	photos
0.4580699361	rows
0.4580699361	acknowledge
0.4580372496	real datasets
0.4580266747	entity linking model
0.4580180913	english and arabic
0.4580180659	concatenating
0.4580180659	shapes
0.4579839765	learning scheme
0.4579351692	decide
0.4579050958	you
0.4578884968	acoustic information
0.4578809129	word attention
0.4578492344	algorithms
0.4578450354	generated samples
0.4578417214	rise
0.4578318725	learning strategies
0.4578260959	degradation
0.4578188338	see
0.4577636476	conversion challenge
0.4576974303	assignments
0.4576574396	paired
0.4576381326	specific user
0.4575592594	pervasive
0.4575519529	themes
0.4575136016	existing research
0.4575134733	end to end st
0.4575130383	phrase based translation
0.4575046707	large amounts of unlabeled
0.4575033607	improved accuracy
0.4574922490	unlabelled
0.4574511256	media mining
0.4574084019	semantic class
0.4574004719	significant difference
0.4573885100	top 50
0.4573824470	requirements
0.4573786192	misleading
0.4573582553	period
0.4573582407	layer
0.4573453436	transform
0.4573272280	existing baselines
0.4573193039	extractive model
0.4573044884	human knowledge
0.4573036949	existing qa
0.4572991313	ranging
0.4572868444	structure learning
0.4572060083	wide applications
0.4571867815	summaries generated
0.4571712447	unclear
0.4571555535	related news
0.4571549551	marked
0.4570809638	model components
0.4570382886	processes
0.4569751969	kind
0.4569347565	network approach
0.4569209370	intermediate
0.4569030881	human supervision
0.4568936774	begin
0.4568679752	poses
0.4568647652	bilingual models
0.4568611196	learning domain
0.4568499704	entire
0.4567982247	including text
0.4567730182	end
0.4566987535	predicates
0.4565996201	speaker data
0.4565856235	linguistic insights
0.4565133230	digitized
0.4565133230	grounds
0.4565133230	throughput
0.4565133230	quantifies
0.4565133230	relevancy
0.4565133230	mentioning
0.4565133230	repeating
0.4563940503	p @ 1
0.4563734198	effective neural
0.4563487294	speech to speech translation
0.4563367001	losses
0.4563344180	lstm and gru
0.4563233886	independent
0.4561762850	considering
0.4561555991	overfitting
0.4561256222	entities mentioned
0.4561050797	experience
0.4560046808	prompts
0.4559703940	hierarchical recurrent neural
0.4559400035	unseen
0.4559096867	discourses
0.4558547022	additional knowledge
0.4558279963	researchers and practitioners
0.4557973518	aggregated
0.4557891373	\ vec \ textit
0.4557460594	neural network based methods
0.4556234243	thereafter
0.4555959978	taking advantage
0.4555577259	producing
0.4555309259	criteria
0.4555292916	written
0.4554817199	$ f_
0.4554792014	respect
0.4554566195	negative and neutral
0.4554390370	\ mathbf
0.4553949326	select
0.4553856525	online text
0.4553327101	buffer
0.4553327101	harmony
0.4553327101	denoting
0.4553327101	cursive
0.4553254174	extensive experiments demonstrate
0.4553233760	existing systems
0.4553057718	bert and roberta
0.4553026973	discriminate
0.4552987810	success
0.4552964538	na \
0.4552366492	automatically
0.4551933173	a deep learning approach
0.4551916417	zero shot learning
0.4551165689	learning ability
0.4550892245	crafted
0.4550673621	semantic patterns
0.4550511196	adaptation approach
0.4550193742	experimented
0.4550163992	society
0.4550126309	end to end asr
0.4549937828	invariant features
0.4549703093	attempting
0.4549211216	exceeding
0.4549054763	model called
0.4548497706	based video
0.4548440715	enormous amount
0.4548287143	multilingual knowledge
0.4548129287	arabic social
0.4547917885	compared
0.4547713085	curate
0.4547713085	entropies
0.4547713085	shuffling
0.4547713085	infants
0.4547713085	concurrently
0.4547713085	pushed
0.4547713085	met
0.4547713085	rounds
0.4547713085	synonymous
0.4547713085	usages
0.4547510847	bert and xlnet
0.4546858589	alignment task
0.4546273868	sets
0.4545175874	\ textit
0.4544843432	elmo and bert
0.4544615091	english french and english
0.4544614415	spoken utterances
0.4544451283	twitter datasets
0.4543619163	quantitative results
0.4542998313	small parallel
0.4542903639	making
0.4542899420	related events
0.4541831606	answers
0.4541710474	fmri
0.4540827903	existing tools
0.4540789813	main question
0.4540206745	pre trained bert model
0.4540079982	extracted
0.4539727751	standard dataset
0.4539673676	evaluation framework
0.4539671549	candidates
0.4539613971	baseline
0.4539228900	parsers trained
0.4538775778	lacks
0.4538403718	strengths
0.4538186980	structural representations
0.4538152620	transcript
0.4537254402	complete
0.4537043237	bodies
0.4537043237	envision
0.4536848234	based classifier
0.4536600355	neural morphological
0.4535681299	author name
0.4535435922	structures
0.4535420083	phases
0.4535099694	word embedding techniques
0.4534632159	simple baseline
0.4534389011	biasing
0.4534389011	opaque
0.4534389011	favors
0.4533934297	configuration
0.4533625230	information needed
0.4533527137	draw
0.4533476502	functionality
0.4533326364	mapped
0.4533260793	shot setting
0.4532928906	discovered
0.4532543727	user's
0.4532445155	learned latent
0.4532185645	learning rates
0.4531004231	online memory
0.4530824032	driven approach
0.4530669304	classes
0.4530234050	however
0.4529757501	ranking models
0.4529330394	alternative
0.4528878067	roughly
0.4528724476	significantly fewer
0.4528473747	domain corpora
0.4528244332	tune
0.4528203801	multi task neural
0.4528167002	downstream
0.4527868715	input length
0.4527774104	heuristically
0.4527774104	inflections
0.4527774104	tight
0.4527774104	recurring
0.4527774104	decidable
0.4527774104	coordinates
0.4527774104	enrollment
0.4527774104	reviewing
0.4527774104	sorted
0.4527774104	legitimate
0.4527774104	adaptability
0.4527598487	relationships
0.4527520883	contexts
0.4527503872	hypotheses
0.4527207014	caused
0.4527126761	outperforms conventional
0.4526200971	subject
0.4525647265	summaries
0.4525473092	contents
0.4525409672	now
0.4525337712	differs
0.4525304595	vast
0.4524373420	based grammar
0.4524348882	levels of abstraction
0.4524253639	measure
0.4522948340	particularly
0.4522593396	strategy
0.4522269155	wmt 2014 english
0.4521548852	variational encoder
0.4521270544	wide margin
0.4521153223	statistical techniques
0.4521074988	down
0.4520335996	wmt 2017
0.4520332231	bert style
0.4520214089	effectively incorporate
0.4520092536	model architecture
0.4519724166	achieves significant improvement
0.4519487201	approach
0.4519362529	resources
0.4519353400	question and answer
0.4519279069	scales
0.4519173353	pose
0.4518832519	alternative approach
0.4518413413	varying
0.4518350060	deployment
0.4518007592	tasks involving
0.4517586033	constituent words
0.4517355863	grammaticality
0.4517355863	segmented
0.4517355863	nominal
0.4516867018	teams
0.4515953389	based parser
0.4514696024	based retrieval
0.4513811582	preprocessing techniques
0.4513387804	achieves comparable performance
0.4513248465	contributed
0.4512797077	social events
0.4512651577	captioning model
0.4512575110	engagement
0.4512351891	character error
0.4512275376	word co occurrence
0.4512156820	simulation experiments
0.4511805747	axioms
0.4511720383	repeated
0.4511720383	corrected
0.4511513233	learned jointly
0.4511080094	based speech recognition
0.4510495686	multilingual bert model
0.4510475980	devlin et
0.4509614123	statistical information
0.4509258724	paper addresses
0.4509073329	surpasses
0.4508671058	specific questions
0.4508601309	theoretically
0.4508391755	productions
0.4508362275	emerge
0.4508134756	conll 2018
0.4508118239	great
0.4507951101	inconsistency
0.4507861132	dans
0.4507784659	sourced
0.4507691846	counterpart
0.4506660116	displays
0.4506660116	loosely
0.4506660116	cheaper
0.4506660116	pertinent
0.4506660116	bottlenecks
0.4506660116	universally
0.4506660116	exposes
0.4506660116	facilitated
0.4506660116	partitions
0.4506660116	uniquely
0.4506660116	delivering
0.4505869207	durations
0.4505869207	anonymous
0.4505869207	specialised
0.4505726435	states
0.4505527699	believed
0.4505376305	evaluation process
0.4505179019	transitions
0.4504907155	input data
0.4504291662	et
0.4503357859	adoption
0.4503350747	fillers
0.4503183538	automated metrics
0.4503115987	insights
0.4502797480	method significantly outperforms
0.4502485854	task oriented dialogue system
0.4502097648	views
0.4501926682	engineered
0.4501922940	hybrid neural
0.4501662282	linguistic feature
0.4501550551	english and spanish
0.4501507061	levels of granularity
0.4500934211	context dependent semantic
0.4500701506	simple word
0.4500627432	comparably
0.4500457144	suffers
0.4500430411	learning to rank
0.4500388381	machine translation tasks
0.4500083165	participants
0.4499857938	handcrafted
0.4499650611	analyzes
0.4499394659	positive and negative
0.4498353099	current systems
0.4498170022	developers
0.4497754726	meaningful representations
0.4497570611	news texts
0.4497305660	proposed model outperforms
0.4495585584	mbert
0.4495506718	nlp task
0.4495422842	interpretations
0.4495030647	phenomenon
0.4494819324	preventing
0.4494393602	ranking task
0.4494028483	achieve high
0.4493916011	data elements
0.4493697244	reaches
0.4493597855	relation specific
0.4493514119	preferences
0.4493295040	large parallel corpora
0.4492842452	real world dataset
0.4492372414	operate
0.4492365288	labeled
0.4492121837	encoded
0.4492116738	means
0.4491638718	impacts
0.4491397509	symbols
0.4491020768	output
0.4490995977	numbers
0.4490987794	undesired
0.4490987794	closes
0.4490987794	degenerate
0.4490987794	propagates
0.4490987794	excel
0.4490987794	supervise
0.4490987794	bands
0.4490987794	happening
0.4490987794	diagonal
0.4490987794	paves
0.4490987794	initiated
0.4490987794	prefixes
0.4490987794	50,000
0.4490987794	collapsing
0.4490987794	repertoire
0.4490987794	additions
0.4490987794	controllability
0.4490987794	modulate
0.4490987794	barely
0.4490987794	magnitudes
0.4490987794	garnered
0.4490987794	undertake
0.4490987794	fitted
0.4490987794	proactively
0.4490987794	venue
0.4490987794	competent
0.4490987794	upto
0.4490987794	10x
0.4490987794	stating
0.4490987794	buying
0.4490987794	reformulated
0.4490987794	discriminability
0.4490987794	hyponym
0.4490987794	dominates
0.4490987794	rankers
0.4490987794	harvested
0.4490987794	mines
0.4490987794	corpuses
0.4490987794	interdependent
0.4490987794	weigh
0.4490987794	folds
0.4490987794	marginally
0.4490987794	rejected
0.4490987794	parametrized
0.4490987794	corroborate
0.4490987794	unpredictable
0.4490987794	incur
0.4490987794	suffered
0.4490987794	plentiful
0.4490987794	undergo
0.4490987794	locates
0.4490987794	admits
0.4490987794	byproduct
0.4490987794	subgroups
0.4490987794	implying
0.4490987794	obey
0.4490987794	communicated
0.4490987794	undecidable
0.4490987794	evenly
0.4490987794	autonomously
0.4490987794	traction
0.4489657984	conventions
0.4489286008	absolute
0.4489140407	aside
0.4488740561	it's
0.4488712763	dominant
0.4488561142	original documents
0.4488504379	relate
0.4488466168	table to text
0.4487581439	participated
0.4487396691	related problems
0.4487086945	document information
0.4487080899	data to text
0.4487041741	text collection
0.4486268761	striking
0.4486110972	continuous vector
0.4485938408	necessarily
0.4485888815	hospitals
0.4485888815	calculations
0.4485888815	industries
0.4485888815	ungrammatical
0.4485507651	french translation tasks
0.4485119591	active research
0.4485056313	synthesize
0.4484755053	influence
0.4484488567	traditional text
0.4484427982	facts
0.4483818388	significantly outperforms existing
0.4483241062	articles published
0.4483023076	probabilities
0.4482857737	increasing
0.4482788321	the lambek grishin calculus
0.4482747996	equality
0.4482747996	questioner
0.4482747996	endeavor
0.4482747996	regularizers
0.4482747996	penalizes
0.4482747996	ups
0.4482747996	complicates
0.4482747996	lays
0.4482747996	sectors
0.4482747996	likelihoods
0.4482747996	groundings
0.4482747996	diverge
0.4482747996	encapsulate
0.4482747996	dominating
0.4482747996	categorisation
0.4482747996	experiencing
0.4482747996	overlook
0.4482747996	possesses
0.4482747996	classifier's
0.4482747996	suffice
0.4482747996	balances
0.4482747996	picks
0.4482747996	supplied
0.4482747996	emulate
0.4482747996	obscure
0.4482747996	simplifications
0.4482747996	noticed
0.4482747996	chronological
0.4482747996	alphabetic
0.4482747996	bear
0.4482747996	feel
0.4482747996	pour
0.4482747996	processors
0.4482747996	quadratically
0.4482747996	mislead
0.4482747996	relieve
0.4482747996	insufficiency
0.4482747996	hinge
0.4482747996	portals
0.4482747996	centroids
0.4482747996	restrictive
0.4482747996	initiate
0.4482747996	empowers
0.4482747996	partitioned
0.4482747996	unnormalized
0.4482747996	contrasted
0.4482747996	govern
0.4481503195	phonetic information
0.4481284434	erroneous
0.4481205318	implicitly
0.4481181263	computational modeling
0.4480598345	equivalents
0.4480598345	copied
0.4480071811	guaranteed
0.4480071111	main focus
0.4479545815	related topics
0.4479347301	tedious
0.4478734233	located
0.4478659679	fine tuned models
0.4478567460	maintaining
0.4478545779	world events
0.4478515322	wider
0.4478334466	word2vec embeddings
0.4477159237	improves
0.4477078055	wise
0.4476859060	alignment model
0.4476606390	special case
0.4475644789	based
0.4475550168	ambiguous word
0.4475480083	research effort
0.4475100187	recursively
0.4473669222	calculate
0.4473580632	extract entities
0.4473010270	recall and precision
0.4472553655	method achieved
0.4471886885	untrained
0.4471886885	concentration
0.4471886885	insertions
0.4471886885	specialists
0.4471886885	sender
0.4471805735	matching text
0.4471524196	decisions
0.4471210679	feedforward neural
0.4470419987	referred
0.4470280727	functions
0.4469942250	syntactical
0.4469459095	enrich
0.4469451469	important information
0.4469332485	upstream
0.4469332485	adjustment
0.4469332485	viewpoints
0.4469332485	roots
0.4469332485	jargon
0.4469206462	devise
0.4468964403	trained end to end
0.4468844047	orders
0.4468592650	held out
0.4468456135	your
0.4468326259	month
0.4468305617	annotated
0.4468173622	storage
0.4467878213	5,000
0.4467878213	irregular
0.4467878213	females
0.4467330044	cover
0.4466335820	automatic metrics and human
0.4466177122	beliefs
0.4465571497	multiple perspectives
0.4465458211	translation output
0.4465035816	morphologically rich language
0.4464259499	media content
0.4464146650	play
0.4463588368	5th
0.4463588368	hops
0.4463356064	inference algorithms
0.4463222649	100k
0.4462863084	filters
0.4462014039	spread
0.4461819988	significantly improving
0.4461609174	gradients
0.4461161844	formalism
0.4461137086	processing step
0.4460897920	repetition
0.4460620472	modify
0.4460529476	thousand
0.4460495883	prevent
0.4460441643	meant
0.4459832380	comprising
0.4459623586	real
0.4459174262	human cognitive
0.4459054597	each other's
0.4459000587	judgments
0.4458702772	frequencies
0.4458602947	q &
0.4458341810	similarity graph
0.4458318834	measures
0.4458199869	vulnerable to adversarial
0.4457589415	deep learning based models
0.4457574209	value
0.4457474280	patterns
0.4457197737	truly
0.4456844904	above mentioned
0.4456253371	learn embeddings
0.4456110097	equally
0.4456053450	speech recognition models
0.4455961051	schemata
0.4455961051	mismatches
0.4455704216	deploy
0.4455609947	session
0.4455574048	clinical information
0.4455422436	structured and unstructured
0.4455190867	capture long range
0.4455126042	arise
0.4454220219	format
0.4454061079	senone
0.4453573007	broader
0.4453186140	coverage
0.4453151153	combined
0.4453087968	official evaluation
0.4452751679	english and japanese
0.4452232911	automatically collected
0.4451837715	logarithmic
0.4451741575	collected data
0.4451741575	data collected
0.4451625818	developments
0.4450900829	combined approach
0.4450898465	learning methods
0.4450839039	perspectives
0.4450523099	decoding strategy
0.4450320371	asr and tts
0.4449865535	continues
0.4449031172	manually annotated data
0.4449019909	baselines
0.4448927392	document matching
0.4448819794	generative and discriminative
0.4448620569	model extends
0.4447495368	textual units
0.4447299431	parameterization
0.4447299431	opening
0.4447018263	good
0.4446960764	data diversity
0.4446790831	flexibility
0.4446204177	embedding approach
0.4446155943	analysis demonstrates
0.4445925619	streaming end to end
0.4445505741	sources
0.4445342777	approach combines
0.4445177514	explicit and implicit
0.4444266223	sheds light on
0.4443365971	sentences
0.4443044094	predictions
0.4441883128	semantic labels
0.4441723019	growing
0.4441608694	scarce
0.4441019335	addition
0.4440816938	proposed architecture
0.4439917614	stated
0.4439725133	intent and slot
0.4439533030	size
0.4439255898	additional
0.4438996139	traditional pipeline
0.4438827720	talk about
0.4438758381	care
0.4438398911	recommendation task
0.4438225133	temporal and causal
0.4437903892	pipeline systems
0.4437727979	mistakes
0.4437267229	comprehension systems
0.4437173211	lower
0.4436948337	relation classification task
0.4436569192	scratch
0.4435954451	search decoder
0.4435933949	rapidly
0.4435856346	goes beyond
0.4435288685	detect
0.4435119214	contributions
0.4434677435	differences
0.4433794181	costly and time consuming
0.4433764709	evolve
0.4433645432	these
0.4433020846	neural sequence to sequence models
0.4432951941	maximize
0.4432574628	concern
0.4432337236	clustering approach
0.4432154234	superiority
0.4431947371	constructed dataset
0.4431744179	twice
0.4431534255	specific embeddings
0.4431077861	arbitrarily
0.4430833553	answerable
0.4430833553	broken
0.4430316462	evaluates
0.4430285776	treated
0.4429999167	language processing systems
0.4429852131	recent models
0.4429360664	raw texts
0.4429311653	vision tasks
0.4429126067	soon
0.4428758354	property
0.4428591697	inference rules
0.4427917743	passed
0.4427766951	recognition tasks
0.4427763476	vowels
0.4427597698	add
0.4427486008	tackles
0.4427337148	model predicts
0.4426916873	updates
0.4426887133	irrelevant
0.4426490858	process
0.4426415160	difficulty
0.4425825201	decades
0.4424584216	architectures
0.4424545196	values
0.4424102371	analysis suggests
0.4423542572	again
0.4423361609	improvement
0.4423085126	novel coronavirus
0.4422423576	centered around
0.4422305971	language embedding
0.4422186219	half
0.4421967900	link prediction task
0.4421618288	anywhere
0.4421242437	learning visual
0.4421145645	shared news
0.4421045387	ongoing
0.4420735099	arrangement
0.4420735099	opened
0.4420735099	200k
0.4420735099	revolutionized
0.4420735099	bullying
0.4420735099	amplify
0.4420735099	exhaustively
0.4420735099	saves
0.4420735099	hurts
0.4420735099	tremendously
0.4420735099	utilise
0.4420735099	activated
0.4420735099	discarded
0.4420735099	sheer
0.4420735099	populated
0.4420735099	preprocessed
0.4420735099	utilised
0.4420735099	altered
0.4420735099	benign
0.4420735099	randomness
0.4420735099	revising
0.4420735099	mature
0.4420735099	softly
0.4420735099	method's
0.4420735099	extensibility
0.4420735099	sought
0.4420735099	initiatives
0.4420735099	retained
0.4420735099	facilities
0.4420735099	optional
0.4420735099	drastic
0.4420735099	returning
0.4420735099	augmentations
0.4420088007	limitation
0.4420039944	word units
0.4419874942	regard
0.4419733736	learned model
0.4419439476	highly similar
0.4419293372	extensive empirical
0.4419178636	generate
0.4419111381	classification approach
0.4419011067	correctly answer
0.4418777196	fully neural
0.4418635655	synthesis model
0.4418443010	occurrences
0.4418326734	field
0.4418320371	smt and nmt
0.4418319070	nlg models
0.4418280197	learning vector
0.4417502070	entries
0.4417018925	correct
0.4416998676	alignment mechanism
0.4416857035	long and short
0.4416833166	based loss
0.4416758010	depend
0.4416432423	calculated
0.4416265353	goals
0.4416134376	sensitivity
0.4415199892	builds upon
0.4415177130	end to end tts
0.4414972553	adjacent
0.4414955371	models achieve
0.4414765537	signal
0.4414641136	selection based
0.4414584489	n ^
0.4414059258	task learning
0.4413969202	specialized
0.4413889183	text to sql generation
0.4413108682	started
0.4413064712	cases
0.4412677624	learning setting
0.4412652343	obviously
0.4412464908	speech datasets
0.4412402078	covid 19 related
0.4411965103	minimize
0.4411823081	huge
0.4411755040	judgment
0.4411482709	specific word embeddings
0.4410225489	fashion
0.4409853910	algorithm
0.4409323782	self adaptive
0.4409126067	5.0
0.4408560826	perturbed
0.4408560826	superficial
0.4408560826	unobserved
0.4408560826	progresses
0.4408560826	terminologies
0.4408560826	deeply
0.4408560826	tightly
0.4408357402	disadvantages
0.4408216405	largest
0.4408046765	pairs
0.4407498187	learning procedure
0.4406433259	errors
0.4405215751	spanning
0.4405070037	ner task
0.4404946630	quantitative and qualitative
0.4404885197	area
0.4404496876	despite
0.4404175540	unimodal
0.4403722667	ranks
0.4403521606	human users
0.4403460142	consideration
0.4403037013	encoder representations from transformers
0.4402759388	tokens
0.4402498302	typically require
0.4401773360	\ em
0.4401433065	sessions
0.4401433065	guides
0.4401394692	omitted
0.4401394692	contiguous
0.4401117455	so
0.4401015322	subtle
0.4399215452	error patterns
0.4398915127	vision and language
0.4398730885	low resource neural
0.4397699571	explanatory
0.4397699571	misspellings
0.4397512749	multiple attention
0.4397509041	learned word
0.4397112785	former
0.4396949292	extent
0.4396920191	responses
0.4396836515	distributed representations of words
0.4396714961	switching data
0.4396616077	simplifies
0.4396559522	notion
0.4396442316	acquire
0.4396270969	neural transformer
0.4396094777	fixed
0.4395643342	word and character level
0.4395360845	based word
0.4395343035	accordingly
0.4394408572	because
0.4393787397	encoder decoder attention
0.4393339353	resulting models
0.4392868707	optimization framework
0.4392861952	mean squared
0.4392699571	players
0.4391787615	retraining
0.4391425743	depends
0.4391021530	label sequences
0.4390515688	students
0.4390388274	widespread
0.4390213846	affected
0.4390170976	channel
0.4390111139	complement
0.4389684471	wish
0.4389632716	driven approaches
0.4389226180	participant
0.4389093234	tree models
0.4388609072	adversarial nets
0.4388450706	accounts
0.4388135565	conflicts
0.4388122914	explain
0.4387989259	due
0.4387921406	ablations
0.4387665272	trend
0.4387183602	candidate
0.4386946281	epochs
0.4386907852	shot classification
0.4386614102	heavy
0.4386455106	end to end speech translation
0.4386362267	framework
0.4386351406	progress
0.4386191766	input image
0.4386138025	everyday
0.4385828983	objective and subjective
0.4385716146	accurately
0.4385700946	unsupervised models
0.4385318761	japanese to english machine
0.4385279076	clauses
0.4384662355	depending
0.4384494887	textual and visual
0.4384494089	reductions
0.4383723096	model generalizes
0.4382719179	supervised domain
0.4382631873	unsupervised topic
0.4382035804	fundamental tasks
0.4381708794	textual resources
0.4380889774	largely ignored
0.4380569988	linguistic annotations
0.4380531629	data representation
0.4380361572	right to left
0.4380354354	high quality speech
0.4379119850	balance
0.4379069157	surpassing
0.4379069157	writers
0.4378922975	assumed
0.4378027534	reference data
0.4377757192	increasingly complex
0.4377628194	attempted
0.4377474163	based query
0.4377451821	conditional language
0.4377178009	specific tasks
0.4377009852	$ _
0.4376517030	schedules
0.4376517030	utilises
0.4376517030	verifies
0.4376517030	cooperate
0.4376517030	50k
0.4376517030	designated
0.4376517030	openly
0.4376517030	impacted
0.4376517030	immense
0.4376517030	epoch
0.4376517030	pays
0.4376517030	rival
0.4376517030	mitigates
0.4376517030	professionally
0.4376517030	placing
0.4376517030	analyser
0.4376517030	issued
0.4376517030	text's
0.4376517030	straightforwardly
0.4376517030	transmitted
0.4376517030	behaviours
0.4376517030	audios
0.4376517030	upcoming
0.4376517030	200,000
0.4376517030	bills
0.4376517030	respects
0.4376517030	7th
0.4376517030	brittle
0.4376517030	understands
0.4376517030	operationalize
0.4376517030	entity's
0.4376517030	discard
0.4376517030	dominate
0.4376517030	tracked
0.4376517030	inherits
0.4376517030	deliberately
0.4376517030	underlie
0.4376517030	averages
0.4376517030	instruments
0.4376517030	machinery
0.4376516522	dramatically
0.4376184661	latency
0.4376181979	subgraphs
0.4376181979	reconstructed
0.4375920722	collecting data
0.4375745754	learning task
0.4375591025	network model
0.4375587508	accuracy
0.4375507714	gates
0.4375431758	retweets
0.4375431758	elicited
0.4375431758	penalizing
0.4375431758	timescales
0.4375411479	such
0.4374962757	typically
0.4373788872	neighbor search
0.4373574230	formed
0.4373384368	medical language
0.4373056578	continuous latent
0.4372956464	times
0.4372865681	existing techniques
0.4372720682	efficacy
0.4372688912	self attention network
0.4372321873	matches
0.4372267014	technologies
0.4372210243	linguistic quality
0.4370916824	natural language processing applications
0.4370660915	discovery and linking
0.4370525791	sentence level sentiment
0.4370497165	raw
0.4370386030	level semantics
0.4369702355	representative
0.4369287928	word or phrase
0.4369233507	parameters
0.4369129250	fully
0.4369066298	families
0.4368784675	entity based
0.4368347895	research problem
0.4368264118	monolingual and multilingual
0.4367507714	parsed
0.4367364983	language information retrieval
0.4366808470	theories
0.4366273628	approach significantly improves
0.4366115132	learning sentence
0.4365371254	millions
0.4365308624	bert based model
0.4364939769	sentence vector
0.4364373818	signals
0.4363531587	word embedding model
0.4363394567	among
0.4363242604	test dataset
0.4362354359	mechanism
0.4361536908	multi speaker speech
0.4361415624	expertise
0.4361070958	pronounced
0.4360941297	source model
0.4360390615	benchmark datasets demonstrate
0.4360285806	method shows
0.4360279076	enabled
0.4359895412	learning models
0.4359803182	modeling choices
0.4359665125	directions for future
0.4358735809	everything
0.4358733087	starts
0.4358725538	variational model
0.4358639185	related
0.4358418856	embeddings based
0.4358368118	carried
0.4358331868	simple vector
0.4358214308	fifth
0.4358073689	interesting patterns
0.4357825937	establish baseline
0.4357825228	discriminative information
0.4357685363	points
0.4357123766	based speaker
0.4357097446	converge
0.4357036569	connect
0.4356114029	data rich
0.4355214287	splits
0.4354946630	static and dynamic
0.4354706039	based emotion
0.4354681947	research question
0.4354565434	approach shows
0.4354350834	scarcity
0.4354002151	gender information
0.4353950595	options
0.4353922277	prediction performance
0.4353788414	scenario
0.4353345804	trains
0.4352172118	english german and english
0.4352009540	held out data
0.4351981280	attempts
0.4351861394	specific
0.4351792149	model quality
0.4351656870	highest
0.4351599181	10 fold cross
0.4351397000	high
0.4351122091	external
0.4351003718	feelings
0.4350116641	sequence to sequence models
0.4349893131	implicit information
0.4349838949	model pre training
0.4349743218	vital
0.4349730163	keep
0.4348634760	top 10
0.4348273356	optimized
0.4347532205	source documents
0.4347439983	machine learning and natural language
0.4347407617	interesting results
0.4346677531	proposed model achieves
0.4346663332	communicate
0.4346195753	her
0.4345960906	frequent
0.4345930442	incorporated
0.4345524310	relative performance
0.4344934704	clustering methods
0.4344692278	french and german
0.4344483509	higher levels
0.4344060111	boundaries
0.4344035397	activations
0.4343696652	data requirements
0.4342928352	passages
0.4342717799	opportunity
0.4342495162	connectivity
0.4342492482	word by word
0.4341936248	effort
0.4341836796	existing knowledge bases
0.4341725191	important features
0.4341551327	difficult
0.4341386425	as
0.4341125472	sampling method
0.4340947857	readily
0.4340283258	advantage
0.4339999019	maybe
0.4339523609	sentence level information
0.4338825937	generate realistic
0.4338173634	emerged
0.4337492787	problem
0.4337243989	pre training data
0.4337230279	supporting
0.4336785214	english to french translation
0.4336128914	many
0.4335467874	based attention
0.4335223734	feed
0.4335195358	back end
0.4334946630	qualitative and quantitative
0.4334627915	method performs
0.4334607575	transformer baseline
0.4334298460	learning and natural language processing
0.4334101233	representation models
0.4333376893	manual evaluations
0.4333364065	partial information
0.4331821364	serves
0.4331152125	advance research
0.4330279614	worked
0.4330279614	completing
0.4330279614	confident
0.4329230339	techniques
0.4329040819	popular word embedding
0.4328780824	infer
0.4328750315	accessibility
0.4328384794	answers to questions
0.4328374729	improve
0.4328097129	standard rnn
0.4327290041	practices
0.4326813856	positive results
0.4326745791	input
0.4326520915	standard words
0.4326431297	encouraging
0.4326386305	ranking tasks
0.4326154760	consisting
0.4326043342	final result
0.4325734851	prohibitively
0.4325713171	model matches
0.4325617983	remarkable
0.4325461115	purely
0.4325108047	twenty
0.4324999376	word to word
0.4324975127	one shot learning
0.4324603075	sub character
0.4323454842	based model
0.4323305787	paper
0.4323108086	human and machine
0.4323041980	hand
0.4323010417	machine learning tasks
0.4322681373	broad
0.4322676937	train models
0.4322535097	one
0.4321908659	categories
0.4321371693	model takes
0.4321327711	small training
0.4321182775	qualified
0.4321182775	emphasized
0.4321182775	sampler
0.4321182775	animation
0.4320514404	large corpus
0.4320294641	date
0.4319893736	warm
0.4319878357	metrics and human evaluations
0.4319670437	relevant
0.4319456837	ten
0.4319316398	speakers
0.4319056378	based systems
0.4318594525	bits
0.4318580178	intents
0.4318232921	really
0.4318056029	series
0.4317916921	research purposes
0.4317903785	preferred
0.4317673942	task for natural language
0.4317537333	correlations
0.4316821979	student learning
0.4316711066	word semantics
0.4316700418	zero shot transfer
0.4316597859	official
0.4316583714	closer
0.4316209698	prior information
0.4316012692	poor
0.4315979117	level supervision
0.4315926570	accents
0.4315444420	threats
0.4315444420	unbalanced
0.4315444420	usable
0.4315444420	conjecture
0.4315070014	model training
0.4314992607	p value
0.4314835147	analyses
0.4314449696	resourced
0.4313217778	parsing approaches
0.4313159591	attention based sequence to sequence
0.4312782066	abstractive models
0.4312142031	models exhibit
0.4311003718	generalizations
0.4310987886	multilingual model
0.4310739893	brings
0.4310562481	correlate
0.4310488449	drawn
0.4310477037	semantic evaluation
0.4310454331	influencing
0.4310454331	exceed
0.4310454331	motivations
0.4310454331	confirming
0.4310454331	centuries
0.4310413225	faster inference
0.4310265187	characteristics
0.4309870823	positions
0.4309578983	abstractive and extractive
0.4309244831	distributions
0.4309180779	evaluation results
0.4308443862	character based models
0.4307813724	popular metrics
0.4307508260	promote
0.4307050192	automatic annotation
0.4307029437	supervised classifier
0.4306966533	framework named
0.4306699490	versions
0.4306592514	stored
0.4306566686	knowledge inference
0.4305307515	masks
0.4305105435	capability
0.4305044181	expensive and time consuming
0.4305043076	there
0.4304301457	occurs
0.4303630505	larger dataset
0.4303477858	classification scheme
0.4303387996	processing and information retrieval
0.4302770409	based neural machine translation
0.4302542400	synthetic dataset
0.4302196839	salient features
0.4301061296	generated sentence
0.4300937833	called `
0.4300849561	side effect
0.4300198934	consuming and expensive
0.4300077761	rich language
0.4299923535	pre trained bidirectional
0.4299765066	computational model
0.4299636907	target based
0.4299604806	daily
0.4299503932	instances
0.4299110295	inherit
0.4299110295	synthesizes
0.4299110295	digitization
0.4299110295	injects
0.4299110295	consume
0.4299110295	assembled
0.4299110295	nontrivial
0.4299110295	curating
0.4299110295	markedly
0.4299110295	requisite
0.4299110295	affixes
0.4299110295	regularly
0.4299110295	transcribe
0.4299110295	notations
0.4299110295	searched
0.4299110295	prevailing
0.4299110295	associates
0.4299110295	replacements
0.4299110295	slowly
0.4299110295	exceptional
0.4299110295	explosive
0.4299110295	marginalized
0.4299110295	commonalities
0.4299110295	inevitable
0.4299110295	interprets
0.4299110295	avoided
0.4298754471	example
0.4298471847	pre trained contextualized
0.4297961880	data augmentation methods
0.4297130329	imply
0.4296881001	superior accuracy
0.4296855716	properties
0.4296640896	human efforts
0.4296500670	multilingual named
0.4296426210	t s
0.4296390813	proposed metrics
0.4296318074	act prediction
0.4296267249	target speakers
0.4296212189	difficult task
0.4296059367	copies
0.4295452284	goal
0.4294854676	annotation efforts
0.4294703250	gestures
0.4294254562	architectural
0.4293635001	tasks
0.4293457119	defined
0.4292772522	scalability
0.4292256961	requiring
0.4292019459	constituent and dependency
0.4291672379	capable
0.4291368824	unstructured information
0.4291308790	ratings
0.4290569321	behavior
0.4289933553	both worlds
0.4289704879	capacity
0.4289661256	stimuli
0.4289578983	humans and machines
0.4289408091	treat
0.4289045606	systematically study
0.4288986887	racist
0.4288914503	detectors
0.4288737138	automatic word
0.4287947279	outline
0.4287795453	unfamiliar
0.4287795453	seldom
0.4287795453	exchanges
0.4287795453	accumulate
0.4287795453	spend
0.4287795453	elucidate
0.4287795453	bearing
0.4287795453	renders
0.4287795453	turned
0.4287795453	recovers
0.4287795453	denote
0.4287795453	puts
0.4287795453	decodes
0.4287795453	rendered
0.4287795453	specifies
0.4287795453	surpassed
0.4287746202	popular datasets
0.4286956860	iteratively
0.4286351767	evidences
0.4286351767	advertisements
0.4285219665	inferred
0.4284195236	labeling tasks
0.4283908980	english and german
0.4283860800	\ pm
0.4283235910	revealed
0.4283054147	models fail
0.4282990777	matched
0.4282654185	test samples
0.4282584009	adapt
0.4280428530	fine grained entity
0.4280142935	diverse outputs
0.4279939684	machine translation system
0.4279261906	training signals
0.4279113387	open knowledge
0.4278760704	matrices
0.4278629266	behaviour
0.4277988449	assigned
0.4277915471	aware word
0.4277816018	figures
0.4277790253	task specific features
0.4277735030	neural network language
0.4277522902	faced
0.4277141555	expectations
0.4276823049	questions involving
0.4276799085	custom
0.4276622835	model leverages
0.4276505641	wide range
0.4276295776	model trained
0.4276252103	research focuses
0.4275978229	devices
0.4275875140	parts
0.4275332605	time spent
0.4274806567	allowed
0.4274189850	improvements
0.4274037041	intent detection and slot
0.4273922891	methodology
0.4273571475	attention neural
0.4273295168	acceptable
0.4272770454	without compromising
0.4272761431	styles
0.4272696988	proposed method outperforms
0.4272434850	multiple baselines
0.4272405635	positive effect
0.4271365429	must c
0.4271203788	ability to generalize
0.4271170849	focuses
0.4270852275	proposed method achieves
0.4270628561	the hebrew bible
0.4270017755	dataset shows
0.4269926300	scale study
0.4269877010	model significantly outperforms
0.4269677290	variant
0.4269657752	representation learning models
0.4269047638	domains
0.4268565629	beamformer
0.4268565629	capacities
0.4268565629	prices
0.4268397382	modified
0.4268333198	supervised and unsupervised
0.4268058789	sarcastic
0.4267273680	guarantee
0.4267199146	transferred
0.4267076441	deaf people
0.4266926591	wnut 2020 task 2
0.4266715672	researches
0.4265828983	subject and object
0.4265470028	entire input
0.4264845035	methods perform
0.4263437109	language modeling task
0.4262831330	updated
0.4262735199	multi task framework
0.4262564040	labeled and unlabeled
0.4262260395	paper examines
0.4262220024	costly
0.4261000285	estimates
0.4260774272	improves generalization
0.4260697530	distributed representation of words
0.4260637728	held
0.4260047116	serious
0.4259420716	extract
0.4259196958	phonemes
0.4259064855	clusters
0.4259054712	visit
0.4259054712	distorted
0.4258812751	extensive evaluation
0.4258713383	co occurrence patterns
0.4258605455	enhance
0.4258404767	bypassing
0.4258404767	underperform
0.4258404767	researcher
0.4258404767	inserting
0.4258404767	incorrectly
0.4258404767	illustrating
0.4258404767	exemplify
0.4258404767	challenged
0.4258404767	elusive
0.4258404767	excerpts
0.4258404767	satisfied
0.4258404767	wording
0.4258404767	benchmarked
0.4258404767	vectorial
0.4258404767	fruitful
0.4258404767	served
0.4258404767	steadily
0.4258404767	achievable
0.4257302070	questions require
0.4256548994	large scale real world
0.4256178340	remaining
0.4256005634	translation approach
0.4255871297	serve
0.4255758071	dimensions
0.4255454888	rich information
0.4254180745	off
0.4253638798	suffer
0.4253570633	level representation
0.4253544522	103
0.4253537094	objects
0.4253397318	datasets
0.4253260337	struggle
0.4252264586	answered
0.4252112357	manually
0.4252083907	potential
0.4251762994	recurrent architecture
0.4251601606	translation based
0.4251525294	ensure
0.4250829026	takes advantage
0.4250584382	based natural language processing
0.4250289039	yields improved
0.4250186990	asr task
0.4250166384	days
0.4249843550	reduce
0.4249761789	generalize
0.4249498384	raw corpus
0.4249487256	need
0.4249312991	m ^
0.4249140274	selection approach
0.4248828056	srl models
0.4248024286	multi task model
0.4247786249	style transfer tasks
0.4247771921	modules
0.4247042781	based question
0.4247006493	social groups
0.4246452091	$ gram
0.4246241344	updating
0.4246228461	the past decade
0.4246162827	tools
0.4246089680	similarity and relatedness
0.4246002024	fine tuning method
0.4245320431	final
0.4244856223	utterances
0.4244406543	internal structure
0.4244031236	fills
0.4244031236	invalid
0.4244031236	investors
0.4244031236	allocate
0.4244031236	enforced
0.4244031236	equip
0.4244031236	assesses
0.4244031236	substantive
0.4244031236	preferable
0.4244031236	lags
0.4244031236	topologies
0.4244031236	rigorously
0.4244031236	audiences
0.4244031236	simplistic
0.4244031236	conveniently
0.4244031236	recognised
0.4244031236	necessitates
0.4244031236	invaluable
0.4244031236	prompted
0.4244031236	thereof
0.4244031236	signaling
0.4244031236	uttered
0.4244031236	recommends
0.4244031236	conveying
0.4244031236	arranged
0.4244031236	proceeds
0.4243806600	the world wide web
0.4243736241	close
0.4243045397	neural retrieval
0.4242868280	seeks
0.4242334580	files
0.4241820932	related entities
0.4241754978	speech recognition system
0.4240570875	global and local
0.4240165030	significant
0.4239977656	compositional structure
0.4239547698	generate sentences
0.4239454228	ability
0.4239396602	entity recognition and relation extraction
0.4238864635	inspect
0.4238864635	retains
0.4238864635	neglecting
0.4238864635	join
0.4238864635	decline
0.4238864635	finetune
0.4238864635	instantiate
0.4238864635	smoothly
0.4238864635	collects
0.4238864635	noted
0.4238864635	borrowing
0.4238864635	discarding
0.4238864635	modifies
0.4238864635	resolves
0.4238864635	tion
0.4238864635	achievements
0.4238864635	suffixes
0.4238422070	deep representation
0.4238404979	speech tagger
0.4238074472	tool
0.4237876703	larger corpus
0.4237664503	option
0.4237527632	modifications
0.4237501763	improves translation
0.4237342601	subtask 2
0.4237137049	industry
0.4237021832	yields consistent
0.4236492286	tagged
0.4235632388	elements
0.4235210197	methods
0.4234251824	pre trained transformer based
0.4233610357	sufficient information
0.4233471043	processing and machine learning
0.4233348756	sequence tagging tasks
0.4232999826	disambiguation problem
0.4232883190	important
0.4232595706	sentiment and emotion
0.4232454977	outperforming previous
0.4232403676	test corpus
0.4232211195	oriented dialogue systems
0.4231661153	abundant
0.4231586462	method employs
0.4230970261	avoid
0.4230921833	schemes
0.4230566737	captured
0.4229982757	standard supervised
0.4229692873	issues
0.4229452727	using
0.4228907129	missing words
0.4228775099	2003
0.4228772418	generated
0.4228715807	content information
0.4228610290	64
0.4228361379	generate text
0.4227432705	suited
0.4227209973	trained model
0.4226252907	pandemic
0.4225972847	extraction model
0.4225926948	richer
0.4225895372	won
0.4225895372	accumulated
0.4225895372	impacting
0.4225895372	refines
0.4225895372	attested
0.4225895372	displayed
0.4225895372	harness
0.4225895372	decided
0.4225895372	recognizes
0.4225895372	founded
0.4225895372	hosted
0.4225895372	featured
0.4225895372	exponent
0.4225895372	formulates
0.4225895372	pave
0.4225895372	reverberation
0.4225895372	primitives
0.4225895372	chooses
0.4225895372	batches
0.4225718178	otherwise
0.4225497883	specific feature engineering
0.4225414333	labels
0.4224799730	specific language
0.4224230006	every day
0.4223894406	syntactic phenomena
0.4223751880	based encoder decoder model
0.4222926574	almost
0.4222521689	longer
0.4222515106	level classification
0.4222165468	transcribed
0.4222159193	want
0.4221985069	freely available
0.4221428291	key challenge
0.4221425622	street journal corpus
0.4221271151	unified approach
0.4221185744	widely spoken
0.4220561482	training method
0.4220410921	site
0.4219741551	chosen
0.4219062132	accessible
0.4218763103	intuitive
0.4218216769	$ \ lambda
0.4217883081	below
0.4217715741	automatic approach
0.4216160552	inserted
0.4216160552	partners
0.4216160552	incoherent
0.4216160552	falls
0.4216160552	decides
0.4216160552	deficiencies
0.4216160552	decreased
0.4216160552	prohibitive
0.4216160552	posit
0.4216160552	characterizes
0.4216160552	breaks
0.4216160552	compositionally
0.4216160552	visits
0.4216160552	optimum
0.4216160552	encompassing
0.4216160552	accepts
0.4216160552	language's
0.4216160552	depict
0.4216160552	gave
0.4216160552	backgrounds
0.4216160552	advantageous
0.4216160552	regularity
0.4215983192	involve
0.4215828983	extractive and abstractive
0.4215678124	except
0.4215590084	zero shot translation
0.4215503969	connections
0.4215441028	trade offs between
0.4215415344	usefulness
0.4215339731	method
0.4214911116	differ
0.4214646640	unlikely
0.4214227583	approach makes
0.4214185280	identification task
0.4212969066	processing task
0.4212963424	et al
0.4212336270	sites
0.4211790505	recent neural models
0.4211659254	major challenge
0.4211391969	we
0.4211124087	leaves
0.4211124087	enhancements
0.4211124087	ratios
0.4211124087	storing
0.4210808255	terms of f1 score
0.4210530423	lstm based models
0.4210277230	important task
0.4210246831	strong improvements
0.4210208250	original
0.4209685220	extraction tasks
0.4209171738	recognition model
0.4209162316	precision and recall
0.4208921031	embedding based entity
0.4208866992	performance
0.4208777735	machine learning and deep
0.4207831397	top k
0.4207395372	passes
0.4206782833	biomedical natural
0.4206780839	2004
0.4206333549	neural encoder
0.4206298018	disciplines
0.4206120461	medium
0.4205705086	phenomena
0.4205402843	seriously
0.4204665803	words with similar
0.4204660597	informative features
0.4204521331	target to source
0.4204478412	applicable
0.4204245670	translation results
0.4204129967	drugs
0.4203864901	sections
0.4203858415	levels
0.4203828563	decide whether
0.4203129841	hierarchically
0.4203129841	filtered
0.4202234693	ner tasks
0.4201487098	promising
0.4201219258	without losing
0.4201120634	main challenge
0.4200309185	91
0.4200250848	perfectly
0.4199862216	understand
0.4199565274	rare
0.4199158538	transforms
0.4199074400	opinions
0.4198948814	mimic human
0.4198606532	language analysis
0.4197761076	model sizes
0.4197746600	ignore
0.4197136640	tendency
0.4197019291	sampled
0.4196965330	semantical
0.4196965330	sensors
0.4196482475	ever
0.4196369877	idea
0.4195811986	greater
0.4195691877	parsing models
0.4195327739	standard neural
0.4195321521	markers
0.4195212800	semeval 2018
0.4194951259	repetitive
0.4194746428	bring significant
0.4194642369	recognized
0.4194493662	correctly
0.4193901294	selection in retrieval based
0.4193715637	processing techniques
0.4193098353	limited availability
0.4192813896	text mining tasks
0.4191388450	task called
0.4191313455	communicate with humans
0.4191169728	based dialogue
0.4190429987	nothing
0.4189828837	reduced
0.4187258897	elaborate
0.4187258897	cited
0.4187055108	large scale pre trained
0.4186963783	hindi and english
0.4186172189	entire text
0.4186130555	materials and methods
0.4185796633	derivations
0.4184770346	leaderboard
0.4184450832	segments
0.4184440597	disambiguation method
0.4184273201	label attention
0.4183994661	solely
0.4183966833	multi hop reading
0.4183397167	effective representation
0.4183274995	convey
0.4182715214	directions
0.4182432141	ive
0.4182326048	perform unsupervised
0.4182132029	adaptively
0.4182132029	membership
0.4182067702	abbreviations
0.4181728985	viewers
0.4181728985	conveys
0.4181728985	proceed
0.4181728985	imposes
0.4181728985	replaces
0.4181728985	overwhelming
0.4181728985	holding
0.4181728985	unchanged
0.4181728985	played
0.4181728985	unifies
0.4181728985	queried
0.4181728985	maintained
0.4181728985	unsolved
0.4181728985	exception
0.4181728985	unrealistic
0.4181728985	moderately
0.4181728985	download
0.4181728985	resembles
0.4181728985	concluded
0.4181728985	profound
0.4181728985	outlined
0.4181421695	lack
0.4181352698	runtime
0.4181092661	learn representations
0.4180992162	similarities
0.4180392501	crucial information
0.4179881271	inform
0.4179819104	structure analysis
0.4179804225	embedding association
0.4179622841	superior
0.4179576738	unsatisfactory
0.4179576738	ing
0.4179576738	converges
0.4179576738	continually
0.4179576738	rising
0.4179576738	planner
0.4179576738	a.k.a
0.4179576738	deemed
0.4179442671	article presents
0.4179291183	build
0.4178749497	user and product
0.4178667399	major languages
0.4178657375	scheme
0.4178258897	paper outlines
0.4178257170	summarization data
0.4177580349	complex systems
0.4177159551	robust models
0.4177070762	based decoding
0.4176686921	human evaluation metrics
0.4176303327	methods require
0.4175948988	costs
0.4175610888	semeval 2019 task
0.4174851943	eg
0.4174222843	produced
0.4173499241	sizable improvements over
0.4172132029	replies
0.4171797824	re weighting
0.4171794600	| y
0.4171674929	recognition results
0.4171340664	a large scale
0.4170667446	why
0.4170371199	85
0.4170180078	reinforcement learning approach
0.4169006523	leading
0.4167057341	drawbacks
0.4166674531	entailment models
0.4166322664	saying
0.4166261033	language knowledge
0.4166238180	fundamental
0.4166193010	perform extensive
0.4166110153	code to reproduce
0.4165514748	sufficiently
0.4165432419	step
0.4165283417	attention recently
0.4164941794	platforms such as twitter
0.4164812246	semantic model
0.4164351894	accessed
0.4164351894	carries
0.4164351894	routinely
0.4164351894	enriches
0.4164351894	unigrams
0.4164351894	locating
0.4164351894	reproduction
0.4164351894	injected
0.4164351894	fuses
0.4164351894	route
0.4164351894	promotes
0.4164351894	simulates
0.4164351894	eliminated
0.4164351894	reused
0.4164351894	enforces
0.4164351894	adults
0.4164351894	separates
0.4164330594	remarkable results
0.4164265430	sequence to sequence learning
0.4164109414	posted
0.4164020564	demands
0.4163454221	formulated
0.4163112107	target translation
0.4162749507	our
0.4162252773	boost
0.4161845253	input representation
0.4161553161	conducted extensive
0.4161241945	consuming
0.4161124396	selection problem
0.4160998724	art methods
0.4160630449	spanish and english
0.4160354404	determined
0.4160210918	fine tuning approach
0.4160078822	application domain
0.4160044357	language data
0.4159581098	benefits
0.4158768611	during
0.4158404835	adopt
0.4157683304	dataset collected
0.4157334830	labelling tasks
0.4157146501	approaches
0.4157074066	strategies
0.4156943006	the winograd schema challenge
0.4156465672	technology challenge
0.4156056325	tasks including
0.4156033765	rnn based model
0.4156023448	achieving high
0.4155167518	qg model
0.4155122501	source texts
0.4154954843	approach enables
0.4154931669	examines
0.4154091182	significant success
0.4153893239	rule based methods
0.4153232133	learning setup
0.4152943605	the voynich manuscript
0.4152939012	antecedents
0.4152822484	sizes
0.4152630968	layers
0.4152114892	lstm based model
0.4151335299	individual components
0.4151062132	exciting
0.4150697167	anaphoric
0.4150647175	consistently improve
0.4150492005	mentioned
0.4150485583	issue
0.4149649816	word level information
0.4149462232	domain examples
0.4149193697	lengths
0.4148972814	model's
0.4148350751	cnn architecture
0.4148316403	overcome
0.4148287328	large scale training
0.4148109392	expand
0.4147819170	ability to capture
0.4147220203	clear
0.4147219920	deep text
0.4145900470	german translation tasks
0.4145516356	image dataset
0.4145484202	constructions
0.4145376289	2005
0.4145358859	arbitrary
0.4145299095	prone
0.4144894968	samples
0.4144662937	effectiveness
0.4143875699	relative error
0.4143584383	operators
0.4143527964	implementations
0.4143497030	non
0.4143445298	stronger
0.4143319630	surrounding
0.4143283883	y |
0.4142876762	capture semantic
0.4142659313	text and speech
0.4142605955	fine grained evaluation
0.4142471876	relation extraction tasks
0.4142057747	optimization algorithm
0.4141384577	formal representation
0.4140212205	similar
0.4139860608	asr and mt
0.4139774960	limited
0.4139540390	powerful
0.4139354350	word sense disambiguation using
0.4139259414	prior research
0.4138362105	2d
0.4138125349	fast and accurate
0.4137932515	mean average
0.4137829102	graph based approach
0.4137680658	human agents
0.4137336939	editions
0.4137191089	neural semantic role
0.4136656970	word in context
0.4136486467	research studies
0.4136088552	modeling language
0.4135993215	generated speech
0.4135925018	extract salient
0.4135739651	supervised model
0.4135322767	significant advances
0.4134961182	citizens
0.4134961182	heterogeneity
0.4134796495	entails
0.4134306334	linguistic elements
0.4134043092	classifications
0.4132464688	utilizes
0.4132276116	closely
0.4131967653	cloze style reading
0.4131615028	relies
0.4131565221	straightforward
0.4131266464	related data
0.4131215811	source to target
0.4131132581	respective
0.4131004379	unidirectional
0.4130667141	final prediction
0.4130081924	do not necessarily
0.4129924153	variants
0.4129815551	validates
0.4129815551	enjoys
0.4129815551	interlocutors
0.4129815551	encompasses
0.4129815551	detrimental
0.4129815551	permits
0.4129815551	albeit
0.4129815551	calculates
0.4129815551	derives
0.4129815551	incoming
0.4129815551	intuitions
0.4129815551	overly
0.4129815551	demanding
0.4129251216	10k
0.4128623325	end to end dialog
0.4127523171	alleviate
0.4127460254	cross lingual text
0.4127314012	vertices
0.4127265550	influences
0.4126887990	labeled sentences
0.4126671270	simple but effective
0.4126519853	core component
0.4126194543	unified architecture
0.4125822792	toolkits
0.4125822792	prefer
0.4125587864	specific representations
0.4124333057	language generation tasks
0.4124194666	media streams
0.4124118778	training models
0.4124033992	sophisticated
0.4123736379	results achieved
0.4123641253	hope
0.4123310877	time
0.4123274808	2009
0.4123028542	guarantees
0.4123028542	strictly
0.4122823070	predicts
0.4122529191	rely
0.4121319072	attractive
0.4120791542	maintain
0.4120738384	generated sentences
0.4120589680	aspect and opinion
0.4120179405	large scale text
0.4120155511	based representations
0.4119971840	spatial and temporal
0.4119654240	directly applicable
0.4119241775	inevitably
0.4119051781	words or phrases
0.4119013947	learning embeddings
0.4118860967	slu models
0.4118676483	amr to text
0.4118194790	individuals
0.4117831206	learned representation
0.4117781652	formal model
0.4117539448	reads
0.4117337083	through
0.4116035192	answering task
0.4115928245	heavily
0.4115560046	feasible
0.4115271464	pre trained bert models
0.4114968683	data types
0.4114902783	builds
0.4114641602	processing models
0.4114371297	leads
0.4114367063	p @
0.4113923319	network features
0.4113779431	at
0.4113543939	learned features
0.4112945189	large scale multilingual
0.4112735867	training datasets
0.4112668190	represent words
0.4112578563	unclear whether
0.4112547598	scores
0.4112321253	researched
0.4112321253	receiving
0.4112321253	satisfies
0.4112321253	hurt
0.4112321253	conducts
0.4112321253	lose
0.4112321253	100,000
0.4112321253	reject
0.4112321253	purchase
0.4112321253	unstable
0.4112321253	restricts
0.4112304870	labeling task
0.4112210916	virtually
0.4112210916	rendering
0.4112210916	uninformative
0.4112210916	physicians
0.4112210916	grown
0.4112210916	optimizations
0.4112210916	filled
0.4112210916	scattered
0.4112210916	ends
0.4112210916	workflows
0.4112210916	manifest
0.4112210916	retrieves
0.4112210916	favorable
0.4112210916	hypothesized
0.4112175999	text to image
0.4111640007	problems
0.4111527389	trained transformers
0.4111386137	simplicity
0.4111296749	regularities
0.4111208622	acoustic speech
0.4111064868	parser achieves
0.4110872272	slow
0.4110435269	a hybrid approach
0.4110416270	past and future
0.4110073633	getting
0.4109691199	model relies
0.4109690136	induce
0.4109614584	simply
0.4109338788	reinforcement learning algorithms
0.4108644739	summarization approaches
0.4108611853	retrieved
0.4108480298	strong baseline models
0.4107519144	past few years
0.4107516617	train
0.4107377012	hybrid systems
0.4107222624	along
0.4106993652	proposed
0.4106878316	human dialogue
0.4105822792	analysts
0.4103921411	\ mu
0.4103771933	pre trained contextual
0.4103541442	learned
0.4102916836	geographical
0.4102598689	whenever
0.4101894900	mitigate
0.4101780327	authors
0.4101369771	people
0.4101259912	qualitative results
0.4101142113	inferior
0.4101077969	learning method
0.4100823372	extracts
0.4100676856	domain model
0.4100349835	attention based recurrent
0.4099916164	questions and answers
0.4099045867	stage training
0.4098638362	input documents
0.4098573613	sufficient data
0.4098266766	authentic
0.4097643990	advance
0.4097284621	speed
0.4097134108	ours
0.4096923013	encoding and decoding
0.4096769954	looks
0.4096243279	exhibit
0.4095957093	achieve
0.4095911063	variations
0.4095887064	offer
0.4095747247	involving
0.4095740601	times faster than
0.4095511696	remove
0.4095370743	users
0.4095139561	desired
0.4094980578	primary
0.4094818044	english and chinese
0.4094222843	gains
0.4094154179	classify
0.4094031489	step forward
0.4093546884	conversation dataset
0.4093067724	trained solely
0.4092581978	reproduce
0.4092171557	generated content
0.4091882143	urgent need
0.4091157551	without needing
0.4090947389	regardless
0.4089832603	embedding based methods
0.4089416164	entities and relationships
0.4088647929	achieving
0.4088280608	scarcity problem
0.4087686853	popular methods
0.4087686251	achieves
0.4087402282	sub word units
0.4087348389	feeding
0.4087348389	ensuring
0.4087077333	reading comprehension tasks
0.4086294604	months
0.4086278912	determine
0.4086263506	embedding approaches
0.4085846926	applications of natural language
0.4085734112	implied
0.4085734112	summarizers
0.4085734112	contrasting
0.4085734112	ingredients
0.4085301681	too much
0.4085124078	1.3
0.4085028895	obtain
0.4084940470	variables
0.4084666213	assign
0.4084108531	evaluation demonstrates
0.4083781057	semeval 2020 task 1
0.4083667852	recorded
0.4083505036	relevant features
0.4083133565	applicability
0.4082937634	appears
0.4082824172	chunks
0.4082707161	applies
0.4082295436	forms
0.4081688257	additional experiments
0.4081653644	cc by
0.4081428193	dataset specific
0.4081391845	f1 =
0.4081005419	higher performance
0.4080857114	traces
0.4080663467	unique
0.4080454108	dissimilar
0.4080330000	proposed approach outperforms
0.4080120226	empirically compare
0.4080028223	submissions
0.4079754811	individual word
0.4079257451	preserve
0.4078742708	exploits
0.4078363455	end to end approaches
0.4078264838	identical
0.4077904659	outperforms previous methods
0.4077716596	2.1
0.4077582695	distinguish
0.4077570881	optimize
0.4077002331	present results
0.4076604023	smaller
0.4076267031	decreasing
0.4076138183	natural language statements
0.4076105031	achieve high accuracy
0.4076008223	enables users
0.4075994039	laborious
0.4075994039	aggregates
0.4075994039	4th
0.4075994039	eliminates
0.4075994039	1,000
0.4075994039	older
0.4075994039	resemble
0.4075994039	decomposes
0.4075994039	expresses
0.4075838223	classifier trained
0.4075649564	fields
0.4075494324	challenging
0.4075428349	contextual words
0.4075180834	dimensional representation
0.4075111069	recently achieved
0.4075070036	fold cross
0.4074857505	outperforms existing methods
0.4074727284	a
0.4074498691	yields substantial
0.4074255467	image and question
0.4073714575	outperforms
0.4073686126	similarity and analogy
0.4072577151	works
0.4071568055	policies
0.4070922211	sequence modeling tasks
0.4070612040	proposing
0.4070574304	overlooked
0.4070303208	complex models
0.4069784899	computable
0.4069782264	end to end automatic speech
0.4068832433	daily basis
0.4068504595	quantitatively
0.4068278928	large collections of documents
0.4066931095	video and language
0.4066890004	accurate results
0.4066555370	term based
0.4065822268	model reaches
0.4065234199	produce
0.4064412518	target dataset
0.4064106546	websites
0.4063654183	non local
0.4063599745	considers
0.4063455980	possibility
0.4062968574	independently
0.4062495916	sentence and document
0.4062027304	traditional word
0.4061803686	supervised pretraining
0.4061642708	formulate
0.4060528332	what
0.4060134092	propose two methods
0.4060115494	represents
0.4059677056	processing model
0.4059459850	e2e models
0.4059263097	transcriptions
0.4058607270	competitive
0.4058338568	curated dataset
0.4057666434	undesirable
0.4057332863	faster training
0.4056594813	contributes
0.4056493317	one hot
0.4056362861	trajectories
0.4055572249	network based models
0.4054824368	later
0.4054242377	augment
0.4054188931	larger datasets
0.4053678978	training phase
0.4052901542	perform on par
0.4052860246	choice
0.4052792656	compare
0.4052052555	top performing
0.4051871219	ignoring
0.4051677868	solution
0.4051007180	the
0.4050940441	carried out
0.4050858872	comprehensive overview
0.4050610052	contextual semantic
0.4050392218	systems perform
0.4050039798	solve
0.4049612139	approach learns
0.4049396233	attention based neural machine translation
0.4048927379	insufficient
0.4048868547	integrate
0.4048432107	regards
0.4048258457	natural language processing and machine
0.4048057385	multilingual machine
0.4047884925	`
0.4047771270	aim
0.4047429673	data processing
0.4047157701	inherently
0.4046343025	jointly model
0.4045432554	shortcomings
0.4045411031	multiple levels
0.4045373925	language understanding task
0.4043947898	language query
0.4043557926	highlights
0.4042798369	multilingual contextual
0.4042271636	according
0.4041885382	current natural
0.4041608525	expensive
0.4041149780	feature structure
0.4040816548	separately
0.4040160000	multiple dimensions
0.4039238897	acquired
0.4039084299	incorporating domain
0.4038882085	understood
0.4038814478	1.1
0.4038235941	compares
0.4038022355	identifiers
0.4037987369	back
0.4036791182	learned knowledge
0.4036716581	based solutions
0.4036627277	bring
0.4036375528	challenging tasks
0.4036253938	encourage
0.4036151702	bill
0.4035122258	detected
0.4034866198	con
0.4034837264	variational neural
0.4034744351	address
0.4034532496	summarizes
0.4034525993	modeled
0.4034471278	column
0.4033738408	biomedical information
0.4033431146	turn
0.4033421981	code and pre trained models
0.4033138946	encoder decoder neural
0.4033118232	repeatedly
0.4033118232	cumbersome
0.4033118232	slower
0.4033118232	understudied
0.4033118232	evolves
0.4033118232	occurred
0.4033118232	10,000
0.4033118232	apparent
0.4033118232	complements
0.4033118232	dates
0.4033118232	releasing
0.4033118232	competitors
0.4032631672	part of speech
0.4032606889	p =
0.4032380235	challenging problems
0.4032376866	issue by proposing
0.4032096710	links
0.4031782872	quantities
0.4031342220	stores
0.4031342220	vastly
0.4031342220	emerges
0.4030739809	solutions
0.4030583626	mathematical framework
0.4029157037	context features
0.4028464674	leverages
0.4027584849	leave
0.4027584849	transfers
0.4027409434	news documents
0.4027371256	human to human
0.4027026193	analyse
0.4026848542	effectively
0.4025734112	famous
0.4025734112	lay
0.4025315339	giving
0.4025207394	approach called
0.4024572904	implies
0.4024244259	character and word
0.4024242190	essential
0.4023985766	sequential models
0.4023806723	graph based neural
0.4023785201	system
0.4023765798	inputs
0.4023644114	provided
0.4023414658	recent transformer
0.4023413663	valid
0.4023237444	hold
0.4023075617	extra
0.4022986749	evaluation data
0.4022523013	until now
0.4022453243	proposed metric
0.4022357268	behaviors
0.4021418375	perform
0.4021144254	end to end automatic speech recognition
0.4021114933	method learns
0.4020993505	semantic and pragmatic
0.4020555287	without hurting
0.4020427298	baseline performance
0.4020384773	happen
0.4020140676	released
0.4018801150	internal
0.4018382907	31
0.4018252299	learning signal
0.4018168246	approach performs
0.4017596954	another
0.4017442313	interest
0.4017034111	conversational setting
0.4017016002	limitations
0.4016880449	evaluate and compare
0.4016720702	full
0.4016334643	classification dataset
0.4016247341	annotators
0.4016074137	media text
0.4015509268	salient
0.4015508820	review texts
0.4015498399	related methods
0.4015247836	noisy input
0.4014257449	theoretical results
0.4014154550	based embedding
0.4013967898	effectively model
0.4013832384	ill
0.4013831790	randomly
0.4013676189	including bert
0.4013592062	selection task
0.4013458716	hour
0.4013382571	trained network
0.4012814133	based architecture
0.4012493630	networks learn
0.4012336286	gain insight into
0.4012063800	shared task dataset
0.4011859511	methods achieve
0.4011719890	tackle
0.4011240417	dimensional
0.4010583626	provide detailed
0.4010399635	weaknesses
0.4010125381	neural network classifier
0.4010021601	choose
0.4009715466	chinese named entity
0.4009053068	turns
0.4008962104	assume
0.4008891401	specific words
0.4008461838	dimensional spaces
0.4008386463	efficiently
0.4008347091	model structure
0.4007643863	video to text
0.4007466116	ranking method
0.4007216223	a large scale chinese
0.4007142672	indices
0.4007027220	running
0.4005435748	approach achieved
0.4005340283	models built
0.4004260132	word and character
0.4004052089	static word
0.4003292950	w.r.t
0.4003292950	maintains
0.4003292950	comprise
0.4003292950	reviewed
0.4003292950	impractical
0.4003173838	directly
0.4002967671	approach significantly outperforms
0.4002225023	individual
0.4002084091	strings
0.4002034654	aware semantic
0.4001090059	employs
0.4000842244	multiple choice reading
0.4000756795	learning neural
0.4000755687	consists
0.4000658882	automatically classify
0.4000417302	impressive
0.4000331115	speech dataset
0.3999782705	1.0
0.3999485495	text pairs
0.3999048778	controls
0.3999048778	investigations
0.3998351422	unless
0.3998340537	challenging dataset
0.3998085658	proposed models
0.3997806961	pretrained word
0.3997526271	output sentences
0.3997450371	provide additional
0.3997051821	significantly
0.3996737371	hybrid dnn
0.3996579421	morphological structure
0.3996482081	potentially
0.3996144566	correlation with human
0.3995857052	exhibited
0.3995857052	bridges
0.3995857052	approximates
0.3995857052	circumstances
0.3995857052	remained
0.3995857052	visualized
0.3995857052	restrict
0.3995857052	weaker
0.3995857052	one's
0.3995857052	fits
0.3995830417	unseen classes
0.3995759012	labeling problem
0.3995520071	natural language input
0.3994935071	framework called
0.3994599610	posting
0.3994416164	recurrent and convolutional
0.3994104381	based transformer
0.3993988610	illustrate
0.3993866873	automated translation
0.3993518193	occur
0.3993333829	language descriptions
0.3992803051	suggested
0.3992801919	ineffective
0.3992801919	functionalities
0.3992658416	training data size
0.3992219850	inference task
0.3991966674	measurements
0.3991467632	considered
0.3991027099	single and multi
0.3990770520	structured prediction tasks
0.3990596017	involved
0.3990453093	formally
0.3990293828	conditional text
0.3989580031	reduction techniques
0.3989546704	biomedical and clinical
0.3989088030	homogeneous
0.3988812480	automatic machine
0.3988687692	quantified
0.3988687692	methodological
0.3988190799	downstream performance
0.3987939528	sequence to sequence model
0.3987128880	who
0.3986875815	included
0.3986795580	proposed framework outperforms
0.3986714868	text and image
0.3985653455	previous systems
0.3985400299	modalities
0.3985175883	image and text
0.3984906367	direction
0.3984662148	task specific models
0.3984449952	performs
0.3984232108	transfer tasks
0.3983496899	important factors
0.3983429972	setup
0.3983420006	occurrence
0.3983088030	immediately
0.3983088030	moves
0.3983088030	unnecessary
0.3983005371	convergence
0.3981355610	module
0.3981327284	it
0.3981142009	level recurrent neural
0.3981104190	public benchmarks
0.3980413914	suggests
0.3980402309	trained
0.3980016040	attitudes
0.3979727405	briefly
0.3979723243	expensive to collect
0.3978200040	cross lingual tasks
0.3977997402	search decoding
0.3977832001	word co occurrences
0.3977253391	part
0.3976722529	inference based
0.3976189084	information extraction tasks
0.3976187826	level encoder
0.3975532810	architecture achieves
0.3975477809	report results
0.3975033371	comprises
0.3974481602	ranked
0.3974416164	cnn and lstm
0.3974042210	benefit
0.3973060656	generalizes
0.3972980234	zhang et
0.3972960508	components
0.3972570570	fine tuned model
0.3972443712	beats
0.3972305476	standard benchmark datasets
0.3972202421	speech and language processing
0.3972112120	identified
0.3972026578	large scale labeled
0.3971147457	simultaneously
0.3970468920	systematic study
0.3970372060	short term memory networks
0.3970329823	neural framework
0.3970250894	word based models
0.3970045810	fails
0.3970008051	learning cross lingual
0.3969936150	encodes
0.3969250113	non standard
0.3968832037	word features
0.3968702423	left to right
0.3968682705	learning problems
0.3968463694	sufficient
0.3968108293	technique
0.3967318946	method leverages
0.3966805247	unseen relations
0.3966803806	large amounts
0.3966580753	\
0.3966543823	accurate and efficient
0.3965982198	highly
0.3965734689	results support
0.3965715015	weights
0.3965655745	recall @
0.3965648786	growing body of
0.3964761853	experiments on real world datasets
0.3964646933	spans of text
0.3964617406	parsing performance
0.3964340022	translation tasks demonstrate
0.3964237498	deployed
0.3963771122	prepared
0.3963771122	obstacles
0.3963639795	existing benchmarks
0.3963512489	interests
0.3963367385	extremely large
0.3963023182	context model
0.3962723593	final answer
0.3961751685	evaluation study
0.3961193211	substantial
0.3961107137	estimate
0.3960441932	task training
0.3960201790	poorly
0.3960159252	speech segment
0.3959812785	relevant text
0.3959724610	evident
0.3959724610	emphasizes
0.3959724610	completed
0.3959724610	documented
0.3959724610	possess
0.3959724610	speeds
0.3959377644	label classification
0.3959369616	aware multi
0.3959058998	result
0.3958828687	newly
0.3958712987	major problems
0.3958218468	lacking
0.3956775537	attracted
0.3956722566	aware bert
0.3956629507	tasks require
0.3956628265	neighboring
0.3956556138	resulting
0.3956097398	pipeline approach
0.3954890439	got
0.3954187334	identify
0.3954151718	local and global
0.3953883297	35
0.3953837307	unsupervised method
0.3953645228	seek
0.3953439963	r =
0.3952907237	well established
0.3952757195	x vectors
0.3952099374	rankings
0.3951538033	based summarization
0.3951459249	point improvement
0.3951252210	extraction method
0.3951231382	everyone
0.3951065683	reported results
0.3950981829	dedicated
0.3950413978	speech and text
0.3950271005	aware language
0.3950049521	supervised systems
0.3949945122	500
0.3949813162	text mining applications
0.3949663048	novel
0.3949057158	summarize
0.3948499211	attention based methods
0.3947445347	an open source toolkit
0.3947319747	existing literature
0.3947279013	leaving
0.3946982464	end to end neural
0.3946628265	penalty
0.3946628265	understandable
0.3946628265	facing
0.3946628265	carrying
0.3946525996	benchmarks
0.3946302008	weighted finite
0.3946248863	neglect
0.3946248863	propagated
0.3945774638	facilitate
0.3945625445	create
0.3945580191	capture
0.3945385564	sequence information
0.3944558205	transfer learning techniques
0.3944441656	generations
0.3944310596	effective approaches
0.3943887637	speaker recognition system
0.3943617091	syntactic or semantic
0.3942932753	labeled training
0.3942794010	supervised method
0.3942694702	reflecting
0.3942679579	37
0.3942668027	steps
0.3942630449	significant and consistent
0.3942556712	front
0.3941677702	includes
0.3940789727	text and images
0.3940689678	graph based models
0.3940344121	redundant
0.3939963057	paradigms
0.3939161003	on
0.3938356000	locations
0.3938090351	harder
0.3937942224	language information
0.3937905742	coherent texts
0.3937549368	summarized
0.3936887583	accuracy improvements
0.3936841555	analyzed
0.3936664796	single neural network
0.3935894710	conditions
0.3935835043	go beyond
0.3935740179	asks
0.3935740179	faces
0.3935672003	procedures
0.3935637885	popularly used
0.3935126899	limits
0.3934842528	words and sentences
0.3934466083	dialogue system
0.3934257091	translation method
0.3933940548	vast number
0.3933849709	english dataset
0.3933816838	an open source
0.3933781162	noisy training data
0.3933034636	github.com
0.3932718874	desirable
0.3932370588	argue
0.3931248710	progressively
0.3930985617	prior methods
0.3930898208	clues
0.3930678676	resource constraints
0.3930658809	employ
0.3930250651	variability
0.3930104952	rouge 1
0.3929587938	hyperparameters
0.3929575135	larger models
0.3929547629	based algorithms
0.3928940985	reduces
0.3928695582	assumptions
0.3928289210	media sites
0.3927801025	equal error
0.3927787139	captures
0.3927641880	text types
0.3927274318	largely
0.3927029275	classification approaches
0.3926996847	annotate
0.3926996780	many natural language processing
0.3926713571	non linear
0.3926307938	immediate
0.3926182769	abilities
0.3926070754	current deep
0.3925992661	decrease
0.3925435339	predefined
0.3925219862	involves
0.3925173142	approach significantly
0.3924927586	data pre processing
0.3924851634	language utterances
0.3924660504	learnt
0.3924297803	automatically answer
0.3923866908	model pretraining
0.3922396677	method works
0.3922316224	75
0.3922261578	slightly
0.3922049433	confirm
0.3921824451	graph to sequence
0.3921585593	based metric
0.3921250727	translates
0.3920802517	training and test data
0.3920514229	received increasing
0.3920218025	whom
0.3919938006	self normalization
0.3919674477	word models
0.3919560287	pre training approach
0.3919297690	formulas
0.3919073512	designs
0.3919073512	dubbed
0.3918617091	gains in accuracy
0.3917609298	domain question answering
0.3916795175	achieve significant
0.3916378126	called
0.3916361855	scenarios
0.3916011842	zero resource languages
0.3915841556	real world datasets demonstrate
0.3915797544	unsupervised approach
0.3915676467	performing
0.3915534873	setting
0.3915453735	~
0.3915286669	task requires
0.3914954464	meaning based
0.3914921099	reflect
0.3914260132	language and vision
0.3913467139	linear model
0.3913404811	changed
0.3913246554	11
0.3913224651	collected
0.3912823300	non autoregressive machine
0.3912806382	two stage
0.3912626338	related task
0.3912405562	peters et
0.3912340463	mtl models
0.3910966555	increases
0.3910821132	network learns
0.3910643142	populations
0.3910380154	customers
0.3910345540	organized
0.3909814253	intrinsically
0.3909814253	comprehensively
0.3909604901	attracting
0.3909604901	exhibiting
0.3909604901	behave
0.3909604901	lets
0.3909604901	lengthy
0.3909604901	twofold
0.3909604901	comparatively
0.3908916715	text classification task
0.3908433383	recording
0.3908040058	sota results
0.3907988631	specifying
0.3907697427	suboptimal
0.3907697427	scholars
0.3907628101	source and target
0.3907553619	seconds
0.3907489749	cross lingual language
0.3906501215	fewer
0.3905962034	increased
0.3905774020	outperformed
0.3905545825	parsing and generation
0.3905179963	q learning
0.3905175764	selected
0.3904367142	research works
0.3903735844	end to end fashion
0.3903213645	monolingual corpus
0.3903169139	method extracts
0.3902859877	handle
0.3902732204	hybrid approach
0.3902542631	research questions
0.3902381125	last year
0.3902073963	machine learning research
0.3902046231	publicly available
0.3901657349	attention scores
0.3901041905	easily adapted
0.3900952030	non native speech
0.3900913362	short sentences
0.3900381032	informative words
0.3900306718	encoding model
0.3899371719	random baseline
0.3899314659	frequently
0.3899290193	clearly
0.3899035365	versus
0.3898922313	higher
0.3898159714	approaches outperform
0.3897667968	helped
0.3897667968	bigger
0.3897667968	delivered
0.3897667968	hinder
0.3897214634	sentence structures
0.3897120757	frameworks
0.3896875153	providing
0.3896772800	sequence to sequence architecture
0.3896568466	outperform
0.3896275621	efforts
0.3895774029	machine translation model
0.3895667294	introduce
0.3894499783	language question answering
0.3894435718	sequentially
0.3893526809	share
0.3893489796	component
0.3892997000	underlying
0.3892628398	name disambiguation
0.3892238583	bleu 4
0.3891671263	organizations
0.3891439821	automatic video
0.3890885805	strongly
0.3890631735	words and phrases
0.3890610793	million
0.3889343159	estimated
0.3889263277	target language text
0.3889247535	surveys
0.3889247535	repositories
0.3889247535	degrade
0.3889109518	certainly
0.3888935266	2.0
0.3888290861	shows
0.3888179190	preliminary analysis
0.3888096256	#
0.3888066510	natural language utterances
0.3887147693	successfully train
0.3886905156	time periods
0.3886707123	covid 19 english tweets
0.3886699671	statistical and neural
0.3886355849	based deep
0.3885295165	procedure
0.3885220599	vocabulary continuous speech recognition
0.3885155436	investigated
0.3885107500	vast amounts of
0.3885065686	capabilities
0.3885045853	proposed approaches
0.3884797697	recent methods
0.3884593959	speech recognition model
0.3884178358	exploited
0.3884106555	multilingual topic
0.3882227099	a preliminary study
0.3882152956	automated methods
0.3882057894	freely
0.3882009954	language communication
0.3881623043	generated outputs
0.3881488799	assess
0.3881485793	explore
0.3881406121	speech recognition tasks
0.3881188450	information source
0.3880626211	co attention
0.3878785532	obtained
0.3878707991	increasingly
0.3878418247	follow up
0.3878404811	individually
0.3877709129	measured
0.3877210080	existing knowledge
0.3877072902	labeled and unlabeled data
0.3876814148	learning agent
0.3876761742	increased performance
0.3876657353	develop
0.3876651595	including
0.3876324562	considerable
0.3876227265	actual
0.3876008973	leveraged
0.3875959361	vaswani et
0.3875713499	lexical representations
0.3875436336	supervised machine
0.3874303683	limiting
0.3873811359	video and text
0.3873449310	valuable
0.3873309612	semeval 2020 task
0.3873180711	extensively
0.3873175297	recognize
0.3872981430	speech audio
0.3872615252	synthetic and real
0.3872486884	natural language models
0.3872459766	complicated
0.3872271344	sequential decision
0.3871677812	carry
0.3871659900	adopted
0.3871455719	systematically
0.3871338839	especially
0.3870966577	question based
0.3870629656	structured representations
0.3870299115	primarily
0.3870117534	properly
0.3869677780	extends
0.3869408779	speed up
0.3869321907	nlp methods
0.3869279218	design and implementation
0.3869072434	approaches assume
0.3869007061	traditional techniques
0.3868979490	domain setting
0.3868649765	ways
0.3868302652	strong generalization
0.3868253681	utilize
0.3868007614	convolutional and recurrent
0.3867939558	multiple labels
0.3867389495	jointly training
0.3866985825	drop in replacement
0.3866691027	relevant entities
0.3866465543	approach produces
0.3866396064	whole word
0.3866281075	provide
0.3866113845	model makes
0.3865671212	outperforming
0.3864881620	significantly outperforms previous
0.3864761118	standard corpus
0.3864478061	doing
0.3864258699	obtains
0.3863926201	identify key
0.3862940112	few
0.3862185187	nlp community
0.3862142111	evaluated
0.3861510779	completely
0.3861334594	image data
0.3861315093	media texts
0.3860970350	varied
0.3860970350	continuously
0.3860706691	inappropriate
0.3859819417	|
0.3859747429	placed
0.3859624115	automatically create
0.3859577515	combine
0.3859391475	noises
0.3859391475	fairly
0.3859391475	flexibly
0.3859266602	leads to significant improvements
0.3859207449	optimization process
0.3859051585	improved neural
0.3858911449	built
0.3858908812	multimodal neural
0.3858808027	significant reduction
0.3858617091	images and texts
0.3858019771	far fewer
0.3857735653	dataset called
0.3857735523	1.4
0.3857677148	natural language applications
0.3857296386	extraction process
0.3856756046	takes
0.3855833821	domain specific data
0.3855371782	widely
0.3855239955	non differentiable
0.3855203651	approach achieves significant
0.3854924885	global view
0.3854752761	1.5
0.3854400966	content and style
0.3854141638	seven
0.3854011949	automatic and human
0.3853767766	80
0.3853276076	decoded
0.3853276076	delivers
0.3853276076	minimizes
0.3853276076	intractable
0.3853094173	embeddings capture
0.3853093883	evaluation scheme
0.3852823155	affects
0.3852357254	large scale chinese
0.3851307823	attained
0.3850693137	statistical machine translation system
0.3850552959	linguistic content
0.3850028567	apply
0.3849984650	unknown slot
0.3849147034	demonstrate
0.3848987814	represented
0.3848785554	yields
0.3848753816	account
0.3848100529	task relevant
0.3847606967	state of art
0.3846820618	back translated data
0.3846586446	complex task
0.3846278032	adapted
0.3845962034	highlight
0.3845931247	adequately
0.3845923381	investigates
0.3844775955	assumption
0.3844708822	http
0.3844655560	follow
0.3844337442	semi supervised text
0.3843695368	applied
0.3843140662	separate
0.3843065776	common features
0.3842507695	just
0.3842506403	one pass
0.3841910117	concerns
0.3841625656	outputs
0.3841561732	equivalent
0.3840721986	collection process
0.3840708971	simple and effective
0.3840326454	vector representations of words
0.3839812270	explores
0.3839400966	speed and accuracy
0.3839280898	every year
0.3839267991	exploit
0.3838954882	connects
0.3838954882	augments
0.3838954882	receives
0.3838954882	attains
0.3838591316	^ n
0.3838525228	input layer
0.3838411166	question pair
0.3838233668	supports
0.3838181562	optimization methods
0.3837812762	translation datasets
0.3837521887	expands
0.3837521887	embeds
0.3837521887	indirectly
0.3837521887	formulations
0.3837521887	exceeds
0.3837521887	neglected
0.3836697628	substantially
0.3836612363	performance loss
0.3836217184	created
0.3834943044	intended
0.3834905439	bidirectional long short
0.3833996036	increase
0.3833814554	generates
0.3833652628	prevents
0.3833652628	causing
0.3833617091	accuracy and efficiency
0.3833589924	german and english
0.3833314719	community structure
0.3833081984	reasons
0.3832865181	end to end speech
0.3832498306	activities
0.3831998518	presents
0.3831189564	setups
0.3831189564	formalize
0.3830877568	satisfying
0.3830877568	highlighted
0.3830877568	tackled
0.3830855220	infers
0.3830855220	positively
0.3830855220	regimes
0.3830855220	experienced
0.3830805457	multilingual word
0.3830681272	image pairs
0.3830044829	semantic vectors
0.3829494882	traditional statistical
0.3829045632	choices
0.3828842101	the wall street journal
0.3828765734	combines
0.3828653337	2014
0.3828224156	readily available
0.3827695186	audio and text
0.3827365973	although neural machine translation
0.3827329551	nearly
0.3827110741	represent
0.3826999752	outperform strong
0.3826736847	achieve superior
0.3826682621	needed
0.3826188308	recall and f1
0.3825671019	qa task
0.3825629695	next word prediction
0.3825170920	nli task
0.3825093435	demonstrated
0.3824236307	level translation
0.3824195823	developing methods
0.3824104413	based opinion mining
0.3823892730	distillation methods
0.3822846021	classification and clustering
0.3822229891	distinct
0.3822102404	graphical user
0.3822037193	overcomes
0.3822037193	rated
0.3822037193	illustrates
0.3822037193	shares
0.3821386209	unlabeled target
0.3821258905	semantic text
0.3820584731	advantages
0.3820147258	reasonably
0.3820144333	years
0.3820036209	recent times
0.3820018273	triples
0.3819400966	efficient and robust
0.3819232697	consistently
0.3819120580	outperforms previous models
0.3818578813	mechanisms
0.3818400966	phrases and sentences
0.3818253681	beneficial
0.3818069109	infeasible
0.3818069109	predominantly
0.3817681292	reveal
0.3817070214	critical
0.3817061505	subtasks
0.3816472454	non parallel
0.3816263931	non autoregressive translation
0.3815153810	regarding
0.3815121177	evaluate
0.3814894336	exponentially
0.3814707290	entity set
0.3814594237	speech activity
0.3814051585	hierarchical neural
0.3813811359	words and entities
0.3813794535	0.5
0.3813757510	commonly
0.3813433369	heavily rely on
0.3813330187	detection on social
0.3813161599	tend
0.3812404083	encode
0.3812034506	display
0.3811118747	professionals
0.3810862724	easier
0.3810841438	multilingual neural
0.3810651929	detects
0.3810555722	sentiment analysis task
0.3810044166	representation learning methods
0.3809763920	increasing attention in recent
0.3808875638	encoder and decoder
0.3808710324	spoken document
0.3808590241	compute
0.3808544548	published
0.3807835771	based dialog systems
0.3807273605	language processing applications
0.3807215993	deeper understanding
0.3806880282	2020
0.3806737378	brings together
0.3806244300	taken into account
0.3805591576	effective methods
0.3805545084	seamlessly
0.3805545084	showcase
0.3805545084	solves
0.3805344521	suggesting
0.3805200942	efficient and accurate
0.3804514934	develops
0.3804446197	non factoid
0.3804367042	text feature
0.3804237611	submitted
0.3804180668	multimodal tasks
0.3803862844	2012
0.3803818651	amounts
0.3803562995	events in text
0.3803341275	ability to learn
0.3803265813	model consistently
0.3803239776	automatic methods
0.3803096690	explicitly
0.3803067797	conduct
0.3803046277	comprehension dataset
0.3802431559	modeling approaches
0.3802211750	raise
0.3802199192	modern language
0.3801626710	non streaming
0.3801272153	algorithm achieves
0.3801083775	scale data
0.3801046113	automated and human
0.3800930593	models obtain
0.3800833111	known
0.3800590332	express
0.3800275216	to
0.3799934455	evidence suggests
0.3799861158	quantify
0.3799430626	information network
0.3799377310	explored
0.3799174035	maximizes
0.3799174035	confirms
0.3799174035	discovers
0.3798875638	entities and relations
0.3798875638	visual and textual
0.3798833707	computes
0.3798833707	motivate
0.3798391142	reveals
0.3798345449	\ beta
0.3798265086	cross domain text
0.3797810711	raised
0.3797810711	pronunciations
0.3797474304	interesting
0.3797366492	computer generated
0.3797015916	provide complementary
0.3796868145	establishes
0.3796868145	grows
0.3796868145	yielded
0.3796783271	subjective information
0.3796635416	incrementally
0.3796332283	allowing
0.3796206482	requires
0.3796092266	verify
0.3795824187	detailed error
0.3795797578	suitable
0.3795357891	attempt
0.3795095754	makes
0.3794914776	approached
0.3794914776	witnessed
0.3794914776	removes
0.3794509551	simulation results
0.3794261407	representation and reasoning
0.3794245335	plays
0.3794181447	forces
0.3794181447	assigns
0.3794181447	publish
0.3794181447	motivates
0.3793801784	recurrent encoder
0.3793757091	discuss
0.3793716461	enable
0.3793253158	greatly
0.3793218056	required
0.3793079049	amongst
0.3793062105	describes
0.3792881001	$ \ beta
0.3792088525	propose
0.3791742819	reaching
0.3791742819	brought
0.3790717898	achieved
0.3790482106	negatively
0.3790482106	argued
0.3790482106	seemingly
0.3789772331	0.89
0.3789614493	fail
0.3789400966	quality and diversity
0.3789259251	best
0.3788997276	models of natural language
0.3788921431	projects
0.3788868145	constitute
0.3788868145	explains
0.3788868145	satisfactory
0.3788868145	raises
0.3788428570	effectively identify
0.3787900416	triplets
0.3787528781	significant attention
0.3787470071	computations
0.3787469249	random search
0.3786661262	handles
0.3786558662	new
0.3786510930	validated
0.3785668586	computationally
0.3785461084	prove
0.3785076429	reach
0.3784986884	training times
0.3784823769	yield
0.3784524565	text or speech
0.3784478316	explicitly models
0.3784174035	extractions
0.3783951665	training approach
0.3783839574	preserves
0.3783655493	quickly
0.3783620999	compositional distributional model of
0.3783467473	include
0.3783193928	all
0.3783085283	aims to identify
0.3782959859	method produces
0.3782618791	interpretable model
0.3782470071	enhances
0.3782249622	term dependencies
0.3781987142	implement
0.3780783381	nine
0.3780676140	audio visual speech
0.3780281947	word and phrase
0.3780245575	suggest
0.3780075385	call
0.3779962403	received
0.3779872876	2013
0.3779691391	model outperforms previous
0.3779676761	training loss
0.3778985469	discover
0.3778701342	reliably
0.3777970172	primarily focused
0.3777790184	negative correlation
0.3777699657	leverage
0.3777085456	sized
0.3776787036	proven useful
0.3776705859	speech synthesis systems
0.3776394404	finds
0.3776243211	convolution neural
0.3775894608	clustering process
0.3774431193	training and testing
0.3774016040	based bi
0.3773616121	neural translation model
0.3773384240	construct
0.3772761864	designed
0.3772218576	developed
0.3772204213	ensures
0.3772204213	constitutes
0.3771912973	results provide
0.3771766159	meaningful
0.3771586247	showing
0.3770795792	produces
0.3770694121	in situ
0.3770036314	computed
0.3769702731	exist
0.3769400966	audio and visual
0.3768950942	effectiveness and efficiency
0.3768713685	graph model
0.3768255197	online data
0.3768227715	when
0.3768165014	0.96
0.3768059141	well understood
0.3767900416	induces
0.3767664710	code and pretrained
0.3767147165	encourages
0.3766751532	observe
0.3766487316	graph based dependency
0.3766394404	incorrect
0.3765952717	metric for evaluating
0.3765638324	natural responses
0.3765531537	knowledge encoded
0.3765477492	incorporate
0.3764429303	far
0.3764074327	manner
0.3764000163	human studies
0.3763972005	reasonable
0.3763804499	question answering over knowledge
0.3763419671	characterize
0.3763265493	automatic approaches
0.3763171475	appropriately
0.3763171475	hinders
0.3763034675	expect
0.3762908443	helpful
0.3762875362	paper introduces
0.3762786820	require
0.3762616329	validate
0.3762169469	more
0.3762026429	each
0.3761816713	test datasets
0.3761468014	exhibits
0.3761467519	shown
0.3761308969	up
0.3760953620	normally
0.3760630165	forming
0.3760630165	severely
0.3760630165	revisit
0.3760630165	inaccurate
0.3760630165	constantly
0.3760521538	classified
0.3760280571	standard benchmark
0.3760201140	analyze
0.3760149857	model works
0.3759927357	reported
0.3759574970	well studied
0.3758875638	lexical and syntactic
0.3758839574	alleviates
0.3758365625	settings
0.3758072707	sentence based
0.3757986272	learns
0.3757970779	formulation
0.3757940024	multiple entities
0.3757544462	key features
0.3757490013	nested named
0.3757168516	monolingual model
0.3757052522	but
0.3756683479	multilingual bottleneck
0.3756627422	unsupervised fashion
0.3756623781	context level
0.3756394404	constructs
0.3755300158	text classification datasets
0.3755187988	degrades
0.3755187988	resolved
0.3755035551	https
0.3754930074	an empirical comparison
0.3754808733	require expensive
0.3754669268	proposed method performs
0.3754541822	carefully
0.3754509755	gained
0.3754009147	embedding framework
0.3753894404	rarely
0.3753837008	while maintaining
0.3753617091	automatic and manual
0.3753339047	bi directional long
0.3753273995	nmt architectures
0.3753034675	formats
0.3751812146	problems in nlp
0.3751683656	chinese and english
0.3751644259	data format
0.3750848687	method combines
0.3750653744	built upon
0.3750349191	semantic based
0.3750176031	showed
0.3749649364	facilitates
0.3748817882	a unified
0.3748631735	efficient and effective
0.3748622969	reflects
0.3748304867	in domain parallel
0.3748165509	bottom
0.3747370033	hypothesize
0.3747046960	fusion approach
0.3746956302	creates
0.3746881212	performances
0.3746713289	summarization performance
0.3746491917	typically trained
0.3746417308	multilingual corpora
0.3746276757	constituents
0.3746108528	gradually
0.3745828927	simultaneously learn
0.3745709354	consecutive
0.3744215220	proven
0.3744164264	multiple speakers
0.3744031816	enables
0.3743905777	far behind
0.3743848019	easily
0.3743728909	proposed recently
0.3743447298	features extracted
0.3742735195	substantial improvements over
0.3742615617	proposed approach achieves
0.3741857102	crucial
0.3741590180	establish
0.3741556114	models of meaning
0.3741097860	effective and efficient
0.3740256045	without degrading
0.3740082202	two layered
0.3740037642	sampling methods
0.3739867091	trained and tested
0.3739740511	conclude
0.3739655313	contribute
0.3739649364	covers
0.3739532185	even
0.3739400966	loss in accuracy
0.3739370171	rouge 2
0.3739318628	5
0.3739180538	fail to capture
0.3739154267	three
0.3738900586	stages
0.3738009725	counterparts
0.3737794581	decreases
0.3737794581	adds
0.3737794581	opens
0.3737794581	scored
0.3737519182	neural machine translation system
0.3737303142	examine
0.3737156447	high quality training data
0.3736703366	examined
0.3736661262	converts
0.3736584470	large numbers
0.3736438826	better
0.3736097860	simple and efficient
0.3735900099	text and knowledge
0.3735254808	ignores
0.3735147909	chinese to english translation
0.3734859162	effectively learns
0.3734504944	purposes
0.3733873903	speedup
0.3733542593	100 million
0.3733241231	text pair
0.3732868119	an
0.3732133372	side
0.3731721835	averaged f1 score of
0.3731633670	additional context
0.3731603482	outperforms traditional
0.3731364469	supervised fine
0.3731004499	aims
0.3729649364	selects
0.3729465700	2017
0.3729288581	augmentation techniques
0.3728994326	benchmark data
0.3728978110	successful
0.3728963606	helps
0.3728693858	varying levels
0.3727794581	implements
0.3727683656	entity and relation
0.3727544036	level fusion
0.3726872598	2016
0.3726854176	derive
0.3726819891	content generation
0.3726642207	rely on hand crafted
0.3726601364	magnitude faster than
0.3726568660	situations
0.3726369575	defines
0.3725866565	relatively small
0.3725810729	wide array of
0.3725754038	shot learning
0.3725338614	sentence translation
0.3724829939	release
0.3723760229	demonstrating
0.3723534125	informative covid 19
0.3723460462	boosts
0.3723460462	holds
0.3722818628	interpret natural
0.3721956302	reached
0.3721956302	shorter
0.3721517565	empirical investigation
0.3721499445	adopts
0.3721044842	non expert
0.3721038781	introduced
0.3720601968	interpret
0.3720598388	model performances
0.3720540570	determines
0.3719936693	approach exploits
0.3719572398	natural language data
0.3719238917	a comparative study
0.3719092550	under
0.3718488658	becomes increasingly
0.3718350271	proposes
0.3718057049	complex interactions
0.3717540419	expressed
0.3717375752	substantial margin
0.3717228496	bert and gpt
0.3717048820	collect
0.3715954477	discusses
0.3715938100	probabilistic generative
0.3715709354	analysed
0.3715709354	uniformly
0.3715431141	number of model parameters
0.3715327622	twelve
0.3715260175	investigate
0.3715093562	incorporates
0.3714742381	classification process
0.3714431193	train and evaluate
0.3714431193	structural and semantic
0.3714183520	standard methods
0.3713539604	extremely
0.3713499027	automatic speech recognition system
0.3713373266	end to end spoken
0.3713358769	0.70
0.3713214259	large data sets
0.3713169218	some
0.3712167314	real applications
0.3712007007	small dataset
0.3711244728	sequence labeling problem
0.3709445628	subtask 1
0.3709426017	mining and sentiment
0.3709357210	out of vocabulary
0.3709263134	rely heavily on
0.3708850116	training techniques
0.3708811359	image and sentence
0.3707880641	statistically
0.3707695548	maintaining competitive
0.3707569971	share information
0.3707096826	assumes
0.3706653655	n =
0.3706500687	also
0.3706135083	constructed
0.3705653572	pairs of languages
0.3705351285	outperform conventional
0.3705211207	successfully
0.3705101281	studied
0.3704425934	word and sentence embeddings
0.3704416298	publicly
0.3704111669	learning and inference
0.3703876289	language and knowledge
0.3703460462	inefficient
0.3703460462	avoids
0.3702999841	series of experiments
0.3702955563	analysis approach
0.3701249249	mrc model
0.3701111352	established
0.3700889488	*
0.3700542956	models of language
0.3699479612	graph embedding models
0.3699249583	research attention
0.3699212940	n best
0.3699104896	partly because
0.3698857926	proved
0.3697794581	classifies
0.3697182599	translation process
0.3696431711	acoustic and linguistic
0.3695194381	us
0.3694887033	length vector
0.3693971924	generative language
0.3693503879	generation step
0.3693195619	areas
0.3691584627	text reviews
0.3691134020	3d
0.3690631735	research and development
0.3690626722	present
0.3689500141	end to end dialog systems
0.3689455837	development process
0.3689111669	speech and language
0.3689100044	labeled corpus
0.3687795541	automatic systems
0.3687317216	systems trained
0.3687089924	short and long
0.3687065399	knows
0.3686932650	detection algorithms
0.3686597732	text domain
0.3683814858	larger
0.3683811359	images and text
0.3683215161	taken together
0.3682895479	semantic and structural
0.3682248013	presumably
0.3681696230	inverse document
0.3681346632	fifteen
0.3681346632	anyone
0.3681346632	sure
0.3680563268	perform extensive experiments
0.3680408750	propose and compare
0.3680107058	0.75
0.3680107058	4.2
0.3680020760	entities in text
0.3679599604	french and english
0.3679504590	six
0.3679464052	text tasks
0.3679407940	learning model
0.3679277282	in depth analyses
0.3678750675	3
0.3678703896	based language models
0.3678609133	presented
0.3677545257	evaluations demonstrate
0.3677062119	the art
0.3676938966	analysis showed
0.3676300839	robustness to noise
0.3675684663	try
0.3675469178	every
0.3675209435	a data driven approach
0.3674702710	define
0.3674528966	remains
0.3674389110	question and answering
0.3673866914	understanding and generation
0.3673690100	art models
0.3673684088	1986
0.3673618223	60
0.3673572704	put
0.3672369456	discussed
0.3672310503	primarily focused on
0.3672039594	vary
0.3671771114	addressed
0.3671683390	a convolutional neural network
0.3671002791	18
0.3670871849	recognition and speaker
0.3670631735	development and evaluation
0.3669683074	five
0.3669129284	2010
0.3668875638	input and output
0.3668772993	evaluation settings
0.3668722128	platforms
0.3667973843	research communities
0.3667961412	based acoustic model
0.3667849831	1988
0.3666787397	line of research
0.3666377268	neither
0.3665628541	summarization framework
0.3665045668	training and evaluating
0.3663323660	sentence retrieval
0.3663098511	training of neural
0.3661611669	model of meaning
0.3661609249	4.6
0.3661531323	semeval 2018 task 1
0.3661097860	syntactic and lexical
0.3660675488	learning based methods
0.3660346000	model achieves competitive
0.3660098203	generate diverse
0.3660011738	experiments on chinese english
0.3659295731	reasons behind
0.3658895654	non autoregressive neural machine
0.3658151521	extend
0.3658151316	1987
0.3657925963	addresses
0.3657504879	standard approaches
0.3657435332	q networks
0.3657230008	an annotation scheme
0.3656374540	difficulties
0.3655934084	performance comparable
0.3655082390	demonstrates
0.3654513991	data source
0.3653958034	employed
0.3653866914	scale to large
0.3653662687	end to end training
0.3653518108	including sentiment analysis
0.3653268563	0.87
0.3652689078	0.63
0.3652601243	metrics and human evaluation
0.3652098973	50
0.3649680548	nmt baseline
0.3649428348	95
0.3648657917	neural network acoustic
0.3648383717	achieves higher
0.3647950457	shown effective
0.3646955111	reasoning ability
0.3646918613	semantic tasks
0.3645515189	offers
0.3645102860	relevant topics
0.3642865295	automated approach
0.3642829819	retrieval process
0.3642747450	needs
0.3642579464	resulting dataset
0.3640878221	robust performance
0.3640683625	unsupervised sentence
0.3640490670	three fold
0.3640443911	relation patterns
0.3640298747	capture complex
0.3639142601	exists
0.3639002093	comprehension model
0.3638899876	aims to reduce
0.3638872440	statistical translation
0.3638649551	mine
0.3638579909	semantic understanding
0.3638293508	utilized
0.3638167341	framework outperforms
0.3638133681	hundred
0.3638065275	method generates
0.3636992334	effectiveness and robustness
0.3636970495	parsing process
0.3636601940	natural language expressions
0.3636393547	encourage research
0.3636188123	very
0.3636097860	trained and evaluated
0.3635341160	higher scores
0.3634803406	substantial performance
0.3634573348	expected
0.3634185789	ability to reason
0.3634152811	output text
0.3633795565	attention based deep
0.3633142199	$
0.3632784606	text annotation
0.3631156718	metrics and human
0.3630168152	a rule based approach
0.3629282641	individual sentences
0.3628887663	global model
0.3628885116	deep language
0.3628493566	incorporating external
0.3628466851	4
0.3628139232	2015
0.3627523573	7
0.3627313309	0.05
0.3627313309	0.74
0.3627313309	0.62
0.3627313309	3000
0.3626564260	precision @
0.3626387974	@
0.3626097860	train and test
0.3626004148	loss in performance
0.3625923887	yielding
0.3625166885	signal to noise
0.3624691528	pay attention to
0.3624278483	day
0.3624213695	embedding techniques
0.3623191257	approaches require
0.3622926335	models achieved
0.3622614514	benchmarks demonstrate
0.3622606388	last decade
0.3622393089	sequence to sequence framework
0.3621622834	much
0.3620988838	14
0.3620862057	specific topics
0.3620598511	datasets and models
0.3620448270	rates
0.3620095019	2018
0.3620045268	same
0.3619910035	semantics interface
0.3619767697	helps users
0.3619704371	implemented
0.3619129881	area of research
0.3618945935	specify
0.3618500340	6
0.3617925446	words representation
0.3617845602	high quality data
0.3617159735	integrates
0.3615955654	models with attention
0.3615672639	top 5
0.3614800192	1
0.3614627975	user data
0.3614611835	least
0.3613963989	comprehensive study
0.3613647866	doing so
0.3612941124	method requires
0.3612840228	e.g
0.3612775231	verification tasks
0.3612168514	end to end models
0.3612142291	neural text to speech
0.3611272387	out
0.3609904449	dense video
0.3609134583	unsupervised and supervised
0.3609118979	based hierarchical
0.3608845514	identifies
0.3607334348	extract features
0.3606944665	extraction and classification
0.3606851071	0.81
0.3605705932	0.65
0.3604322305	training and decoding
0.3604037194	conducted
0.3604021061	provide rich
0.3603872779	30
0.3603866914	extraction of entities and relations
0.3603784399	cross lingual semantic
0.3603583212	network classifiers
0.3603311699	order logic
0.3602603575	move
0.3602310857	effectively extract
0.3601731276	expensive to obtain
0.3601132561	by
0.3600878818	extensive experiments conducted on
0.3600562146	important and challenging
0.3599751252	ner performance
0.3599456512	performed
0.3599361669	training and test
0.3599236008	shed
0.3599045668	domains and languages
0.3598992398	compares favorably with
0.3598845514	considerably
0.3597387116	few shot relation
0.3596950132	2019
0.3596938736	word statistics
0.3596827842	1999
0.3596743868	comprehension models
0.3596480899	6.6
0.3596480899	0.73
0.3596480899	1970
0.3595974673	at semeval 2020 task 12
0.3595803060	1997
0.3594969946	whole
0.3594399402	machine learning systems
0.3594230645	0.90
0.3594230645	0.88
0.3593143080	transfer learning methods
0.3593077293	i.e
0.3592883062	grounded dialog
0.3592608674	shows superior
0.3592530042	task specific training
0.3592458459	development and test
0.3591852580	part of speech tags
0.3591568991	increasingly difficult
0.3591158144	model generated
0.3590889600	consistent performance
0.3590489329	acoustic and language
0.3589742147	entropy loss
0.3589348869	accuracies
0.3588297984	large unlabeled
0.3587779949	\ subseteq
0.3586632081	related concepts
0.3586593599	probabilistic modeling
0.3586585670	models and code
0.3586177376	noisy texts
0.3585950717	review data
0.3585896334	help
0.3585658736	extensive research
0.3585319139	recognition and relation
0.3585279069	shows promising
0.3585068857	achieved competitive
0.3585021169	run time
0.3584643047	end to end learning
0.3584568627	processed
0.3583937199	perform experiments
0.3583933656	visual and linguistic
0.3583840799	large scale data
0.3583230317	visual data
0.3583192626	tasks and domains
0.3583036668	identification in social media
0.3582666085	medical entity
0.3582443983	tested
0.3582415421	arabic speech
0.3582038158	media messages
0.3581072825	for
0.3580901204	90
0.3580730943	an overview
0.3580285609	gram based
0.3580142007	end to end model
0.3579967395	structure based
0.3579647913	information extraction systems
0.3579361669	visual and language
0.3579205706	translation shared
0.3576809118	9
0.3576691760	two
0.3576520760	word and sentence
0.3575439721	detection dataset
0.3575143220	2
0.3575049295	non verbal
0.3574442288	introduces
0.3574201349	not necessarily
0.3574011587	remain
0.3572978958	relies heavily on
0.3572873621	^ 3
0.3572187406	code and datasets
0.3572147218	effective approach
0.3571826017	learning tasks
0.3571327389	small corpora
0.3570630667	amounts of training data
0.3569604795	elementary discourse
0.3569520162	an efficient
0.3569346472	source of data
0.3568420577	conventional attention
0.3567648295	achieves significant
0.3567220187	4.5
0.3565091792	original sentence
0.3564997040	understanding and reasoning
0.3564754234	a natural language interface
0.3563851717	semantic and syntactic
0.3563851717	syntactic and semantic
0.3563453416	while retaining
0.3563003138	specific case
0.3561820698	analysis in twitter
0.3561597731	method makes
0.3560620943	fine grained semantic
0.3560207366	models in real
0.3559782403	systematic comparison
0.3559736128	using deep neural networks
0.3559717284	classification datasets
0.3559676239	general framework
0.3559574726	two fold
0.3559473300	noise ratio
0.3559404282	english and english german
0.3559158424	external resource
0.3558903644	chinese datasets
0.3558538259	+
0.3557545218	shall
0.3557187406	lexical and semantic
0.3557116369	roberta model
0.3557044608	captioning task
0.3556826541	inference tasks
0.3555961663	popular approach
0.3555119044	fine grained level
0.3554957074	upon
0.3554515087	textual input
0.3552458400	learning classifiers
0.3552366194	including machine translation
0.3551816596	on wall street journal
0.3551700510	english translation tasks
0.3551566797	indicating
0.3549377641	detecting fake
0.3547567116	added
0.3546816848	learning based approaches
0.3546403255	parallel speech
0.3546173538	covering
0.3545788967	detection and classification
0.3543855032	bring together
0.3543657637	like
0.3543467604	decoding algorithms
0.3543032154	prediction errors
0.3542872277	significantly improve performance
0.3540737532	1990
0.3539807493	paper evaluates
0.3539008342	architecture called
0.3537788576	observed
0.3536319431	sentiment analysis models
0.3536152685	models of word
0.3536065439	self supervised pre training
0.3535194113	against
0.3534453448	8
0.3533263403	bert architecture
0.3533095762	highest accuracy
0.3532663127	noisy user
0.3531730122	language patterns
0.3531206117	similarity dataset
0.3531199931	existing text
0.3530310387	small subset
0.3530123851	previous datasets
0.3529531874	develop models
0.3528034926	classification error
0.3527996936	never
0.3527065384	hmm based speech
0.3526980976	end to end dialogue
0.3526426406	resulting embeddings
0.3525855202	way
0.3525270978	main challenges
0.3524458888	they
0.3524238495	switchboard corpus
0.3523031277	based technique
0.3522912252	$ ^ 2
0.3522268686	three dimensional
0.3520928202	automatically discover
0.3519676310	5.1
0.3519676310	79
0.3518973194	41
0.3518638670	2020 shared task
0.3515936528	method of generating
0.3515926130	learning technique
0.3515416877	large collections
0.3515035521	specific aspects
0.3513651175	dataset and code
0.3513613256	an open source python
0.3513493822	classification and sentiment
0.3512961477	query by example
0.3512718931	6.3
0.3512718931	7.5
0.3512718931	6.4
0.3512718931	8.8
0.3512718931	0.61
0.3512718931	0.71
0.3512718931	8.5
0.3512718931	0.2
0.3512718931	138
0.3512718931	120
0.3512718931	8.1
0.3512718931	5.9
0.3512718931	0.79
0.3512477833	representation scheme
0.3510906555	end to end deep
0.3510153477	not
0.3510141816	generated words
0.3510107012	eleven
0.3510005563	training and inference
0.3509779196	never seen
0.3509282754	large and diverse
0.3509040357	representation structures
0.3509040130	proved useful
0.3508032305	000
0.3508032096	bag of word
0.3507367522	comprehensive analysis
0.3507068991	leverage unlabeled
0.3506702278	tries
0.3504852850	having
0.3503902321	little
0.3503765601	thereby
0.3503648509	physical world
0.3503500606	hierarchical multi
0.3503417922	bleu improvement
0.3502923825	memory model
0.3502009142	real human
0.3500213852	0.76
0.3500098107	achieve good performance
0.3499632991	other
0.3498801213	analysis datasets
0.3498735050	efficient methods
0.3498381185	four
0.3495945429	enables researchers
0.3495805021	code and data
0.3495564685	so far
0.3495399571	important parts
0.3494082201	emerging research
0.3493767594	and non neural
0.3493721742	transformer based model
0.3493682553	adversarial example
0.3493412335	information encoded
0.3493212854	average improvement
0.3492818320	automatic fact
0.3492675271	analysis of natural language
0.3492186548	human conversation
0.3492058793	state and action
0.3491599208	test results
0.3491562596	propose and evaluate
0.3490953391	language corpora
0.3490615990	several
0.3490258900	natural language generation tasks
0.3490204164	no
0.3489686528	code and dataset
0.3489042355	requires understanding
0.3488972220	recent deep
0.3488938273	popular approaches
0.3488790058	10
0.3488778336	generate natural
0.3488698066	unsupervised model
0.3487669874	analysis techniques
0.3486900508	classification systems
0.3486142591	publicly available at https
0.3485937329	20
0.3485681483	specific task
0.3485601109	inference framework
0.3485312596	dataset and evaluation
0.3485312596	code and models
0.3484150251	text to text
0.3483345886	modern neural
0.3483106098	i vector
0.3482486646	10 fold
0.3482029570	base population
0.3481289584	target corpus
0.3480836719	level policy
0.3480329542	extraction models
0.3479882702	domain specific text
0.3478775386	16
0.3478548829	with
0.3478267546	number of training examples
0.3477944654	accurate predictions
0.3477695103	at semeval 2020 task 9
0.3477471687	data and code
0.3476450435	text information
0.3474989597	a large scale corpus
0.3474921308	current version
0.3474334356	bag of words model
0.3474114690	popular method
0.3474005563	tasks and datasets
0.3472923598	at wnut 2020 task 2
0.3472230438	paper analyzes
0.3471711067	dialog system
0.3471100811	provide insight
0.3470555492	commonly used
0.3470151023	annotation tasks
0.3469648467	linear programming
0.3469148998	interesting research
0.3468978843	time consuming and expensive
0.3468198492	analysis tasks
0.3468096624	automatically evaluate
0.3467884583	high and low
0.3467341793	easy first
0.3467123425	arrive at
0.3467060691	empty
0.3466645480	general approach
0.3466627844	generator networks
0.3466603128	multiple layers
0.3465859868	extraction and verification
0.3465851031	relatively little
0.3465676708	1996
0.3465357396	non compositional
0.3464656701	asr tasks
0.3464446981	capable of handling
0.3464395482	related research
0.3463942454	level semantic
0.3463645930	training and prediction
0.3463587885	above
0.3463239786	non english languages
0.3462149357	0.69
0.3462149357	0.68
0.3462108017	art baseline
0.3461846472	datasets and tasks
0.3460191730	2011
0.3459987342	1.4 bleu
0.3459971523	popular models
0.3459964538	entity recognition and relation
0.3459779332	clinical tasks
0.3459437310	his
0.3458401175	attention in recent
0.3457983002	frame semantic
0.3456048941	applications including
0.3455846688	much smaller
0.3455027355	2006
0.3455008777	simpler model
0.3454320468	well resourced
0.3453701091	less frequent
0.3453294248	good and bad
0.3453263319	based reasoning
0.3452578448	radford et
0.3452520556	recommendation system
0.3452336609	1993
0.3452169329	a higher degree
0.3449709154	based unsupervised
0.3449305472	g \
0.3449088975	language translator
0.3448534367	^ 2
0.3448490635	therein
0.3448073918	on device
0.3447615472	telephone speech
0.3447562956	crafted rules
0.3446685888	limited in size
0.3445642132	report experiments
0.3445451440	domain test sets
0.3445116388	deep architecture
0.3444850066	resource domains
0.3444130966	language users
0.3444130528	an information theoretic
0.3444018358	liu et
0.3443832643	throughout
0.3443074654	large dataset
0.3442837090	models employ
0.3442578448	shen et
0.3441846472	results of experiments
0.3441404764	a pilot study
0.3440800302	benchmark models
0.3440297612	model significantly
0.3439899646	low performance
0.3439400711	large scale online
0.3438411782	vinyals et
0.3438192813	recognition and translation
0.3438065167	translation techniques
0.3437182474	1991
0.3437131217	pass model
0.3436718704	achieves high
0.3436356197	a semi supervised approach
0.3436184094	approach works
0.3435129469	chen et
0.3434642560	language processing techniques
0.3434589575	number of parameters
0.3434303921	require large
0.3434258739	scarcity of labeled
0.3434148351	2001
0.3433949309	task in natural
0.3433855960	back translations
0.3433617812	followed
0.3433544368	mt models
0.3433532900	downstream natural language
0.3433252760	learned models
0.3432589539	challenging datasets
0.3432048566	ignored
0.3429542695	facilitate future
0.3428911639	news datasets
0.3428319477	languages and domains
0.3428257366	system combination
0.3427775570	based representation
0.3427647002	detection model
0.3427081861	based similarity
0.3426728073	framework enables
0.3425961766	ones
0.3425437628	complex problem
0.3425183748	proposed methodology
0.3424891754	exactly
0.3424738236	input query
0.3423949309	model of language
0.3423046326	learning semantic
0.3422915143	linking task
0.3422042473	model requires
0.3421357469	cross lingual models
0.3420946636	in domain training
0.3420746067	tried
0.3420520518	sequence labeling task
0.3420005563	training and evaluation
0.3418912578	nlp datasets
0.3418894680	multilingual semantic
0.3418524772	intermediate step
0.3417792683	paper explores
0.3417545800	models of natural
0.3417545800	model of word
0.3417426079	english to german translation
0.3417369897	rate reduction
0.3416621312	useful
0.3416609886	1995
0.3415684900	speech synthesis system
0.3415060420	language and speech
0.3414779077	tends
0.3413995399	performs on par
0.3413136383	significantly improves performance
0.3411643831	outperform previous
0.3411277483	probabilistic framework
0.3409912225	directly applied
0.3409221768	showed promising
0.3408664219	a neural network approach
0.3408297706	sometimes
0.3405705232	1992
0.3405482387	ability to understand
0.3405365617	within
0.3405017724	current evaluation
0.3404785430	1994
0.3404266597	more importantly
0.3404188914	associated
0.3402999465	experiments and analysis
0.3402559884	languages like english
0.3401336714	memory component
0.3400515634	necessary
0.3400382311	^
0.3400005563	source of information
0.3398834638	yield significant
0.3398732119	huge amounts of
0.3398240194	downstream classification
0.3397662213	across
0.3397145999	text encoder
0.3396759332	this paper describes
0.3396487351	applications in natural
0.3395793989	surprisingly good
0.3395479102	much larger
0.3395151241	attracted much
0.3394850066	unsupervised dependency
0.3393102106	performed experiments
0.3392779292	year
0.3392538014	recent trends
0.3391858249	latter
0.3391564457	available
0.3391410215	deep q
0.3391062428	speech recognition task
0.3391060984	opinion mining and sentiment
0.3391050999	this paper proposes
0.3390899761	relatively
0.3390231845	transfer techniques
0.3390205768	progress in recent years
0.3389574530	special focus
0.3389439606	build models
0.3389171599	highly dependent
0.3387961110	dimensional feature
0.3387833392	massive amounts of
0.3387743287	systems require
0.3387340168	metrics such as bleu
0.3387078979	text to sql task
0.3384089907	self attention weights
0.3383850346	network based methods
0.3383800115	too
0.3383764868	amount
0.3383657774	provide empirical
0.3383268623	sequence to sequence neural
0.3382531570	difficult to train
0.3382524772	guide future
0.3381634463	optimal performance
0.3381492041	standard sequence to sequence
0.3381394838	capture long term
0.3381091606	outperforming existing
0.3378737393	valuable resource
0.3376999585	english and english
0.3376739740	comes
0.3376641022	tv show
0.3376553317	correction rate
0.3375721760	causes
0.3375311375	joint semantic
0.3375244817	19
0.3374780973	2008
0.3374549222	linguistic tools
0.3374421578	proposed hybrid
0.3374249818	supervised text classification
0.3373511665	retrieval and question
0.3372898145	important problem
0.3372730982	work
0.3372270402	robot interaction
0.3372206779	1998
0.3371666572	this paper presents
0.3371266085	disease detection
0.3369689438	significant impact
0.3369294103	important tasks
0.3369190735	techniques developed
0.3369132002	important factor
0.3368319059	widely used
0.3366371956	thereby allowing
0.3365829357	+ l
0.3365289834	around
0.3365248202	plus
0.3364719583	well
0.3364622577	until
0.3363497111	aimed at
0.3363349690	how
0.3362577259	end to end framework
0.3362291579	highly sensitive
0.3361911062	textual patterns
0.3361618893	language spoken
0.3361217349	pipeline approaches
0.3360717702	104
0.3360717702	went
0.3360717702	6.8
0.3360717702	280
0.3360717702	7.2
0.3360717702	3.0
0.3360436071	task in nlp
0.3359589636	data settings
0.3359443703	agent reinforcement
0.3358628048	an object oriented
0.3358165685	dictionary induction
0.3358043223	outperforms several strong
0.3357904950	achieve results
0.3357857358	high correlation
0.3357617191	particular
0.3356971497	end to end architecture
0.3356846980	model outperforms state
0.3355994028	between
0.3355455716	large vocabulary continuous
0.3354887434	quite
0.3354809212	growing research
0.3354268356	gold standard corpus
0.3353407737	textual sentiment
0.3353214373	before
0.3352668435	transfer learning method
0.3351974981	at semeval 2020 task 11
0.3351742981	66
0.3351742981	3.1
0.3350803125	dataset annotated
0.3350606656	stage 1
0.3350503471	seen
0.3350217163	cause
0.3350187921	without
0.3349969151	complex semantic
0.3349744851	paper includes
0.3349370878	wikipedia data
0.3348457515	key problem
0.3348297354	idea behind
0.3348214540	binary classifier
0.3347907020	applications require
0.3347614626	strong correlation
0.3346384158	two pass
0.3345875425	text features
0.3345677195	good enough
0.3345115975	processing and information
0.3344708749	0.6
0.3342989915	tasks in natural
0.3342956273	1d
0.3342635970	studies reveal
0.3341837780	human and automatic
0.3341757259	based language model
0.3341496784	important questions
0.3340666623	under consideration
0.3339674073	doesn't
0.3339585774	simple to implement
0.3339454552	experiments showing
0.3339094288	generation component
0.3338862073	worse than
0.3338462995	transfer knowledge
0.3338315459	achieves performance
0.3334960577	pre trained multilingual
0.3334583274	generate speech
0.3332657856	range dependency
0.3332594991	outside
0.3332494971	forth
0.3332437619	extraction accuracy
0.3332149730	take place
0.3331939398	seem
0.3331787910	both
0.3331114720	method to generate
0.3330943521	underlying data
0.3330734270	out of domain
0.3330118477	significant improvements over
0.3329786969	languages other than english
0.3328738306	standard word
0.3328445965	natural language processing task
0.3328427534	strong empirical
0.3326237163	top 1 accuracy
0.3326130298	get
0.3325571319	less
0.3325434579	supervised speech
0.3325343421	response generation model
0.3324215637	inference method
0.3323534279	information extraction task
0.3323382754	77
0.3322676232	10.5
0.3322676232	3.9
0.3322676232	4.4
0.3322676232	0.67
0.3321987050	than
0.3321654939	5000
0.3321639842	proposed method improves
0.3320981113	vision and language tasks
0.3320656581	training of deep
0.3320302867	neural trf
0.3319577621	used
0.3318010456	outperforms prior
0.3317092588	means clustering
0.3316285753	2002
0.3316285753	54
0.3316125350	art accuracy
0.3315668770	achieve better performance
0.3315159790	under explored
0.3314209332	disambiguation tasks
0.3313742905	non overlapping
0.3313722884	detection systems
0.3313107105	well defined
0.3312815350	an end to end fashion
0.3312721192	model represents
0.3311740308	lingual entity
0.3311645079	or
0.3311293414	said
0.3310917116	toolkit for neural machine translation
0.3310755067	3.8
0.3310755067	6.5
0.3310755067	5.2
0.3310755067	3.4
0.3310011691	even though
0.3308972063	a comprehensive survey
0.3308861077	proposed solution
0.3308797852	concerning
0.3308695124	\ lambda
0.3308610905	original model
0.3308263457	domain dependency
0.3308234505	different
0.3308088350	human reference
0.3307542833	language systems
0.3306595676	task model
0.3306061399	inference methods
0.3305825826	news challenge
0.3304606558	keeps
0.3303862027	automatically evaluating
0.3303519373	machine learning algorithm
0.3303418003	takes advantage of
0.3303405250	enables learning
0.3302490687	a pre trained model
0.3301554788	existing algorithms
0.3300456480	recognition problem
0.3299949193	task transfer
0.3299586789	network based model
0.3299099626	large space
0.3298889313	large models
0.3298747142	multiple language pairs
0.3297621965	remarkably well
0.3297454459	controlled english
0.3297133698	studies suggest
0.3296070016	achieved significant
0.3295977577	trying
0.3295746549	provide extensive
0.3295646248	well known
0.3295519146	name matching
0.3294198065	sub questions
0.3293507959	left and right
0.3292596046	merely
0.3291445115	method effectively
0.3291275631	a large text corpus
0.3291173422	example based
0.3290547327	0.92
0.3290531372	trade off between
0.3290405308	trained bert model
0.3290352297	non collaborative
0.3290148381	entirely
0.3288743971	training labels
0.3287927635	large gap
0.3287132397	sources of information
0.3285895930	capable of generating
0.3285734130	the shelf
0.3285496890	0.7
0.3285213265	supervised fashion
0.3284958538	84
0.3284827234	recent results
0.3283669615	non native english
0.3282846135	96
0.3282609334	a joint model
0.3282573474	a comparative evaluation
0.3282550521	2000
0.3282267222	large pre trained language
0.3281574361	of
0.3281234429	achieves substantial
0.3280474785	article proposes
0.3280458005	generate coherent
0.3280393281	3.7
0.3280358447	dialogue generation models
0.3279921646	consists of two parts
0.3278670935	aims to extract
0.3278367078	extraction algorithm
0.3277799415	base models
0.3277099185	without requiring
0.3277012802	94
0.3276829102	natural language based
0.3275614153	essential step
0.3274944088	improves accuracy
0.3274742461	15
0.3273075757	89
0.3272490602	ai applications
0.3272435231	use
0.3272123607	4.3
0.3272123607	43
0.3272123607	2.7
0.3272031763	simple bag of words
0.3271622304	multiple sets
0.3270971636	specific linguistic
0.3270539221	74
0.3270539221	52
0.3270427113	neural machine translation using
0.3270334823	dataset consisting
0.3268943839	clinical decision
0.3268667274	translation corpus
0.3268458357	vs
0.3267834297	\ mathbb
0.3267305985	limited amounts
0.3267133965	a systematic analysis
0.3267041647	related issues
0.3266065110	research challenge
0.3265664973	while keeping
0.3264204520	various
0.3263452586	taken
0.3263100057	classification based
0.3262113593	alone
0.3262001336	taking advantage of
0.3259881535	effectively and efficiently
0.3258709265	self attention mechanism
0.3258514762	automatically predict
0.3258259782	0.83
0.3258199659	ability to predict
0.3258124466	deep multi
0.3257820012	53
0.3257820012	0.93
0.3257820012	49
0.3257820012	0.82
0.3257820012	4.1
0.3257820012	saw
0.3257820012	ourselves
0.3257597799	probabilistic language
0.3257595360	large text corpus
0.3256864394	detection algorithm
0.3255763858	domain information
0.3255670256	based rules
0.3255346135	88
0.3254488415	better suited
0.3253587075	13
0.3253456194	attention for neural
0.3253456194	based and neural
0.3253312057	still
0.3252259804	take into account
0.3251457399	unlike prior
0.3250048905	proposed techniques
0.3249948686	paper studies
0.3249923726	each iteration
0.3248610487	33
0.3247852024	identify relevant
0.3247633670	a transfer learning approach
0.3245524315	related documents
0.3245393281	3.5
0.3244350387	related questions
0.3244309390	outperforms previous state of
0.3244026986	$ o
0.3243905558	publicly available dataset
0.3243697787	corresponding
0.3243508574	appropriate
0.3243263108	language related
0.3242910254	varying degrees of
0.3241860222	extend existing
0.3241585824	from scratch
0.3241038680	recurrent convolutional
0.3239997873	at semeval 2020 task 4
0.3239694259	effective models
0.3238848522	make
0.3238372368	meaningful information
0.3237755574	paper discusses
0.3237486899	important aspects
0.3236346396	usually
0.3235797069	gaps between
0.3234948869	algorithm called
0.3233863212	any
0.3233500597	learning systems
0.3233297111	th
0.3233037353	social media such as twitter
0.3232654900	resulting word
0.3232538383	one to many
0.3230870090	about
0.3230440678	seems
0.3228041932	global sentence
0.3228030599	detection in social media
0.3227919285	non experts
0.3227900969	relation recognition
0.3225566176	a deep reinforcement learning
0.3224785928	word classification
0.3224645492	groups of words
0.3224478234	fed into
0.3224437454	reasoning over
0.3223303046	approach outperforms existing
0.3221518507	87
0.3221193533	generate responses
0.3221149509	specific characteristics
0.3221040561	generate synthetic
0.3219972473	67
0.3219907814	parsing framework
0.3219534052	2007
0.3219340275	choice question answering
0.3217490967	named entity recognition using
0.3217457508	an encoder decoder framework
0.3217012066	additional parameters
0.3216838407	take
0.3216714085	follows
0.3213873220	problems in natural
0.3213697205	2.9
0.3213697205	0.86
0.3213697205	1.9
0.3213697205	8.3
0.3213697205	56
0.3213571062	challenging setting
0.3213559085	task involves
0.3212921803	divided into
0.3212538142	eight
0.3212516559	two stage training
0.3212178995	3.6
0.3211290286	limited training
0.3211104030	72
0.3209520216	related knowledge
0.3209040783	compares favorably to
0.3208701233	mostly
0.3207854287	others
0.3207395892	a broader range
0.3206728119	sentiment classification task
0.3206625948	identifying key
0.3205993358	model consistently outperforms
0.3205988795	4.7
0.3205988795	61
0.3205988795	else
0.3205988795	63
0.3205988795	elsewhere
0.3205988795	somehow
0.3205285678	0.91
0.3201970968	an intermediate step
0.3201844948	detail
0.3201778906	unsupervised setting
0.3201566775	2017 shared task
0.3199221439	paper reports
0.3199058150	observe significant
0.3198311746	analysis model
0.3198264314	sequence to sequence pre training
0.3197404959	intelligence systems
0.3197350709	improved model
0.3196653619	sensible
0.3196320744	distance dependencies
0.3195150310	first pass
0.3194534052	98
0.3194324484	transformer based neural
0.3194106521	from
0.3192547127	present experiments
0.3192484690	important issues
0.3192231609	resource machine translation
0.3192076276	a neural language model
0.3191823986	model parameter
0.3191671874	challenge 2020
0.3191659887	dimensional data
0.3191498661	critical role
0.3190524446	relevant content
0.3189979461	k =
0.3189776815	38
0.3188981874	generate captions
0.3188186607	acoustic and language models
0.3186101732	outperform existing
0.3185926912	well suited
0.3185731877	top n
0.3185608804	theoretic approach
0.3185398953	resource based
0.3183570227	attention based end to end
0.3182532315	language description
0.3182285302	decoder model
0.3181034568	outperforms several baselines
0.3179026478	input images
0.3178318168	\ mathrm
0.3177813468	does not necessarily
0.3177606062	achieves promising
0.3176068133	input language
0.3176036979	art solutions
0.3175114660	decomposed into
0.3173350709	structured models
0.3173220888	smt system
0.3172760138	achieving comparable
0.3172500126	convolutional and recurrent neural
0.3172465585	multiple semantic
0.3171777533	12
0.3171044290	segmentation based
0.3170770968	time aware
0.3170535205	the alexa prize
0.3170472869	come
0.3169979681	specific data
0.3168834397	bilingual sentence
0.3168642564	5.8
0.3168642564	51
0.3168642564	someone
0.3168642564	com
0.3168642564	57
0.3168642564	83
0.3167727997	based training
0.3167582475	neural sequence to sequence
0.3167296297	training for neural
0.3166527066	extraction and sentiment
0.3166527066	tasks in recent
0.3165945785	complex syntactic
0.3165938382	encoder based
0.3164525033	semantic gap
0.3163910590	achieving competitive
0.3163526221	great interest
0.3162760138	dynamic nature
0.3162095430	65
0.3161551162	speech to speech
0.3161023372	achieves consistent
0.3161001066	happens
0.3159959420	subtask a
0.3159366196	the rational speech
0.3158512514	a conversational agent
0.3158088821	able
0.3157922012	achieves better performance
0.3157296327	non task oriented
0.3157296297	learning of word
0.3157296297	language in social
0.3156707543	important applications
0.3154277852	robust enough
0.3153962322	over
0.3153524889	nlp benchmarks
0.3153445193	93
0.3151448159	source content
0.3151445942	outperforms competitive
0.3151211191	aims to generate
0.3150735105	on in domain
0.3150598695	heavily relies on
0.3148214731	question answering over
0.3147151412	a context aware
0.3146197072	summarization based
0.3145859512	23
0.3144414514	improved word
0.3144220743	information improves
0.3143559439	answering model
0.3143412623	labeled corpora
0.3143400973	lexical data
0.3142613467	learn meaningful
0.3140898580	text messages
0.3140457982	model builds
0.3139504029	growing number
0.3139327603	an iterative
0.3138798944	capable of capturing
0.3138707747	modeling methods
0.3138487539	conneau et
0.3138143196	label based
0.3137488505	this report describes
0.3136214565	amounts of labeled data
0.3135943781	faster than
0.3134557967	automatically obtained
0.3134518894	analysis task
0.3134135992	26
0.3133457777	99
0.3132936274	document specific
0.3132442943	expressed in natural language
0.3132439824	simple approach
0.3131078816	only
0.3130884340	applying machine learning
0.3130394350	propaganda techniques in news
0.3130240611	multilingual settings
0.3130074552	experimental results indicate
0.3129723340	modeling tasks
0.3128394016	increased interest
0.3128392766	sequence to sequence speech recognition
0.3128067283	indicates
0.3128066462	a by product
0.3127012278	sufficient training
0.3126642476	inference process
0.3125756095	end to end systems
0.3125618224	a deep learning based
0.3125110975	2.2
0.3123853228	accuracy scores
0.3123273327	a daily basis
0.3123092123	rich linguistic
0.3120551988	including question answering
0.3120409567	likely
0.3120291976	free word
0.3120148322	achieves significant improvements over
0.3119074366	large scale empirical
0.3115182450	online hate
0.3114880828	extract meaningful
0.3114372763	always
0.3113174607	dimensional embedding
0.3113172177	top level
0.3112859512	1000
0.3112850553	0.5 bleu
0.3111887107	transition system
0.3111673121	specified
0.3111652134	those
0.3110452438	neural network methods
0.3109864540	end to end trained
0.3109194536	determine whether
0.3108974762	related language
0.3108532564	generate multiple
0.3108508210	simple linear
0.3107756587	text to sql models
0.3107713037	non contextual
0.3106929046	based search
0.3106778526	1.2
0.3106545091	dialogue task
0.3106518687	a wide variety
0.3105993998	in domain and out of domain
0.3105711505	an interactive
0.3104952420	correspondences between
0.3104876201	promising approach
0.3104648743	information retrieval tasks
0.3103844310	art performance
0.3102032681	possible
0.3101368981	based extraction
0.3100931399	outperforms baseline
0.3100755515	reading comprehension datasets
0.3099828204	modern deep learning
0.3099692173	significant number
0.3099585938	large amounts of
0.3099407876	trained directly
0.3098774429	approach outperforms previous
0.3098066462	of in domain
0.3097450227	achieve similar
0.3097424496	came
0.3097424496	600
0.3097424496	800
0.3097424496	0.95
0.3097204141	main features
0.3097189830	chinese english and english
0.3096568992	21
0.3095351063	end to end speech to text
0.3094987351	outperforms previously
0.3094443262	interplay between
0.3094366265	popular topics
0.3094121117	achieve promising
0.3093483539	document classification task
0.3091589519	reasoning over multiple
0.3091387545	$ z
0.3089716972	detection problem
0.3088863468	done
0.3088527838	smaller training
0.3088086390	conclusions about
0.3087409965	82
0.3084788070	state automata
0.3084619159	standard corpora
0.3084450499	previous neural
0.3083838785	method proposed
0.3083538693	relative importance
0.3083336500	crf models
0.3081244789	automatically learned
0.3080081038	$ grams
0.3080033079	performing model
0.3079453266	powerful tool
0.3078918673	shows significant
0.3078740745	whether
0.3078714868	a deep neural network
0.3078610286	efficient approach
0.3077568956	model encodes
0.3077348830	100
0.3077265641	becoming
0.3076938689	obtain competitive
0.3076362155	significant role
0.3076121099	empirical comparison
0.3074458269	graph to sequence model
0.3074362402	large amounts of training data
0.3073864330	paper attempts
0.3073675917	an unsupervised
0.3072534965	2.5
0.3071402370	points higher
0.3070288267	with recurrent neural networks
0.3069330348	efficient manner
0.3068293340	obtain significant
0.3068276017	techniques improve
0.3068216880	describe
0.3066001000	becomes
0.3065925678	large scale parallel
0.3065182582	actually
0.3065147879	promising research
0.3065042707	attention architecture
0.3064981912	training neural
0.3064959559	via
0.3064755508	corpus shows
0.3064753681	d vector
0.3064247719	few shot classification
0.3063660955	gets
0.3063643833	available at https
0.3063109444	evaluating text
0.3062308096	language similarity
0.3062059804	more engaging
0.3062010462	model demonstrates
0.3061760903	enough
0.3061662915	computational approach
0.3061084616	important research
0.3060512671	certain
0.3059372279	next sentence
0.3059152698	0
0.3058907536	shown significant
0.3058675215	reason about
0.3058273554	behind
0.3058174166	manual feature
0.3057625828	field of natural language processing
0.3057347861	amounts of unlabeled
0.3056705626	42
0.3056470938	traditional linguistic
0.3056204676	contrast to traditional
0.3055132007	a generative model
0.3054924005	own
0.3054723464	predict human
0.3054040362	respectively
0.3053655145	show
0.3053498252	methods for word
0.3052991976	crucial step
0.3052590711	learning to detect
0.3051658846	publicly available datasets
0.3051265500	existing pre trained
0.3051108440	autoregressive neural machine
0.3050567135	recent advances in deep learning
0.3050096617	code publicly
0.3049882701	time series data
0.3049684362	re scoring
0.3049423794	natural language interfaces to
0.3048491046	improve machine
0.3047923121	an unsupervised approach
0.3047455807	92
0.3047295309	neural information
0.3047214380	multiple words
0.3047021651	directly generate
0.3046799100	classifiers trained
0.3046491849	much faster
0.3045552293	already
0.3045502453	relations between words
0.3045252226	attention method
0.3045237330	very deep
0.3044896973	a pretrained language model
0.3044220348	main goal
0.3042925093	specific training data
0.3042699835	into
0.3042377968	improve accuracy
0.3042313481	made
0.3041950295	text classification using
0.3041318775	many real world applications
0.3040837769	recognition models
0.3039730751	and
0.3039679033	fixed number
0.3039416752	so called
0.3039127054	clinical natural language
0.3038382484	81
0.3038382484	39
0.3038258712	analysis method
0.3038016245	found
0.3037812245	facilitate research
0.3036877018	understanding models
0.3036848954	word word
0.3035045334	art extractive
0.3034535737	400
0.3034535737	71
0.3034448409	query auto
0.3034410357	large scale study
0.3034392869	previous unsupervised
0.3033597452	popular benchmark
0.3033382484	0.3
0.3033382484	4.0
0.3032474076	public benchmark
0.3031617272	beforehand
0.3031617272	3.3
0.3031617272	110
0.3031617272	73
0.3031365909	an ontology based
0.3030766995	requires reasoning
0.3030273345	larger number
0.3030008598	supervised deep learning
0.3029938446	underlying model
0.3029406816	learning based method
0.3029076186	yield better
0.3028752717	outperform standard
0.3027892441	bidirectional encoder representations from
0.3027670113	2.4
0.3027670113	3.2
0.3027227974	large scale speech
0.3027179890	converted into
0.3025106020	self supervised tasks
0.3023479166	input question
0.3023196548	28
0.3022766773	multi way
0.3021353181	generative dialogue
0.3021251165	often
0.3021134829	class classification
0.3020942954	relation extraction model
0.3020087712	extract relevant
0.3019990405	on ms coco
0.3019630833	per token
0.3018564134	questions about images
0.3018164728	side information
0.3017160073	34
0.3017160073	5.5
0.3016872904	45
0.3016596876	semeval 2020 task 4
0.3015750410	transcription system
0.3015588557	results showing
0.3014892042	a posteriori
0.3014463837	an interdisciplinary
0.3014117502	value decomposition
0.3013516413	fundamental task in natural
0.3012645959	learning language
0.3011696802	described
0.3011499087	information retrieval system
0.3011164231	multiple context
0.3010639831	method consistently
0.3010070543	exploratory study
0.3010034744	2.6
0.3010034744	47
0.3010034744	58
0.3010034744	46
0.3010034744	can't
0.3010034744	2.8
0.3009951594	wide range of
0.3009219940	representations from transformers
0.3008588028	level labels
0.3007946387	code changes
0.3006618544	a challenge set
0.3006444963	adaptation for neural machine translation
0.3005775667	extraction module
0.3005399544	mainly
0.3004579334	allows
0.3003927478	coverage grammar
0.3002630839	translation data
0.3002362324	sequence to sequence neural network
0.3001992081	hierarchical text
0.3001283627	full text
0.3000931074	meanings of words
0.3000632048	without knowing
0.3000172086	supervised baseline
0.2999445473	either
0.2998370829	wide variety of
0.2997672992	appear
0.2997302323	55
0.2997302323	140
0.2997302323	250
0.2997302323	78
0.2997302323	150
0.2997302323	kept
0.2997232783	character level word
0.2995747193	17
0.2994556463	76
0.2994520671	pair encoding
0.2994478037	\ approx
0.2993576897	approach based
0.2992797573	learns representations
0.2990212623	48
0.2989437427	higher bleu
0.2989217756	a 10
0.2988565963	methods suffer
0.2987986588	high computational
0.2987549260	generation performance
0.2987507631	standard text
0.2986481694	consider
0.2986387735	typically rely
0.2985982330	an end to end manner
0.2985225211	achieve significantly
0.2984944912	an increasing number
0.2983074427	provides
0.2981707548	aim to improve
0.2981501484	most frequent
0.2980701808	an important role
0.2980356942	improve neural
0.2979533676	improves upon
0.2979382912	joint goal
0.2978307982	require large amounts of
0.2977915996	multilingual document
0.2977033651	joint extraction
0.2976490037	natural language sentence
0.2974581082	\ url
0.2973961726	supervised manner
0.2972862097	collaboration between
0.2972788808	modern deep
0.2972071044	tasks demonstrate
0.2971742149	automatic method
0.2971472137	outperforms baselines
0.2970554865	generalizes well
0.2970447325	unseen during training
0.2970226114	multi channel speech
0.2970093487	co training
0.2968487577	type classification
0.2968160647	based parsing
0.2966147429	computational framework
0.2965828520	based keyword
0.2964639874	oriented dialogues
0.2964251215	wnut 2020 task
0.2963571030	very low resource
0.2962480043	approach named
0.2962243830	testing data
0.2959347378	tradeoff between
0.2959218820	nor
0.2958081535	decoder network
0.2957827866	studies focus
0.2957346790	indicate
0.2957248214	=
0.2957111578	uses
0.2956790011	a wide coverage
0.2956486025	low resource speech
0.2956399875	self attention mechanisms
0.2956005818	label text
0.2955118615	using naive bayes
0.2954830935	25
0.2954168046	generate natural language
0.2954001380	an unsupervised fashion
0.2952473591	a hybrid
0.2951699873	many to many
0.2951628163	supervised baselines
0.2950905459	designed to capture
0.2950807988	improve model performance
0.2950671204	end to end neural network
0.2950292267	existing word embedding
0.2949675415	benefited from
0.2947951436	viz
0.2947951436	1.7
0.2947617764	advances in natural language processing
0.2947556888	field of study
0.2946782833	resource scenarios
0.2946549048	a latent variable
0.2945052431	cross lingual entity
0.2944494260	&
0.2943675511	0.4
0.2943675511	0.9
0.2943675511	44
0.2943675511	69
0.2943675511	68
0.2943443440	source and target languages
0.2942583400	very small
0.2942439886	software quality in use
0.2942231449	recurrent convolutional neural
0.2941387088	a probabilistic model
0.2941209858	teams participated in
0.2940833117	an online fashion
0.2938947101	significant improvement over
0.2938732915	training task
0.2938299353	a conditional random field
0.2937563616	in domain training data
0.2937446131	occurrence patterns
0.2935901230	non autoregressive neural
0.2935268371	cross lingual dependency
0.2934787313	being
0.2934614599	trained from scratch
0.2933230370	mean opinion
0.2932780573	an essential step
0.2932720863	domain based
0.2931910858	existing word
0.2931295431	models capture
0.2931052424	machine translation based
0.2929793972	neural natural
0.2929648343	model achieves significant
0.2929466979	supervised word
0.2928818098	tasks in natural language processing
0.2928317578	learning cross
0.2928302694	experimental results show
0.2927751436	1.6
0.2927751436	0.8
0.2927121126	source data
0.2926323384	an empirical evaluation
0.2926277424	level training
0.2925713135	piece of text
0.2925572434	shows competitive
0.2924936855	a deep learning
0.2924575304	tweets annotated
0.2924545636	time domain
0.2923598239	based dialogue systems
0.2923393495	relation extraction using
0.2920944912	the cnn daily
0.2920780820	an annotated dataset
0.2919508566	multiple types
0.2919466660	model outperforms existing
0.2917459160	key aspects
0.2916814355	end to end text to speech
0.2916405348	end to end manner
0.2916195106	62
0.2916134089	rather than
0.2915909333	adhere to
0.2915671576	become
0.2915225365	multiple source
0.2915220963	paper makes
0.2915093852	a powerful tool
0.2915091656	cross lingual data
0.2914763357	sentiment analysis using
0.2914687465	experiment results show
0.2913588822	86
0.2912357014	track 2
0.2912320799	pre trained on large
0.2911758737	language domain
0.2911520616	neural systems
0.2911309851	language understanding and generation
0.2910953759	traditional supervised
0.2910929790	semeval 2010 task
0.2910922290	end to end approach
0.2909408552	supervised aspect
0.2908001323	simple methods
0.2907629953	aims to predict
0.2907482647	a multi task learning framework
0.2907120791	processing research
0.2906755826	an information extraction
0.2905902844	a long short term memory
0.2905661043	parametric speech
0.2905587419	human based
0.2904793857	findings indicate
0.2902935187	language models such as bert
0.2901705883	part of speech tagger
0.2901681429	for task oriented dialog systems
0.2901660956	existing method
0.2900521250	rich contextual
0.2899552478	automated fact
0.2896968987	a survey
0.2894567420	graph to text
0.2893917199	available at http
0.2893521777	joint entity and relation
0.2892768486	unsupervised bilingual
0.2892441783	self supervised speech
0.2892226531	memory neural
0.2891689100	networks for text classification
0.2890067671	jointly models
0.2889649946	most probable
0.2888683585	domain translation
0.2887352555	bleu points over
0.2886922766	generate answers
0.2886694265	state of
0.2885686355	must
0.2885495893	methods improve
0.2884297898	based reinforcement
0.2883838562	$ \ times
0.2883588822	2.3
0.2883588822	1.8
0.2882789480	errors introduced
0.2882615238	an upper bound
0.2882463831	changes
0.2882105086	contains
0.2880928719	likelihood training
0.2878327923	computer vision and natural language processing
0.2876363572	vast majority of
0.2875837391	containing
0.2875338301	gives
0.2874858124	probably
0.2874437692	large scale real
0.2874323382	its
0.2874259409	standard transformer
0.2874154719	self attention based
0.2873057312	an annotated corpus
0.2870776207	large monolingual
0.2869940761	something
0.2869344843	at least
0.2869005860	sacrificing performance
0.2868080385	$ k
0.2867957625	explicit modeling of
0.2867750715	self attention layers
0.2867321058	general text
0.2866961877	aims at
0.2866600095	domain dataset
0.2866420129	traditional topic
0.2866302830	important challenge
0.2864979823	significant challenge
0.2864367306	best performing
0.2864157384	require reasoning
0.2862666460	english tasks
0.2862535122	a neural architecture
0.2862417274	no answer
0.2860483893	performs significantly
0.2860300691	training approaches
0.2858876737	number of clusters
0.2858121390	training mechanism
0.2857306621	allow
0.2856857982	a broad coverage
0.2856397294	media users
0.2856297613	automatic quality
0.2856152924	36
0.2856152924	29
0.2854648548	recurrent language
0.2853798327	task b
0.2850842731	many to one
0.2850722666	data driven method
0.2850156582	governed by
0.2848459216	search systems
0.2847649911	using machine learning
0.2846288749	find
0.2845482623	perform significantly
0.2844839120	baselines and state of
0.2843687068	trained on large scale
0.2842233955	if then
0.2842071276	weighted f1
0.2841041089	two phase
0.2840844295	took
0.2840739477	automatic question
0.2840332272	generate summaries
0.2840116379	and vice versa
0.2838752717	rnn encoder
0.2838481584	thoroughly
0.2836887644	speech recognition performance
0.2836574614	up to date
0.2836093151	english statistical
0.2835674170	q network
0.2835400894	achieved high
0.2835121981	general language
0.2834925508	important aspect
0.2834827123	networks trained
0.2834345523	hardly
0.2833645233	200
0.2832654434	extracting relevant
0.2832538383	one to one
0.2830219526	where
0.2830158291	capture rich
0.2829533551	datasets validate
0.2828927860	indicated
0.2828591345	level concepts
0.2828489121	wherein
0.2828411581	method significantly improves
0.2827827213	model types
0.2827200006	study aims
0.2826957026	generation method
0.2826838493	key step
0.2825943973	n gram language model
0.2825942109	build upon
0.2824876265	deep neural network model
0.2824364190	contain
0.2823797941	connected layer
0.2823619295	strong correlation between
0.2822988499	paper demonstrates
0.2819530603	maintaining high
0.2818626643	entity and relation extraction
0.2818477452	language inference task
0.2818441477	non commercial
0.2817604087	a 20
0.2817602932	hundreds of millions
0.2816784245	speech systems
0.2816228652	diachronic word
0.2815478416	existing dataset
0.2815231957	27
0.2813804480	difficult to obtain
0.2813518821	translation problem
0.2812314401	resort to
0.2812073726	correct meaning
0.2811997043	a small fraction
0.2811029960	deep reinforcement learning for
0.2810834695	neural network based model
0.2810699269	generated dataset
0.2810637467	partly due
0.2809993773	points on average
0.2809691336	ability to detect
0.2809359725	parser based
0.2808761372	this dissertation
0.2808596173	a neural approach
0.2808416617	correlates well with
0.2805690446	identify important
0.2804402592	aim to learn
0.2804043285	a deep architecture
0.2803946952	24
0.2803322719	order to enable
0.2802717706	question answering using
0.2801839494	latent information
0.2801767392	learning based systems
0.2799937839	22
0.2799421832	aims to detect
0.2798917961	70
0.2798623568	challenge 2019
0.2798610757	training large
0.2798045821	should
0.2797496227	using recurrent neural networks
0.2797036281	types of noise
0.2796730382	cross lingual model
0.2796023622	somewhat
0.2795126750	generate candidate
0.2794682939	encoding models
0.2794002158	specific text
0.2793572553	both in domain
0.2793372540	language comprehension
0.2793100921	towards building
0.2792858895	more than 50
0.2791492062	document based
0.2791237894	method to automatically generate
0.2790587864	mean field
0.2790327269	standard language
0.2790022180	generation aims
0.2789692965	knowledge domain
0.2789378734	mt system
0.2789310952	general semantic
0.2789268428	outperforms standard
0.2789150136	language texts
0.2788942030	truth labels
0.2788148104	resorting to
0.2788042188	tagging models
0.2787923210	yields significant
0.2786376687	growing interest
0.2786174325	language models like bert
0.2784883733	this article presents
0.2784469696	achieve strong
0.2784431795	previous state
0.2784413400	this paper
0.2783491517	relationships among
0.2783315572	relative improvement over
0.2783126239	300
0.2783126239	32
0.2783126239	became
0.2783015990	progress towards
0.2782880619	rnn t model
0.2782418202	large scale corpus
0.2780620381	97
0.2780593258	outperform traditional
0.2780448595	specific fine tuning
0.2780438738	itself
0.2779665952	give
0.2779280264	insight into
0.2778791669	40
0.2778257096	generate questions
0.2774739915	hampered by
0.2773642002	sentence prediction
0.2772951192	area of natural language
0.2772946351	a data driven
0.2772622812	a substantial margin
0.2772390514	bias in word embeddings
0.2771468938	approach consists
0.2770910915	obtained results
0.2770387497	correlate well with human
0.2769544548	a large scale multi
0.2768268977	world networks
0.2767956293	could
0.2767912001	representation techniques
0.2767848461	whereby
0.2767402936	the stanford question
0.2767000381	based document
0.2766935794	click through
0.2766812570	range of nlp tasks
0.2766798300	namely
0.2765832605	a deep neural
0.2764563925	point out
0.2762761523	seen during training
0.2762208153	far less
0.2761402247	large real world
0.2760656604	may
0.2760198305	supervised approach to
0.2759903074	believe
0.2759568051	based acoustic models
0.2759486544	time period
0.2758467352	$ calculus
0.2758415737	recognition research
0.2758405056	existing end to end
0.2758148104	resorted to
0.2756359727	learning module
0.2756327424	sub word
0.2756123835	change over time
0.2755842233	existing entity
0.2754824608	was
0.2752603440	etc
0.2752396009	much lower
0.2751269698	end to end multi
0.2750650250	method significantly
0.2750597270	detection based
0.2750096706	based reinforcement learning
0.2749893359	them
0.2749422498	time and memory
0.2749031252	model to jointly
0.2745971732	their
0.2745127854	paper considers
0.2744655264	has
0.2744655264	is
0.2744352130	second place
0.2743343861	mikolov et
0.2743035626	larger training
0.2742488193	requires large amounts of
0.2740908924	seen and unseen
0.2740605208	two stage approach
0.2740371902	paper extends
0.2739591408	benchmark for evaluating
0.2739482993	were
0.2738876507	coherence model
0.2738238209	at semeval 2020 task
0.2737989937	will
0.2737768620	whose
0.2737619107	previous work
0.2736589120	quality in use
0.2735828875	can
0.2734655264	been
0.2734655264	be
0.2733075350	first person
0.2732947857	very large
0.2732375186	audio text
0.2731141750	that
0.2730776588	tend to produce
0.2729751342	might
0.2728267895	word corpus
0.2727859086	a systematic comparison
0.2727570080	\ `
0.2727428631	qa data
0.2726878851	approach substantially
0.2726153967	@ 1
0.2725896741	an incremental
0.2724655264	have
0.2724655264	which
0.2724356938	n gram language models
0.2723402911	a lightweight
0.2723263936	user based
0.2722965281	onto
0.2722830704	selection framework
0.2722720382	analysis framework
0.2721593416	document summaries
0.2721539736	end to end deep learning
0.2720047442	name entity
0.2718710038	hybrid approach for
0.2717871950	english task
0.2717769307	limitations of existing
0.2716292157	specific corpora
0.2715915992	ability to generate
0.2714655264	are
0.2714153074	1.0 bleu
0.2713406296	large amounts of annotated
0.2712755785	answer pair
0.2712618937	current context
0.2710418001	learning and deep learning
0.2707963584	time varying
0.2707430268	an empirical
0.2706837056	multiple knowledge
0.2706539364	input graph
0.2705961344	results prove
0.2705873000	a benchmark dataset
0.2705627949	had
0.2705416580	neural data to
0.2705318089	art deep learning models
0.2704835819	aiming at
0.2703930030	case based
0.2702758549	computational analysis
0.2702483276	character level language
0.2701639029	method outperforms previous
0.2700772890	large amounts of data
0.2700475761	based optimization
0.2700241206	language model trained
0.2699292367	text query
0.2699027576	themselves
0.2698933922	approach requires
0.2697648433	two step
0.2694579571	attention in recent years
0.2693516173	task aims
0.2693463628	model outperforms strong
0.2693320898	based matching
0.2692838560	decoder architecture
0.2692357108	viewed as
0.2692138464	cannot
0.2692014874	media platform
0.2690566980	distant language
0.2689690402	pre trained deep
0.2686058536	attempts to address
0.2685295733	obtain high
0.2683771121	does not require
0.2683422177	cross lingual sentence
0.2682877489	semantic context
0.2682674175	benefiting from
0.2682328716	model inference
0.2682068826	next token
0.2681119349	become increasingly
0.2680130262	opinion score
0.2679104645	performance improves
0.2679076184	$ n
0.2678805131	would
0.2677619296	an unsupervised manner
0.2677505727	an adaptive
0.2676890087	named entity recognition and relation
0.2676724846	natural language processing methods
0.2675908170	approaches based
0.2675565326	transformed into
0.2675561917	training language
0.2672672261	conventional machine
0.2672193345	succeeded in
0.2671739674	sequence of words
0.2671513087	baseline system
0.2671201949	medical language system
0.2670913429	traditional approach
0.2670377175	based ner
0.2670354151	at semeval 2019 task
0.2670002482	varying levels of
0.2669311468	in domain and
0.2669235029	shown to outperform
0.2668822664	categorized into
0.2667535932	give rise to
0.2667209811	achieves similar
0.2666827236	an encoder decoder model
0.2666031304	few attempts
0.2662803779	linking model
0.2662799484	obtains new state of
0.2662177157	models for speech recognition
0.2659409986	few shot setting
0.2658636915	distributed vector
0.2658465456	external linguistic
0.2658271053	models provide
0.2657221059	two level
0.2655833532	performs significantly better
0.2655047747	\ sigma
0.2653804169	in task oriented dialog
0.2653694588	experiments on two benchmark datasets
0.2653548398	embeddings provide
0.2650481477	dominated by
0.2650023194	model achieves state of
0.2648067878	detection method
0.2647880447	10 ^
0.2647176592	a simple baseline
0.2646546136	related to covid 19
0.2646495112	approaches rely
0.2644993308	end to end spoken language
0.2644759396	pairs of words
0.2644670978	word level language
0.2644146688	extensive experiments conducted
0.2642796396	a crucial component
0.2642602326	a 4
0.2642373567	in one language
0.2640956442	based solution
0.2639411926	\ epsilon
0.2638952914	current sentence
0.2638831986	choice question
0.2638407088	sequence to sequence approach
0.2637290381	frequency words
0.2637171801	recognition error
0.2635895700	inspired by
0.2635706828	learning architecture
0.2635280083	global latent
0.2634692613	syntactic and semantic information
0.2634392441	model bias
0.2633850043	large pretrained language
0.2632621936	presented here
0.2632147212	research dataset
0.2629871041	speech models
0.2629548373	using convolutional neural networks
0.2629475614	automatic generation of
0.2628718474	coping with
0.2628660907	oriented spoken
0.2628581795	trivial task
0.2628413114	a multi modal
0.2628088815	regarded as
0.2626924077	prior work
0.2626824708	input document
0.2626811598	suffer from
0.2626771118	two dimensional
0.2626621448	based topic
0.2626160588	a probabilistic framework
0.2625928174	across multiple languages
0.2624512184	an improved
0.2623752935	step process
0.2623727467	a semi automatic
0.2623163698	based word embeddings
0.2622373567	the two methods
0.2622373567	the different methods
0.2622314909	time of writing
0.2622159562	tune bert
0.2620482934	common language
0.2620402240	time expressions
0.2620232795	character level neural machine
0.2618133631	incorporated into
0.2617964167	input word
0.2616909738	based learning
0.2616449186	on demand
0.2616365317	distinction between
0.2616320050	generate meaningful
0.2616279337	languages such as english
0.2615909489	high quality training
0.2615416500	learning settings
0.2613557803	representation learning model
0.2612887303	state of art results
0.2612717182	part of speech taggers
0.2612273808	still unclear
0.2611915893	an attentional
0.2610947435	a hybrid model
0.2609501925	a hidden markov model
0.2608139592	learning applications
0.2607951752	relationships between
0.2607937852	the art methods
0.2606311463	^ 6
0.2605439940	level model
0.2605351072	based graph
0.2604693031	represented as vectors
0.2604287639	probabilistic approach
0.2604137894	features generated
0.2604049843	results in improved
0.2603114847	novel objects
0.2602328112	the two models
0.2600535143	an ensemble model
0.2600250338	class words
0.2599056522	current state of
0.2597898022	a critical component
0.2597727425	recognition system
0.2597642565	one dimensional
0.2597535069	supervised information
0.2596837554	transformer based language
0.2596197217	aims at identifying
0.2595935535	existing machine
0.2595008900	code analysis
0.2594709668	simple model
0.2592915417	mediated by
0.2592523088	pre training language
0.2591111211	identify words
0.2591078765	domain text
0.2590598573	aim to develop
0.2589738209	at semeval 2017 task
0.2589681944	a hierarchical
0.2589243674	existing document
0.2588773444	verification systems
0.2587406745	paper focuses
0.2585625047	influenced by
0.2583463907	tend to generate
0.2582938952	even worse
0.2582474714	the teacher student
0.2581929099	pre trained sentence
0.2580801690	media based
0.2579951828	multi class text
0.2579639684	looked at
0.2579305317	based dialog
0.2578981915	translating between
0.2578459454	representations of words
0.2577536130	leads to improved
0.2575789942	time consuming process
0.2574415586	high quality dataset
0.2573558820	distributions of words
0.2572720681	enable users
0.2572222389	does not
0.2571310752	did not
0.2570597908	semantic relations between
0.2570214799	sequence to sequence based
0.2570169160	a new benchmark dataset
0.2569175463	style reading comprehension
0.2568820075	machine translation methods
0.2568520809	explicit representation
0.2564449299	outperforms existing models
0.2563021943	into account
0.2562200422	task 1
0.2560194225	unsupervised discovery of
0.2559610737	large scale human
0.2559154659	a step forward
0.2558998111	neural machine translation for
0.2558724157	final translation
0.2557672538	lead to improved
0.2557348379	detection datasets
0.2557157113	unavailability of
0.2557005659	the large number
0.2556901432	learning based models
0.2556480172	neural dialog
0.2553710330	while preserving
0.2552436934	an automated
0.2551761454	1.5 bleu
0.2551382819	one or multiple
0.2550881558	for authorship attribution
0.2550746055	descriptions of entities
0.2550669368	a controlled natural language
0.2550192402	over fitting
0.2549883522	large pool of
0.2549825401	past work
0.2549774058	level analysis
0.2548738474	an attentive
0.2548544543	speech recognition with
0.2547074420	and accurate neural
0.2547035307	achieves new state of
0.2546665696	humans do
0.2545317108	correlations with human
0.2545152925	balance between
0.2545148407	statistical machine learning
0.2544812111	transfer task
0.2544060089	work in progress
0.2543876692	the new york times
0.2543097043	a priori
0.2542497951	an experimental
0.2541201660	native speech
0.2538933610	in spite
0.2537109523	present experimental results
0.2536637784	this article describes
0.2536200743	specific dataset
0.2535672918	model accuracy
0.2535551042	n gram features
0.2535200656	critical task
0.2534479867	complex neural
0.2533756197	represent entities
0.2532373567	to new tasks
0.2530880309	answering system
0.2529245849	2019 shared task
0.2529205532	adversarial neural
0.2528213819	wider range of
0.2528178891	existing evaluation
0.2527837701	recent advances in neural
0.2527726553	comprehensive set of
0.2527142918	recurrent neural network language
0.2526018565	accompanied by
0.2525931537	computational social
0.2525771599	modeling algorithms
0.2525111639	extraction framework
0.2524848379	training algorithms
0.2524674455	each time step
0.2524026225	distributional model
0.2523985151	make sense
0.2523937541	framework for building
0.2522392191	joint task
0.2522278098	a noisy channel
0.2521996212	re estimation
0.2521300769	number of speakers
0.2520932625	speech recognition using
0.2520778934	out of vocabulary words
0.2520692799	inference dataset
0.2520511582	method consists
0.2520470196	non negative
0.2520024352	2018 shared task
0.2518308099	recurrent neural network model
0.2517489278	vocabulary words
0.2517328571	task 2
0.2516621650	an introduction
0.2516420965	rely on
0.2515267071	speech and text data
0.2514262898	corpus consisting
0.2512758855	field of research
0.2512368195	temporal relations between
0.2512179792	broad range of
0.2511614911	\ ^
0.2511255268	systems achieve
0.2511106327	a computational framework
0.2510590501	data resources
0.2510323290	model achieves comparable
0.2509436411	bridge between
0.2509109949	achieves state of
0.2507709356	methods proposed
0.2507468455	performs better than
0.2501169692	supervised parsing
0.2500981599	language classification
0.2499946628	search task
0.2499472784	sentence level semantic
0.2498876317	a deep
0.2498147478	computer vision and natural
0.2497270088	existing attention
0.2496946203	task 11
0.2495894979	towards understanding
0.2495742307	success of deep learning
0.2494693137	this short paper
0.2493725740	several strong baselines
0.2493665015	a deeper understanding
0.2493595202	this paper investigates
0.2492305741	proposed systems
0.2490110206	detection and slot filling
0.2490019300	large amounts of parallel
0.2488348933	a computational model
0.2487729499	input speech
0.2487458349	from free text
0.2487331646	on wmt
0.2485904689	decoder to generate
0.2485602485	conduct experiments on
0.2485173715	an xml
0.2483759095	in recent years
0.2482261479	representation language
0.2482015183	combines multi
0.2481937852	the art results
0.2481628649	popular neural
0.2480974703	a large margin
0.2480292581	sentence embeddings using
0.2480066909	relying only on
0.2479692543	without parallel data
0.2479525174	performs on par with
0.2478854455	a data efficient
0.2478722118	pre trained neural
0.2478604518	the art performance
0.2477870788	language understanding models
0.2477751496	consisting of
0.2476619506	embeddings encode
0.2476250708	a multi task
0.2476023318	vast number of
0.2475241489	available on github
0.2474870412	generalization to new
0.2473681377	achieve state of
0.2473595202	this paper explores
0.2473503249	network training
0.2472976635	caused by
0.2472534520	well below
0.2472398025	millions of users
0.2471185957	popular word
0.2470305218	human like
0.2469848145	discriminate between
0.2468882377	rules based
0.2468747543	gpt 2 model
0.2468162841	reasonably well
0.2468120366	intuitive way
0.2467868867	more accurate
0.2467614209	art systems
0.2466844400	knowledge distillation for
0.2466068882	resource language
0.2465717159	required to train
0.2464423851	hypothesis only
0.2464321732	focuses on
0.2463086663	high quality text
0.2462883786	supervised learning method
0.2462173134	based decoder
0.2461632530	learn features
0.2461524611	name recognition
0.2461422519	disagreement between
0.2461260829	research shows
0.2461184166	publicly available corpus
0.2460935040	trained transformer models
0.2460831599	data annotated
0.2460089848	six languages
0.2459822276	human dialog
0.2459707360	lingual tasks
0.2459312161	method for detecting
0.2458408955	\ sim
0.2457907828	compact representation of
0.2457549457	differences between
0.2456840337	first and second
0.2456814074	labeling model
0.2455367222	text generated
0.2455332860	d \
0.2454677411	develop methods
0.2454434213	language processing models
0.2454309028	compared to conventional
0.2453914374	n gram based
0.2452361193	gram model
0.2451871322	side monolingual data
0.2451549788	analysis results
0.2451368492	general method
0.2450111843	turn conversation
0.2449010504	+ +
0.2448317731	experiments on real world
0.2447959686	effective technique
0.2447873276	language sentence
0.2447309490	largest dataset
0.2446975063	domain dialogues
0.2446480555	$ \ textbf
0.2446403435	depending on
0.2445189784	an adversarially
0.2445090483	an open question
0.2445056618	art supervised
0.2444994779	of such models
0.2443672050	` s
0.2443343995	vision and natural language processing
0.2443048219	this paper introduces
0.2443039395	obtained from https
0.2442583956	the other hand
0.2442487069	based sequence
0.2441499416	mismatch between
0.2441373567	from one language
0.2441213184	an exploration
0.2440587933	three stage
0.2440559490	framework consists
0.2440215564	domain dialogue state tracking
0.2439215610	based lstm
0.2439073631	i vectors
0.2438912266	grouped into
0.2437968073	compared to previous
0.2436968404	a flexible
0.2436589487	information based
0.2434764704	reasoning task
0.2434421188	a quantum
0.2434402703	imperative to
0.2434097389	do not
0.2433901867	one billion
0.2433841699	this end
0.2433249368	detection using
0.2433009552	a closer look at
0.2432520471	query by example spoken
0.2432060046	the last few years
0.2431925133	network framework
0.2431424280	frequently used
0.2430440054	outperform baseline
0.2430114432	a cross domain
0.2428454718	level feature
0.2427907903	embeddings represent
0.2427794879	neural tensor
0.2427435463	improve upon
0.2427172689	cater to
0.2426898649	reasoning about
0.2426752663	determining whether
0.2426738727	at semeval 2018 task
0.2426517822	large document
0.2425514556	general model
0.2424477447	space time
0.2423165506	proposed model significantly
0.2423023387	effective strategy
0.2422657029	greater than
0.2422038397	automated method
0.2421340261	an unbiased
0.2421186693	modeling performance
0.2420502005	art natural language processing
0.2420203266	vast amount of
0.2418950996	establish new state of
0.2418578477	crucial task
0.2418210429	detection of fake
0.2417971199	series data
0.2415989564	recent advances in
0.2415937008	ranging from
0.2415657397	recent neural network
0.2415479771	an arabic
0.2414841307	rely only on
0.2413930615	an effective
0.2413506950	based convolutional neural
0.2413297868	achieved state of
0.2412932678	learning literature
0.2411745915	relations between entities
0.2410974547	processing methods
0.2410836206	valuable source
0.2409233314	training from scratch
0.2408564654	systems developed
0.2408232370	a unified model
0.2407217509	recognition from speech
0.2406404623	example sentences
0.2405862421	investigate whether
0.2404951169	this article
0.2404493109	trained to predict
0.2404182067	information networks
0.2404087056	performance results
0.2403315109	sequence to sequence speech
0.2403237006	self learning
0.2403190329	hierarchical reinforcement
0.2402814580	based text
0.2402765947	the support vector machine
0.2402108036	proposed measure
0.2401670018	collected from twitter
0.2401522435	evidence to support
0.2401461284	a review
0.2401010504	based neural model
0.2400996276	in natural language processing
0.2400202807	akin to
0.2400190288	non adversarial
0.2399823508	aims to address
0.2399166229	large scale pre
0.2399005491	based encoder decoder
0.2398931313	an efficient approach
0.2398612163	achieve better results
0.2398081195	annotated text
0.2397707994	for myanmar
0.2397583266	suffering from
0.2397563988	a holistic
0.2397389555	a bayesian approach
0.2396125610	eight languages
0.2395261527	highly dependent on
0.2395098512	recurrent neural network with
0.2394545256	method relies
0.2393736717	without retraining
0.2393731064	using machine learning techniques
0.2393435657	very little
0.2392006519	model significantly improves
0.2390302591	according to
0.2389190819	an enhanced
0.2389178809	an interpretable
0.2388657291	extend previous
0.2387707681	the sake
0.2386019535	neural machine translation with
0.2385065171	conversational system
0.2384560922	novel neural network
0.2384099321	modern machine
0.2383546470	baseline trained
0.2382498691	studied problem
0.2382296519	data generation
0.2382270509	efficient model
0.2381665146	to tune
0.2381398166	a scalable
0.2380987483	called word
0.2379351163	based ctc
0.2379035558	in neural machine translation
0.2378786046	denoted by
0.2378261247	multilingual parallel
0.2377931829	existing corpus
0.2377787943	embeddings models
0.2377590382	collections of documents
0.2376564662	art neural network
0.2376124874	task c
0.2375858555	a unified framework
0.2375165496	language questions
0.2374812382	scale pre
0.2373622022	large number of
0.2373205808	network language model
0.2373021821	there exist
0.2372877102	a and b
0.2372791682	large amount of unlabeled
0.2372259281	a corpus
0.2370239327	an open
0.2370234331	relationship between
0.2370141198	characterized by
0.2368809533	temporal knowledge
0.2367993838	ability to perform
0.2367986848	multi head self
0.2367381373	little training data
0.2367237483	similar word
0.2367232658	10 million
0.2367090062	paper shows
0.2366781720	tend to
0.2366320520	information obtained
0.2366080129	network based approaches
0.2365032908	short term memory network
0.2364344278	simple neural network
0.2363983639	a text mining
0.2363933844	shown to improve
0.2363359360	lead to
0.2363161814	performance on downstream tasks
0.2362713026	an end to end
0.2362474570	simple question
0.2362036075	a one to
0.2361426041	multiple public
0.2360501743	a unifying
0.2359983520	classification framework
0.2359719366	including data
0.2359662477	the sigmorphon
0.2358946798	serve as
0.2358870412	focus more on
0.2358798323	end to end automatic
0.2358423254	language use
0.2357794879	solely based
0.2357794879	based solely
0.2357418527	synthesis system
0.2357048511	for word sense disambiguation
0.2356692031	trained on large amounts
0.2356094254	simple and effective method
0.2354260565	model pre trained
0.2354123044	original speech
0.2353175862	before and after
0.2353099227	a fast
0.2352561828	multiple relation
0.2351407690	terms of bleu score
0.2351252716	a thorough evaluation
0.2350960936	strong neural
0.2350335036	difficult to understand
0.2350321308	representation method
0.2350205709	a deep learning model
0.2349836279	very large scale
0.2349656335	demonstrate improvements
0.2349594727	insights into
0.2349591406	retrieval techniques
0.2349576798	large volumes of
0.2349424392	existing neural
0.2349073646	leads to
0.2347943900	modified version of
0.2347588077	level knowledge
0.2347199640	architecture outperforms
0.2347064067	pertaining to
0.2346527917	relying on
0.2346231843	a practical approach
0.2345355237	study proposes
0.2345218048	an open vocabulary
0.2344918855	increase performance
0.2344829340	average f1 score of
0.2344408341	problem in natural language
0.2343307707	second subtask
0.2343273342	based word embedding
0.2343079809	an in depth
0.2342863604	aim to provide
0.2342701954	advances in deep learning
0.2341665022	related text
0.2340964115	turn response
0.2339927914	simple technique
0.2339397922	based network
0.2339207918	focus on
0.2337348483	multi task learning with
0.2337230716	state machines
0.2335855926	proposed task
0.2335820139	fall into
0.2335709392	bias problem
0.2335703875	unlike previous work
0.2335466206	lambek calculus with
0.2334777690	provide strong
0.2334496369	based purely on
0.2333768420	performance significantly
0.2333608188	neural machine translation by
0.2332991711	language sentences
0.2332480353	clear whether
0.2331467552	classification using
0.2331382781	supervised topic
0.2330906725	specific semantic
0.2330746248	prediction problems
0.2329149162	connection between
0.2329014364	automatic extraction
0.2328594490	experimental results on
0.2328246542	compared to traditional
0.2328006763	data scenarios
0.2328000377	use case
0.2327882874	recent advances in natural language processing
0.2327806044	consistent improvements over
0.2327538963	for statistical machine translation
0.2327252732	based sequence to sequence
0.2327240204	detection approaches
0.2326146705	techniques including
0.2325959754	demonstrate strong
0.2325863433	publicly available data
0.2325770139	virtue of
0.2325469111	the winograd schema
0.2324631034	unclear if
0.2324236491	lack of sufficient
0.2323601488	able to
0.2323505930	correlation between
0.2322531355	parsing approach
0.2322215414	important step
0.2321082691	model based approach
0.2320239154	slightly better than
0.2319699796	a generalized
0.2319185052	correspondence between
0.2318144293	\ alpha
0.2316451809	accounted for
0.2316393960	workings of
0.2315982615	step toward
0.2315705980	underlying language
0.2315579894	a corpus based
0.2314994298	close to human
0.2314914835	existing data
0.2312473144	based annotation
0.2312246695	comparison to existing
0.2311848492	real world datasets show
0.2311606295	for low resource languages
0.2311542118	last few years
0.2310829301	achieving state of
0.2310100649	via policy gradient
0.2310088613	distance between
0.2309696732	relies on
0.2309299390	study presents
0.2309045227	challenge 2017
0.2308653952	popular research
0.2308181289	$ p
0.2307980962	leads to significant
0.2306287169	for multi turn response selection
0.2305838439	at various levels
0.2304714535	previous state of
0.2304613506	key task
0.2303882081	chinese named
0.2303798273	shed light on
0.2303367656	very few
0.2302360617	significantly outperforms state of
0.2301939946	within and across
0.2301790562	the spirit of
0.2300718322	set of candidates
0.2300559703	a neural network architecture
0.2299418069	followed by
0.2299403644	comparable or better
0.2299201916	motivated by
0.2298354176	test time
0.2298298557	model performed
0.2297506095	look at
0.2297392019	data conditions
0.2297360976	speech information
0.2295431068	transfer model
0.2295346366	important component
0.2294781149	automatic recognition
0.2293965790	a neural network based
0.2293835217	a neural based
0.2293714027	knowledge extracted
0.2293084436	trained language models
0.2292412853	an order of magnitude
0.2292376463	level embeddings
0.2292350278	lack of training data
0.2291587877	devoted to
0.2291369512	level accuracy
0.2291235957	benchmark task
0.2291088442	flow of information
0.2290880494	standard sequence
0.2290640003	knowledge base question
0.2289941136	across languages
0.2289133162	ongoing work
0.2289127081	data generated
0.2287892924	a finite state
0.2287827591	wealth of
0.2286486042	for spoken language understanding
0.2285587787	relationships between words
0.2284378290	proposed approach significantly
0.2284135833	model augmented
0.2283771434	achieves significantly
0.2283539001	some preliminary
0.2281517789	level context
0.2281108247	of such systems
0.2280819859	biased towards
0.2280162530	aims to improve
0.2279405944	still lacks
0.2278305848	to normalize
0.2277816492	comprehensive overview of
0.2276801399	a two stage
0.2276192658	method achieves state of
0.2275198076	time and space
0.2275156972	embeddings generated
0.2275057141	based classification
0.2274962877	deals with
0.2274820798	speech taggers
0.2274630604	sentiment classification using
0.2273388523	learning scenario
0.2272960624	source speech
0.2272837420	method named
0.2272607970	rnn language
0.2272343263	chat dataset
0.2272309412	task dataset
0.2272235122	step towards
0.2271925421	learn word
0.2270607428	emotion recognition from
0.2270581607	benchmark results
0.2270576912	varies across
0.2269964993	explore methods
0.2268635733	essential role in
0.2267756085	a multi
0.2267603675	suffers from
0.2267211797	public data
0.2267095850	a graph neural
0.2266923489	embeddings learned
0.2265130515	in response to
0.2265048428	body of research
0.2264096424	sentiment analysis model
0.2263283791	large training
0.2263254543	owing to
0.2262522045	tagging problem
0.2262026009	prediction methods
0.2261330636	speech classification
0.2260939467	identification using
0.2260846436	dataset provided
0.2259650519	results in significant
0.2259368471	a machine learning
0.2258727244	set of images
0.2258501834	t s learning
0.2258454616	task in natural language processing
0.2258374756	computation time
0.2256416154	a portable
0.2256030855	become one of
0.2255944883	multitask learning for
0.2255738765	learns to generate
0.2255362517	based speech
0.2254972316	require human
0.2254727356	approaches perform
0.2254397425	second language
0.2253196754	serves as
0.2253047879	a lot
0.2252308075	a neural network model
0.2252223031	dominant approach
0.2251395064	understanding module
0.2250239103	original bert
0.2249899006	this study explores
0.2249741748	standard attention
0.2249419379	lead to substantial
0.2249154546	experimenting with
0.2248992223	@ 10
0.2248940713	this issue
0.2248510588	improve model
0.2248452090	for visual question answering
0.2248232267	based entity
0.2247509884	data for training
0.2247507700	a probabilistic
0.2246657135	datasets from two
0.2246199327	aims to learn
0.2245915495	known in advance
0.2245431068	based tasks
0.2245365444	the ibm
0.2243453682	for audio visual
0.2243431295	this position paper
0.2242692374	lingual learning
0.2242692278	designed to measure
0.2242367408	improvement over
0.2241843030	n 1
0.2241728438	existing benchmark
0.2241711942	magnitude larger than
0.2241480709	based encoder
0.2240816598	an individual's
0.2239981831	an efficient algorithm
0.2239688761	information conveyed by
0.2238730734	refers to
0.2238480011	tree structured neural
0.2236291357	underlying knowledge
0.2236187804	model achieves state
0.2235522806	performs significantly better than
0.2235383947	conduct extensive experiments on
0.2234638671	pre trained language models like
0.2234594591	a smart
0.2234494074	model for joint
0.2234121334	huge amount of
0.2234054607	experiments on two public
0.2233069067	a low resource setting
0.2232154258	due to
0.2230132233	specific training
0.2228886376	recent work
0.2227950235	caption pairs
0.2227407254	with limited training data
0.2226973933	induction task
0.2226821647	gives rise to
0.2226628938	sequence of events
0.2225356491	shown to capture
0.2225187917	a multilingual
0.2225066354	much simpler
0.2224929955	propose methods
0.2224748273	based alignment
0.2224054491	based embeddings
0.2223817544	for neural machine translation
0.2223598916	this thesis
0.2223367081	using convolutional neural
0.2222611449	received much
0.2221128187	a simple
0.2220537552	transferring knowledge from
0.2220341292	language queries
0.2220305826	belongs to
0.2220185928	using deep neural network
0.2220090015	experiments on benchmark
0.2219726616	subjected to
0.2219072747	gained much
0.2218886232	multi modal neural
0.2218754429	without affecting
0.2218295850	denoted as
0.2217862605	hindered by
0.2217353304	learning for low resource
0.2217350517	based semi supervised
0.2217089810	art end to end
0.2216615292	a recipe
0.2216548213	automatic identification of
0.2216472482	on chinese english
0.2215562673	significant gains over
0.2215247697	monolingual training
0.2215193214	empirical experiments
0.2215016886	automatic discovery of
0.2214527801	task data
0.2213992322	proposed features
0.2212790911	after fine tuning
0.2212468681	results indicate
0.2212147520	ask whether
0.2211546037	more sophisticated
0.2211438359	a multitask learning
0.2210997918	distinguishes between
0.2210680186	specific models
0.2210119960	composed of multiple
0.2209890737	nlp approaches
0.2209270770	higher than
0.2208177192	prediction using
0.2207816896	retrieval method
0.2207352131	10 times
0.2206318689	approach relies
0.2205937008	focusing on
0.2205678656	based end to end speech recognition
0.2205470551	multiple text
0.2205397401	belong to
0.2204552536	trained neural
0.2203924139	gap between
0.2203678224	the art baselines
0.2203455496	the proposed model
0.2202501340	resolution task
0.2200566114	open source toolkit for
0.2199935316	meaning of words
0.2199754884	self training method
0.2199620939	end to end memory
0.2199404458	shown to achieve
0.2199188645	fine tuned on
0.2198994733	features obtained
0.2198897175	performance based
0.2196448344	a machine learning approach
0.2194275397	an integrated
0.2193592762	time frequency
0.2193384334	data based
0.2193111215	contrast to previous
0.2191515464	achieved great success in
0.2191452111	learning to generate
0.2190597467	representation based
0.2189952746	embeddings trained
0.2189937917	$ \ textit
0.2188692278	designed to support
0.2188486914	a new paradigm
0.2188238922	experiments provide
0.2187692374	lingual text
0.2186254399	conform to
0.2185733314	of 100
0.2185059971	compared to
0.2184745213	the u.s
0.2184718652	difference between
0.2184170026	information contained in
0.2183628395	a case
0.2182916149	multi task learning for
0.2182279034	provide insight into
0.2182060238	part of speech tag
0.2181067420	novel words
0.2180698592	problem by introducing
0.2180390053	learns to predict
0.2179450537	on line
0.2179421131	still remains
0.2178765944	zero shot setting
0.2178404037	an innovative
0.2178302094	existing dialogue
0.2177117342	text generation using
0.2176109924	system description
0.2175272357	a deep generative
0.2174994688	a stable
0.2174969998	learn to predict
0.2174718848	a robust
0.2174577837	syntactic and semantic features
0.2173891859	relations between events
0.2173832743	pieces of text
0.2172169099	automatic construction
0.2172120230	until recently
0.2171734510	based information extraction
0.2170776532	an intelligent
0.2170249111	hate speech detection on
0.2169333232	demonstrate improved
0.2169191801	errors introduced by
0.2169151725	with other models
0.2168785266	$ l
0.2168424432	online user
0.2168311974	range of topics
0.2168089855	important task in natural
0.2167437899	derived from
0.2167248072	experiments with three
0.2167181182	$ m
0.2167118477	model enables
0.2166963635	found at https
0.2165998270	directed towards
0.2165676207	neural network based approach to
0.2165231912	discrepancy between
0.2165121199	important semantic
0.2163650604	end to end method
0.2163609667	based image
0.2162746405	problem of classifying
0.2162744562	based recurrent neural
0.2162587576	next step
0.2162183586	reading comprehension model
0.2162100464	macro averaged f1 score of
0.2161938980	higher accuracy than
0.2161831344	f 1
0.2161822598	the internet
0.2161770342	relied on
0.2161748834	written in different languages
0.2161579287	non english
0.2161021424	the language of
0.2160940271	a weakly supervised
0.2160894966	specific problem
0.2160155367	a joint
0.2159590768	a neural
0.2159348158	correspond to
0.2159316141	level nmt
0.2158921595	outperforms state of
0.2158681223	a recurrent neural network
0.2158345992	a bilingual
0.2158064136	largely focused on
0.2157547087	variety of nlp tasks
0.2156728464	generation approaches
0.2156089847	for english german
0.2156030948	with and without
0.2155476683	availability of large
0.2155253050	a hierarchical model
0.2155140656	large language
0.2155084075	level quality
0.2154688313	loss based
0.2154664096	including word
0.2154147186	wide range of tasks
0.2154103675	depend on
0.2152603675	dealing with
0.2151446334	evaluation based
0.2151071215	a three stage
0.2151021424	the classification of
0.2149500697	features derived
0.2149147275	set of terms
0.2149029114	neural end to end
0.2148718647	towards robust
0.2148695739	issues related
0.2147551980	towards automated
0.2147397309	network trained
0.2147325772	the semantics of
0.2145525734	a cascade
0.2144597904	transfer method
0.2144437773	based solely on
0.2144053347	semantic analysis of
0.2143752630	modeling word
0.2143454196	inference time
0.2142024911	a bi lstm
0.2141021424	the tasks of
0.2141021424	the learning of
0.2140975674	distributional word
0.2140824211	interactions between
0.2140580776	an application
0.2139950806	corpus data
0.2139719706	complex information
0.2138564228	n gram models
0.2138075800	based classifiers
0.2136470988	cross lingual named
0.2136138868	automatic analysis
0.2136034140	performance compared
0.2134547776	a fixed size
0.2134302094	domain test
0.2133975515	outperforms current
0.2133734404	for named entity recognition
0.2133069078	extensive experiments on
0.2132831332	a comprehensive
0.2132717348	availability of large amounts of
0.2132509605	compared with previous
0.2132411398	extensive experimental results show
0.2132353492	of magnitude larger
0.2132080575	language processing task
0.2131742176	task 5
0.2131112189	a pretrained language
0.2129404966	task for many
0.2129134806	re examine
0.2128584496	larger than
0.2128368491	methods learn
0.2127671680	evaluation experiments
0.2127532759	interacts with
0.2127344825	make full use
0.2127301360	a single
0.2125599144	expensive and time
0.2125312457	new approaches
0.2125238309	experiments conducted on
0.2124689352	level annotation
0.2124119154	domain datasets
0.2123685037	an ambiguous word
0.2122585318	distinguish between
0.2122526052	domain adaptation for neural
0.2121761252	filling task
0.2121683336	experiments on large scale
0.2121021424	the parameters of
0.2121021424	the requirements of
0.2120939067	consist of
0.2120138452	a feature rich
0.2120099075	looks at
0.2119911472	linking system
0.2119797394	widely used datasets
0.2118143834	comparisons between
0.2118107637	concern about
0.2117835252	challenging task in natural
0.2116778079	susceptible to
0.2116078980	learning frameworks
0.2116047823	models with different
0.2116008252	based on recurrent neural networks
0.2115554639	a reply
0.2115428072	local semantic
0.2115406912	treated as
0.2115270621	learns word
0.2115219995	a revised
0.2114838930	learning scenarios
0.2114728175	integrated into
0.2114278143	summarization system
0.2114212306	no longer
0.2113940820	in domain data
0.2113477070	dialog task
0.2113315241	decoder networks
0.2112278195	among others
0.2112068646	assumptions about
0.2111984203	number of classes
0.2111948660	performing models
0.2111021424	the boundaries of
0.2109086284	improvement compared to
0.2108645012	semantics of words
0.2108545746	not enough
0.2108418754	synthesis systems
0.2108411402	a generative adversarial
0.2106591324	learn word embeddings
0.2106501370	information system
0.2106200772	in order to
0.2105965864	a text classification
0.2105861851	language understanding systems
0.2105786358	focused on
0.2105740620	multimodal sentiment
0.2103219658	only monolingual data
0.2103180460	far beyond
0.2102518864	using long short
0.2102388911	an ensemble
0.2101203801	lack of large scale
0.2101021424	the patterns of
0.2101021424	the weights of
0.2100661492	latent variable model for
0.2100642783	online product
0.2099546942	standard approach
0.2099383302	level embedding
0.2099375313	f1 score of
0.2098789520	latent variable models for
0.2098682992	many natural language processing tasks
0.2098527672	explicit feature
0.2098423341	grounded word
0.2098012330	irrespective of
0.2097971287	arising from
0.2096777076	advent of
0.2096457264	a two level
0.2096374758	improvements of up to
0.2095566527	task of classifying
0.2094655238	task of predicting
0.2094373156	total number of
0.2094040628	capable of
0.2093874246	use cases
0.2093806862	multiple language
0.2093404633	prediction systems
0.2092838537	discriminating between
0.2092377910	building on recent
0.2091856541	compared to standard
0.2091021424	the models on
0.2091021424	the scores of
0.2091021424	the sentiments of
0.2091021424	the contexts in
0.2091021424	the names of
0.2091021424	the contents of
0.2091021424	the concepts of
0.2090162774	capable of learning
0.2090030873	vital role in
0.2089592643	extensive analysis
0.2089427174	language generation systems
0.2088825386	compared with existing
0.2088222223	language documents
0.2087236947	based sentence
0.2086531190	datasets collected
0.2086128792	training word
0.2085835043	a comprehensive study
0.2085586010	long term goal of
0.2085407760	for people to
0.2085311780	various natural language processing tasks
0.2085142941	terms of bleu
0.2085123551	time steps
0.2084818847	aims to
0.2082482749	language understanding system
0.2082375134	few examples
0.2082101818	during training
0.2081462243	computer interaction
0.2081030948	in part to
0.2081021424	the issues of
0.2081021424	the probabilities of
0.2081021424	the predictions of
0.2081021424	the supervision of
0.2081021424	the roles of
0.2080727268	methods rely
0.2080605734	belonging to
0.2080281188	unsupervised language
0.2080152477	no clear
0.2079372676	compared to existing
0.2078149302	the present work
0.2078071723	a dutch
0.2077591345	in end to end speech
0.2076854134	sota results on
0.2076845630	results validate
0.2075176434	english training
0.2075036094	provide baseline
0.2074799710	standard test
0.2074609186	in most cases
0.2074357321	gains of up
0.2073021424	the authors of
0.2073021424	as shown in
0.2072592837	multimodal model
0.2071921786	based information
0.2071487801	both automatic metrics and
0.2071021424	the sentences of
0.2071021424	and frequency of
0.2071021424	the topics of
0.2071021424	the entities in
0.2071021424	the approach of
0.2071021424	the rules of
0.2070751296	based on distributional
0.2070114760	human level performance on
0.2069968555	a crucial step
0.2069516367	an rnn based
0.2068716750	of english hindi
0.2067567996	neural network based language
0.2067407760	of time and
0.2065961184	the advent
0.2064932187	multiple data
0.2064819774	efficient end to end
0.2064184264	for fake news detection
0.2063848505	types of entities
0.2063595917	art algorithms
0.2063560274	based on
0.2062759714	a data set
0.2062066339	short time
0.2061896719	shown strong
0.2061891599	at test time
0.2061718980	an attention based
0.2061670717	models tend
0.2061317190	alignment models
0.2061038634	depends on
0.2061021424	of text in
0.2061021424	of entities in
0.2061021424	the sentences in
0.2060687260	a major problem
0.2060322372	a pointer network
0.2059947806	framework based
0.2059731977	dozens of
0.2059722116	important application
0.2059691485	memory architecture
0.2059579314	co occurrence networks
0.2059374134	non task
0.2059062087	more or less
0.2059033526	approach consistently
0.2057924023	to count
0.2057897757	acting as
0.2057726104	trained embedding
0.2057475599	word use
0.2057068271	relationships between entities
0.2056551050	robustness against
0.2055973155	framework for evaluating
0.2055630531	answering questions about
0.2055564222	contextual language
0.2054481374	a modular
0.2054000631	fine grained analysis of
0.2053955422	type system
0.2053909216	affected by
0.2053369727	approach for improving
0.2052452644	fundamental task
0.2052309125	provide insights into
0.2052142987	specific datasets
0.2051995038	assess whether
0.2051021424	the embeddings of
0.2051021424	the words of
0.2051021424	the findings of
0.2051021424	the models with
0.2051021424	the details of
0.2050824299	all neural
0.2050807348	variable model
0.2050686649	data obtained
0.2050223324	the speech to
0.2050029867	quality text
0.2049742439	network called
0.2049068630	unsupervised algorithm
0.2048964561	different levels of granularity
0.2048618270	methods for learning
0.2048041486	benchmark performance
0.2047053594	problem in natural language processing
0.2046919053	reliance on
0.2045632714	an industrial
0.2045534034	learning for text classification
0.2045074870	semi supervised method
0.2044942721	unable to
0.2044744704	to date
0.2044653782	based end to end
0.2044489444	a long standing
0.2044384827	based convolutional neural network
0.2043992517	level word embeddings
0.2043361664	principled way
0.2043090203	like units
0.2043021424	the performances of
0.2042923186	target model
0.2042664068	for task oriented dialogue
0.2042147404	association between
0.2042123347	existing sentence
0.2041487603	less likely
0.2041206735	an essential component
0.2041021424	a network of
0.2041021424	the domains of
0.2040590275	task of detecting
0.2040231569	of deep learning in
0.2039610241	from clinical text
0.2039064903	the words used
0.2038810564	systematic analysis
0.2038566707	$ ^
0.2038513592	a level
0.2038436895	language question
0.2038175483	languages with different
0.2038100800	based contextual
0.2036646734	test bed for
0.2036634188	different granularities
0.2036555802	significant research
0.2035904085	without modifying
0.2035803332	based on long short
0.2035737152	a versatile
0.2035396524	relative word
0.2035306683	yet challenging task
0.2035210384	the 2016
0.2035136365	errors made by
0.2034795803	similarity of words
0.2034594104	coming from
0.2034500820	a platform
0.2034463388	ranked second
0.2034249027	method for extracting
0.2033976457	three way
0.2033021424	the challenges in
0.2032894842	similarities between
0.2032611536	generator model
0.2032169066	important to understand
0.2031697681	the united states
0.2031346314	a note
0.2031021424	of sentences in
0.2031021424	the statistics of
0.2030835752	much easier
0.2030670847	an open domain
0.2029610208	supported by
0.2029064903	with humans in
0.2028883250	language applications
0.2027191541	focus on extracting
0.2024850673	a 5
0.2024617386	captured by
0.2024260610	next generation
0.2024147783	new facts
0.2024113838	an encoder decoder
0.2024028796	equipped with
0.2023925491	dataset of tweets
0.2023734813	dependencies among
0.2023021424	the limits of
0.2023021424	the paper also
0.2023021424	the retrieval of
0.2023021424	a task with
0.2022417925	perform multi
0.2022346384	smaller model
0.2021881425	level task
0.2021826120	a significant margin
0.2021530284	texts written by
0.2021456487	this paper examines
0.2021021424	the structures of
0.2021021424	the perspectives of
0.2020149302	between pairs of
0.2020003627	encoder representation
0.2018934387	task 6
0.2018791246	task of determining
0.2018655619	to end automatic speech recognition
0.2017998222	experimental results on multiple
0.2017231305	experiments on synthetic
0.2017120470	approach for detecting
0.2016756197	domain task
0.2015930818	received little
0.2015290064	automatic extraction of
0.2014968291	based pre
0.2014063593	future work
0.2014040761	based on deep neural networks
0.2014003643	diverse text
0.2013021424	the applications of
0.2012180331	so as to
0.2011989284	becomes even more
0.2011428379	into consideration
0.2011115880	extracted from
0.2010998915	end to end neural model
0.2010742988	applied to
0.2010440390	a closer look
0.2010253477	generated by
0.2010165696	significantly better than
0.2009064903	the distributions of
0.2009053102	based translation
0.2008462877	corresponds to
0.2007615061	an external knowledge
0.2007360250	meaningful semantic
0.2007026181	complex language
0.2006764973	a high performing
0.2005738842	models like bert
0.2004051519	a joint learning
0.2003794032	achieve performance
0.2003305746	bag of
0.2003021424	the contributions of
0.2002588379	approach leads
0.2001756247	different word embeddings
0.2001634360	for robust speech recognition
0.2001591956	interactions among
0.2001500199	task of answering
0.2001195990	via reinforcement
0.2000372432	embeddings of words
0.2000198030	experiments on benchmark datasets
0.1999450364	originate from
0.1998521754	stanford question answering
0.1997991283	full sentence
0.1997642987	perform tasks
0.1997385571	automatic detection of
0.1996298873	models pre trained on
0.1996192536	improvement in accuracy
0.1995542685	automated detection of
0.1994952390	while avoiding
0.1994647689	language in social media
0.1994641679	a heterogeneous graph
0.1993896765	a chinese
0.1993021424	the outputs of
0.1993021424	in relation to
0.1992622284	experiments with different
0.1992398236	the language used
0.1992271522	for cross language
0.1991445364	space and time
0.1991021424	different methods of
0.1990914394	extraction approaches
0.1989598509	level visual
0.1989064903	the baselines in
0.1988705370	task in natural language
0.1988630350	an important
0.1987914089	produced by
0.1987300293	on social network
0.1986478516	of 10
0.1986376654	based adversarial
0.1986329545	all languages
0.1985626289	large amounts of labeled
0.1985266057	powerful model
0.1985090843	out of distribution
0.1984681518	cope with
0.1984514992	based sequence to sequence models
0.1984372928	a low resource language
0.1983799991	a pre trained language model
0.1983075525	building upon
0.1983021424	the challenges of
0.1983021424	the ways in
0.1983020683	a drop in replacement
0.1982622284	experiments with two
0.1982564266	on lstm
0.1981205054	two speaker
0.1981021424	the problems of
0.1981021424	for classification of
0.1980259482	tasks such as question answering
0.1979800834	method using
0.1979302529	based on pre trained
0.1979149941	the proposed approach
0.1979074426	of people in
0.1979064903	of language in
0.1977314945	as opposed to
0.1977144416	as many as
0.1976998822	sentence relations
0.1976677266	an empirical analysis
0.1976197224	a generative
0.1976156622	to solve
0.1975976492	the 2017
0.1975923226	a graph based
0.1975510488	replaced by
0.1974966458	performance on downstream
0.1974949242	thousands of
0.1973819368	consists of
0.1973201146	convolutional neural network for
0.1973175666	suffer from error
0.1973021424	the purposes of
0.1973021424	the fields of
0.1972488837	of machine learning and
0.1971774875	a multitask
0.1971478147	with or without
0.1971293705	as far as
0.1971236088	a general purpose
0.1970442393	millions of
0.1969959843	including language
0.1969445616	the art approaches
0.1968477564	think about
0.1968333004	more realistic
0.1968112838	large knowledge
0.1968000110	a comparison
0.1967688243	attention network for
0.1967650041	led to
0.1967324632	this article proposes
0.1967128780	datasets from different
0.1964843461	using pre trained
0.1964251373	several hundred
0.1964180268	classified into
0.1964171275	a python
0.1963404360	an encoder decoder architecture
0.1963056470	of attention in
0.1962004437	advances in natural language
0.1961316040	answering models
0.1959751871	baseline for future
0.1959519591	a new data
0.1959403592	massive amount of
0.1959169455	the kullback leibler
0.1959149941	the proposed method
0.1959071953	generalization across
0.1959064903	of documents in
0.1958711045	a massive
0.1958283112	a goal oriented
0.1957660788	term dependency
0.1956852429	methods for detecting
0.1955682401	a bayesian
0.1955436966	$ \
0.1955210728	based lexical
0.1955121105	on glue
0.1953815172	a sequence labeling problem
0.1953607523	framework improves
0.1953546284	based on deep learning
0.1953473240	deal with
0.1953323394	mining methods
0.1953230183	model to learn
0.1952515803	more interesting
0.1952510565	range of applications
0.1952131496	a multi source
0.1951304204	a factorized
0.1950871447	cosine similarity between
0.1950608076	number of
0.1950571953	with attention based
0.1950511770	biases present in
0.1950510488	formulated as
0.1950446736	the covid 19 open research
0.1949619142	an automatic method
0.1949584535	obtains state of
0.1949465121	improving machine
0.1948576198	no need
0.1948533857	commands for
0.1948322500	a proposal
0.1947566724	this paper addresses
0.1947347713	from one or
0.1947308108	problems in natural language
0.1947254949	task 4
0.1946976893	an imitation
0.1946574723	sql task
0.1946369431	aims to automatically
0.1946154892	significantly better performance
0.1946107818	classifier to predict
0.1945912676	product attention
0.1945018047	the national
0.1944932595	successfully applied to
0.1944798098	model outperforms state of
0.1944680422	gender bias in
0.1944513311	a large scale dataset
0.1944501507	by large margins
0.1944486137	a special type
0.1944398236	of terms in
0.1943022718	detection in tweets
0.1942960006	simple yet
0.1942450549	to address
0.1941367066	as little as
0.1941073837	in low resource settings
0.1941021424	one language to
0.1940848581	the art performances
0.1940435367	model to predict
0.1939953459	free grammar
0.1939883308	highly sensitive to
0.1939826944	a challenging task
0.1939106481	a recurrent neural
0.1938490871	novel object
0.1938431642	word representations from
0.1938115690	wide range of applications
0.1938110779	pre trained on
0.1937572012	extended version of
0.1937401434	diverse set of
0.1937240549	number of topics
0.1936753935	far field speech
0.1936253949	a web application
0.1935776279	new large scale
0.1935510102	largest publicly available
0.1934448245	based on machine learning
0.1933766490	directly from speech
0.1933371138	the cl
0.1933048672	methods based
0.1933021424	of sentences with
0.1933021424	for languages with
0.1932397277	name entities
0.1932393246	module networks
0.1932353231	based smt
0.1932119427	a key step
0.1931217843	top 1
0.1931064903	the textual and
0.1930631911	vary across
0.1930335157	a vietnamese
0.1930290923	not well understood
0.1930060996	a sequence labeling task
0.1928273000	a character level
0.1927691702	compared against
0.1927414439	method for generating
0.1926785938	sub linear
0.1926479024	distances between
0.1926322236	learning to map
0.1926146492	hundreds of millions of
0.1925921960	a topic model
0.1925397625	ground truth for
0.1925319550	estimation using
0.1925199973	this paper discusses
0.1925167255	to learn
0.1924620448	a comparative analysis
0.1924454198	as well as
0.1924270071	focus only on
0.1923324494	represented as
0.1922973075	model output
0.1922948264	very low
0.1922765404	text classification with
0.1922432617	detection system
0.1922220300	non latin
0.1922043261	an intrinsic
0.1921409099	the art models
0.1921314884	for multi hop question answering
0.1920632473	becoming more and more
0.1920491446	media data
0.1920086704	amounts of data
0.1919602151	better representation
0.1918998106	on multiwoz
0.1917669417	embeddings obtained
0.1917522643	for sequence modeling
0.1917232172	tweets using
0.1916487516	f1 score on
0.1916426305	art unsupervised
0.1916215399	current neural
0.1915113065	models achieve state of
0.1915102648	more fine grained
0.1914964426	field of nlp
0.1914793220	proposed end to end
0.1913737047	prone to
0.1913493465	drawn from
0.1913148000	considerable amount
0.1912735147	a multi task learning
0.1912168224	an image
0.1911732289	non autoregressive models
0.1910130560	period of time
0.1909832975	a compositional
0.1909825471	multiple related
0.1909765433	yields better performance
0.1909565127	models fine tuned
0.1909295738	recognition system for
0.1909179511	apply deep
0.1909008645	evidenced by
0.1908507236	datasets in different
0.1908365154	a study
0.1908247580	to pretrain
0.1908085906	periods of time
0.1908059617	$ f
0.1907614774	recent state of
0.1907526904	novel word
0.1907023027	network for visual
0.1906785819	lstm language
0.1906045995	network to learn
0.1905751380	few works
0.1905209374	supervised relation
0.1905164575	task at hand
0.1904683376	sub tasks
0.1904617671	datasets of different
0.1904096665	level word
0.1903300832	for automatic speech recognition
0.1902571074	outperforms existing state of
0.1902535615	quality models
0.1902389491	an integer
0.1902285923	fact extraction and
0.1902125435	a b
0.1899866822	a self attentive
0.1899708808	language model using
0.1899688940	leading to
0.1898933957	better than
0.1898876938	gives rise
0.1898734698	online fashion
0.1898529169	clinical named
0.1898437652	obtain state of
0.1897607780	word error rate on
0.1897588200	reach state of
0.1897454581	an analysis
0.1897434273	key aspects of
0.1895469364	recognition techniques
0.1894398236	between questions and
0.1893039822	a concise
0.1892562057	a large
0.1892013858	in addition
0.1891640561	yet effective
0.1891306979	a crowd
0.1891064903	between human and
0.1890978968	the maximum entropy
0.1889723922	recognition using
0.1889608925	a small scale
0.1888896696	mapping between
0.1888478412	previously known
0.1888135129	a multimodal
0.1887873472	running time
0.1887816351	small number of
0.1886942286	a comparative
0.1886711881	representations learned by
0.1886409163	language processing methods
0.1885498901	linear combination of
0.1885167255	to improve
0.1884808385	a natural language understanding
0.1884624207	a two step approach
0.1884337587	significant increase
0.1884276952	based on recurrent neural
0.1884215799	an ablation study
0.1883137681	to predict
0.1883021424	both accuracy and
0.1883016936	each target word
0.1883004722	based language
0.1882417316	more than 90
0.1882120771	word representations with
0.1881992035	as low as
0.1881483374	refer to
0.1881097869	extraction approach
0.1880926527	a pragmatic
0.1880854116	research paper
0.1879885528	generalize well
0.1879864086	including sentiment
0.1879610745	texts written
0.1879078362	paper reviews
0.1877392406	become ubiquitous
0.1877207587	supervised semantic
0.1877184629	sub task a
0.1877008582	responsible for
0.1876712120	exemplified by
0.1876159059	m \
0.1875991520	not always
0.1875734958	methods fail
0.1875305471	a new challenge
0.1874742538	deep learning system
0.1874378116	language words
0.1874216848	by 3
0.1873021424	the ideas of
0.1873021424	in experiments with
0.1872710637	monolingual word
0.1872138233	more fluent
0.1872074775	variety of sources
0.1870717880	automated detection
0.1870640946	built on top of
0.1869945021	labeling models
0.1869922780	large scale dataset for
0.1869627103	domain conversation
0.1869397993	generalizes better
0.1869113985	conditioned on
0.1868333004	not clear
0.1867897218	studies demonstrate
0.1867837713	language expressions
0.1867777649	recent advancements in
0.1867027958	accurate models
0.1866316967	a critical
0.1866231805	originated from
0.1865816912	model to generate
0.1865649720	trained on
0.1865621369	makes use of
0.1865431411	a library
0.1864539979	this problem
0.1864251021	neural ner
0.1864245310	the skip gram model
0.1864173071	time dependent
0.1863735650	released under
0.1863548659	representations derived
0.1863167515	large scale natural
0.1862782735	trained weights
0.1862427276	advances in nlp
0.1862331384	models produce
0.1862249184	representations of entities
0.1862165369	from structured data
0.1861003826	a statistical model
0.1860381403	an attention based encoder decoder
0.1859923108	representation of words
0.1859835372	as much as possible
0.1859582128	set of seed
0.1859519180	domains demonstrate
0.1859284029	the pre trained model
0.1859219675	supervised deep
0.1859161368	automatic generation
0.1859134892	as high as
0.1857753992	the wild
0.1857587213	large scale evaluation
0.1857503759	relatedness between
0.1857487794	the microsoft
0.1857220561	represented by
0.1857213132	time scales
0.1857081344	non technical
0.1856522346	more than
0.1855777358	a neural machine translation
0.1855610113	for code switching
0.1855459223	present paper
0.1855216843	a semi supervised
0.1855046878	switch between
0.1854627553	the xtag
0.1854398236	and code for
0.1854111412	amenable to
0.1853224721	as long as
0.1853003472	topic model for
0.1852741506	spite of
0.1852693101	some cases
0.1852340851	new methods
0.1852235140	table of
0.1852223452	a novel
0.1852158531	the bi lstm
0.1850922765	a continuous space
0.1850484883	differs from
0.1849722604	similarity between
0.1849713062	qa over
0.1849705550	recent advances in deep
0.1849230826	fewer parameters than
0.1849222720	= 1
0.1849172367	help identify
0.1849154531	techniques to improve
0.1849134892	as features in
0.1849134892	with applications in
0.1848904029	shared across
0.1848176572	thanks to
0.1848001260	long training
0.1847949632	an important factor
0.1847937548	nlp field
0.1847845287	metrics to evaluate
0.1847681284	similar or better
0.1847354540	set of rules
0.1846963499	approach using
0.1846604335	correlations between
0.1846150655	particularly challenging
0.1845558847	better results than
0.1845539904	without changing
0.1843069460	induction using
0.1843033502	easily adapted to
0.1842769885	costly and time
0.1842607112	reference less
0.1842463596	distribution of words
0.1842460460	one class
0.1841795173	lots of
0.1841451885	the coronavirus
0.1841214551	tasks in natural language
0.1841145812	more precisely
0.1840985575	on iwslt
0.1840613431	large scale analysis of
0.1840266899	extract relations
0.1839940115	a vital role
0.1839353626	a state
0.1839227827	based on word
0.1838932851	general problem
0.1838696498	more nuanced
0.1838560580	machine translation using
0.1838467212	an augmented
0.1838241638	requires large
0.1838223960	traditional neural
0.1837739957	propose to leverage
0.1837488703	propose multi
0.1837458619	for task oriented dialogue systems
0.1837430185	1 2
0.1837172376	a repository
0.1836722315	an open challenge
0.1835763807	on several text
0.1835397970	outperform state of
0.1835371637	based on statistical
0.1835195254	in retrieval based chatbots
0.1834004588	different attention
0.1833586198	access to
0.1833500589	to generate
0.1833265327	in term of
0.1833117166	fine tuning on
0.1833038991	per character
0.1833013244	trained embeddings
0.1832684534	achieving new state of
0.1832447441	recognition experiments
0.1832237315	experiments show
0.1831861974	challenging task because
0.1831308805	across tasks and
0.1831017749	domain performance
0.1830853278	lot of attention
0.1830344364	based on rhetorical
0.1830207985	a multi turn
0.1829912699	of one or
0.1828808287	dynamic time
0.1828229252	nlp system
0.1828120077	communication system
0.1826831647	relations between
0.1826030498	not change
0.1825956760	combined model
0.1825805265	existing model
0.1825431623	to identify
0.1825166091	image question
0.1824982854	the smallest
0.1824507023	wants to
0.1824393125	the art results on
0.1823484229	large collections of
0.1823388429	a discriminative
0.1820435753	this work
0.1820369683	a quantitative analysis
0.1820296186	stance towards
0.1820292927	answering datasets
0.1820206877	sub event
0.1819387094	kinds of
0.1819294048	encoder trained
0.1818951960	the development set
0.1818847292	take advantage of
0.1818179217	translation methods
0.1817478571	hundreds of
0.1817453719	domain adaptation for
0.1816890478	for grammatical error correction
0.1815915196	of 15
0.1815467208	towards automatic
0.1815248364	an automatic
0.1814870234	to overcome
0.1814394405	not seen during training
0.1814381390	based on bert
0.1813512841	huge number of
0.1813265327	with attention for
0.1812815946	trained word embedding
0.1812616281	best performing model
0.1811817795	number of studies
0.1811747981	a new concept
0.1811736051	focus on learning
0.1810753277	distinctions between
0.1810544858	set of features
0.1810245544	model's ability to
0.1808260315	best f1 score
0.1808081941	end to end neural machine
0.1807935782	language processing and machine learning
0.1807118035	more stable
0.1806353783	evaluation approach
0.1806308805	and testing of
0.1806136800	the same
0.1806043970	aware knowledge
0.1805217168	end to end text
0.1804996749	network for multi
0.1804506756	two decades
0.1804398236	of meaning in
0.1804286553	highest performance
0.1803265327	in support of
0.1802101456	most importantly
0.1801830797	concentrate on
0.1801828839	less explored
0.1801741291	much fewer
0.1801594423	score improvement
0.1801398937	for task oriented
0.1801349180	an auxiliary task
0.1801145812	an elegant
0.1800371527	the manifold
0.1800261801	detection in english
0.1799696244	in china
0.1799630950	generate target
0.1799539390	to understand
0.1799490354	improve translation
0.1799307389	approach achieves state of
0.1799221582	attention at
0.1799134892	of text from
0.1798203832	with respect to
0.1797472515	the fly
0.1797185514	equally well
0.1797184043	comparison of two
0.1797105334	deep learning architecture for
0.1796933662	set of candidate
0.1796858338	terms of accuracy
0.1796764239	focus on improving
0.1795970509	explained by
0.1795919903	made publicly
0.1794577548	information from text
0.1794561521	learning approach based
0.1794427195	analysis systems
0.1794236234	an attention mechanism
0.1793881705	impact on performance
0.1793704602	issues related to
0.1793650523	under translation
0.1793412287	the encoder decoder framework
0.1793306711	pieces of information
0.1792460075	skip gram with
0.1792199305	compared to baseline
0.1792062105	aims to provide
0.1791684718	for task oriented dialogues
0.1789911458	hierarchical semantic
0.1789269229	fusion model
0.1789150657	reduction compared to
0.1788114974	3 4
0.1788041522	concerns about
0.1787203498	a meta
0.1786898686	alleviated by
0.1786875511	a chart
0.1786825569	approach for generating
0.1786229592	people express
0.1786200137	2016 shared task
0.1785982595	out performs
0.1785644878	many nlp tasks
0.1785366916	based on reinforcement learning
0.1784701053	to learn semantic
0.1784381583	to extract
0.1783945951	two main challenges
0.1783515041	for indian languages
0.1783321870	based multi
0.1783265327	with applications to
0.1782885932	under resourced language
0.1782786481	an emotional
0.1781721239	the proposed method outperforms
0.1781507819	trained on large
0.1781457078	a real world
0.1781226536	set of words
0.1781060310	an evaluation
0.1780492935	an investigation
0.1779850943	improvement in performance
0.1778926132	$ y
0.1778653933	benefit from
0.1777539390	to build
0.1777520785	disambiguation using
0.1776611428	million people
0.1776552166	learning for chinese
0.1776162747	the present paper
0.1775914435	begin by
0.1775760857	contrary to
0.1775359306	for urdu
0.1774922319	performs comparably to
0.1774566679	lack of resources
0.1774560695	make use of
0.1774538805	neural networks for text
0.1774512817	a language agnostic
0.1773815770	unsupervised framework
0.1773201951	based on neural networks
0.1772142201	method to train
0.1771936524	meaning of sentences
0.1771274442	model pre trained on
0.1771138612	more than 10
0.1771104741	a simple approach
0.1769531347	self attention module
0.1768601315	learning to learn
0.1768221524	\ times
0.1768193857	the latent structure
0.1768130745	re rank
0.1767595338	each element
0.1767491132	referred to as
0.1767317482	to detect
0.1767216231	a general model
0.1766987900	emerged as
0.1766578884	real time speech
0.1766436408	the blank
0.1766215373	large parallel
0.1765383942	search system
0.1764690669	\ &
0.1764673819	of out of vocabulary words
0.1764491302	space based
0.1764400225	a free
0.1764302480	not only
0.1764158606	learn latent
0.1763826695	the proposed model outperforms
0.1763787080	a novel neural architecture
0.1763265327	as sequences of
0.1763246032	of rules for
0.1763072444	a high resource language
0.1762366641	computational analysis of
0.1761416048	models trained on
0.1761319456	learn to generate
0.1760448640	features derived from
0.1759853159	regardless of
0.1759776810	compared to baselines
0.1759660640	learning technologies
0.1758944182	existing unsupervised
0.1758892515	text generation based
0.1758593895	improvements in accuracy
0.1758457307	a high quality
0.1758447053	ranges from
0.1758083933	task of identifying
0.1756158447	an open problem
0.1755719872	analysis models
0.1755706705	non parallel data
0.1755239804	information about
0.1754661377	annotated training
0.1754305792	term detection
0.1753576402	multimodal speech
0.1752652535	a new text
0.1752541324	trained to produce
0.1752404576	language system
0.1752381944	results compared
0.1752272401	the art result
0.1752270244	longer than
0.1752175537	a power law
0.1752024672	$ r
0.1751964333	potential to improve
0.1751318770	this task
0.1751229594	a transferable
0.1751170899	decisions made
0.1751139611	the european
0.1751047618	approach to tackle
0.1750823580	data augmentation for
0.1748273042	framework significantly
0.1747825477	a black box
0.1747738835	partly due to
0.1747145922	types of errors
0.1746716321	on librispeech
0.1745618155	a new semantic
0.1745064809	task setting
0.1745023406	embeddings produced
0.1744918513	twitter using
0.1744705691	model showed
0.1744393125	the art performance on
0.1743265327	with application to
0.1742491333	decoder framework
0.1741671447	i vector based
0.1740712396	provided by
0.1740630222	vision and natural language
0.1740539569	improvements over
0.1740036885	on top of
0.1739048536	small fraction of
0.1738813451	unsupervised discovery
0.1738246032	between text and
0.1738174915	methods for generating
0.1738139233	more than 80
0.1738096359	art results on
0.1737612036	new algorithm
0.1736311800	the birth
0.1736275647	current study
0.1735424140	as good as
0.1735349381	to produce high quality
0.1735280365	new word
0.1735206092	gram models
0.1735039216	especially true
0.1734954945	an ad hoc
0.1734786802	large annotated
0.1734682569	understanding evaluation
0.1734622337	continuous bag of
0.1734001928	competitive results on
0.1733931176	text without
0.1733402163	fine tuned for
0.1733246032	to german and
0.1732896177	a sign
0.1731960351	translation using
0.1731849974	determine if
0.1731824123	level language
0.1730832963	key feature of
0.1729916556	order to improve
0.1729373383	information from multiple
0.1728750370	similar semantic
0.1727802894	progress in natural language
0.1727672523	a series of
0.1727579726	particularly useful
0.1727294474	based on contextual
0.1727116642	the penn treebank
0.1726918608	time step
0.1726213974	generation using
0.1726009035	more robust
0.1725867487	order to understand
0.1725672628	collection of documents
0.1725472027	to train
0.1724689677	based on deep
0.1724642138	and phrases in
0.1724122095	a few hundred
0.1723336694	of deep neural networks
0.1723019050	a real world dataset
0.1722598604	this technical report
0.1722552819	representations of meaning
0.1722301779	the past few years
0.1721989153	considerable amount of
0.1721599981	model benefits
0.1720742391	a multilayer
0.1720527186	many real world
0.1720328898	a computational
0.1720324157	on 10
0.1720266385	in terms of
0.1719870315	generation of natural language
0.1719623887	challenging because
0.1719347932	a principled
0.1718643146	different nlp tasks
0.1718595494	a task to
0.1718408949	billions of
0.1718246032	of experiments with
0.1718246032	the parsing of
0.1717516091	participate in
0.1716420539	to split
0.1716359497	mined from
0.1715425658	an integer linear
0.1715327248	performs well
0.1714807066	an adversarial learning
0.1714705181	proposed to solve
0.1714212503	better performance than
0.1714152377	art performance on
0.1713795023	between modalities
0.1713740648	framework for learning
0.1713568098	the visual dialog
0.1713326980	does indeed
0.1713206983	a latent space
0.1713188910	task of translating
0.1712778362	a transition based
0.1712620587	a formal
0.1712461969	extracting information from
0.1712348377	inspiration from
0.1712192541	for multi turn response
0.1711583010	including named entity
0.1711456123	existing state of
0.1711154236	machine reading comprehension with
0.1710575159	hybrid system
0.1709623926	appropriate responses
0.1709325644	distributed representations of
0.1709298707	target document
0.1708010006	contrast to previous work
0.1707785343	text written
0.1707309933	smaller set of
0.1706503488	in 2016
0.1706434305	key component of
0.1705749132	task of named
0.1703280695	one or more
0.1702950580	large numbers of
0.1702585173	dealt with
0.1702337352	too small
0.1701866805	special focus on
0.1700765983	a single vector
0.1700747165	perform better than
0.1700593536	by introducing
0.1699431690	large scale corpus of
0.1698886685	the covid 19 pandemic
0.1698522435	closely related to
0.1697650262	the art systems
0.1697574663	the essence
0.1697287332	a domain independent
0.1697217869	on test clean
0.1696624042	substantial amount
0.1695115038	neural networks to learn
0.1694977159	asr system
0.1694609107	in real world scenarios
0.1694367491	over 1 million
0.1694259697	large extent
0.1693936215	a surrogate
0.1693731880	an alternative
0.1693205254	via multi task learning
0.1692186189	space of possible
0.1691400852	step approach
0.1691255274	a morphologically rich language
0.1691132185	effective method
0.1690654641	for aspect based sentiment
0.1690117334	posed by
0.1689437975	at wnut 2020
0.1689207004	specific corpus
0.1688730202	qa system
0.1688548533	a rigorous
0.1688175726	a web based
0.1687953777	the stanford question answering
0.1687431088	models such as bert
0.1687114642	advantages over
0.1685734192	detection on twitter
0.1685467223	more than half
0.1684873125	compared to prior
0.1684612581	a deeper
0.1682236588	much less
0.1682192114	interpreted as
0.1681564518	during inference
0.1681333244	well designed
0.1680819300	distinguishing between
0.1680210888	a uniform
0.1680154908	high levels
0.1680029264	a large dataset
0.1679821765	from raw text
0.1679556370	to generate high quality
0.1679248439	training on large
0.1679079089	framed as
0.1678440685	aim at
0.1678188184	words in context
0.1678012513	improving language
0.1677646416	new possibilities
0.1677488552	the pdtb
0.1677048250	a purpose
0.1676806552	detection in social
0.1676642817	reasoning based
0.1676567639	existing language
0.1676378845	an open source toolkit for
0.1676324188	a qualitative
0.1675899700	links between
0.1675593825	imposed by
0.1675329579	improvements over baseline
0.1674929370	across domains
0.1674566257	based extractive
0.1674127760	concerned with
0.1674083766	classifier trained on
0.1673229118	accounting for
0.1673024302	the addition of
0.1672149958	the proposed method achieves
0.1671891536	for short text
0.1671724287	to fine tune
0.1671577498	relatively few
0.1670036885	more and more
0.1669959267	a variety of
0.1669475196	plenty of
0.1669272709	significantly less
0.1669132320	representations of text
0.1668883379	and evaluate several
0.1668563237	above challenges
0.1668438231	a balanced
0.1668414151	a step towards
0.1668153100	in areas such
0.1667982806	set of documents
0.1667912131	a new
0.1667893632	not need
0.1667455820	one million
0.1667336000	to create
0.1667095033	an attacker
0.1666217260	method outperforms state of
0.1666173671	the biocreative
0.1665647584	performing systems
0.1665423821	experimented with
0.1664994775	more likely to
0.1664947777	operates at
0.1664938142	the space
0.1664750314	projected into
0.1664277410	representation of sentences
0.1664079527	results show
0.1663494611	a wide range of
0.1661995850	increasing amount
0.1661196434	most existing works
0.1660167973	datasets from different domains
0.1660161366	widely available
0.1659699770	with 4
0.1659463469	first steps
0.1658518436	extraction based
0.1658361782	to detect fake
0.1658077824	this study
0.1657755587	standard nlp
0.1656631025	different domains
0.1655529719	3 class
0.1655268596	extensive experiments show
0.1655102274	typically rely on
0.1655058866	active learning for
0.1653853408	a benchmark
0.1652451965	the research community
0.1652161548	for bangla
0.1651878557	few studies
0.1651416980	fundamentally different
0.1651157691	a serious
0.1650971997	a large number
0.1650880978	the kl divergence
0.1650608750	extraction from text
0.1650478229	two stages
0.1650316446	approach does not require
0.1649362242	attention mechanisms for
0.1648733363	more complex
0.1648569447	method based
0.1648021567	trained only on
0.1647689574	a call
0.1647479596	answer questions about
0.1647345844	crawled from
0.1647319824	\ log
0.1647231826	words do
0.1646157700	a dataset
0.1645950379	more than 20
0.1645719791	more attention
0.1645487952	data from multiple
0.1645462185	the extraction
0.1645154122	rely on hand
0.1644914431	to gauge
0.1644192478	aware models
0.1642808836	effective training
0.1642014716	the fastest
0.1641841282	external language
0.1641708818	model component
0.1641523478	sub task
0.1640851470	the universal dependencies
0.1640571939	simple yet effective approach
0.1640568500	best result
0.1640374374	f1 score over
0.1639557468	along with
0.1639336968	experiments on multiple
0.1639266350	approach to machine translation
0.1638680082	of text for
0.1638680082	of texts in
0.1638680082	of parameters in
0.1638505214	with weak supervision
0.1638148243	model proposed
0.1637686046	autoregressive language
0.1637533607	a systematic
0.1637242352	each other
0.1637207017	model to capture
0.1637047724	an increasingly important
0.1636458845	the original
0.1635740885	semantic relatedness between
0.1635684600	the last decade
0.1635406365	representation of text
0.1635337234	a fuzzy
0.1635038105	system evaluation
0.1634280484	recent developments in
0.1634156760	yields state of
0.1634029295	comprehensive evaluation of
0.1634028879	type of data
0.1633836066	approach to improve
0.1633414972	for morphologically rich languages
0.1633229640	to tackle
0.1632584639	a gold standard
0.1631535767	a time series
0.1631302962	techniques in news articles
0.1629640376	a theoretical analysis
0.1629374592	terms of f1
0.1629249570	in depth error
0.1629096400	deep learning approach for
0.1628766384	high levels of
0.1628501808	for natural language processing
0.1628288351	the indo european
0.1628206612	multilingual named entity
0.1627837465	independent models
0.1627730268	compared to strong
0.1626661382	at different levels
0.1626495037	based opinion
0.1626198619	a feature based
0.1625888125	vietnamese word
0.1625608590	consists of multiple
0.1625558110	the interplay
0.1625079925	looking at
0.1625056641	identification models
0.1625041494	a sentence
0.1624818597	an audio
0.1624808982	two main
0.1624805713	even if
0.1624764046	original data
0.1624664158	semi supervised learning for
0.1624510671	high degree of
0.1624165036	framework to improve
0.1624098577	a wide margin
0.1624098145	during decoding
0.1623943681	personalized word
0.1623937161	a 2d
0.1623858616	new techniques
0.1623811285	vector space models of
0.1623665693	three tasks
0.1623580988	language grammars
0.1623467456	an arbitrary number of
0.1623073188	the chinese
0.1623048455	method to create
0.1622692155	empirical comparison of
0.1622324299	task of generating
0.1621595673	generalize better
0.1621595448	score based
0.1621433357	extraction using
0.1621157221	a lemma
0.1620484887	essential component of
0.1620355298	more subtle
0.1618842976	of 50
0.1618588412	during fine tuning
0.1618480138	a deterministic
0.1617705865	a language independent
0.1617698478	this paper outlines
0.1617221111	an ontology
0.1615707756	a bert based
0.1615655329	a population
0.1614452454	evaluated against
0.1614036676	for multi hop question
0.1613794351	whether or not
0.1613605643	of natural language processing
0.1613245293	text as input
0.1612961417	~ \
0.1612494098	comparable or even
0.1612452771	the danish
0.1611989765	while ignoring
0.1611730442	approach to address
0.1611186513	a user study
0.1610939834	the target language
0.1610839637	towards better
0.1610754667	not just
0.1610586101	named entity recognition with
0.1610044972	an obstacle
0.1609419242	react to
0.1609360527	baselines for future
0.1609310167	relationship among
0.1608983304	next word
0.1608444047	second stage
0.1608427312	the acl
0.1608054364	recent advances in natural
0.1607818820	based parsers
0.1607787652	experiments performed
0.1607535265	many nlp
0.1607408167	a grounded
0.1606656408	concentrates on
0.1606640516	improvements over strong
0.1605861477	experiments on
0.1605261690	cross lingual transfer of
0.1605156914	for open domain question answering
0.1605125871	task 8
0.1605063313	a binary classification task
0.1604948504	stems from
0.1604618335	starting from
0.1604232218	thus far
0.1604086202	without supervision
0.1603880998	to obtain
0.1603450198	important natural
0.1603106310	chinese word segmentation with
0.1603017298	improved performance over
0.1602446546	lower than
0.1601995048	an input sentence
0.1601879141	level machine
0.1601662019	rich set of
0.1601368979	a hierarchical attention
0.1601105749	very limited
0.1601092172	the ground truth
0.1600693509	tasks such as sentiment analysis
0.1600539430	interaction between
0.1599785826	recent advances in natural language
0.1599172411	does not allow
0.1599101091	evaluated on
0.1599065234	originating from
0.1599014010	occurrence information
0.1598386821	the zipf
0.1598171375	transformer based model for
0.1598033390	method to extract
0.1597705151	sensitive word
0.1597688655	an ai
0.1596818017	problem of learning
0.1596462942	a diachronic
0.1596438270	an essential role
0.1596405081	propose to build
0.1596110301	a neural encoder decoder
0.1595750983	in word error rate
0.1595223321	an automatic speech recognition
0.1595062161	of information for
0.1594625787	propose to learn
0.1594474514	empirical results show
0.1593825396	not simply
0.1593699438	with one or
0.1593363261	best first
0.1593085731	trained to generate
0.1592763694	large body of
0.1592364375	by fine tuning
0.1591311368	sentiment analysis for
0.1591260139	learn to perform
0.1591180916	the total number
0.1590861768	rule based system
0.1590736684	relative reduction in
0.1590675580	translation between
0.1590012964	dataset for evaluating
0.1589676076	for task oriented dialog
0.1589134143	method to identify
0.1587906963	framework to learn
0.1587901433	proposed to improve
0.1587778431	superiority over
0.1587592169	in low resource languages
0.1587491957	the art neural models
0.1587392495	order to address
0.1587366303	by 2
0.1587288562	summarization using
0.1587097695	both labeled and unlabeled
0.1586888209	framework to generate
0.1586793996	tends to
0.1586772001	at inference time
0.1586307061	perform poorly on
0.1586189490	a binary classifier
0.1585978159	probability distribution over
0.1585932804	potentially useful
0.1585774679	propose to combine
0.1585763352	learned through
0.1584432709	compared with
0.1584354449	the german
0.1583968857	based on long short term
0.1583707717	starting point for
0.1582412412	obtained from
0.1582357667	such as
0.1582035174	semantic similarity between
0.1581919725	on timit
0.1581712803	too many
0.1581419535	level performance
0.1581386320	oriented dialogue
0.1580294275	begins with
0.1580169176	experiments on standard
0.1579940936	for aspect based sentiment analysis
0.1579638125	captioning datasets
0.1579519725	based lms
0.1578629576	making process
0.1578214834	empirical investigation of
0.1578081580	new algorithms
0.1577672430	in spite of
0.1577644900	proposed to address
0.1577581002	identification of informative
0.1577492500	this area
0.1577240407	related work
0.1576752614	trained contextual
0.1576443548	lot of research
0.1576143850	between source and
0.1575062161	of sentiment in
0.1575003361	in natural language understanding
0.1574754103	french translation
0.1573359422	a thousand
0.1573338680	impressive performance on
0.1573176345	indistinguishable from
0.1573074408	one step
0.1573063495	both automatic metrics and human
0.1573015225	a song
0.1572996657	correlations among
0.1572922443	representation learning with
0.1572845670	training tasks
0.1572776783	insensitive to
0.1572618843	based on recurrent
0.1572451446	based on local
0.1572316238	detection in online
0.1572182329	in real world applications
0.1571919466	based speech synthesis
0.1571851269	any feature engineering
0.1570492740	both objective and
0.1570418424	training deep neural
0.1570266944	prior state of
0.1570175016	report state of
0.1570149003	earlier work
0.1568943658	a large corpus
0.1568866824	see if
0.1568565193	for low resource neural
0.1567861121	set of tweets
0.1567802190	learning to predict
0.1567731375	at semeval 2020
0.1566599058	the set
0.1566025881	seen at training time
0.1565877592	quality data
0.1565791605	an increasingly popular
0.1565240831	word embeddings trained on
0.1565157586	very encouraging
0.1564666145	to tell
0.1563656481	dataset to train
0.1563503403	new domains
0.1563175487	based inference
0.1562694454	amount of
0.1562504233	tested against
0.1562342629	main tasks
0.1561474876	language statements
0.1561410676	a vector space
0.1561238697	an initial
0.1561071880	multilingual natural
0.1561065584	an information retrieval
0.1560927822	quality of machine translation
0.1559939847	level of performance
0.1559517689	based on distributed
0.1559300643	than 50
0.1558937822	an extended
0.1558797072	three subtasks
0.1557596470	complex natural
0.1556866737	to facilitate
0.1556751766	classifiers trained on
0.1556671734	tweets containing
0.1556316986	similarity model
0.1555579354	a multi label classification
0.1555406188	but also
0.1555339735	semantic role labeling with
0.1555209566	data driven approach to
0.1555084860	recurrent neural language
0.1555060329	a regularized
0.1554824898	the conll
0.1554488739	at semeval 2016 task
0.1553909375	learning experiments
0.1553114136	neural network trained
0.1552718676	news using
0.1552185061	framework to address
0.1552123059	method to incorporate
0.1552037306	by 20
0.1551419535	architecture based
0.1551189358	many nlp applications
0.1550885842	more powerful
0.1549710703	in sanskrit
0.1549386806	features extracted from
0.1549345690	level structure
0.1548892995	to determine
0.1548453066	the t
0.1548445961	for open domain
0.1548260171	interact with
0.1548148226	captioning systems
0.1548107859	tip of
0.1548065996	representations of sentences
0.1547980817	borrowed from
0.1547319879	portion of
0.1547253016	scores compared
0.1547156575	sentences annotated
0.1546784089	approach for learning
0.1546775123	a new multimodal
0.1546755334	a 3d
0.1546686235	resulted in
0.1546658707	much higher
0.1546642267	the european parliament
0.1545831174	conventional text
0.1545648664	the art accuracy
0.1545608462	large improvements over
0.1545587282	by 4
0.1544729834	model trained on
0.1544642550	more reliable
0.1544605806	lot of
0.1544543186	analysis of text
0.1543816604	the proposed
0.1543571444	the 2015
0.1543494126	f1 scores of
0.1543133820	the main
0.1542354062	adaptation using
0.1542003047	overlap between
0.1541713313	tuned model
0.1541587447	work done
0.1541522540	a small number
0.1541514264	over 100
0.1541326174	more robustly
0.1540936383	paucity of
0.1540893326	propose to train
0.1540609234	obtained state of
0.1540583238	make better
0.1540150778	positive impact on
0.1539712451	a transformer based model
0.1538395494	not scale to
0.1537890907	obtained by
0.1537609124	to capture
0.1537059288	composed of
0.1536938221	domain language
0.1536228775	substantially better than
0.1536140927	applications ranging from
0.1536042926	representations learned from
0.1535835873	speech enhancement and
0.1535087135	results on english
0.1534683370	to text
0.1534524468	the art neural machine translation
0.1534513748	without fine tuning
0.1534284727	different scales
0.1534105629	a tractable
0.1533529574	f1 measure of
0.1533485436	task 9
0.1532965804	recognition datasets
0.1532751792	english languages
0.1532416262	a persona
0.1532109696	determined by
0.1532027904	the upper bound
0.1531653846	right contexts
0.1531446494	version of
0.1531133318	solely based on
0.1530829874	propose to incorporate
0.1530466129	outperforms several state of
0.1530465224	thought of as
0.1530005885	retrieval using
0.1529895887	mechanism based
0.1529375531	recent progress in
0.1529351202	generation system
0.1529300109	model to perform
0.1528170177	time complexity
0.1527830900	fixed number of
0.1527455610	proposed dataset
0.1527025347	the mention
0.1526766809	space representation
0.1526694408	problem of generating
0.1526063428	goodness of
0.1524305057	gains of up to
0.1523637096	to produce
0.1523266651	meaningful information from
0.1522916472	similarities and differences between
0.1522768938	participated in
0.1522421001	the ted
0.1522253596	by 1
0.1521641017	related to
0.1520911837	surprisingly well
0.1520293269	a new neural
0.1520083785	level understanding
0.1520003771	combination of multiple
0.1519817808	tts system
0.1518718180	aspects of interest
0.1518040169	does not hold
0.1517964880	large model
0.1517078238	the pre trained language model
0.1516836391	approach to extract
0.1516197114	language adversarial
0.1516174737	compatible with
0.1516035539	on low resource languages
0.1515251086	an embedded
0.1514781267	level language model
0.1514569455	+ 1
0.1514352131	per class
0.1514337846	questions about
0.1513808760	variety of languages
0.1513535710	model to identify
0.1513329264	user needs
0.1513271011	a new corpus
0.1512782461	require parallel
0.1512777565	order to produce
0.1512332209	mechanism to learn
0.1511205631	the first place
0.1510935722	the art supervised
0.1510884550	a diagnostic
0.1510846068	an opportunity
0.1510362910	original english
0.1510244825	method to improve
0.1510141078	single framework
0.1509763869	for low resource
0.1509545581	a conversational
0.1509389560	proposed to learn
0.1509024672	r \
0.1508940650	the world's languages
0.1508829744	an edge
0.1508732109	a fine grained
0.1508657550	the bot
0.1508585085	the word error rate
0.1507727418	the art techniques
0.1506587605	an attention based neural
0.1505939670	combined with
0.1505736186	for knowledge base completion
0.1504728426	a descriptive
0.1504712271	deep learning model for
0.1504537196	the proposed framework
0.1504387482	to avoid
0.1503723668	the data
0.1503445700	the clevr
0.1503187553	baseline neural
0.1502984994	on par with
0.1502339186	between english and
0.1502018391	order to support
0.1501591063	text generation by
0.1501247362	large quantities of
0.1501175629	and relations in
0.1501086191	of 60
0.1501014798	a sparse
0.1500477904	as few as
0.1498501366	key component in
0.1498456351	the transformer
0.1498201290	the art baseline
0.1497594534	both automatic and human evaluations
0.1497444820	pre trained model for
0.1497368601	a manner
0.1497316348	model for automatic
0.1497147292	a practical
0.1496876885	simpler and more
0.1496676142	connections between
0.1496463735	statistical analysis of
0.1496425275	by leveraging
0.1496213046	deep learning models for
0.1495841332	keep track
0.1495685816	this paper tackles
0.1495354066	with long short term memory
0.1495256745	increasing interest
0.1495222726	based on graph
0.1494952164	pay more attention to
0.1494792802	classification experiments
0.1494750034	order to generate
0.1494497022	framework for generating
0.1494300211	simple data
0.1494002167	as special cases
0.1493772890	from pre trained language models
0.1493588513	proposed network
0.1493570187	achieves better performance than
0.1493423399	collected from
0.1493157997	in low resource scenarios
0.1492115043	based on human
0.1491971670	language without
0.1491958992	an f1 score of
0.1491402055	order to achieve
0.1491367170	a very large
0.1491365990	associations between
0.1491175629	of context in
0.1491175629	of languages in
0.1491059894	a data augmentation method
0.1490987349	the processing
0.1490827731	relative improvement in
0.1490532613	the rhetorical
0.1489768448	correct meaning of
0.1489543085	machine learning models for
0.1489470455	dataset consisting of
0.1489229464	number of documents
0.1489183377	model via
0.1489115306	a report
0.1488990128	processing text
0.1488680414	to combat
0.1488408275	a context
0.1488303398	method to learn
0.1487972372	automatic creation of
0.1487722546	present empirical
0.1487704449	attempt to
0.1487694118	the art neural
0.1486972654	simple method
0.1486660191	good performance
0.1486633334	including part of speech
0.1486313510	the transfer
0.1485675248	lack of labeled
0.1485642166	framework for modeling
0.1484918676	value to
0.1484762759	to pre train
0.1484385697	clearly outperforms
0.1484198525	tasks such as image
0.1483986180	to reduce
0.1483846023	model for learning
0.1483454581	an svm
0.1482707263	tasked with
0.1482234762	this shortcoming
0.1481275043	less than
0.1481065313	text using
0.1480860291	the recognition
0.1480826166	of syntax and
0.1480694612	architecture to learn
0.1480665496	a language model
0.1480292606	apart from
0.1480110545	art result on
0.1480045956	models to generate
0.1479760247	model for natural language
0.1479528383	information to improve
0.1479409507	while minimizing
0.1479320551	a set of
0.1479221236	neural network architecture for
0.1478987657	existing topic
0.1478811061	the paragraph
0.1478309584	the eu
0.1478049348	each word
0.1477205927	the new york
0.1477082427	data to train
0.1476974872	using machine
0.1476423162	more accurately than
0.1475906712	based on deep neural
0.1475860709	a character based
0.1475785761	approach to machine
0.1475566105	the help of
0.1475561048	competitive performance on
0.1474795154	improvements in performance
0.1474732911	the proposed model achieves
0.1474691883	a transformer based
0.1474510523	like to
0.1473606881	zero shot performance
0.1473602961	model substantially
0.1472178732	similarity between words
0.1471891917	mainly focused on
0.1471859201	on social media
0.1471732398	these issues
0.1471692921	this limitation
0.1471546816	approach towards
0.1471457553	the edge
0.1471241253	training time
0.1471081520	for cross lingual transfer
0.1471020518	automatic evaluation metrics for
0.1470826166	between training and
0.1470790923	a single layer
0.1470556274	an effective technique
0.1470461681	tens of
0.1470094587	limited amounts of
0.1469971119	a bidirectional long short
0.1469598574	a pre trained
0.1469425228	examine whether
0.1469229579	operate at
0.1469177483	aspects of language
0.1469110563	development of deep learning
0.1468787381	performs better
0.1468667412	close to
0.1468643589	multi task learning to
0.1468314824	workshop on
0.1467531300	field of natural language
0.1467065266	extraction problem
0.1466719781	specific resources
0.1466379218	reason over
0.1466083967	available at
0.1465477904	make better use of
0.1464868465	increasing number of
0.1464448352	applicable to
0.1464328979	improve user
0.1463986419	of 8
0.1463858423	the source domain
0.1463799416	produce high
0.1463737062	problem of identifying
0.1463647257	model prediction
0.1463454343	performance on
0.1463246469	a multi layer
0.1461539314	problem of detecting
0.1461099832	method for learning
0.1460826166	of noise in
0.1460790256	experiments on real
0.1460756976	the art deep learning models
0.1460668999	to follow
0.1460387203	a significant increase
0.1460208817	expressed in natural
0.1460154217	the art solutions
0.1460095601	the word level
0.1459950631	associated with
0.1459917290	exposed to
0.1459374743	transformer language
0.1459370887	divergence between
0.1459256278	the wiki
0.1459084686	over time
0.1458777267	shared task on
0.1458678379	for automatic speech
0.1458191476	the art model
0.1457631029	the one hand
0.1457570825	the english
0.1456894895	set of labels
0.1456773505	original training
0.1456724588	growing number of
0.1456703757	analysis system
0.1456615495	divided into two
0.1455758182	an ideal
0.1455003889	knowledge from text
0.1454875733	an integral part
0.1454830144	the path
0.1454643762	the multi task learning
0.1453955658	triggered by
0.1453517381	based on linguistic
0.1453495559	small amounts of
0.1452770183	present state
0.1452753243	90 \
0.1452641078	based grammars
0.1452442620	a foreign language
0.1451561685	an end to end trainable
0.1451440882	without explicit
0.1451275501	entire model
0.1450955201	contribute to
0.1450826166	of data available
0.1450218292	different dimensions
0.1450052593	tagging with
0.1449884505	by means of
0.1449414230	order to identify
0.1449366719	model to select
0.1449115072	problem of finding
0.1448275823	models for learning
0.1447853245	on real world data
0.1447823515	factors like
0.1447596014	on social media platforms
0.1447434585	internal structure of
0.1447313681	approach to learn
0.1447028109	translated into
0.1446967960	algorithm based
0.1446876264	few hundred
0.1446707421	as soon as
0.1446455610	a collaborative
0.1446421860	a gaussian
0.1446155135	method does not require
0.1446096588	the model
0.1445379550	given rise to
0.1445183061	upper bound on
0.1444494503	very competitive
0.1444256007	fraction of
0.1444226018	across multiple domains
0.1443942337	data to improve
0.1443864974	including machine
0.1443273159	at scale
0.1442975318	on 4
0.1442867393	the low resource setting
0.1442632991	an adversarial
0.1442610412	a fixed set
0.1442526432	the demand
0.1441856636	language corpus
0.1441515009	in electronic health records
0.1441433672	over 90
0.1441215256	corpus consisting of
0.1441212882	a text corpus
0.1440826166	in tasks such
0.1440809288	towards learning
0.1440564229	a natural language question
0.1440471212	a knowledge base
0.1439818109	the complementary
0.1439377884	model to produce
0.1438867930	significantly better
0.1438821191	the order
0.1438415337	over 30
0.1437419337	propose to augment
0.1437320551	the problem of
0.1437189337	a low dimensional
0.1437025347	the causal
0.1436901268	rich source
0.1436847551	the physical world
0.1436683717	each cluster
0.1436612336	for chinese word segmentation
0.1436535008	from natural language text
0.1436506855	the latter
0.1436491381	a fair
0.1436379470	the glue benchmark
0.1436204449	fail to
0.1435041674	do not need
0.1435021732	a lower dimensional
0.1434380770	conveyed by
0.1433979560	neural network architectures for
0.1433682913	in depth analysis
0.1433566428	the structure
0.1433516771	technique based
0.1433365239	on penn treebank
0.1433098388	learn better
0.1432137300	set of data
0.1431983106	take advantage
0.1431561895	approach for automatic
0.1431219147	the gold
0.1430906330	of 20
0.1430826166	for tasks such
0.1430811279	this purpose
0.1429985372	modeling task
0.1429209333	in many cases
0.1429144849	task 3
0.1428476491	head attention
0.1427460493	a novel semantic
0.1426347481	task of extracting
0.1426177443	space embeddings
0.1426033715	more often than
0.1425874877	a real time
0.1425859918	the entire
0.1425386368	2020 task 2
0.1424488422	the state of
0.1424431931	a more comprehensive
0.1423115730	\ ~
0.1423098297	non sequential
0.1422901645	insights about
0.1422639553	dependencies between
0.1422355138	the ten
0.1422046217	full advantage of
0.1421758464	algorithm for learning
0.1421678110	a new evaluation
0.1421546230	model with attention
0.1421519427	by proposing
0.1420935474	understanding task
0.1420826166	on datasets from
0.1420575195	millions of people
0.1420201554	computer vision and natural language
0.1419542605	the impact
0.1419346146	across multiple
0.1419058919	more difficult
0.1418923049	each token
0.1418553904	method to automatically
0.1418317445	semantic relationship between
0.1418132061	systems achieved
0.1418121707	a role
0.1417824355	the official evaluation
0.1417675762	fine tuned with
0.1417492273	inspired by recent
0.1417329575	second phase
0.1416838223	re training
0.1416371108	automatic construction of
0.1416309103	original document
0.1416083472	the largest
0.1415719726	preliminary experiments on
0.1415349623	a simple yet effective
0.1414643169	an urgent
0.1413855282	approach to build
0.1413285633	the art deep learning
0.1413173742	a caption
0.1413014072	these models
0.1412892061	over 2
0.1412874691	a declarative
0.1412409611	variety of tasks
0.1412301254	type of information
0.1412271320	system responses
0.1412073999	an active research
0.1412065904	dearth of
0.1411810621	representations of language
0.1411805289	the identified
0.1411609360	a cross lingual
0.1411597520	pre trained language models such
0.1411042818	in social media
0.1410906620	language through
0.1410895516	still far from
0.1410890715	inferred from
0.1410816966	vector model
0.1410558998	representations for words
0.1410464838	for machine reading comprehension
0.1410393169	then fed
0.1410340098	based on transformer
0.1409807566	the greek
0.1408910529	a pure
0.1408751886	entailed by
0.1407849263	training text
0.1407546249	provision of
0.1407385790	order to obtain
0.1407043153	a manual
0.1406965529	by formulating
0.1406838821	to alleviate
0.1406838563	designed to
0.1406689778	plethora of
0.1406477925	first experiments
0.1406450200	improvements in translation
0.1405836149	the training set
0.1405478079	a new task
0.1404893151	many languages
0.1404680536	a long way
0.1404558897	per word
0.1404209125	progress made
0.1404072568	specific feature
0.1403846001	corpora for training
0.1403790168	co occurrence information
0.1403741737	to select
0.1403158032	assigned to
0.1402795429	reflected by
0.1402779653	approach to generate
0.1401778904	gram language
0.1401358826	more precise
0.1400975307	experimenting on
0.1400827456	improves performance on
0.1400357385	then fine tune
0.1400257064	arise from
0.1399851600	specific model
0.1399061578	comparison between
0.1398451079	model to automatically
0.1397849214	models to predict
0.1397836697	sub problems
0.1397816837	sentiment analysis on
0.1397558089	across different languages
0.1397272971	three main
0.1397120465	pre trained models for
0.1397034867	early stages of
0.1396867283	a support vector machine
0.1396784799	number of words
0.1396520282	impressive results on
0.1396330353	approach to train
0.1396211528	the lambek calculus
0.1396170550	recurrent sequence
0.1396152213	based on context
0.1395691096	significant portion of
0.1395556406	submission to
0.1395467972	dataset of real
0.1395434534	an autoencoder
0.1394457991	tasks such as machine translation
0.1394197127	a challenging problem
0.1393819555	comparative study of
0.1393516823	the semantic web
0.1393383528	decoding time
0.1393128903	improvement over previous
0.1392481804	using multi
0.1392327412	the use of
0.1391936718	much longer
0.1391007002	$ d
0.1390193424	detailed description of
0.1389918149	performance with respect
0.1389307745	comments based
0.1389107952	used to train
0.1389025627	to sequence
0.1388685519	f1 score by
0.1388622711	a cognitive
0.1388594918	to guide
0.1388446211	across documents
0.1387750684	to target
0.1387732392	models for neural
0.1387529206	examples with
0.1387274516	two way
0.1386452024	parsing using
0.1386345677	of 85
0.1386245627	different feature
0.1385366737	to enhance
0.1385173558	fine tuning with
0.1385091592	on one hand
0.1384937750	a complete
0.1384581977	the target word
0.1384429041	n gram model
0.1384426993	approach to automatically
0.1384386898	to handle
0.1383736513	word error rate of
0.1383681457	amount of labeled data
0.1383502030	pre training on
0.1383062623	a diverse range
0.1382619920	machine translation for
0.1382337809	various natural language processing
0.1381745203	an event
0.1381192614	natural language interface for
0.1381097772	methods for natural language
0.1381053704	on 8
0.1380765146	the proposed method improves
0.1380612874	notion of
0.1380604549	a structure
0.1380442199	the art language models
0.1380077850	the caption
0.1379864974	based cnn
0.1379078395	part of
0.1379050744	quality summaries
0.1378790550	model for predicting
0.1378731857	correlate well with
0.1378522909	english german and
0.1378090496	up to 20
0.1378030775	a transformer
0.1377641304	vocabulary problem
0.1377082312	model based on
0.1376375144	best reported results
0.1375859708	for open domain question
0.1375647079	in e commerce
0.1374823776	systematic analysis of
0.1374572993	the unique
0.1374478873	learning framework for
0.1374133116	confronted with
0.1373834516	two real world datasets
0.1373260237	pair of languages
0.1373109358	to perform
0.1373042183	for hate speech detection
0.1372766896	a per
0.1372753075	based on lexical
0.1372721704	on two benchmark datasets
0.1372656467	the proposed methodology
0.1372500120	language datasets
0.1372286535	for use in
0.1372073450	this paper contributes
0.1372039663	$ s
0.1371699102	a stochastic
0.1371536408	the out of domain
0.1371263828	the art extractive
0.1371094557	\ absolute
0.1370517410	the latency
0.1370496859	a dual
0.1370275212	amounts of
0.1370103867	shared task at
0.1369826288	in open domain dialogue
0.1369460211	an average
0.1369457569	of speech tagging
0.1369388805	to ensure
0.1368690608	a collection
0.1367514766	better accuracy than
0.1367184747	task of finding
0.1366981462	results on standard
0.1366195475	a positive effect on
0.1366170035	the verb
0.1365766565	based on conditional
0.1365546549	the f1
0.1364410555	using multi task learning
0.1364318399	on 5
0.1363884680	text written by
0.1363753622	more than 100
0.1363547593	the task specific
0.1363198726	correlation among
0.1362703896	performance compared to
0.1362371013	improvements of up
0.1362172906	perform well
0.1362135970	from plain text
0.1361884472	experiments on two benchmark
0.1361657544	with multi task learning
0.1361385165	mutual information between
0.1361316715	the training data
0.1361238219	processing problems
0.1360869324	experiments to investigate
0.1360776113	no access
0.1360731878	attached to
0.1360676093	model to encode
0.1360480428	during pre training
0.1360419487	s \
0.1360149302	order to reduce
0.1360119077	proposed strategy
0.1359967696	three major
0.1359959267	different types of
0.1359730660	ability to model
0.1359279639	closer look at
0.1358923200	statistical properties of
0.1358821668	mitigated by
0.1358739512	on real world datasets
0.1358359776	the mathematical
0.1358090328	interested in
0.1357975712	method leads
0.1356979367	an alternate
0.1356833020	the character level
0.1356708317	structure trees
0.1356673932	coupled with
0.1356253280	documents written in
0.1355654899	a testbed
0.1355586486	information extraction from
0.1355470332	the similarity
0.1354772741	both automatic and human evaluation
0.1354165201	a correlation
0.1354070056	current version of
0.1353939408	respect to
0.1353048828	a surprising
0.1352854817	representations across
0.1352815059	a macro averaged
0.1352555188	judged by
0.1352275072	propose to model
0.1352230641	a transfer
0.1351997114	the natural language processing
0.1351488981	based on multiple
0.1351093948	simple framework
0.1351054628	on squad
0.1350952381	named entity recognition in
0.1350008398	the art machine learning
0.1349861877	the italian
0.1349773262	a short
0.1349767088	standard machine
0.1349050744	quality responses
0.1348866826	the feature
0.1348829729	natural language processing of
0.1348563584	performance comparable to
0.1347908918	knowledge about
0.1347856376	propose to improve
0.1347848568	performance gap between
0.1347749598	the target domain
0.1347442057	macro f1 score of
0.1347305289	the phonetic
0.1346739302	and long short term memory
0.1346271153	still need
0.1346107300	an additional
0.1345983545	the soft
0.1345925335	more important than
0.1345462697	better semantic
0.1345301887	model for chinese
0.1345135064	large scale neural
0.1344825100	do not explicitly
0.1344664492	of cross lingual transfer
0.1344616494	an optimized
0.1344586367	a very
0.1344148144	model to improve
0.1343628476	a neural model
0.1343615246	time sensitive
0.1343078442	the long short term memory
0.1342959346	a retrieval
0.1342746621	the parallel
0.1342595416	used to
0.1342490544	to better understand
0.1342431252	the search
0.1342379541	algorithm using
0.1342362905	four language pairs
0.1342230271	a rule based
0.1341979454	made for
0.1341525705	research focuses on
0.1341442731	for natural language inference
0.1340820067	method to address
0.1340532395	the dual
0.1340527398	to resolve
0.1340317936	portions of
0.1340170195	a wide range
0.1339312022	the direct
0.1339307232	the experimental results demonstrate
0.1338553447	as input
0.1338075198	more challenging
0.1337922674	absence of
0.1337750549	downloaded from
0.1337621926	the pre trained models
0.1337306283	quite challenging
0.1336884057	more than 30
0.1336880921	limited availability of
0.1336734666	domain questions
0.1336122172	of question answer pairs
0.1335786493	model to achieve
0.1335486206	of pre trained language models
0.1335145556	the pre trained language
0.1334982717	powerful language
0.1334349488	performance of existing
0.1334258936	the art algorithms
0.1334170270	set of sentences
0.1334047495	each sentence
0.1333850744	generating adversarial
0.1333714529	speech translation system
0.1333661898	a style
0.1333642654	by comparing
0.1333064348	relevant information from
0.1333034909	the science
0.1332936735	whether to
0.1332825541	the present study
0.1332136879	test other
0.1332074710	an entity
0.1332034318	for real world applications
0.1332011133	based generation
0.1331808251	the multi head attention
0.1331627019	the art word
0.1331246373	set of
0.1331011597	this approach
0.1330831119	a self supervised
0.1330461104	the same time
0.1330348360	approach to identify
0.1330135239	difficult to
0.1329854819	processing algorithms
0.1329678629	important linguistic
0.1329477441	primarily due
0.1329123098	deviations from
0.1329016207	various nlp tasks
0.1328664709	written in natural
0.1328644275	neural model for
0.1327906523	lingual data
0.1327877468	computer vision tasks
0.1327032080	the span
0.1326953212	reported here
0.1326645310	the reward
0.1326599194	for dialogue state tracking
0.1326197579	types of information
0.1326102608	gain over
0.1325934055	rapid growth of
0.1325755061	based only on
0.1325700546	the improvement
0.1325671576	challenging research
0.1325517810	a large number of
0.1325208341	a website
0.1324956049	extractive text
0.1324766129	most notably
0.1323291651	achieves strong
0.1322989781	variety of applications
0.1322794656	the fever
0.1322622722	trained word
0.1322617755	the lens of
0.1322571872	semantic relationships between
0.1322328312	of user generated content
0.1322290475	a probability
0.1322162091	the sake of
0.1322058972	task of mapping
0.1321513451	better modeling
0.1321329404	for knowledge graph completion
0.1320941058	number of examples
0.1320792528	less data
0.1320460251	a sub
0.1320304396	classified as
0.1320037887	available corpora
0.1319885252	released at
0.1319152878	each modality
0.1319054925	this work proposes
0.1318925722	about images
0.1318137595	a supervised manner
0.1318112199	an indispensable
0.1317992822	features to improve
0.1317671976	a procedure
0.1317429202	studies focus on
0.1317027362	a total of
0.1316806745	from multiple sources
0.1316744762	the covid
0.1316398343	question of whether
0.1316307976	large scale dataset of
0.1316284228	fine tuning of
0.1316088594	the target style
0.1315488777	the source language
0.1315323793	a convolutional
0.1315174262	versions of
0.1315047116	the fine grained
0.1314352591	the art method
0.1314202534	the historical
0.1314028792	three sub tasks
0.1313544587	made available
0.1312784342	a parallel corpus
0.1312754090	a value
0.1312693247	made publicly available
0.1312669136	lead to significant
0.1312655776	multi task learning in
0.1312614735	several natural language processing
0.1312424125	the dialog
0.1312352530	the style
0.1312344279	complex linguistic
0.1312200442	achieve new state of
0.1312144438	recent language
0.1311899762	the generation
0.1311630824	model to translate
0.1311475562	embeddings as input
0.1311329879	on 3
0.1311236799	lack of data
0.1311182972	comparative analysis of
0.1310391039	an account
0.1310388263	a word sequence
0.1309772069	performance improvements over
0.1309559513	the pre trained bert
0.1308581233	slightly better
0.1308410287	this idea
0.1308379776	instantiations of
0.1308156668	2 3
0.1307731664	split into
0.1307467340	the reasoning
0.1307204494	the spatial
0.1307053075	an entire
0.1306745857	\ vec
0.1306689329	strong performance on
0.1306645310	the service
0.1306557560	the stanford question answering dataset
0.1306548904	confined to
0.1306452813	an active area of
0.1305722838	level user
0.1305227657	short paper
0.1304824926	machine learning approach to
0.1304807501	significantly outperforms other
0.1304164263	the ambiguity
0.1303886370	the proposed technique
0.1303593797	an accurate
0.1303593414	new opportunities
0.1303569479	do not generalize
0.1303276455	a content
0.1303029034	performance on standard
0.1302462647	the proposed approach outperforms
0.1302038122	this study aims
0.1301791433	supervised sequence
0.1301417447	evaluation of text
0.1301400452	hundreds of thousands of
0.1301309535	the open
0.1301285948	of social media users
0.1300931910	right context
0.1300637696	experiments on chinese
0.1300556434	a sentiment
0.1300266790	described here
0.1300226567	problems related to
0.1300132627	$ score
0.1300114264	up to
0.1299536624	a category
0.1299515332	general enough to
0.1299448168	for fact checking
0.1299303682	wide variety of tasks
0.1299024956	different modalities
0.1298615502	the function
0.1298098764	the proposed model significantly
0.1297948007	the input
0.1297766129	more broadly
0.1297537499	translated data
0.1297315075	a parallel
0.1297069328	similarities among
0.1297046707	based approach to
0.1296766703	problem in nlp
0.1296746196	generate adversarial
0.1296566428	the prediction
0.1296302893	experiment with
0.1296079834	the privacy
0.1296054221	the population
0.1295843490	a controlled
0.1295700546	the aspect
0.1295689894	less attention
0.1295670035	the subject
0.1295411315	suitable for
0.1295393147	do not perform well
0.1294872710	task consists
0.1294762113	performance achieved
0.1294679185	approach to
0.1294526375	small set
0.1294429004	to mitigate
0.1293866816	the vector
0.1293631124	same author
0.1293396497	variety of
0.1293152680	a classification
0.1293058797	nature of language
0.1292458729	the de facto
0.1292404068	speech into
0.1292301014	a great deal
0.1292219359	the arabic
0.1292085929	one sentence
0.1291533492	the social
0.1291056448	critical role in
0.1291054163	a low resource
0.1291017474	a promising direction
0.1290994078	the performance of
0.1290966498	the story cloze
0.1290532308	the novel
0.1290055525	together in
0.1289871093	a sufficient
0.1289092134	a quantitative
0.1288994759	ability to
0.1288944754	evaluation results show
0.1288901764	learned from
0.1288609913	for multi task learning
0.1288546217	by virtue of
0.1288209718	the background
0.1287910094	encoder decoder model for
0.1287803167	the long
0.1287577882	parts of
0.1287237048	a mapping
0.1287079652	a limitation
0.1286817880	most existing methods
0.1286536048	the matrix
0.1286474388	lingual model
0.1286463954	a strong baseline
0.1285888613	the transformation
0.1285856692	approach to neural
0.1285400217	significant impact on
0.1284986957	advances in neural
0.1284513676	time delay neural
0.1284483470	propose to evaluate
0.1284445006	to work
0.1284063717	the intermediate
0.1283983620	more specifically
0.1283849710	number of samples
0.1283694514	widely adopted in
0.1283481094	model trained with
0.1283378564	various nlp
0.1283153568	method based on
0.1283137394	an early
0.1282840771	approach for sentiment
0.1282758168	types of knowledge
0.1282704539	task of unsupervised
0.1282385572	accuracy compared to
0.1282268077	task 7
0.1282210002	yields better
0.1281833576	zipf's law for
0.1281734320	based on recent
0.1281576419	works focus on
0.1281528555	an adversary
0.1281231076	then fine tuned
0.1280990439	and better
0.1280854843	an important step
0.1280749698	to speech
0.1280570091	for open ended
0.1280322901	side of
0.1280072659	defined by
0.1280039438	based analysis
0.1279680297	2020 challenge
0.1279560380	error rate by
0.1279525347	the minimal
0.1279513527	the art text
0.1278735568	systems rely
0.1278386748	text only
0.1278239018	form of natural language
0.1278206718	of 9
0.1277859016	an easy to use
0.1277738755	to frame
0.1277330148	the recurrent neural network
0.1277151777	a precision
0.1277135239	nature of
0.1276787156	the art natural language
0.1276488790	models for text
0.1275932547	attends to
0.1275769473	4 bleu
0.1275015805	the strong
0.1275005591	learning to solve
0.1274950380	recurrent neural networks with
0.1274658697	a small set of
0.1274643193	the detection
0.1274584215	level speaker
0.1274411176	larger set of
0.1274396335	different from previous
0.1274387943	the french
0.1273965751	an n gram
0.1273587675	people from
0.1273559974	shown to
0.1273144558	compared to human
0.1273059527	a handful of
0.1272855487	$ t
0.1272557699	an integral
0.1272325579	more expressive
0.1272261333	learning community
0.1271840287	limited amount of
0.1271539144	match accuracy
0.1271459845	the sequence to sequence
0.1271445637	improving upon
0.1271355566	the preliminary
0.1271302680	particularly on
0.1271040871	no training
0.1271000612	best case
0.1270834863	analysis of
0.1270787048	a relative
0.1270517389	on 7
0.1269682890	in spoken dialogue systems
0.1269659487	a class
0.1269460156	obtained results show
0.1269421397	core component of
0.1269273150	building block for
0.1269139285	for natural language generation
0.1269106057	the loss
0.1268898941	the focus
0.1268844593	set of experiments
0.1268230142	ones in
0.1267927357	of 4
0.1267878233	order to solve
0.1267437966	an average f1
0.1267103277	tens of thousands of
0.1266612055	the cnn daily mail
0.1266600273	easily extended to
0.1266458152	the capability
0.1266312090	intrinsic evaluation of
0.1266020293	a negative
0.1265969917	used extensively
0.1265919220	contributes to
0.1265837279	a loss
0.1265497520	a transfer learning
0.1265381086	the proposed algorithm
0.1265289309	a sequence to sequence model
0.1264973112	on average
0.1264914616	new types
0.1264532080	the category
0.1264482936	an important issue
0.1263993880	a siamese
0.1263899001	accomplished by
0.1263697886	the best
0.1263658912	an ongoing
0.1263188762	the real world
0.1263138009	applying machine
0.1263081783	and often
0.1262640305	the final
0.1262348658	based chatbots
0.1261899704	much more difficult
0.1261814428	models trained with
0.1261079834	the taxonomy
0.1260847249	remarkable success in
0.1260677808	the art approach
0.1260625850	to indicate
0.1260363219	help users
0.1260362297	the missing
0.1260069211	the art pre trained
0.1259646039	a neural network
0.1259567002	to steer
0.1259298599	information provided by
0.1258836127	based on multi
0.1258724556	approach to word
0.1258670701	the art unsupervised
0.1258559236	a high level
0.1258285327	hidden state of
0.1257789085	effectiveness of
0.1257658080	to question
0.1257553575	by jointly training
0.1257487866	neural network model for
0.1257336143	an essential
0.1257191192	the end to end
0.1256841902	strategy based on
0.1256626118	serving as
0.1256424503	for zero shot
0.1256300015	about covid 19
0.1256269703	analysis dataset
0.1255982204	a causal
0.1255786199	method for identifying
0.1255759001	attacks against
0.1255111427	the transition
0.1255076635	performance of word
0.1254803691	the switchboard corpus
0.1254688795	comes from
0.1254647852	highly correlated with
0.1254597628	widely used in natural language
0.1254382135	the unknown
0.1254026147	glove word
0.1253445592	fine tuned to
0.1253387207	by applying
0.1253231564	for learning word embeddings
0.1253194242	no additional
0.1252905056	a personalized
0.1252473540	for multi document summarization
0.1252008090	a few shot
0.1251968589	model to tackle
0.1251580861	a straightforward
0.1251475138	able to achieve
0.1251180088	better understand
0.1251125464	a pre trained language
0.1251082224	the treebank
0.1250987342	the scale
0.1250763548	in house
0.1250734112	recent trends in
0.1250559900	the sequential
0.1250422773	the visualization
0.1250281354	model to detect
0.1249799829	a linear transformation
0.1249561512	the art generative
0.1248877287	transfer learning from
0.1248746196	aware dialogue
0.1248426301	speech tag
0.1247778332	the art speech recognition
0.1247722961	integral part of
0.1247703393	embeddings learned from
0.1247549202	the digital
0.1247097507	to classify
0.1247058996	a communication
0.1246885697	very costly
0.1246809535	the tree
0.1246711749	a signal
0.1246609477	a small subset of
0.1246246181	extrinsic evaluation of
0.1246100696	model for text
0.1245811970	advances in machine
0.1245699110	large amount of data
0.1245562354	deep neural networks for
0.1244963439	with 100
0.1244809858	the post
0.1244770662	relations among
0.1244654180	the pair
0.1244356648	model for sentence
0.1243004152	p \
0.1242887474	acts as
0.1242664122	the state
0.1242582119	the conversational
0.1242492428	respond to
0.1242483604	variety of datasets
0.1242465060	the concept
0.1242081185	of 6
0.1242006152	dataset of human
0.1242000757	different words
0.1241994611	a wide variety of
0.1241206193	neural approach
0.1241164263	the probability
0.1240937362	the highest
0.1240587621	non autoregressive model
0.1240509995	enabled by
0.1240256668	the use
0.1240214731	in community question answering
0.1240206154	effective transfer
0.1240124053	evolution of language
0.1240018967	tasks such as sentiment
0.1240011032	some extent
0.1239582810	experiments on english
0.1239499556	an lstm based
0.1239398244	for natural language understanding
0.1239064274	to improve robustness
0.1238884460	an explosion
0.1238688040	sub network
0.1238681969	the art neural network
0.1238488828	based on real
0.1238286346	for finding
0.1238050764	quality of generated
0.1237443490	the computer
0.1237404775	up to 6
0.1236956926	known about
0.1236726700	benchmark datasets show
0.1236390490	analysis of social
0.1236372821	current user
0.1236221754	to suggest
0.1236216771	of 35
0.1236192528	on 20
0.1235828161	a control
0.1235771272	a dynamic
0.1235717411	documents written
0.1235401470	the art deep
0.1235237138	does not exist
0.1234956923	still suffers
0.1234788423	most likely
0.1234690338	specific type of
0.1234682716	a top down
0.1234656811	a measure
0.1234654180	the distributional
0.1233707949	neural network models for
0.1233575265	named entities in
0.1233449610	a module
0.1233210661	alternative method
0.1233152927	the distribution
0.1232845413	in multiple languages
0.1232680929	range of domains
0.1232668146	for research purposes
0.1232337166	a constrained
0.1232231839	results compared to
0.1232075122	new corpus
0.1231947078	the hope
0.1231888118	at word level
0.1231833667	the action
0.1231703993	the sentence level
0.1231551508	new challenges
0.1231519730	art results by
0.1231450405	retrieval system
0.1231185877	f1 score for
0.1230994078	the quality of
0.1230546743	very important
0.1230526067	evaluation of word
0.1230252005	contrast to existing
0.1229898016	tree like
0.1229603037	network systems
0.1229245522	a form
0.1229181415	operates on
0.1228493895	whole sentence
0.1228329476	many downstream tasks
0.1228268162	tested on
0.1228230954	a detailed description
0.1228183442	in task oriented dialogue
0.1228098360	try to
0.1227727040	protocol for
0.1227509230	the effect of
0.1227451315	shown promising results in
0.1227396769	a sequential
0.1227244695	set of tasks
0.1227207381	over strong baselines
0.1227142887	designed features
0.1227122149	most influential
0.1226990582	a natural language
0.1226914480	the cross lingual
0.1226722896	a composition
0.1226067201	the pre trained
0.1225452291	a light weight
0.1225430065	for example
0.1225385970	the basic idea
0.1225340188	and labor intensive
0.1225069905	the art machine
0.1224983912	an online
0.1224867213	the adversarial
0.1224850470	the diversity
0.1224809858	the token
0.1224340459	approach based on
0.1224039753	set of human
0.1223781431	unlike other
0.1223768683	the special
0.1223597896	a view
0.1223378313	the meaning
0.1223283889	representations of documents
0.1223211753	language models trained on
0.1222590610	the continuous
0.1222360555	specifically designed to
0.1221987496	a single sentence
0.1221856982	an instance
0.1221764550	a formalism
0.1221709110	large set
0.1221651980	two steps
0.1221497293	to assist
0.1221339461	relatively simple
0.1221299275	non hierarchical
0.1221213542	call for
0.1220976571	help with
0.1220778238	to encourage
0.1220452502	take on
0.1220369009	to become
0.1220033404	automatic classification of
0.1219609129	in real life
0.1219594896	range of tasks
0.1219412198	to part of speech
0.1219210253	significant advances in
0.1219158621	the error
0.1219058052	more accessible
0.1218883410	the first stage
0.1218216694	important source
0.1217903852	with regards to
0.1217769266	one single
0.1217766288	a structural
0.1217706360	the translated
0.1217085636	more concise
0.1217072693	further investigation
0.1216847720	the covid 19
0.1216804850	the narrative
0.1216378322	useful as
0.1216220396	and more
0.1216188040	non recurrent
0.1215585889	the relation
0.1215537747	language requires
0.1215310020	depth first
0.1215178317	a broad range of
0.1215122570	paper deals with
0.1214992513	process using
0.1214659487	a tree
0.1214425768	a change
0.1214350094	order to evaluate
0.1214319327	a name
0.1214247246	the table
0.1214224321	to contain
0.1214004282	qualitatively different
0.1213997974	other non
0.1213972632	an essential part of
0.1213730836	new words
0.1213686388	propose to use
0.1213550412	based on user
0.1213345618	by up
0.1213130116	an information
0.1212845618	a bidirectional lstm
0.1212750055	the clinical
0.1212748350	different languages
0.1212561188	further improvements
0.1212420242	an implementation
0.1212266654	advances in natural
0.1212036549	a multi level
0.1211765242	of pre trained language
0.1211662359	a matrix
0.1211658792	a graph neural network
0.1211523975	to minimize
0.1211049672	a dependency
0.1211015687	based on feature
0.1210811400	a macro averaged f1 score of
0.1210486055	of 91
0.1210248670	each node
0.1209915789	an accuracy of
0.1209776024	to achieve
0.1209620363	knowledge extracted from
0.1209367285	semantic parsers from
0.1209347742	a semantic
0.1209176757	available training data
0.1208966062	the art vqa
0.1208786056	aspects of
0.1208600185	the proposed architecture
0.1208456491	the biomedical domain
0.1208065726	a zero shot
0.1208062612	dataset collected from
0.1208007002	$ c
0.1207794367	performance of machine
0.1207782508	outperforms other state of
0.1207585887	attributed to
0.1207202881	ubiquitous in natural
0.1206729667	a distance
0.1206664263	the difficulty
0.1206459914	does not always
0.1206455799	the f1 score
0.1206043793	selection using
0.1205896327	the need for
0.1205683101	original word
0.1204893100	the modified
0.1204665653	the inference
0.1204316826	the study
0.1204261156	transfer knowledge from
0.1204157545	the final prediction
0.1204056506	learn about
0.1203899563	with 10
0.1203802766	multi task learning of
0.1201438769	to achieve high accuracy
0.1201192241	the discourse
0.1201134154	on mobile devices
0.1200465869	the conceptual
0.1200264399	do not exist
0.1200248605	scale evaluation
0.1200242291	approach to unsupervised
0.1200135116	the encoder decoder
0.1199939611	classifier based on
0.1199923827	the best models
0.1199570160	interpretable word
0.1199548676	a mechanism
0.1199423197	pair of sentences
0.1199184130	a significant challenge
0.1198782506	the gender
0.1198743575	comprehensive experiments on
0.1198533114	to represent
0.1198493594	models lack
0.1198482436	model for question
0.1197905367	sampled from
0.1197873584	performed better than
0.1197812619	the target
0.1197394812	present experimental
0.1197194547	language syntax
0.1197178133	noisy training
0.1196613398	a path
0.1196604436	an auxiliary
0.1196425613	a freely available
0.1196326679	as much
0.1196205015	much more
0.1196109428	to infer
0.1196060960	comparable to
0.1196009402	to unseen
0.1195983545	the constraint
0.1195888634	a single unified
0.1195618133	neural named
0.1195435717	simple text
0.1194884609	to employ
0.1194746530	a universal
0.1194744092	number of applications
0.1194531090	make full use of
0.1194430672	at training time
0.1194185464	a co
0.1193687486	convolutional neural networks for
0.1193545906	a description
0.1193533061	the target speaker
0.1193031594	from 10
0.1193027702	differ from
0.1192861601	the very
0.1192845824	requiring less
0.1192838280	the art neural machine
0.1192417659	task of automatically
0.1192185332	experiment results on
0.1192132535	the comparison
0.1192075130	embeddings improve
0.1191993348	the likelihood
0.1191964867	set of baselines
0.1191958048	by adding
0.1191638048	a novel neural network
0.1191394491	amounts of labeled
0.1191241027	away from
0.1190994078	the effectiveness of
0.1190993294	modeled as
0.1190891679	for knowledge graph embedding
0.1190738948	the pattern
0.1190657243	mappings between
0.1190554381	using pre trained word
0.1190497775	experiments on three
0.1190255263	a huge amount
0.1190227608	deep learning approaches for
0.1190221120	conducted experiments on
0.1190116826	the entity
0.1190105750	a word sense disambiguation
0.1189903323	small set of
0.1189821202	to out of domain
0.1189648787	model for semantic
0.1189316826	the representation
0.1189119910	a large vocabulary
0.1189002368	two case studies
0.1188892288	model for sentiment
0.1188664263	the raw
0.1188266190	unsuitable for
0.1187961737	$ bleu
0.1187888964	report results on
0.1187517045	set up
0.1187178881	the relational
0.1186916220	a modern
0.1186224884	a translation
0.1186105944	a single document
0.1185895325	made to
0.1185840676	a single model
0.1185548572	large performance
0.1185421011	the discrete
0.1185301819	language research
0.1184885913	an acoustic model
0.1184867213	the character
0.1184854209	to enable
0.1184544321	at sentence level
0.1184522606	a fully
0.1184312022	the implementation
0.1184049142	the selection
0.1182975416	through extensive experiments
0.1182709860	transliteration of
0.1182582686	structure of language
0.1182563099	an implemented
0.1182435708	a brief
0.1182418155	to maximize
0.1182373321	a scheme
0.1182291124	multitude of
0.1182098049	to answer questions
0.1181466623	embeddings derived
0.1180943346	an argument
0.1180644282	by combining
0.1180623001	weighted average of
0.1180406026	the opinion
0.1180328074	the emotional
0.1180278434	improves performance over
0.1180002922	the rule
0.1179885643	the progress
0.1179839973	interacting with
0.1179755488	to assess
0.1179143051	more flexible
0.1179020645	a bi directional
0.1178754867	the legal domain
0.1178660234	an f score of
0.1178601413	on two real world datasets
0.1177743279	very successful
0.1177704980	to sql generation
0.1177663905	the human brain
0.1177644993	the purpose
0.1177622813	very challenging
0.1177175139	extensively used
0.1176944517	the loop
0.1176906874	of 12
0.1176653895	a labeled
0.1176292551	needs of
0.1176038152	a pattern
0.1175907122	the task of
0.1175810706	the partial
0.1175743434	distribution over
0.1175728881	proportional to
0.1175315095	model to extract
0.1175049286	the art speech
0.1174869533	parser based on
0.1174633398	limited set
0.1174616289	alignments between
0.1174536712	an end to end framework
0.1174511803	by 6
0.1174505289	specific type
0.1174083680	from unstructured
0.1174052738	reflected in
0.1173939358	to appear
0.1173891528	over 50
0.1173708271	the output layer
0.1173625237	considered as
0.1173602052	hypotheses about
0.1173572909	specifically designed for
0.1173523359	models for language
0.1173457208	ask questions
0.1173383204	and machine learning techniques
0.1173278778	relatively low
0.1172992866	empirical evaluation of
0.1172793798	a major challenge
0.1172534335	$ relative
0.1172519624	the described
0.1172285525	the long term
0.1172199775	paths between
0.1171918672	results with respect
0.1171636442	in terms of bleu score
0.1171619952	the input text
0.1171422167	pre training for
0.1171330127	rapid development of
0.1171173850	multiple types of
0.1171028530	an agent
0.1170816457	the wall street
0.1170655859	fill in
0.1170652530	the medical
0.1170476959	information to learn
0.1170269513	properties of word
0.1170262285	either on
0.1170091149	this paper studies
0.1169880136	a compact
0.1169550795	further boost
0.1169540148	two or more
0.1168480843	to word model
0.1168228832	the main idea
0.1168149557	shaped by
0.1167688529	the test set
0.1167362642	small amount of
0.1167305289	processing community
0.1167193827	to day
0.1167179328	a fact
0.1167082539	the proposed approach achieves
0.1166950783	human performance on
0.1166648257	the large scale
0.1166573119	currently available
0.1166254900	this paper reviews
0.1166109808	a small
0.1166005492	a new dataset
0.1165794828	given passage
0.1165748588	a social
0.1165534672	unsupervised learning of
0.1165449960	as fast as
0.1165192241	the emotion
0.1164928204	theoretical analysis of
0.1164916057	the expert
0.1164653651	inference system
0.1164376548	trained on monolingual
0.1164369701	the co occurrence
0.1164144783	a phoneme
0.1163946979	an end to end neural
0.1163914029	of 64
0.1163668461	propose two novel
0.1163607483	over knowledge bases
0.1163439161	a natural language processing
0.1163085814	the essential
0.1163049459	the input sentence
0.1162858823	the original text
0.1161867177	systems submitted to
0.1161809535	the linear
0.1161593308	mining system
0.1161357477	the computation
0.1161335976	offered by
0.1161184648	the legal
0.1161014798	a candidate
0.1160487502	based machine
0.1160316413	a monolingual
0.1160288936	the targeted
0.1160200622	properties of
0.1160020616	the interaction
0.1159345623	the mimic iii
0.1158958899	the art transformer
0.1158760906	the cross
0.1158317561	as well
0.1158186297	the contribution
0.1158145460	in order
0.1157957528	this paper shows
0.1157621449	control over
0.1157506467	the fast
0.1157157803	the ability
0.1157096326	the covid 19 open
0.1156926289	the type
0.1156808209	a copy mechanism
0.1156730540	audio only
0.1156723517	often fail
0.1156702133	to group
0.1155778929	comprised of
0.1155775336	facts about
0.1155716465	driven approach to
0.1155657557	out of
0.1155537048	correlate well
0.1155276321	the gold standard
0.1155204869	first stage
0.1154969912	a preliminary
0.1154818638	more complicated
0.1154730553	small size of
0.1154466400	the massive
0.1154071030	types of features
0.1153964259	these methods
0.1153890213	the final answer
0.1153684691	self attention models
0.1153554204	a limited set of
0.1153482713	able to generate
0.1153034636	easy to use
0.1153017182	the perplexity
0.1153013666	the particular
0.1152994078	a number of
0.1152859156	significant number of
0.1152691676	by exploiting
0.1152443275	able to capture
0.1152181510	the theory
0.1152101843	deeper understanding of
0.1151995350	a finite set of
0.1151964931	to encode
0.1151812022	the critical
0.1151579905	number of languages
0.1151359746	$ x
0.1151189447	the generalization
0.1150977017	the categorical
0.1150909851	novel attention based
0.1150855240	the pre
0.1150828491	learning better
0.1150283298	a feed forward
0.1150116848	do not require
0.1149588252	rules from
0.1149413340	a long short term
0.1149410107	two phases
0.1149294513	a target language
0.1149001651	significant improvements in
0.1148920370	trained on english
0.1148209270	a temporal
0.1148041628	respectively on
0.1147990438	yet to
0.1147597436	the fine tuning
0.1147250334	the parameter
0.1147116893	the art embedding
0.1147067146	number of tasks
0.1147052827	approach to automatic
0.1146863116	a prior
0.1146782587	to post
0.1146048376	a scientific
0.1145838885	recently become
0.1145622155	with attention mechanism
0.1145596336	a point
0.1145453775	self attention model
0.1145431959	very time consuming
0.1145088414	does not rely on
0.1145078265	a result
0.1145038239	a vocabulary
0.1145020720	several real world
0.1144968343	a re
0.1144902123	named entity recognition for
0.1144773933	no labeled data
0.1144671835	the multi task
0.1144580608	a field
0.1144522855	help to
0.1144013498	types of data
0.1143608025	accuracy compared
0.1143550839	preferred by
0.1143459839	a fixed length
0.1143393190	model for unsupervised
0.1142814757	the bi directional
0.1142624038	multiple sets of
0.1142581668	lack of large
0.1142408304	order to provide
0.1142170507	superior performance on
0.1142157887	including named
0.1141845267	bleu scores on
0.1141808634	the former
0.1141523931	good for
0.1141305766	task of natural language
0.1141270628	a lot of
0.1140455454	en de and
0.1140360104	an answer
0.1140220757	a keyword
0.1140146046	a diversity
0.1140045119	based on convolutional
0.1140015811	the dialogue history
0.1139954536	translation tasks show
0.1139949464	the nlp community
0.1139894244	a relational
0.1139498762	perform well on
0.1139464917	comparative evaluation of
0.1139431613	the technical
0.1139366840	need of
0.1139110874	for semantic role labeling
0.1138697633	a sense
0.1138541350	an optimal
0.1138368173	a hierarchy
0.1137903280	a few
0.1137655310	the design
0.1137560650	the argument
0.1136929217	the extent
0.1136909191	this gap
0.1136677990	the negative
0.1136576049	the google books
0.1135963704	the distance
0.1135596557	of machine reading comprehension
0.1135432381	the noise
0.1135239278	joint learning of
0.1134919968	a grammatical
0.1134832918	different positions
0.1134754452	impact on
0.1134669138	the machine translation
0.1134261313	conducted on
0.1133899232	a pre
0.1133607201	and right to left
0.1133411746	constituency parsing with
0.1133067670	appropriate response
0.1133060246	the keyword
0.1133039596	more frequent
0.1132922243	a search
0.1132918780	the policy
0.1132730797	first place
0.1132304166	this paper demonstrates
0.1132152888	this paper focuses
0.1131959346	to stabilize
0.1131462806	manual annotation of
0.1131080661	work by
0.1130852127	to outperform
0.1130409482	to score
0.1130361023	news articles from
0.1130316867	the part of speech
0.1130229772	this paper extends
0.1130169411	machine translation systems for
0.1130032562	subset of
0.1129941077	order to extract
0.1129744125	meta learning for
0.1129449912	empirically show
0.1129350454	classifiers based on
0.1129287398	model to focus
0.1129104079	the proposed methods
0.1128997904	over knowledge graphs
0.1128801958	task of natural
0.1128669838	a segment
0.1128623596	power of neural
0.1128386819	the box
0.1128075732	a patient
0.1127748891	the segment
0.1127705783	paired with
0.1127652189	a frame
0.1127038658	the most informative
0.1127007097	the correct answer
0.1126980952	tweets related to
0.1126745769	resource speech
0.1126652558	a constraint
0.1126473723	to end to end
0.1125961855	easy to
0.1125947195	enable users to
0.1125913913	the program
0.1125888527	the left
0.1125795195	models tend to
0.1125501969	important aspects of
0.1124827498	the spoken
0.1124651943	important parts of
0.1124615745	only slightly
0.1124524680	the interface
0.1124473241	to learn word embeddings
0.1124070322	suited for
0.1124036048	the cognitive
0.1123883282	superior performance over
0.1123346364	the logic
0.1123254085	yet powerful
0.1122723093	trained on human
0.1122690578	testing time
0.1122444365	improvements over existing
0.1122361956	a noise
0.1122274935	the implicit
0.1122159451	a layer
0.1122063099	some recent
0.1121614680	different language pairs
0.1121477683	to change
0.1121455967	best model achieves
0.1121334336	each segment
0.1121292101	4 score
0.1121181035	a graph convolutional
0.1121096472	new loss function
0.1120836937	released dataset
0.1120793682	very promising
0.1120766860	handled by
0.1120488422	the art nmt
0.1120312094	large portion of
0.1119988220	each layer
0.1119943856	a phonetic
0.1119141331	body of work
0.1118649506	a word level
0.1118647370	number of labeled
0.1118360921	to english translation
0.1117737784	a hidden
0.1117307849	a constraint based
0.1117162099	the ensemble
0.1117128256	an end to end neural network
0.1116847641	the noun
0.1116731971	translation system
0.1116728448	a voice
0.1116609516	do so
0.1116262858	research based
0.1116173886	order to build
0.1115435271	go to
0.1115339722	formed by
0.1115282749	quality dataset
0.1114999514	transfer models
0.1114992493	out of context
0.1114635354	a news
0.1114599136	emergent communication in
0.1114490291	different nlp
0.1114387662	cause of
0.1114365140	broad set of
0.1114227110	in f score
0.1114150280	the work
0.1113990158	the graph based
0.1113943416	to action
0.1113725305	the early
0.1113423925	the manual
0.1113014876	zipf's law of
0.1112997695	the language model
0.1112981665	bleu points on
0.1112757810	range of
0.1112672140	covered by
0.1112615210	a macro f1 score
0.1112562993	the impact of
0.1112386766	information encoded in
0.1112134099	a pointer generator
0.1112110007	model for multi
0.1111880247	large sets
0.1111512621	networks for multi
0.1111485277	the results obtained
0.1111370872	this paper considers
0.1111347489	as features for
0.1111346517	a pretrained
0.1111289184	network acoustic
0.1111281623	by fusing
0.1111111831	with pre trained language
0.1110447359	machine learning approach for
0.1109909907	a text to speech
0.1109603949	the product
0.1109508293	the i vector
0.1109500496	computational framework for
0.1109493989	neural architectures for
0.1109457981	a dialogue system
0.1109133959	a new word
0.1109031622	on par
0.1108975932	of multiword expressions
0.1108881905	domain speech
0.1108839382	results on two datasets
0.1108736394	a news article
0.1108360129	this paper reports
0.1108344728	first dataset
0.1108284549	approaches based on
0.1108150000	the vast amount
0.1107413712	more effective than
0.1107323975	the final result
0.1107053711	a combined
0.1106971182	go through
0.1106947639	the czech
0.1106945739	more interestingly
0.1106702491	on 11
0.1106694339	the point
0.1106663426	contain rich
0.1106566515	very good
0.1106311049	model does not require
0.1106175026	the word embeddings
0.1106006646	these problems
0.1105994751	with deep neural networks
0.1105962430	large amount of
0.1105941246	relation extraction with
0.1105901568	the explicit
0.1105703848	a multi agent
0.1105693445	more generally
0.1105558784	a reinforcement learning
0.1105331846	the art single
0.1105221634	a spatial
0.1105189617	perform experiments on
0.1104783277	more efficient than
0.1104640768	a large collection
0.1104461847	existing generative
0.1104116756	based on machine
0.1103959390	on ptb
0.1103774662	the signal
0.1103615330	the communication
0.1103602128	approaches rely on
0.1103588656	by incorporating
0.1103480421	ones with
0.1103392729	a downstream
0.1103246614	the proposal
0.1102716694	clinical natural
0.1102594635	proven to
0.1102583580	dataset based on
0.1102143658	the art sentence
0.1101829887	corpus of news
0.1101829435	an effective way
0.1101826691	more generalizable
0.1101764990	the proposed method significantly
0.1101746738	a person's
0.1101626829	for zero shot cross lingual
0.1101563036	the skip gram
0.1101106328	for text classification
0.1101032782	the attention mechanism
0.1100994078	the context of
0.1100990842	used for
0.1100726387	distributed representation of
0.1100694030	the tag
0.1100642858	the unstructured
0.1100426589	a log linear
0.1100278966	a world
0.1100135804	this annotation
0.1099989837	pre training with
0.1099758906	the first
0.1098915727	theoretic approach to
0.1098571848	to introduce
0.1098497632	speech synthesis using
0.1098441653	aim to
0.1098212963	for low resource language
0.1098028921	the morphological
0.1097948007	the current
0.1097621960	vector representations of
0.1097549572	known for
0.1097462901	able to learn
0.1097431239	based character
0.1097232835	automatic evaluation of
0.1096469280	model towards
0.1096117609	present study
0.1096000982	question answering via
0.1095898055	knowledge to improve
0.1095801248	a game
0.1095557561	a type
0.1095177407	deep learning techniques for
0.1094640673	achieve better performance than
0.1094565945	experiments on three real
0.1094258304	the time series
0.1093769710	large volume of
0.1093684958	the multilingual
0.1093523037	very useful
0.1093389545	a service
0.1093359442	real world use
0.1093090024	an annotated
0.1092838991	$ german
0.1092746366	the conditional
0.1092207208	ease of use
0.1092195782	the key idea
0.1092126351	for end to end speech recognition
0.1092063208	the value
0.1091884118	a sequence of
0.1091481474	by decomposing
0.1091423113	differences among
0.1090999418	types of language
0.1090626474	a distribution
0.1090583895	pre trained language model to
0.1090219085	recently seen
0.1090133475	50 \
0.1089953585	the subword
0.1089735742	ie systems
0.1089649502	the abstract
0.1089629923	a multi head
0.1089603949	the phrase
0.1089421494	bert like
0.1089375023	paper focuses on
0.1089299171	to elicit
0.1089230472	discussion about
0.1089205139	one side
0.1089122612	a group
0.1088660406	the spanish
0.1088634885	of natural language text
0.1088459805	the proposed approach significantly
0.1088293435	the rank
0.1088093896	the word based
0.1088024144	the grammatical
0.1088014907	future research on
0.1087575857	this survey
0.1087521596	the iterative
0.1087495432	the slot
0.1087380234	modelled by
0.1087378262	modeling objective
0.1087118113	\ c
0.1087069136	the integrated
0.1086430515	number of training
0.1086284269	faced by
0.1086019426	for multi turn
0.1085990756	and fine tune
0.1085591133	approach results in
0.1085529521	to rescore
0.1085251764	the alignment
0.1085124597	metric based on
0.1085059365	a function
0.1084997257	the formal
0.1084523071	a linguistically
0.1084516025	a strategy
0.1084276327	new challenges for
0.1084216392	a close
0.1084011194	still in
0.1083983262	vector representation of
0.1083789749	the collected
0.1083783478	the black box
0.1083552419	widespread use
0.1083089280	models to classify
0.1082883531	able to handle
0.1082779494	trained on data
0.1082707648	a range of
0.1082497722	the software
0.1082267455	such as lexical
0.1081919223	a memory
0.1081656491	decoder neural
0.1081357477	the intent
0.1081308439	augmentation using
0.1081176312	a focus
0.1081066194	quality of machine
0.1081064659	tuning method
0.1080817806	an important application
0.1080564056	the last layer
0.1080538671	the source sentence
0.1080301107	more effective
0.1079919427	the model learns
0.1079886571	the book
0.1079881564	these challenges
0.1079757458	the generative
0.1079706247	the x vector
0.1079585665	several nlp tasks
0.1079457937	concentrated on
0.1079445486	to sequence framework
0.1079351503	based neural networks for
0.1079281338	scale knowledge
0.1079160115	a morphological
0.1079125457	broader range of
0.1078934522	a macro f1 score of
0.1078797369	the k
0.1078690645	to text generation
0.1078366348	hours of
0.1078201512	the size
0.1077989463	learning for neural
0.1077718911	spelling correction in
0.1077381278	approach relies on
0.1077313809	supposed to
0.1077102239	at semeval 2017
0.1076925258	a part of speech
0.1076920975	surge in
0.1076850789	explore whether
0.1076401556	the text based
0.1076189452	predicting whether
0.1076125881	answering over
0.1075938350	the complexity
0.1075915096	the customer
0.1075749849	significant amount
0.1075264460	based tts
0.1074892567	to compute
0.1074769039	the person
0.1074673903	generative question
0.1074507617	a reward
0.1074498401	between sentence pairs
0.1074215550	the coarse grained
0.1073770518	but effective method
0.1073579928	times more
0.1073528862	of natural language sentences
0.1073346634	a paradigm
0.1072971548	examples per
0.1072868882	simpler than
0.1072614956	this chapter
0.1072241728	quality in terms
0.1072110757	work with
0.1071642495	the movie
0.1071266980	this study presents
0.1071086796	the suggested
0.1070749125	a weight
0.1070707648	the role of
0.1070635248	model learned
0.1070249434	instead of
0.1070231965	the practical
0.1070119213	in isolation
0.1070118420	difficult because
0.1070112779	a representation
0.1070093016	a problem
0.1070028240	models for nlp
0.1069772290	a large set of
0.1069477197	the vocabulary size
0.1069457438	for image captioning
0.1069275049	the teacher
0.1069160808	technique based on
0.1069156960	differentiate between
0.1068942755	the operation
0.1068476233	the inner workings
0.1068185197	significant improvements on
0.1067974532	the position
0.1067728439	of pre trained models
0.1067549972	the functional
0.1067430910	the known
0.1067378731	a speech
0.1067113320	the message
0.1067039583	very sparse
0.1066516527	a text
0.1066210049	\ precision
0.1065704114	an extensive
0.1065646230	speech tagging
0.1065600459	publicly available at
0.1065545096	used in conjunction with
0.1065500913	pre trained with
0.1065473517	an intriguing
0.1065451708	new applications
0.1065263878	information extraction system
0.1064698323	90 accuracy
0.1064532478	a ranking
0.1064494925	texts written in
0.1064406719	based on convolutional neural
0.1064288644	also provide
0.1064206065	the name
0.1064012232	an f1 score
0.1063571645	the self
0.1063407850	text written in
0.1063028571	systematic comparison of
0.1062949912	making use of
0.1062541302	a sequence labeling
0.1062381180	a need
0.1062117811	modelled as
0.1062063717	the surface
0.1062055577	a correct
0.1061878873	the search space
0.1061825710	to sql task
0.1061818990	results on
0.1061609447	bias towards
0.1061389074	a bilingual dictionary
0.1061313107	experiments performed on
0.1061307654	function based
0.1061226577	new languages
0.1061190246	this paper compares
0.1061068466	the art multilingual
0.1060919925	use supervised
0.1060773933	three benchmark datasets
0.1060767115	the combined
0.1060568447	combination of
0.1060509539	a single task
0.1060469280	modeling using
0.1059957399	non textual
0.1059217462	an exhaustive
0.1059019231	the generalized
0.1058849920	from social media
0.1058843378	the embedded
0.1058819664	external knowledge into
0.1058690427	the weight
0.1058656931	this short
0.1058634935	looking for
0.1058563034	the text corpus
0.1058350426	per second
0.1058211868	the biomedical
0.1057986394	without relying on
0.1057783493	continues to
0.1057760680	to +
0.1057567404	at https
0.1057491098	organized into
0.1056975164	to generate natural language
0.1056877556	the tagger
0.1056835889	a test
0.1056809314	in particular
0.1056625334	supervised framework
0.1056432834	the team
0.1056368417	the slot filling
0.1056339863	a language modeling
0.1056323631	an expert
0.1056151340	a new set
0.1056090974	the main challenges
0.1055945127	order to create
0.1055944474	for out of domain
0.1055801475	re evaluate
0.1055646230	sense knowledge
0.1055632905	multiple levels of
0.1055400694	variational autoencoder for
0.1055343980	to fulfill
0.1055328377	run on
0.1055201392	gradient method
0.1055160943	about 1
0.1054750968	a word based
0.1054381812	the start
0.1054044966	the information contained
0.1054044673	for chinese ner
0.1053835251	both directions
0.1053510463	achieved by
0.1053402378	two aspects
0.1053387865	novel multi task
0.1053222395	\ relative
0.1052942448	the syntax
0.1052852716	increasing amount of
0.1052806283	four major
0.1052654703	the expansion
0.1052596030	to query
0.1052583088	empirical studies on
0.1052278525	soon as
0.1052265714	a character
0.1051843334	better models
0.1051801292	prior knowledge of
0.1051692961	the behavior
0.1051493259	different styles
0.1051242430	a main
0.1051160618	the manner
0.1051134452	outperforms other
0.1050881692	a generator
0.1050867408	driven method
0.1050707648	the presence of
0.1050665795	a clean
0.1050339527	the attribute
0.1050018967	search using
0.1049957123	domain knowledge into
0.1049896101	further enhance
0.1049876516	the source text
0.1049872011	the flexibility
0.1049587384	of 1.3
0.1049497002	used in
0.1049387400	larger number of
0.1049175032	good quality
0.1049014009	main focus of
0.1048975458	the proposed solution
0.1048739219	the near
0.1048335382	a question answering
0.1048248963	preliminary work
0.1047990519	the arts
0.1047886854	performance of deep
0.1047866588	the original sentence
0.1047706522	time scale
0.1047589784	a generalization
0.1047503949	a surge
0.1047375768	a mathematical
0.1047157224	the node
0.1047092930	general framework for
0.1047046927	pre training of
0.1047007522	via reinforcement learning
0.1046897044	over 80
0.1046819798	for 5
0.1046781157	for open domain dialogue
0.1046678959	pre trained language models such as
0.1046475707	the bottleneck
0.1046441673	or better
0.1046354725	for testing
0.1046334388	fine tuning for
0.1046141856	by pre training
0.1046116083	world tasks
0.1045372727	model leads
0.1044956082	promising way
0.1044875494	the residual
0.1044856350	the word embedding
0.1044727494	standard data
0.1044579829	for multi hop
0.1044556952	trained sentence
0.1044149161	help researchers
0.1044126834	the uncertainty
0.1043916157	wer over
0.1043801252	last layer
0.1043741914	the hierarchy
0.1043684208	a lot of interest
0.1043495656	opinions about
0.1043446503	solved by
0.1043404590	a search engine
0.1043352829	effect on
0.1043348924	fed to
0.1043347026	strong results on
0.1043179274	a parameter
0.1042729792	significant improvement in
0.1042625328	twice as
0.1042594473	the power law
0.1042409629	and error prone
0.1042293251	various natural language
0.1042260521	the news
0.1042252045	a filter
0.1042180577	a label
0.1042046674	a crucial role
0.1041769760	the text
0.1041669777	an absolute improvement
0.1041625889	a resource
0.1041131106	the art recurrent
0.1041014778	to discover
0.1040749767	the experimental results
0.1040731281	value in
0.1040659122	synthesis using
0.1040272919	the art nlp
0.1040228498	kind of
0.1040197096	accepted by
0.1040068567	the analyzed
0.1040017510	types of word
0.1039980000	a goal
0.1039958871	by conducting
0.1039923233	the fine tuned
0.1039874759	more informative
0.1039603949	the frequency
0.1039593898	to localize
0.1039586052	a named entity recognition
0.1039449474	distillation for
0.1039418108	of machine learning models
0.1039093568	end to end training of
0.1039050063	linguistic properties of
0.1038779273	the rest
0.1038763792	well suited for
0.1038750451	a noisy
0.1038680715	a limited number of
0.1038528686	differences in
0.1038434370	the re
0.1038412869	the dialog history
0.1038299799	the optimization
0.1037880982	for end to end speech
0.1037638177	through crowdsourcing
0.1037384349	the bilingual
0.1037368777	to discuss
0.1037162308	a side
0.1036973834	various kinds
0.1036943319	a tag
0.1036942602	for analyzing
0.1036901937	cause extraction
0.1036899572	a single language
0.1036897663	indicative of
0.1036768567	a perspective
0.1036595890	a question answering system
0.1036569381	comparable performance to
0.1036493498	the inner
0.1036491476	domain question
0.1036268262	the formalism
0.1036106143	an inter
0.1035990538	the description
0.1035898024	best published
0.1035837163	determination of
0.1035827201	the detailed
0.1035584142	to generate diverse
0.1034894602	still suffer
0.1034804103	performance gains over
0.1034776224	per language
0.1034714629	mostly focused
0.1034680909	the natural language
0.1034581088	a web
0.1034366793	the current state of
0.1034364624	built from
0.1034348112	to estimate
0.1034321674	a selected
0.1034272838	the few shot
0.1034262951	significant reduction in
0.1034228320	the mapping
0.1034131879	datasets shows
0.1034116623	robust against
0.1034108707	a model based
0.1034026423	very high
0.1034025021	model for generating
0.1033347397	a knowledge graph
0.1033208853	the polarity
0.1033069596	task 12
0.1033005950	this type
0.1032896861	a day
0.1032896585	multiple layers of
0.1032886946	the specified
0.1032822157	the knowledge base
0.1032722960	to make
0.1032425768	a theory
0.1032339629	a target word
0.1032161646	defined as
0.1032159487	such features
0.1032002290	f1 score in
0.1031954075	out of domain data
0.1031927434	emotion detection in
0.1031787714	the explanation
0.1031784504	the technology
0.1031569617	a generation
0.1031512447	corpus of documents
0.1031429092	a machine learning model
0.1031200761	alignment between
0.1031047205	two major
0.1030782956	the effort
0.1030553812	the cluster
0.1030502084	for handling
0.1030494979	guided by
0.1030318331	representations based on
0.1030159071	the machine
0.1029807146	methods suffer from
0.1029551071	the machine learning
0.1029538493	better accuracy
0.1029535118	continuous word
0.1029501507	empirical study on
0.1029497449	the feedback
0.1029417492	to covid 19
0.1029400479	experiments indicate
0.1029322967	the model performance
0.1029254720	the form
0.1028823788	for data to text
0.1028796732	representation learning for
0.1028723278	for representing
0.1028271685	substantial amount of
0.1028242112	applications like
0.1028164394	the target task
0.1028137361	relation between
0.1028052102	experiments on three benchmark
0.1027554812	word embeddings from
0.1027508910	the input sequence
0.1027425913	that goal
0.1026549918	level evaluation
0.1026148831	a step
0.1025953456	network for chinese
0.1025870154	in natural language processing tasks
0.1025822808	estimated by
0.1025691666	transfer learning for
0.1025552844	promising results on
0.1025514866	from electronic health
0.1025154455	network for text
0.1025152537	high cost of
0.1025133508	each group
0.1025042770	extract information from
0.1024961495	an oracle
0.1024872204	or less
0.1024562865	deep understanding
0.1024510268	more efficient
0.1024461000	the paper describes
0.1024381175	the importance of
0.1024243217	data source for
0.1024239796	to sequence neural
0.1023871047	the russian
0.1023859094	neural machine translation to
0.1023695780	a model
0.1023694886	relative importance of
0.1023326965	a team
0.1023206069	bounds on
0.1023148232	published results on
0.1023121053	achieving good
0.1022930804	the softmax
0.1022531007	the weak
0.1022516014	the chair
0.1022493866	the separation
0.1022489990	most prominent
0.1022438958	the latent space
0.1021987743	use pre trained
0.1021781039	the vector space model
0.1021599757	3 bleu
0.1021585553	level tasks
0.1021538807	powered by
0.1021317347	a map
0.1021176740	a frequency
0.1021100831	further analysis shows
0.1021017523	the chatbot
0.1020928594	act as
0.1020741380	the control
0.1020647180	to search
0.1020433797	approach leads to
0.1020395571	full of
0.1020323153	comes at
0.1020301969	baseline results on
0.1019941813	acquired by
0.1019924446	changes over time
0.1019767900	better on
0.1019705716	the nature
0.1019447129	an external
0.1019345640	to end
0.1019200090	important role in
0.1019177335	returned by
0.1019123055	the location
0.1018833115	based baselines
0.1018752386	an input sequence
0.1018357599	more advanced
0.1018181835	a model agnostic
0.1017857267	more diverse
0.1017415169	not at
0.1017113788	overall accuracy
0.1017056915	the generated captions
0.1017040105	from raw
0.1017024628	architecture consists of
0.1016970901	effective framework
0.1016908675	the proposed models
0.1016704171	improvement in terms
0.1016393094	induced from
0.1015756198	small amount of data
0.1015572764	a question
0.1015372424	crucial for
0.1015226019	to end framework
0.1014918235	a context free
0.1014538939	of interest
0.1014305406	the use of machine
0.1014219825	measured by
0.1014189543	the document level
0.1014144042	the answer selection
0.1014046383	authored by
0.1014037402	method for training
0.1013909845	the co
0.1013808744	the reduced
0.1013676304	a cluster
0.1013395518	for computing
0.1013324278	non zero
0.1013304110	by utilizing
0.1013215069	a semantic parser
0.1013080794	a small subset
0.1013041707	the old
0.1012931127	a score
0.1012910533	the utility
0.1012833621	important step towards
0.1012769165	special case of
0.1012718431	ranked list of
0.1012659071	the sequence
0.1012635897	the platform
0.1012429694	attention networks for
0.1012328774	representations obtained
0.1012008368	for instance
0.1011799194	the self attention
0.1011773127	a diverse range of
0.1011624606	the feature based
0.1011519788	performance on multiple
0.1011421123	the definition
0.1011368778	2019 shared
0.1011283307	previous studies on
0.1011233322	the pre training
0.1011025217	the art asr
0.1010694504	a named entity
0.1010685929	number of experiments
0.1010581417	automatic summarization of
0.1010539436	in depth
0.1010539115	the japanese
0.1010294808	the latent variables
0.1010229884	the speech recognition
0.1010190186	to recognize
0.1010108174	gathered from
0.1010065611	a specific domain
0.1010024797	a sentence level
0.1010008894	not readily
0.1009918801	to sample
0.1009889853	the visual question answering
0.1009579527	chinese speech
0.1009428596	a distinct
0.1009379992	recognition applications
0.1009241435	combinations of
0.1008985800	a multi class
0.1008829616	and higher
0.1008662914	the big
0.1008662726	encourage further
0.1008554391	almost as
0.1008417529	question answering with
0.1008391954	and so
0.1008197864	for code switched
0.1008070254	no explicit
0.1008049918	sequence attention
0.1007372449	implications for
0.1007286766	the autoregressive
0.1007279699	achieve more
0.1006985139	a reduced
0.1006945739	while remaining
0.1006567953	the text classification
0.1006390894	a relevant
0.1006253778	the trade offs
0.1006232658	effective way
0.1005786549	competitive with state
0.1005655836	area of natural
0.1005350601	systems rely on
0.1005343464	correlate with
0.1005263062	the middle
0.1005221566	the performance of models
0.1005163544	the long range
0.1005039742	a novel text
0.1004709026	particularly effective
0.1004527425	exposure to
0.1004503322	for code mixed
0.1004327896	test whether
0.1004236037	a substantial improvement
0.1003952978	a public dataset
0.1003863454	available from
0.1003782722	a multi dimensional
0.1002907409	to evaluate
0.1002895380	essential task
0.1002707648	the basis of
0.1002626406	the back translation
0.1002231629	and non native
0.1001929293	$ +
0.1001898513	agreement between
0.1001784504	the testing
0.1001573832	the phoneme
0.1001565211	a single neural network
0.1001525246	lower bound on
0.1001177611	representation learned
0.1001108109	variety of neural
0.1000989393	model consists
0.1000721973	approach to learning
0.1000708093	recurrent neural network for
0.1000615526	a diverse
0.1000581267	an end to end model
0.1000348247	best performing models
0.1000084499	these results suggest
0.1000004663	a non linear
0.0999798840	mismatch between training
0.0999757458	the dependency
0.0999721250	fields like
0.0999634079	exploitation of
0.0999533214	any parallel data
0.0999436508	the exposure bias
0.0999222073	the variation
0.0999146646	algorithm based on
0.0999073452	and out of domain
0.0998967042	the proposed approaches
0.0998765487	linguistic characteristics of
0.0998680961	network to generate
0.0998474336	the utterance level
0.0998368113	b \
0.0998320671	written by
0.0998216710	in multi agent
0.0998025600	case study on
0.0997825477	more compact
0.0997703934	deep neural networks with
0.0997582194	selection models
0.0997566409	for sarcasm detection
0.0997485405	a set of sentences
0.0997160768	the deep learning
0.0997031404	the whole
0.0996957490	the collection
0.0996811938	estimated from
0.0996774988	most existing approaches
0.0996556150	the convolutional neural network
0.0996304536	this paper makes
0.0996302244	to sound
0.0996213843	but do not
0.0996183194	a human
0.0996158854	particularly in
0.0995811064	adversarial training for
0.0995412648	pos tagging for
0.0995341815	a specific topic
0.0995156211	the attention based
0.0995079482	examination of
0.0994891445	to observe
0.0994844928	encoded by
0.0994631693	a rich source
0.0994622633	a machine translation
0.0994583122	the meta
0.0994248276	of news articles
0.0993969157	of eight
0.0993857317	a scenario
0.0993808236	room for
0.0993786244	neural models for
0.0993739693	a task specific
0.0993711545	some special
0.0993592497	like word2vec
0.0993555488	the task
0.0993528305	pair of words
0.0993440539	the degree
0.0993312027	for sentiment analysis
0.0993243226	seem to
0.0993064670	information present in
0.0992977199	a new sentence
0.0992780714	models based on
0.0992733633	learning representations of
0.0992591969	fixed set of
0.0992279933	the transformer based
0.0992137468	a visualization
0.0992057024	a definition
0.0991554332	an environment
0.0991510185	contained in
0.0991446181	2017 challenge
0.0991087640	existing work
0.0990724665	the length
0.0990634755	the universal
0.0990591899	those obtained
0.0990534605	the act
0.0990393353	correlated with
0.0990141751	instead on
0.0990093889	with task specific
0.0990077353	gains over
0.0989937478	both machine
0.0989872793	resource ones
0.0989758103	time frame
0.0989720001	presence of
0.0989461214	experiments on four
0.0989361160	exclusively on
0.0989319094	the enhanced
0.0989293731	on two datasets
0.0989070794	for out of vocabulary
0.0989009821	good result
0.0988957018	manually annotated with
0.0988919524	a social media
0.0988861431	a gated
0.0988842393	by jointly learning
0.0988839881	opposed to
0.0988751300	the frame level
0.0988587422	a distributional
0.0988364543	the passage
0.0988151613	a transformation
0.0987798851	collected during
0.0987719452	the english language
0.0987713705	the baseline model
0.0987693723	improvements over state of
0.0987479073	the solution
0.0987405642	for reading comprehension
0.0987310112	the use of pre
0.0987214303	an hmm
0.0987166365	the translation
0.0987161646	builds on
0.0986933963	the experimental
0.0986598361	up to +
0.0986596515	such as latent dirichlet allocation
0.0986581180	made of
0.0986566186	automatic analysis of
0.0986533599	method relies on
0.0986420230	a cost
0.0986397920	not yet
0.0986379910	to accomplish
0.0986328312	benefits from
0.0986249892	the user
0.0986144765	the model training
0.0985799174	a variation
0.0985491474	posed as
0.0985420386	need to
0.0985366761	the unlabeled
0.0985323300	to view
0.0985299036	different senses
0.0985144460	the wmt18
0.0985093779	the interactive
0.0985004981	of in domain data
0.0984952715	very effective
0.0984861785	a japanese
0.0984524680	the instance
0.0984381175	a corpus of
0.0984209314	the list
0.0984158968	the stance
0.0984065224	a software
0.0984042080	mainly focused
0.0983987375	to generate meaningful
0.0983919678	the long short term
0.0983589699	joint modeling of
0.0983448904	learning based on
0.0983431347	the adaptation
0.0983427638	the clinical domain
0.0983278661	responses based on
0.0983201805	the sample
0.0983183926	the superior
0.0982662390	to focus
0.0982563370	the extended
0.0982366755	the comparative
0.0982021948	to set
0.0981877744	the scientific
0.0981718536	a cross modal
0.0981435195	adapted to
0.0981434755	the relevance
0.0981293032	statistical word
0.0981130234	aided by
0.0981124175	framework based on
0.0981026365	a syntactic
0.0980690739	large sets of
0.0980686099	the model architecture
0.0980610774	improvement of up to
0.0980598099	the russian language
0.0980568291	to teach
0.0980477049	a second order
0.0980400160	the reported
0.0980225856	the great
0.0979887928	the dense
0.0979768302	with high accuracy
0.0979673472	a given
0.0979629489	a margin
0.0979584418	a text based
0.0979465111	learned by
0.0979389225	semantic parsing with
0.0979221046	the same model
0.0979121851	the ultimate goal
0.0979106115	for end to end
0.0979102180	to code
0.0978841271	three approaches
0.0978808604	the positive
0.0978713292	further improve
0.0978514349	the language modeling
0.0978359370	a listener
0.0978235969	models generally
0.0978142595	same time
0.0978110142	an important task
0.0978065746	full potential
0.0977810382	different views
0.0977623240	most previous studies
0.0977467839	emergence of
0.0977427132	used as
0.0977395941	three steps
0.0977386866	this paper evaluates
0.0977045007	the rating
0.0977027421	latent representation of
0.0976993206	driven methods
0.0976967849	trying to
0.0976760990	the training process
0.0976596520	approaches typically
0.0976533687	sentiment analysis of
0.0976211415	other nlp tasks
0.0976210683	near human
0.0976176432	better translation quality
0.0976162720	the conversion
0.0976093677	for abstractive summarization
0.0975913650	a first
0.0975585378	a true
0.0975437793	and more accurate
0.0975334909	a fusion
0.0975254681	a digital
0.0975135073	performance improvement over
0.0975110165	a pairwise
0.0974594962	the inside outside
0.0974587340	the promising
0.0974415959	the audio signal
0.0974153863	the target sentence
0.0973984210	addressed by
0.0973978311	the history
0.0973635839	largely due
0.0973626435	many language pairs
0.0973571166	an extension of
0.0973440539	the unified
0.0973145130	a spoken
0.0973123524	the evidence
0.0972845558	a domain specific
0.0972465610	information from
0.0972434094	reading comprehension with
0.0972357857	text to speech system
0.0972039091	sensitive to
0.0971994068	the subsequent
0.0971952467	the n best
0.0971853132	the downstream
0.0971719715	in favour
0.0971681707	existing studies on
0.0971474951	english to
0.0971205007	a personal
0.0971117273	expressed by
0.0971107404	the forward
0.0971028081	combination of word
0.0970728307	a first step
0.0970673551	conditioning on
0.0970603437	information extracted from
0.0970469825	operate on
0.0970387419	the success
0.0970375517	thus making
0.0970118748	the metric
0.0970114216	wish to
0.0970037504	lack of parallel
0.0969757109	the seq2seq
0.0969430874	different genres
0.0969416947	to automate
0.0969294219	the development of
0.0968889945	of particular interest
0.0968719678	the visual
0.0968701403	taken to
0.0968695044	the auxiliary
0.0968149581	structured representations of
0.0967315062	the answer
0.0967204381	the modality
0.0966816390	than others
0.0966641981	learning for
0.0966496187	for extractive summarization
0.0966488377	the well established
0.0966479181	distributions over
0.0966431851	based upon
0.0966266610	the word representations
0.0965877501	an average accuracy
0.0965792647	a document level
0.0965711085	the work presented
0.0965660800	this mechanism
0.0965570689	a regular
0.0965541359	the schema
0.0965482835	the ontology
0.0965443490	the sub
0.0965349227	value for
0.0965336302	any additional
0.0965306989	achieve better
0.0964650229	the successful
0.0964577717	adversarial examples for
0.0964418170	the symbolic
0.0964381175	the goal of
0.0964303352	a tree structured
0.0964229009	a framework
0.0964003806	a novel transformer
0.0963931127	the transcription
0.0963532296	a rule
0.0963193786	rely on large
0.0962867565	a source domain
0.0962800054	as latent variables
0.0962730495	evaluation metrics for
0.0962707648	the aim of
0.0962495216	the model achieves
0.0962487370	the voynich
0.0962400326	the generated responses
0.0962087580	the most popular
0.0962049637	the number of
0.0962034476	experiments on large
0.0961983931	comprehensive analysis of
0.0961962064	a knowledge
0.0961804659	the best model
0.0961610938	keep track of
0.0961567744	neural architecture for
0.0961541522	an abundance of
0.0961494615	the model parameters
0.0961478566	time points
0.0960993102	large collection of
0.0960736251	embedded into
0.0960705005	a constituent
0.0960632251	the discovered
0.0960485179	to pick
0.0960470689	the results of
0.0960276365	an instruction
0.0960196432	advantage over
0.0960144548	without considering
0.0960079965	a conventional
0.0959881302	the hearer
0.0959818638	two subtasks
0.0959771737	completely different
0.0959751055	the first step towards
0.0959709208	arbitrary number of
0.0959591522	the area
0.0959551554	qualitative analysis of
0.0959461048	representations for text
0.0959456497	only 1
0.0959411070	of speech
0.0959294219	the lack of
0.0959060992	a property
0.0958837280	consistent across
0.0958765437	a lot of attention
0.0958248579	a physical
0.0958102163	the immediate
0.0958039115	the group
0.0958018176	human judgments of
0.0957989462	responding to
0.0957972073	a standalone
0.0957903129	training data for
0.0957890278	a target speaker
0.0957529850	a key component in
0.0957382831	a set of features
0.0957254076	to prune
0.0957223400	more accurately
0.0957176090	a specific task
0.0957008536	the predictive
0.0956971083	at par
0.0956927242	the normalized
0.0956891698	a thorough
0.0956865318	existing ones
0.0956697054	the biggest
0.0956532449	of 31
0.0956436685	an important challenge
0.0956303458	important aspect of
0.0956115769	all possible
0.0956055251	the loss function
0.0956029632	two layer
0.0956019945	the long tail
0.0955947761	variety of domains
0.0955929929	nearly as
0.0955848715	curse of
0.0955745038	processed by
0.0955645012	the discriminator
0.0955587031	a difficult task
0.0955556598	submitted system
0.0955296206	line of work
0.0955226968	recent works on
0.0955119769	for multi label
0.0955074382	also show
0.0954975808	the fixed
0.0954868926	the benefits
0.0954842810	method for text
0.0954699741	a novel dataset
0.0954521140	the debate
0.0954514959	a multi step
0.0954434903	seeks to
0.0954423470	a novel end
0.0954287822	suited to
0.0954163565	evaluation framework for
0.0954100557	very common
0.0954100438	posted by
0.0953681212	method for neural
0.0953539066	compared to state of
0.0953486125	the new state
0.0953348696	the adaptive
0.0953308913	mainly focus on
0.0953254536	detailed analysis of
0.0953066557	of part of speech
0.0952995594	common representation
0.0952959971	the art natural
0.0952946168	to phoneme conversion
0.0952924886	wisdom of
0.0952899668	100 languages
0.0952760377	done for
0.0952588141	an alternative approach
0.0952501375	a significant improvement
0.0952397759	a restricted
0.0952386990	each training
0.0952128864	modeled by
0.0951919432	problem in natural
0.0951884882	help in
0.0951841567	the rapid growth
0.0951814777	significant differences in
0.0951813382	on out of domain data
0.0951784718	intend to
0.0951699325	well trained
0.0951679108	superior performance of
0.0951573832	the expression
0.0951456501	four languages
0.0951324960	the medical domain
0.0951020201	the first model
0.0950981509	still limited
0.0950950173	the task of identifying
0.0950917945	a large portion
0.0950834564	three language pairs
0.0950642858	the close
0.0950632587	methods rely on
0.0950526481	a meta learning
0.0949996983	others in
0.0949852394	a foreign
0.0949847936	a simple but effective method
0.0949826231	ready to
0.0949752963	the seed
0.0949720690	other languages
0.0949682317	a hypothesis
0.0949646705	sub networks
0.0949615043	the model trained
0.0949515318	problem of predicting
0.0949492940	the paper presents
0.0949491430	the consistency
0.0949469934	the speech signal
0.0949306995	labeled data from
0.0949246921	form of
0.0949120584	predict whether
0.0949039652	with multi level
0.0948888110	an essential task
0.0948732632	mainly because
0.0948728506	both intrinsic and extrinsic
0.0948539809	the performance of word
0.0948506803	the token level
0.0948416291	the first step
0.0948392254	oriented language
0.0948228713	put into
0.0948201297	lack of
0.0948113464	a logic
0.0947917764	measured in terms of
0.0947863891	the out of vocabulary
0.0947825953	edition of
0.0947698142	linked to
0.0947394679	the question
0.0947198485	the shared task
0.0947108215	text generated by
0.0947008437	the world
0.0946687107	the derived
0.0946645066	types of
0.0946620050	new ways
0.0946557646	the meaning of
0.0946506690	characterisation of
0.0946453723	a common
0.0946423924	a novel end to end
0.0946378378	results on benchmark
0.0946242797	a word embedding
0.0946197099	view learning
0.0946052471	analysis based on
0.0946039054	test clean and
0.0945942071	scale datasets
0.0945932083	parser trained on
0.0945853971	models suffer from
0.0945784428	a system of
0.0945709755	an experiment
0.0945461855	advances in
0.0945449088	stage approach
0.0945331451	of 11
0.0945249580	learning in order
0.0945125241	the confidence
0.0945118488	for ad hoc
0.0945067302	the personalized
0.0944708354	two separate
0.0944491430	the mixed
0.0944418607	both automatic evaluation
0.0944361897	the best published
0.0944348924	paid to
0.0944236755	to remember
0.0944157071	other users
0.0944152158	on twitter
0.0944069986	induced by
0.0943714435	a table
0.0943713243	different ways
0.0943403632	results on multiple
0.0943192034	this literature
0.0943120390	a strong
0.0943037932	to foster
0.0942916845	overview of
0.0942894924	to re
0.0942887707	models in order
0.0942863046	computed by
0.0942849861	the context
0.0942785105	the sparse
0.0942317549	the extractive
0.0942293300	the well known
0.0942292046	vulnerable to
0.0942148414	the recall
0.0942128426	the class
0.0941986077	attention mechanism into
0.0941685153	to generate adversarial
0.0941489840	text to
0.0941414858	to speak
0.0941364477	a memory augmented
0.0941354840	a handful
0.0941327953	the regular
0.0941270628	a large amount of
0.0941219463	abundance of
0.0941153807	the most frequent
0.0941116917	segmentation using
0.0941079173	a setting
0.0940919262	not seen during
0.0940841842	the validation
0.0940762816	the low resource
0.0940460619	a slot
0.0940450397	in practice
0.0940121834	variety of language
0.0940084983	a particular
0.0939636802	the paper discusses
0.0939573873	the neural models
0.0939549998	previous research on
0.0939497449	the change
0.0939464564	named entity recognition on
0.0939174146	a biomedical
0.0939049932	becoming more
0.0939041968	a pair
0.0938443576	the word frequency
0.0938418448	a discriminative model
0.0938373408	embeddings trained on
0.0938340380	likely to
0.0938317189	driven by
0.0938314446	the effects
0.0938148351	driven language
0.0938067812	to standardize
0.0938033460	a new state of
0.0937815030	initialized with
0.0937775375	to sequence to sequence
0.0937571517	the synthesis
0.0937401801	a bottom up
0.0937399741	proved to
0.0937341801	1 million
0.0937247071	the provided
0.0937177897	steps towards
0.0936823213	a meaning
0.0936569530	to re rank
0.0936548476	more appropriate
0.0936465389	a variable length
0.0936182664	a forward
0.0936181611	each year
0.0936120473	to transfer knowledge
0.0935940632	the conll 2014
0.0935907561	able to predict
0.0935884781	a small corpus
0.0935848022	overall sentiment
0.0935832438	a novel language
0.0935511687	semantic understanding of
0.0935481747	the paradigm
0.0935145778	meant to
0.0935083965	the model size
0.0935014706	to generate text
0.0934981398	a random forest
0.0934895856	to automatically generate
0.0934858208	of pre trained word
0.0934677612	the importance
0.0934645066	development of
0.0934498118	of 16
0.0934413827	the part of
0.0934370218	the agreement
0.0934349650	this shared
0.0934321370	the item
0.0934277053	an attractive
0.0934053140	plug in
0.0933938985	method for automatically
0.0933846958	a dependency tree
0.0933840265	an error analysis
0.0933655436	mechanism based on
0.0933461681	the model's
0.0933134408	collection of
0.0932785865	advantage of
0.0932730620	efficient way
0.0932707648	a pair of
0.0932614717	the salient
0.0932389447	a multi view
0.0932292046	attending to
0.0932142429	knowledge learned from
0.0931915628	the frequent
0.0931778558	for training
0.0931735327	the posterior
0.0931507855	of words
0.0931382855	language models based on
0.0931277667	a novel architecture for
0.0931258627	the semi supervised
0.0931111575	an important component
0.0931026431	consists of three
0.0930950173	the task of predicting
0.0930910692	a well
0.0930851141	the financial
0.0930584837	the shallow
0.0930536228	mainly rely
0.0930470116	many machine learning
0.0930467055	a wide range of applications
0.0930357290	often suffers
0.0930330806	data set of
0.0930302793	various types of
0.0930300073	consisted of
0.0930234932	first attempt
0.0930158228	thus enabling
0.0930039695	effective technique for
0.0929931841	to project
0.0929930633	questions based on
0.0929920493	trained language model to
0.0929853446	the native
0.0929753819	the classical
0.0929479356	the university of
0.0929363905	results based on
0.0929272245	the entropy
0.0929136332	novel training
0.0928923863	a weak
0.0928894878	the transformer translation
0.0928844564	expressed through
0.0928826705	the right
0.0928733871	properties of natural
0.0928582360	a process
0.0928541359	the property
0.0928338474	with human judgments
0.0928185170	the perception
0.0928105027	the number of training
0.0928071395	approach compared
0.0927887700	data collected from
0.0927830662	a global view
0.0927522595	both objective and subjective
0.0927384097	a book
0.0927303472	features based on
0.0927292355	the computational
0.0927225616	mainly focuses on
0.0927033788	a non autoregressive
0.0926872593	the variance
0.0926837098	same as
0.0926760668	concludes with
0.0926759769	the hybrid
0.0926692156	the transformer architecture
0.0926583712	tasks such as text
0.0926527439	the mutual
0.0926452813	to quantify
0.0926180995	sequence to sequence models with
0.0926169315	an end to end deep
0.0925962193	the perceived
0.0925890344	way for
0.0925823933	entity mentions in
0.0925806369	special type of
0.0925763262	the most probable
0.0925598926	same model
0.0924833752	an approximate
0.0924805759	the knowledge
0.0924268219	first ever
0.0924195747	a large text
0.0923864495	different areas
0.0923779304	together with
0.0923726686	beneficial for
0.0923718277	at run time
0.0923535035	the output sequence
0.0923514552	the end to end model
0.0923422843	most existing
0.0923259107	the aligned
0.0923205548	little to
0.0923151032	a synthetic
0.0923140065	the input image
0.0923102580	several orders of magnitude
0.0923015741	a known
0.0922885791	only use
0.0922574600	end to end models for
0.0922565970	understanding of natural
0.0922226141	the face
0.0922215426	the constituent
0.0922012846	the system
0.0921747205	sentence embeddings for
0.0921731503	the stylistic
0.0921680755	the domain specific
0.0921419681	allows to
0.0921315379	needs to
0.0921291200	a logistic regression
0.0921279281	the patient
0.0921146586	for part of speech tagging
0.0921129215	this work investigates
0.0921092493	the highest performance
0.0920992329	the produced
0.0920917344	competitive with
0.0920715106	ie system
0.0920410282	to succeed
0.0920252147	from high resource
0.0920219566	the unit
0.0920202531	smaller than
0.0919883275	strong baselines on
0.0919840024	uniqueness of
0.0919746662	automatic methods for
0.0919645251	a high
0.0919462452	enables researchers to
0.0919438943	to mimic human
0.0919325663	the correlation
0.0919315074	semantic properties of
0.0919303868	attempts at
0.0919214243	the issue
0.0918850969	from six
0.0918784405	existence of
0.0918743306	to assign
0.0918675532	expansion in
0.0918633915	a strong correlation
0.0918553578	the classification task
0.0918549376	obtains new
0.0918022579	better with
0.0917851996	english french and
0.0917815285	evaluation of machine
0.0917789745	encoder representations from
0.0917743375	and human evaluations
0.0917327121	the existing systems
0.0917206067	mapped to
0.0916986346	a symbolic
0.0916953939	a gold
0.0916787855	to adopt
0.0916702864	dataset consists of
0.0916686044	a key problem
0.0916682705	this direction
0.0916626582	good results
0.0916500918	from unstructured text
0.0916473450	a selection
0.0916278306	a general
0.0916136344	the test
0.0916020831	comparable to state of
0.0915992684	the identification
0.0915851141	the string
0.0915549452	attend to
0.0915461545	at http
0.0915424195	a project
0.0915355626	first phase
0.0915306505	an end to end learning
0.0915279259	end to end system
0.0915172267	text attribute
0.0915123776	a numerical
0.0914909605	the preprocessing
0.0914842810	framework for text
0.0914800440	system ranks
0.0914581763	the adapted
0.0914542077	a challenge
0.0914483440	set of word
0.0914389981	the qualitative
0.0914381175	the field of
0.0914353949	the dynamic
0.0914321370	the kernel
0.0914009867	perform better
0.0913821499	discovered by
0.0913795949	sentence based on
0.0913659176	the context information
0.0913573346	two distinct
0.0913564916	the sentiment
0.0913541586	a *
0.0913346537	the conclusion
0.0913251084	future directions for
0.0913240716	this regard
0.0913166087	signal to
0.0913159856	bias in word
0.0912992921	added to
0.0912938273	created by
0.0912887080	any external
0.0912773753	proliferation of
0.0912762032	the mixture
0.0912663601	the art sequence
0.0912593905	german english and
0.0912562896	great success in
0.0912534449	the user utterance
0.0912435802	neural network model with
0.0912254076	to pinpoint
0.0912127072	the coherence
0.0911953030	attention model for
0.0911848300	the translation task
0.0911792293	native speakers of
0.0911754569	to interact
0.0911648456	par with
0.0911632425	the multi hop
0.0911563914	different types
0.0911529333	all word
0.0911509678	the transfer learning
0.0911434183	text based on
0.0911330810	promising performance on
0.0911302882	thus allowing
0.0911287909	an inference
0.0911214559	metrics based on
0.0911149485	the performance of automatic
0.0911136538	to generate captions
0.0911121933	the syntactic and semantic
0.0911062266	to ignore
0.0911052187	a metric
0.0910934068	recent studies on
0.0910835441	the ground
0.0910518810	together to
0.0910497045	empirical results on
0.0909954146	the first time
0.0909848852	this study focuses
0.0909383152	the bag
0.0909321208	for part of speech
0.0909312916	the voice
0.0909107997	many natural language
0.0909077305	a learner
0.0908964442	consistency between
0.0908793569	the word
0.0908579297	evaluation metric for
0.0908565746	become popular
0.0908535127	from non parallel
0.0908401578	the practice
0.0908388410	the acoustic
0.0908374527	well structured
0.0908366832	acquired from
0.0908358005	a general framework
0.0908295202	art method for
0.0908037071	the convergence
0.0907995399	the constructed
0.0907973616	well performing
0.0907971197	dataset provided by
0.0907834287	the word2vec
0.0907733637	a treebank
0.0907732076	levels of
0.0907682510	an abstract
0.0907666365	the attention
0.0907604502	experimental evaluation on
0.0907334666	progress in recent
0.0907300440	each dimension
0.0907144210	this field
0.0907037859	the aim
0.0906842650	to translate
0.0906823901	the phenomenon
0.0906615148	an example
0.0906531634	the wikisql dataset
0.0906469499	the informal
0.0906357219	framework for
0.0906005305	a large annotated
0.0905973904	the theoretical
0.0905958483	system actions
0.0905942794	a distant
0.0905866365	the information
0.0905800795	to provide
0.0905532560	hidden representations of
0.0905465542	to text translation
0.0905360385	a novel neural network architecture
0.0905245963	a lexicalized
0.0905233410	the interplay between
0.0905195289	unsupervised methods for
0.0905163063	a post processing
0.0905106482	the disease
0.0904906095	a fine tuned
0.0904901063	the triple
0.0904832125	adaptation for
0.0904784975	form of text
0.0904713720	integrated with
0.0904612174	a prototype
0.0904600873	to generate fluent
0.0904186802	a differentiable
0.0903977660	also discuss
0.0903966416	the hidden
0.0903888392	the translation process
0.0903839267	the sentence length
0.0903826051	the encoder
0.0903808920	a word's
0.0903710449	learning for text
0.0903653482	the heart of
0.0903512574	system level
0.0903481090	the performance of neural
0.0903201480	extraction datasets
0.0903080868	a new state
0.0903013285	the translation model
0.0903003212	pieces of
0.0902919234	the short term
0.0902718448	the multi turn
0.0902491339	while requiring
0.0902436409	the front end
0.0902423321	a paraphrase
0.0902407974	characterization of
0.0902403319	precision at
0.0902387956	the high level
0.0902369619	the pairwise
0.0902224266	linguistic analysis of
0.0902176164	the experience
0.0902138256	the influence
0.0901955791	points over
0.0901817918	in contrast
0.0901817726	an utterance
0.0901777224	rules based on
0.0901773623	the variational
0.0901677524	avenues for
0.0901642660	the marginal
0.0901475409	the first neural
0.0901459988	a discrete
0.0901242951	obtained via
0.0901195127	the retrieved
0.0901168366	over 20
0.0901130810	the psychological
0.0901130291	the ambiguous
0.0901123524	the event
0.0901049568	from english to
0.0900973011	an exemplar
0.0900902681	the generic
0.0900630821	appear as
0.0900579876	different data sets
0.0900470689	the accuracy of
0.0900391058	attempts to
0.0900344728	the above mentioned
0.0900257809	use only
0.0900195118	a major
0.0899916083	to generate coherent
0.0899868749	model learns to
0.0899842497	different countries
0.0899701065	interest for
0.0899580064	the title
0.0899442363	the results showed
0.0899436936	any kind
0.0899405097	the setting
0.0899294219	the form of
0.0899249982	to customize
0.0899141105	the place
0.0898903234	all words
0.0898744740	a community
0.0898733831	both translation
0.0898722005	an independent
0.0898643154	on two different datasets
0.0898622364	significant role in
0.0898444121	question generation from
0.0898340563	supervised approach for
0.0898300898	current methods for
0.0898238039	proportion of
0.0898148249	the dialogue
0.0898085518	a head
0.0897957294	the modular
0.0897925340	the filter
0.0897910081	very efficient
0.0897840050	an rnn
0.0897374044	unlike most
0.0897321461	retrieved from
0.0897297642	dedicated to
0.0897292355	the technique
0.0897205524	of articulation
0.0896964638	according to different
0.0896809355	the kind
0.0896781723	communicate with
0.0896725602	models for speech
0.0896701387	translation based on
0.0896682842	the brain
0.0896647479	for future research
0.0896601745	the various
0.0896564916	the topic
0.0896397578	to bridge
0.0896351648	a semantic parsing
0.0896285358	a comment
0.0896258908	success in natural
0.0896174056	to decide
0.0896086958	especially with
0.0896050897	powerful tool for
0.0896039207	semantic representation of
0.0896027443	one hand
0.0895996587	full texts
0.0895929176	found on
0.0895831636	a novel word
0.0895801229	corrupted by
0.0895732094	a multi lingual
0.0895719049	the need
0.0895610974	the gain
0.0895557789	framework consists of
0.0895553348	the video
0.0895520351	contextual information from
0.0895512655	this work provides
0.0895429221	not covered by
0.0895356778	a matter of
0.0895253726	extensive analysis of
0.0895228444	available online
0.0895086339	works well
0.0895029843	substantial improvement in
0.0894937182	a recommendation
0.0894805685	the cross entropy
0.0894750767	an end to end neural model
0.0894742373	different sizes
0.0894707648	the efficacy of
0.0894666291	the most common
0.0894632331	the arabic language
0.0894433963	the high
0.0894230503	the entailment
0.0894189822	a single framework
0.0893832349	to name
0.0893824817	new knowledge
0.0893823891	the index
0.0893818391	this concept
0.0893603197	the scene
0.0893573381	look into
0.0893571297	the source code
0.0893476122	in advance
0.0893470535	encoded into
0.0893429594	the same amount
0.0893259378	the prior
0.0893017529	last years
0.0892919605	and potentially
0.0892902617	use of natural
0.0892750280	to remedy
0.0892707648	the extent to
0.0892707648	an improvement of
0.0892586879	the citation
0.0892473908	large corpora of
0.0892471493	applications such as machine
0.0892450301	a document
0.0892445015	\ improvement
0.0892437570	data sets for
0.0892406363	by 10
0.0892369201	the extra
0.0892345912	the probability distribution
0.0892063253	about 20
0.0891832264	any explicit
0.0891636545	tweets about
0.0891611274	the morphology
0.0891583087	a citation
0.0891459386	a classical
0.0891223988	in particular for
0.0891144752	arises from
0.0891136320	an attempt
0.0890736410	introduced by
0.0890539129	the end user
0.0890483457	shared among
0.0890296246	in favor of
0.0890106409	methods based on
0.0889798384	a mixed
0.0889531976	a new loss
0.0889515116	sensitivity to
0.0889401376	for data to text generation
0.0889195316	commonsense validation and
0.0889107125	the relatedness
0.0888961009	to convert
0.0888926718	a given query
0.0888837298	to attract
0.0888831763	the specialized
0.0888639838	an end to end approach
0.0888509980	a high dimensional
0.0888464480	such as bert
0.0888411351	a small number of
0.0888271219	a schema
0.0888263354	different kinds of
0.0888250867	methods tend to
0.0888236139	and wikitext 2
0.0888204506	two downstream tasks
0.0888083269	the truth
0.0888054950	the generated text
0.0887985704	achieved good
0.0887866622	the knowledge graph
0.0887792969	for providing
0.0887666365	the domain
0.0887664938	seen at training
0.0887525908	the precision
0.0887444571	the evolution
0.0887399022	these large
0.0887316169	the object
0.0887172057	a word error rate
0.0886921518	a continuous
0.0886901214	each document
0.0886681592	an in depth analysis
0.0886648370	to adapt
0.0886446892	different platforms
0.0886362681	the document
0.0886302004	end to end speech recognition with
0.0886201002	degrees of
0.0886132902	ranking model for
0.0886019225	results obtained by
0.0885994323	these shortcomings
0.0885975692	a task oriented
0.0885860475	each language
0.0885856993	on two large scale
0.0885828619	the most important
0.0885729236	the logical
0.0885661651	ranked first in
0.0885557646	the size of
0.0885412927	approach to text
0.0885389195	achieve good
0.0885349645	the system with
0.0885135785	evaluation of
0.0885096005	embeddings based on
0.0885089974	the obtained
0.0884768728	at risk
0.0884673884	feature engineering or
0.0884558309	most previous methods
0.0884546509	a controllable
0.0884495737	the tongue
0.0884375693	little attention
0.0884251583	a source language
0.0884080408	extraction from
0.0884035744	element of
0.0883981582	the memory
0.0883874221	an overview of
0.0883766664	different time
0.0883655436	summarization based on
0.0883642848	to sequence models
0.0883607037	various approaches
0.0883568007	a continuous vector
0.0883206857	the syntactic
0.0883186063	appeared in
0.0883179155	improvement in
0.0883156761	the interpretability
0.0883156262	performance compared with
0.0883104289	with at least
0.0883011012	correlates with
0.0883005060	significant progress in
0.0882845810	the pseudo
0.0882812944	two sub
0.0882746056	availability of
0.0882707648	the utility of
0.0882547036	detect whether
0.0882509685	to deal with
0.0882216960	the multi modal
0.0882186936	made great
0.0882097066	existing works on
0.0882021304	improvements compared to
0.0881897737	reported results on
0.0881867212	on whether
0.0881791494	this work presents
0.0881728349	to word sense disambiguation
0.0881454791	the open source
0.0881434183	language based on
0.0881311499	the output space
0.0881294219	in addition to
0.0881257633	order to learn
0.0881198869	further improvement
0.0881076362	word like
0.0881066674	the training corpus
0.0881030869	mixed data
0.0881008390	attention over
0.0880957343	such as gpt 2
0.0880896327	the purpose of
0.0880855516	dependency parsing with
0.0880629702	task 10
0.0880586629	an emergent
0.0880548983	models for automatic
0.0880455041	personalized language
0.0880435091	a discussion
0.0880337212	appearing in
0.0880241902	a conceptual
0.0880196289	mostly focused on
0.0880057105	based on pre
0.0879849909	and almost
0.0879758555	end to end speech to
0.0879721491	from clinical notes
0.0879600770	a reader
0.0879557043	a baseline model
0.0879434968	based model for
0.0879146892	computational approach to
0.0879131049	inner workings of
0.0879067045	to english machine
0.0879064532	referred to
0.0879018335	as shown by
0.0878872658	distinguished from
0.0878859811	embeddings generated by
0.0878852285	a longstanding
0.0878793569	the semantic
0.0878719678	the quality
0.0878674200	the ranking
0.0878659929	generalize across
0.0878591327	model outperforms other
0.0878489452	over 5
0.0878446127	a distributed
0.0878424300	models trained using
0.0878415341	german to
0.0878366855	the detected
0.0878225710	words based on
0.0878051161	a transition
0.0878045435	multilingual dataset for
0.0877992844	to look
0.0877934301	the utterance
0.0877921309	to manage
0.0877850982	freely available at
0.0877736770	network based on
0.0877478746	a linguistic
0.0877477434	large variety of
0.0877421963	model benefits from
0.0877375688	the contextual
0.0877206067	relate to
0.0877186551	for low resourced
0.0876769760	the results
0.0876743456	the hidden states
0.0876639219	an attribute
0.0876421143	the shared
0.0876406625	the research
0.0876398021	each turn
0.0876097953	sentences containing
0.0875893172	the advantages
0.0875780467	new dataset
0.0875728613	the reason
0.0875713243	to realize
0.0875623405	a different language
0.0875606340	a diverse set of
0.0875290190	the term
0.0875278468	a new language
0.0875111424	approach to language
0.0874743692	the recent years
0.0874703846	the encoded
0.0874561517	the phrase based
0.0874523437	analysis using
0.0874481672	participation in
0.0874460533	the authorship
0.0874381175	the process of
0.0874354225	an abstractive
0.0874330102	\ text
0.0874236661	this challenging
0.0874080028	the art approaches in
0.0873938299	trained with
0.0873883581	a closed
0.0873678799	these different
0.0873582008	layer by
0.0873573346	each utterance
0.0873511467	a constant
0.0873368541	a test set
0.0873338294	distributed representations for
0.0873316879	the trained model
0.0873221820	to cross
0.0873163231	a little
0.0872730620	previous ones
0.0872729494	building blocks of
0.0872716103	to benefit
0.0872674541	test set of
0.0872673038	the latest
0.0872578394	to penalize
0.0872457848	the semantic gap
0.0872329281	rather than on
0.0872313406	the contextual information
0.0871814064	the static
0.0871583203	the depth
0.0871407391	the semantic space
0.0871377919	executed by
0.0871267046	to sql
0.0871189501	from one language to
0.0871179599	dependency among
0.0871029366	results compared with
0.0871022741	with regard to
0.0870903570	amount of unlabeled data
0.0870829815	language model for
0.0870753377	account for
0.0870745352	to do so
0.0870657902	a hierarchical structure
0.0870642103	detection based on
0.0870629781	to influence
0.0870564916	a domain
0.0870546208	of vision and language
0.0870349861	the sentence
0.0870107083	a combination
0.0870022258	a near
0.0869947908	a human expert
0.0869894523	significantly more
0.0869713991	on multiple datasets
0.0869273249	the graph
0.0869096294	across six
0.0868944539	insights from
0.0868735029	each component
0.0868734866	scoring system
0.0868251918	model for word
0.0868216499	for achieving
0.0868166757	the 2nd
0.0868149502	the database
0.0867989991	images with
0.0867951899	a policy
0.0867934395	bleu points for
0.0867878766	a text classifier
0.0867801032	the methodology
0.0867574928	a dependency parser
0.0867432841	a new system
0.0867382870	by presenting
0.0867330863	to ground
0.0867272713	the available
0.0867145188	not trivial
0.0867071253	the competition
0.0867001662	and efficiently
0.0866797472	distilled from
0.0866662343	investigation into
0.0866631574	a patient's
0.0866582775	to improve generalization
0.0866543672	particularly for
0.0866480605	this measure
0.0866457047	a sentence to
0.0866334333	the model's ability
0.0866312378	and up to
0.0866109698	do not account
0.0865844452	the test data
0.0865787603	appears to
0.0865599256	method for automatic
0.0865537072	the attacker
0.0865349189	a neural machine
0.0865099647	a good
0.0864995208	the system for
0.0864973535	a new pre
0.0864931149	the virtual
0.0864562904	research interest
0.0864539897	substantial improvements in
0.0864497004	annotated corpus of
0.0864163169	to parallelize
0.0864031751	a customer
0.0863852455	the model for
0.0863710449	learning for language
0.0863624401	especially useful
0.0863440579	improved performance on
0.0863366668	expressed as
0.0863241660	a span
0.0863232125	the recursive
0.0863082012	the semeval 2010
0.0862763959	the broad
0.0862659570	the proper
0.0862560219	the weighted
0.0862468471	constructed by
0.0862454739	a common practice
0.0862305004	accuracy compared with
0.0862172061	the coverage
0.0862083795	a modality
0.0861952891	instead of relying
0.0861941363	same task
0.0861904025	shared task 2
0.0861862540	for hindi
0.0861861670	task of learning
0.0861861206	predicted by
0.0861743406	the latent variable
0.0861725602	performance of neural
0.0861624529	subsets of
0.0861550445	two step approach
0.0861512850	the crisis
0.0861387190	the brown corpus
0.0861299380	used on
0.0860938037	a query
0.0860885028	those sentences
0.0860775209	this tutorial
0.0860754401	of nine
0.0860741930	and co reference
0.0860716203	by augmenting
0.0860693585	a multi hop
0.0860509854	characteristics of
0.0860490381	a sentiment analysis
0.0860366299	the same text
0.0860330129	the generation process
0.0860253089	score based on
0.0860242757	the relationship
0.0860223111	accuracy on
0.0860113077	classification based on
0.0860001614	to serve
0.0859979257	a pre defined
0.0859821623	the same language
0.0859738775	key role in
0.0859700464	a title
0.0859682522	the law
0.0859565895	the survey
0.0859555774	at hand
0.0859531976	a new form
0.0859380235	the art performance of
0.0859366367	augmented with
0.0859215597	standard sequence to
0.0859149933	a bidirectional long short term
0.0859057147	the most suitable
0.0858835927	a promising approach
0.0858741261	this kind of
0.0858723204	effective method for
0.0858649452	a technical
0.0858596602	to exploit
0.0858538547	the closed
0.0858440911	better generalization
0.0858434270	the first large scale
0.0858425774	the political
0.0858175134	all subtasks
0.0858037225	less than 1
0.0858027535	for segmenting
0.0858012317	the comment
0.0857936538	jointly trained with
0.0857689468	distributional model of
0.0857532350	the level
0.0857523274	to sequence labeling
0.0857429235	a high precision
0.0857349841	poetry from
0.0857268097	the same author
0.0857260981	an important role in
0.0857168818	single best
0.0857155600	than 90
0.0857064232	the same set
0.0857059734	about events
0.0856874480	the beam
0.0856756192	promising results in
0.0856692574	the efficiency
0.0856677548	the use of language
0.0856599384	response generation with
0.0856319222	to open
0.0856231212	a dictionary based
0.0856111501	the student model
0.0856100630	the architecture
0.0856023716	lot of work
0.0855986722	without loss of
0.0855952471	as well as with
0.0855825635	the incremental
0.0855722368	each model
0.0855510808	the proposed attention
0.0855485322	the code switching
0.0855304162	the audio visual
0.0855298198	the previous state
0.0855150718	the computational power
0.0855012968	two orders of magnitude
0.0854890979	as part of
0.0854837263	system using
0.0854732403	the attention weights
0.0854697672	a reading
0.0854521271	based sequence to
0.0854449381	more abstract
0.0854424101	different perspectives
0.0854247000	on github
0.0853956501	this assumption
0.0853950732	systems based on
0.0853891369	generative models for
0.0853843214	tool for
0.0853805652	the proposed dataset
0.0853793569	the network
0.0853784770	previous works on
0.0853745943	integration of
0.0853731020	generation based on
0.0853714641	the main challenge
0.0853701710	six datasets
0.0853637996	to play
0.0853541980	the log
0.0853484142	further extend
0.0853473616	development of natural
0.0853365163	techniques based on
0.0853264461	a lexical
0.0853218952	to use for
0.0853161188	the second step
0.0853010105	superior to
0.0852847692	different users
0.0852763114	a downstream task
0.0852566357	a template
0.0852154483	each individual
0.0852131980	produces better
0.0851924270	one modality
0.0851650712	a language
0.0851635122	to let
0.0851075432	a difference
0.0850902214	learning representations for
0.0850772920	a high resource
0.0850758103	a preference
0.0850732824	to develop
0.0850721265	a passage
0.0850652794	an important aspect
0.0850617330	the bi
0.0850571925	solely on
0.0850427115	the low level
0.0850392951	the rule based
0.0850308700	a taxonomy
0.0850224774	an active learning
0.0850193982	a virtual
0.0850133140	language models for
0.0850094682	a post
0.0849993073	these limitations
0.0849868496	many researchers
0.0849810995	at semeval 2018
0.0849783948	using reinforcement learning
0.0849666807	to say
0.0849519774	about 10
0.0849238420	dialogue generation with
0.0849202209	the wrong
0.0849193385	the mentioned
0.0849147866	notions of
0.0849058212	the prosody
0.0848996796	the learned representations
0.0848990198	the following
0.0848987277	the semantic relationship
0.0848982952	results obtained from
0.0848971696	a regularization
0.0848889175	and directly
0.0848812918	the annotation
0.0848805346	the decision
0.0848623429	the business
0.0848601691	the 1st
0.0848492200	different lengths
0.0848475875	number of features
0.0848427725	textual descriptions of
0.0848338202	known as
0.0848262452	inability to
0.0848190050	the self attention mechanism
0.0848152877	a method
0.0848136008	a new architecture
0.0847996369	the induced
0.0847805124	for producing
0.0847802139	past few
0.0847665345	by estimating
0.0847660369	the trend
0.0847640384	the conll 2003
0.0847592971	a desired
0.0847577895	a confidence
0.0847481349	an alignment
0.0847441669	three kinds
0.0847366358	degree of
0.0847325690	representation of
0.0847295156	many tasks in natural language
0.0847216442	the previous works
0.0847183383	a letter
0.0847083795	a legal
0.0847068074	the source
0.0846866468	distributed across
0.0846812706	a logical
0.0846735539	a hand
0.0846658542	a dense
0.0846645757	overfit to
0.0846614917	also known as
0.0846507855	the dataset
0.0846472492	not all
0.0846381175	a combination of
0.0846323039	texts based on
0.0846288928	attention mechanism over
0.0846224779	as soon
0.0846118014	the aforementioned
0.0845964411	predictive power of
0.0845919871	way classification
0.0845784494	like machine translation
0.0845600863	termed as
0.0845401833	performance of text
0.0845336837	the human
0.0845309936	the line
0.0845095085	all cases
0.0844696035	the empirical
0.0844595650	the integration
0.0844553149	learning to improve
0.0844313578	essential task in
0.0844267222	the crowd
0.0844185788	the model from
0.0844175507	semantic meanings of
0.0844157830	by replacing
0.0844002515	task of automatic
0.0843962714	a new question
0.0843900413	the process
0.0843598929	the multi head
0.0843537720	the approximate
0.0843458029	rate at
0.0843426209	a deep convolutional neural
0.0843276382	fewer than
0.0843231582	the evaluation
0.0843121699	a node
0.0843096879	embeddings learned by
0.0843064146	the corpus
0.0843014820	predictions about
0.0842994457	the cnn dailymail
0.0842961355	adapt to new
0.0842871011	to sequence model
0.0842751171	the binary
0.0842716207	such as long short term
0.0842714704	to fuse
0.0842271598	in sub task
0.0842257315	and empirically
0.0842248414	world data
0.0842149256	the word sense disambiguation
0.0841871047	the employed
0.0841781803	the connection
0.0841606110	models perform better
0.0841552531	parallel corpora for
0.0841470392	both recurrent
0.0841245875	marked by
0.0841221367	best reported
0.0841105147	hidden layers of
0.0841069726	the lack of large scale
0.0840589879	the first of
0.0840221666	able to model
0.0840083018	those generated
0.0840066553	an on line
0.0840012463	the bottom
0.0839969517	for end to end asr
0.0839948614	the learner
0.0839940828	a long term
0.0839922058	system by
0.0839910664	an infinite
0.0839831490	the resulting
0.0839700333	ranked first
0.0839651321	the art end to end
0.0839583236	enables users to
0.0839578074	the observed
0.0839569045	with self supervised
0.0839528350	system provides
0.0839391499	starting with
0.0839340994	the vector space
0.0839329461	the simplified
0.0839300329	to beat
0.0839180964	the mental
0.0839162127	a reduction
0.0839138005	the second stage
0.0839128262	the most salient
0.0839045597	generalize to new
0.0838865673	to reuse
0.0838817246	computational models of
0.0838753715	a cost effective
0.0838747095	architectures based on
0.0838630644	the family
0.0838585346	a predictive
0.0838512010	to cope
0.0838477248	performance of
0.0838429150	the computational cost
0.0838400290	average improvement of
0.0838371544	evaluation methods for
0.0838361127	to facilitate future
0.0838001832	on out of domain
0.0837916529	the compression
0.0837890269	the existing models
0.0837796805	value memory
0.0837727983	the persian language
0.0837726963	a softmax
0.0837620747	many ways
0.0837472209	to better capture
0.0837368605	for text to sql
0.0837033586	a macro f1
0.0837022879	benchmark datasets for
0.0836951193	pipeline system
0.0836809695	a subject
0.0836798159	and thereby
0.0836794784	tests show
0.0836760334	and easily
0.0836508658	a variable
0.0836453251	new datasets
0.0836401300	by projecting
0.0836381175	an analysis of
0.0836180814	inference based on
0.0836027325	the capacity
0.0835974136	this notion
0.0835945881	trained on large amounts of
0.0835793546	content analysis of
0.0835700559	the equivalent
0.0835579639	annotation scheme for
0.0835555026	dependent on
0.0835546297	non linguistic
0.0835530455	the candidate
0.0835445519	languages like
0.0835370608	the metadata
0.0835283693	a systematic study
0.0835146979	the extension
0.0835136442	the relative importance
0.0835069177	a novel neural
0.0834969102	and easier
0.0834926913	none of
0.0834821871	to construct
0.0834803997	point of
0.0834794910	and visual modalities
0.0834774098	also present
0.0834720392	both supervised
0.0834718269	the vietnamese
0.0834447949	the nlp field
0.0834409838	by human experts
0.0834407919	the proposed strategy
0.0834360710	more sensitive
0.0834206379	the theme
0.0834130474	compensate for
0.0834027306	the different tasks
0.0834012104	an important feature
0.0833706133	an out of domain
0.0833703794	found by
0.0833642858	the choice
0.0833626144	by 15
0.0833471058	pursuit of
0.0833399890	to know
0.0833316014	work focuses
0.0833087809	the hierarchical
0.0833034948	as opposed
0.0832809779	the combination
0.0832757871	replaced with
0.0832736696	the board
0.0832645263	a heterogeneous
0.0832619490	to incorporate
0.0832451957	two sets
0.0832397492	3 different
0.0832353788	a word
0.0832339135	with limited labeled
0.0832195311	the range
0.0832157506	light on
0.0832111691	a simple method
0.0832067379	the training
0.0832057701	the state of art
0.0831978079	tailored to
0.0831913751	the duality
0.0831729005	determinants of
0.0831565253	a contextual
0.0831492034	by providing
0.0831426640	these findings
0.0831397089	a student
0.0831384936	the format
0.0831384294	language pairs show
0.0831359141	a premise
0.0831272713	the non
0.0831081742	to push
0.0830922324	to execute
0.0830889778	sets of
0.0830701023	transferred to
0.0830698985	the claim
0.0830698207	sharing across
0.0830540949	the way in
0.0830361238	the korean
0.0830353132	the presented
0.0830341809	the proof
0.0830309943	this trend
0.0830098680	a target domain
0.0829980606	another language
0.0829942612	communication between
0.0829919819	expressed in
0.0829893273	a low rank
0.0829893098	each corpus
0.0829852982	to connect
0.0829755708	by human judges
0.0829689295	corpus contains
0.0829578338	already trained
0.0829319196	compositional structure of
0.0829214307	a comparable
0.0828863060	stored in
0.0828832337	the training phase
0.0828828515	the semantic information
0.0828784703	a simple model
0.0828774651	to spread
0.0828550369	variety of natural
0.0828462440	sentences annotated with
0.0828436426	the algorithm
0.0828354345	the discriminative
0.0828306703	able to leverage
0.0828221944	the bag of words
0.0827793401	second one
0.0827783124	for neural machine
0.0827549467	fundamental problem in
0.0827318225	to state of
0.0827167995	sequence to sequence models for
0.0827128814	grammatical structure of
0.0827117345	to map
0.0826963371	understanding of
0.0826832853	discourse structure of
0.0826749327	by outperforming
0.0826737164	to force
0.0826564916	the content
0.0826554345	the scenario
0.0826520168	quantitative analysis of
0.0826434776	a high degree
0.0826386587	the lattice
0.0826342352	a noun
0.0826322599	the tested
0.0826316165	a double
0.0826258803	attention mechanism for
0.0826249692	the density
0.0826242226	the topical
0.0826185055	the student
0.0826034191	the conditional probability
0.0825928418	each candidate
0.0825903167	the teacher model
0.0825719049	the possible
0.0825582569	seen in
0.0825557075	with minimal
0.0825265065	the input document
0.0825058764	the precise
0.0824768450	to use in
0.0824742229	a decade
0.0824439203	the role
0.0824412170	and very
0.0824317395	stands for
0.0824271401	analysis indicates
0.0824268141	intends to
0.0824261658	across different
0.0823986738	used for text
0.0823985476	to merge
0.0823956376	stance detection on
0.0823952175	performance close to
0.0823738893	by employing
0.0823686150	the generated summaries
0.0823666338	on 13
0.0823626393	more human
0.0823594725	a non trivial
0.0823562472	built on
0.0823526347	the first phase
0.0823494708	jointly learning to
0.0823416291	used to predict
0.0823398897	the annotation process
0.0823275417	this research
0.0823168927	and consequently
0.0823075118	the best results
0.0823047641	the rapid
0.0822811275	hoping to
0.0822490970	writing system
0.0822256541	the pitch
0.0822231583	the task of extracting
0.0822150807	referring to
0.0822123428	the investigation
0.0822115123	various machine learning
0.0822109129	same word
0.0822066286	existing methods for
0.0821961206	relevant to
0.0821809779	architecture based on
0.0821774860	the basis
0.0821764852	based method for
0.0821667705	most previous works
0.0821503659	impressive performance in
0.0821305066	neural network for
0.0821052363	not only for
0.0820978534	in recent times
0.0820705674	the existing methods
0.0820689390	to compensate
0.0820605393	think of
0.0820587411	the most commonly
0.0820533635	relative improvement of
0.0820507834	research directions for
0.0820495142	for multi document
0.0820492354	a subjective
0.0820324515	but also for
0.0820254664	the expensive
0.0819978072	the main goal
0.0819890979	by up to
0.0819850482	a useful tool
0.0819776033	a picture
0.0819699415	each category
0.0819690985	neural network with
0.0819588568	to work on
0.0819569364	or just
0.0819464144	the deterministic
0.0819237062	this experiment
0.0819110829	each source
0.0819041231	the added
0.0818817587	the insight
0.0818585154	the occurrence
0.0818319020	link between
0.0818277136	the field of natural language processing
0.0818231582	the accuracy
0.0818062211	the use of deep
0.0817997543	small subset of
0.0817858048	the same as
0.0817856209	a popular approach
0.0817758868	success of neural
0.0817719792	to start
0.0817619479	information needs
0.0817613191	the masked
0.0817514648	mainly due
0.0817455146	a retrieval based
0.0817417327	by using
0.0817238309	mixture of
0.0817191534	representations of
0.0817157122	a relationship
0.0817120291	learning approach to
0.0817069555	to cluster
0.0817038409	different kinds
0.0816960583	an explicit
0.0816872004	adversarial learning for
0.0816832836	other domains
0.0816735967	hours of training
0.0816604025	faced with
0.0816588590	begun to
0.0816565613	the intention
0.0816517297	the real
0.0816516822	each sense
0.0816513718	an ablation
0.0816481002	different features
0.0816381175	a subset of
0.0816371563	the obtained results
0.0816073346	different meanings
0.0816054541	possible for
0.0816041364	of sexism
0.0815996824	lies in
0.0815991755	the adopted
0.0815522928	for paraphrase generation
0.0815507498	results comparable to
0.0815452404	operating on
0.0815277537	features related to
0.0815276862	infrastructure for
0.0814913459	a selective
0.0814840038	the typical
0.0814717993	leads to better
0.0814578074	the structural
0.0814514734	a self
0.0814246865	the library
0.0814206761	the paired
0.0814146133	the huge
0.0814125592	amounts of training
0.0813623524	the grammar
0.0813512764	the mean
0.0813426247	the boundary
0.0813271689	systems in terms
0.0813164525	understand whether
0.0813164233	neural networks with
0.0813017242	both image
0.0812917541	the usefulness
0.0812885698	of different languages
0.0812790190	the review
0.0812790190	the bias
0.0812480926	understood as
0.0812342316	utilization of
0.0812254476	the response
0.0812185885	searching for
0.0812145173	for describing
0.0812104425	a power
0.0812053521	an asymmetric
0.0811928292	to text generation with
0.0811894067	the translation quality
0.0811741795	a corresponding
0.0811733734	an important part of
0.0811694968	very short
0.0811671824	over 10
0.0811514376	the same or
0.0811421961	tasks such as
0.0811340519	to acquire
0.0811177775	sequence to sequence model with
0.0811120632	sequence to sequence model for
0.0810996591	for multi domain
0.0810947832	amount of parallel data
0.0810805347	applications such as
0.0810773036	the movement
0.0810721982	mixture model for
0.0810597024	a novel graph based
0.0810281997	a multi domain
0.0810180681	the system to
0.0810178635	a speech recognizer
0.0809887673	different roles
0.0809468015	re use
0.0809426558	the pipeline
0.0809400132	given image
0.0809366510	an extensive evaluation
0.0809337809	the audio
0.0809206896	used to generate
0.0809006529	to interpret
0.0808868803	well explored
0.0808823624	a sample
0.0808685212	a visual
0.0808447756	to achieve high
0.0808426314	with others
0.0808374966	significant increase in
0.0808347762	a hierarchical encoder
0.0808342409	other kinds
0.0808304039	data provided by
0.0808283628	two data sets
0.0808222699	benchmark dataset for
0.0808200297	very fast
0.0808125859	a discourse
0.0808111097	chinese to
0.0808044338	the system also
0.0808020365	and effectively
0.0807902841	of out of
0.0807890979	a way to
0.0807811756	word vectors to
0.0807736246	a novel model
0.0807608950	a full
0.0807524962	the most frequently
0.0807474480	but rather
0.0807459765	surge of
0.0807451755	in turn
0.0807431922	algorithms based on
0.0807372018	by adopting
0.0807361355	for selecting
0.0807249925	the model to generate
0.0807148322	to computer
0.0807125002	applications such as question
0.0807104376	become more and more
0.0807072756	able to generalize
0.0806839464	empirical evidence of
0.0806829421	knowledge base of
0.0806802237	not readily available
0.0806771868	the lowest
0.0806730881	the existing approaches
0.0806692973	a dialog
0.0806645221	aims to make
0.0806614027	a triple
0.0806544254	to respond
0.0806501595	the idea
0.0806454071	little work
0.0806273703	to tag
0.0806135818	an excellent
0.0806115576	investigation of
0.0806104931	compare against
0.0805646610	no training data
0.0805627774	two sub tasks
0.0805603859	the head
0.0805548245	further study
0.0805539671	for visual dialog
0.0805448178	a human annotated
0.0805431741	to exhibit
0.0805427356	a relation extraction
0.0805425554	for machine reading
0.0805368377	different regions
0.0805344388	the region
0.0805252314	unsupervised approach for
0.0804890224	the story
0.0804872501	in mind
0.0804840024	myriad of
0.0804779206	for sequence to sequence learning
0.0804632854	this model
0.0804501218	source to target and
0.0804441759	the stack
0.0804424910	substantially better
0.0804411573	the dictionary
0.0804395567	as good
0.0804381175	a collection of
0.0804283699	a phrase based
0.0804255630	and more than
0.0804247233	to run
0.0804235859	agree on
0.0804173174	the strict
0.0804136555	to initialize
0.0804003901	these facts
0.0803874616	characteristics such as
0.0803795720	the construction
0.0803640942	word embeddings via
0.0803597972	such as document
0.0803206857	the label
0.0803179517	the prosodic
0.0802974141	the general domain
0.0802958563	emphasis on
0.0802929927	the outcome
0.0802735040	a slight
0.0802473172	networks for language
0.0802354862	a cooperative
0.0801884118	the superiority of
0.0801869680	the prototype
0.0801856906	a simple yet
0.0801549718	a rank
0.0801535493	the late
0.0801526332	curriculum learning for
0.0801311826	results in
0.0801217451	the course of
0.0800665922	a self attention mechanism
0.0800536678	definition of
0.0800356582	the linguistic
0.0800325753	the end
0.0800263269	to increase
0.0800254664	the frame
0.0800224971	the mask
0.0800091439	three sub
0.0800072593	such as news
0.0800067454	into two
0.0800054277	this dataset
0.0799727613	both word
0.0799648086	the amount of training
0.0799354166	a key challenge
0.0799294219	a dataset of
0.0799127458	a comprehensive evaluation
0.0799004474	the previous state of
0.0798921143	the lexical
0.0798888892	a pivot
0.0798694341	a standard benchmark
0.0798621074	a rumour
0.0798437879	with limited training
0.0798282600	to propagate
0.0798210633	long time
0.0798200297	four popular
0.0797971653	an author
0.0797957779	evaluation based on
0.0797852112	the most influential
0.0797849104	an unsupervised way
0.0797816241	a reliable
0.0797810216	to write
0.0797810216	to induce
0.0797755609	a specified
0.0797740515	the bidirectional
0.0797731524	a statement
0.0797657518	a bag of words
0.0797620749	for out of
0.0797512111	as possible
0.0797393548	corpus based on
0.0797370218	the quantitative
0.0797329934	to relate
0.0797105630	an order
0.0797074706	to represent words
0.0797064117	and also to
0.0797055850	the course
0.0796935656	the surface form
0.0796904073	comprehensive survey of
0.0796864786	using n gram
0.0796813646	the auto
0.0796699632	the first method
0.0796484393	to retrieve
0.0796453169	the target sequence
0.0796423803	value pairs
0.0796311084	novel information
0.0796248621	analogous to
0.0796247071	the experiment
0.0796117583	a high performance
0.0796112136	the literature and
0.0795988427	a multi speaker
0.0795894563	by humans
0.0795850103	often lack
0.0795812665	the first pass
0.0795798352	the predicate
0.0795789206	not only to
0.0795783473	consists of two
0.0795706249	resulting in
0.0795535302	favorably to
0.0795451480	paper aims to
0.0795430047	distributional models of
0.0795315318	the estimated
0.0795281021	an extensive empirical
0.0795242088	joint training of
0.0795154703	for aligning
0.0795040995	many types
0.0794983596	as well as on
0.0794950219	a gap
0.0794738301	these tasks
0.0794717819	this bias
0.0794381175	the robustness of
0.0794296945	the readability
0.0794046576	to disambiguate
0.0793985592	the persian
0.0793837960	well above
0.0793787536	the image
0.0793756647	that training
0.0793740949	the time and
0.0793705578	of new words
0.0793689190	the target of
0.0793506378	architectures with
0.0793275877	a reinforcement
0.0793253470	only 20
0.0793193002	new state of
0.0793093307	an interesting
0.0793044175	the word order
0.0793016819	of 18
0.0792996880	the naive
0.0792996255	the extreme
0.0792843401	sequence into
0.0792698110	the problem of data
0.0792676686	to model
0.0792619490	to combine
0.0792535232	a dictionary
0.0792485687	exist between
0.0792466294	available data
0.0792317475	various aspects
0.0792272103	large enough
0.0792242511	an aspect
0.0792108667	to generalise
0.0792062727	the task of generating
0.0791999441	language models like
0.0791986404	different embedding
0.0791924509	the decision making
0.0791799543	a two step
0.0791631972	time than
0.0791474898	the word vectors
0.0791352495	sequential nature of
0.0791314253	a crisis
0.0791305093	a stack
0.0791209664	automatic recognition of
0.0791094155	to add
0.0790952800	to deploy
0.0790873312	the toolkit
0.0790872165	able to produce
0.0790863040	an unsupervised method
0.0790758183	to grow
0.0790619709	the two languages
0.0790583520	for capturing
0.0790464860	the regularization
0.0790354985	a special case
0.0790350445	the code and data
0.0790343558	methods for text
0.0790219420	method for
0.0790156033	the globe
0.0790149723	several deep learning
0.0790030229	still lack
0.0789963644	2019 task 6
0.0789950660	the fact
0.0789935039	a formalization
0.0789869552	the standard transformer
0.0789825367	by analyzing
0.0789818775	several language pairs
0.0789602325	a bridge
0.0789507855	the performance
0.0789488563	on benchmark datasets
0.0789309345	while achieving
0.0789209755	of two different
0.0789192879	by product of
0.0789128572	the interpretation
0.0789118596	to help
0.0788835477	assumed to
0.0788783198	for word sense
0.0788754416	used to model
0.0788640710	a cross
0.0788574936	other datasets
0.0788518518	four categories
0.0788503698	the gaussian
0.0788428780	the measure
0.0788328302	particularly important
0.0788228483	the academic
0.0788215206	the data sparsity
0.0788194136	the spectral
0.0788167767	a shared task
0.0788127152	some others
0.0788109247	co occur in
0.0788036146	a basis
0.0787929092	a series
0.0787923036	more human like
0.0787858283	the cause
0.0787850070	structure of
0.0787826238	a pseudo
0.0787797202	the robot
0.0787642383	the human effort
0.0787177296	the most appropriate
0.0787145891	as well as human
0.0787060103	the terminology
0.0786969312	methods for
0.0786891499	fails to
0.0786888930	a product
0.0786763027	to circumvent
0.0786625078	in contrast to
0.0786592524	a well defined
0.0786558195	systematic study of
0.0786394918	the discrimination
0.0786381175	the case of
0.0786376837	draws on
0.0786336075	an n
0.0786313029	a string
0.0786262990	the literal
0.0786188225	the power
0.0786186028	the least
0.0786094040	performance of models
0.0785980421	of noun phrases
0.0785935494	as if
0.0785857089	encoder decoder with
0.0785814731	for on device
0.0785795580	the input data
0.0785794769	to extract relations
0.0785657113	equivalent to
0.0785598451	for goal oriented
0.0785561280	bleu points in
0.0785392042	the normal
0.0785313753	the margin
0.0785289048	the difference
0.0785219529	a proof of concept
0.0785103873	a database
0.0785043146	implementation of
0.0785009581	the risk
0.0784949951	method leads to
0.0784948124	high performance in
0.0784901183	differences across
0.0784886963	enough for
0.0784851696	similarity among
0.0784822786	unsupervised method for
0.0784615560	to categorize
0.0784605127	at semeval 2019
0.0784509475	such as lstm
0.0784350175	a sequence to sequence
0.0784136344	the method
0.0784112456	the affective
0.0784064647	the current study
0.0784036885	the coreference
0.0784001433	joint extraction of
0.0783689190	the word in
0.0783625099	to offer
0.0783594673	the end task
0.0783583790	a resource rich
0.0783534071	encouraged to
0.0783478974	function based on
0.0783424897	majority of
0.0783300983	this goal
0.0783298966	based framework for
0.0783266388	the view
0.0783180283	model achieves better
0.0783169295	a trainable
0.0783101835	the social network
0.0783098503	information flow in
0.0783002348	simple approach for
0.0782948626	assessed by
0.0782887987	a spoken language
0.0782862126	both dependency
0.0782846685	an input text
0.0782803984	the pronunciation
0.0782712799	the best performance
0.0782606054	the largest publicly available
0.0782483262	better with human
0.0782346156	method consists of
0.0782317475	also release
0.0782271500	generates more
0.0782157830	an intuitive
0.0782029601	work on neural
0.0781927347	discuss possible
0.0781826781	interest of
0.0781810289	a variety of language
0.0781751707	these semantic
0.0781685124	discovery of
0.0781678805	a text document
0.0781665960	various ways
0.0781633084	the language models
0.0781628480	the physical
0.0781371696	number of word
0.0781080564	natural language processing tasks such as
0.0780956702	a novel approach
0.0780857411	the main task
0.0780839537	task based on
0.0780822545	relates to
0.0780763535	the high quality
0.0780713579	several ways
0.0780682508	quantity of
0.0780555026	essential for
0.0780532149	model consists of
0.0780399799	defined over
0.0780178497	for sub task
0.0780149277	an unprecedented
0.0780142621	the intrinsic
0.0780117512	of twitter users
0.0780000648	new tool
0.0779951051	the dominant approach
0.0779826964	strategies such as
0.0779802281	as far
0.0779731082	a possible
0.0779675065	the desired
0.0779655181	a gradient
0.0779629728	more detailed
0.0779476609	each class
0.0779313780	the mismatch
0.0779187813	language models with
0.0779187037	the trained models
0.0778991055	this theory
0.0778961090	the sentiment polarity
0.0778942247	pay more
0.0778913366	new domain
0.0778850702	the clean
0.0778838353	constrained by
0.0778749692	the numerical
0.0778670159	a model trained
0.0778665567	naturalness of
0.0778602429	the baseline models
0.0778550214	this phenomenon
0.0778432841	a novel system
0.0778403002	a second language
0.0778181562	obtained through
0.0778173827	to distinguish
0.0778089994	competitive with state of
0.0778040306	information related to
0.0777874040	a high accuracy
0.0777849104	an efficient way
0.0777818603	for commonsense reasoning
0.0777763269	this challenge
0.0777695095	growing interest in
0.0777688014	only about
0.0777668618	to bootstrap
0.0777635797	to explicitly model
0.0777609029	more interpretable
0.0777573261	the strongest
0.0777560002	a minimum
0.0777528538	a stacked
0.0776851014	corpus consists of
0.0776809779	the cost
0.0776722734	consistent with
0.0776680713	a plausible
0.0776676735	directly into
0.0776616839	the exponential
0.0776616614	a robot
0.0776458774	topic modeling with
0.0776426640	by integrating
0.0776399806	predictability of
0.0776359878	the task of semantic
0.0776272074	also demonstrate
0.0776016860	to transform
0.0775997989	sentiment polarity of
0.0775822087	as well as for
0.0775770107	to choose
0.0775739464	the usefulness of
0.0775709874	a detailed analysis
0.0775703630	three real world
0.0775623141	more general
0.0775592329	significantly outperforms several
0.0775574227	to ground truth
0.0775570118	particularly with
0.0775393437	the syllable
0.0775286130	the thesis
0.0775265278	the stochastic
0.0775190018	model performance on
0.0775092688	based models for
0.0775073085	converse with
0.0775032204	to integrate
0.0774885730	on detecting
0.0774810916	does not need
0.0774781579	questions related to
0.0774768040	the same semantic
0.0774759108	two well known
0.0774757632	paragraphs from
0.0774687799	this class
0.0774673307	the joint
0.0774634060	a new method
0.0774587368	more accurate than
0.0774526887	hard to
0.0774491137	each language pair
0.0774388205	these representations
0.0774283398	to target language
0.0774259476	for obtaining
0.0774230369	a transformer model
0.0774136262	a message
0.0774131295	and lower
0.0774004338	illustrated by
0.0773888359	variant of
0.0773876431	a historical
0.0773784280	the new approach
0.0773740385	built in
0.0773697884	the n
0.0773697884	the example
0.0773689190	the corpus of
0.0773662912	interpreted by
0.0773602444	experiments on two
0.0773588424	bias in
0.0773488607	limited number of
0.0773396421	tries to
0.0773312338	favorably with
0.0773193227	via multi task
0.0773059672	the module
0.0772970779	a variational
0.0772930360	the squad dataset
0.0772914784	to utilize
0.0772631458	by enforcing
0.0772612448	the corpus consists
0.0772590502	current work
0.0772422223	high performance on
0.0772371719	by choosing
0.0772317475	more efficiently
0.0772297917	a secondary
0.0772026852	automatic annotation of
0.0771798544	improves over
0.0771361330	with human judgements
0.0771287205	this benchmark
0.0771272693	superior performance in
0.0771239350	the baseline methods
0.0771222642	the complete
0.0771141925	processing using
0.0770950464	a thesaurus
0.0770942888	a list
0.0770924496	a pre training
0.0770851100	search over
0.0770703217	entity discovery and
0.0770602710	5 different
0.0770424382	the topology
0.0770404568	assigned by
0.0770246554	solution to
0.0770212066	the first task
0.0770157880	a mobile
0.0770157583	very expensive
0.0769918253	the game
0.0769828327	new sentences
0.0769746970	a novel method
0.0769741755	the simultaneous
0.0769727204	very long
0.0769720692	for word alignment
0.0769542043	the neighborhood
0.0769451014	during testing
0.0769342077	the word vector
0.0769156726	exactly one
0.0769110706	want to
0.0768996652	the network architecture
0.0768963042	a significant impact
0.0768928634	training framework for
0.0768885745	over existing methods
0.0768840725	the last
0.0768595759	validated by
0.0768539614	an important research
0.0768535556	present results on
0.0768503482	a superior
0.0768254664	and manually
0.0768207941	an entailment
0.0768137452	three public datasets
0.0767982519	to facilitate research
0.0767945868	the framework
0.0767940447	such as named entity recognition
0.0767921553	of whether
0.0767851011	much more challenging
0.0767817475	by aligning
0.0767623238	a given word
0.0767589974	the objective
0.0767489909	the first such
0.0767442115	treatment of
0.0767438830	machine translation with
0.0767424869	proceedings of
0.0767380319	make predictions
0.0767273413	such as gender
0.0767222890	particular domain
0.0767120388	a pronunciation
0.0767102741	this objective
0.0767063261	learnt by
0.0766972513	perspective of
0.0766927024	an evidence
0.0766889778	distribution of
0.0766773249	the computational complexity
0.0766726503	than previous approaches
0.0766601296	struggle with
0.0766599764	main idea of
0.0766506690	ubiquity of
0.0766407616	the dataset of
0.0766331051	neural networks for
0.0766204827	a fixed
0.0766027978	the most
0.0765938039	a large scale dataset for
0.0765923803	from different sources
0.0765854343	the salience
0.0765802855	the propagation
0.0765793121	performed on
0.0765766638	the next word
0.0765710513	a document's
0.0765695469	the literature in
0.0765681240	for under resourced
0.0765446011	production system
0.0765208127	data sets from
0.0765190361	than 30
0.0765180211	encoded in
0.0765141233	the environment
0.0765087311	starts by
0.0765049505	as much as
0.0765008988	task of text
0.0764864356	the baseline
0.0764814150	the period
0.0764703010	for determining
0.0764438757	begin with
0.0764416096	the most relevant
0.0764409811	the previous
0.0764381937	key challenges in
0.0764252652	to account
0.0764185823	relevant information for
0.0764151865	to extract relevant
0.0763875575	familiar with
0.0763766380	the premise
0.0763687573	vector representations for
0.0763519096	a number
0.0763497904	scenarios such as
0.0763348670	the pretrained
0.0763342695	the task of detecting
0.0763178639	new method
0.0763093539	performances than
0.0763005609	causes of
0.0762968362	to anticipate
0.0762945467	the acquired
0.0762738984	research on
0.0762572504	full dataset
0.0762473172	networks for text
0.0762423174	the validity
0.0762384025	this gap by
0.0762183618	the way of
0.0762102514	the condition
0.0762088161	emotion recognition in
0.0761867019	not incorporate
0.0761478222	a debate
0.0761429150	the success rate
0.0761375971	these steps
0.0761346263	a single word
0.0761295831	popular approach to
0.0760993703	the novelty
0.0760942652	semantic information from
0.0760927364	reasoning with
0.0760742544	changes in
0.0760571013	of word usage
0.0760487283	test set with
0.0760470689	the output of
0.0760415186	and more attention
0.0760236851	a narrative
0.0760192042	the explainability
0.0760111789	the agent's
0.0759805129	to generate responses
0.0759786177	the nmt model
0.0759635428	the benchmark
0.0759621327	the last years
0.0759582210	the input features
0.0759465456	the speed
0.0759460333	several years
0.0759347580	sequence to
0.0759173241	the best reported
0.0759165095	the spelling
0.0758971848	a tree based
0.0758947639	the norm
0.0758683240	a component
0.0758290178	model for
0.0758290122	to shed
0.0758289206	not only in
0.0758022158	an explanation
0.0757946265	to scale
0.0757888545	to bring
0.0757817475	by examining
0.0757798189	game between
0.0757720831	of information in
0.0757491525	by 5
0.0757463339	different groups
0.0757357907	this data set
0.0757348631	the usual
0.0757320362	a lattice
0.0757195095	contributing to
0.0757068625	a logistic
0.0756987111	the art models by
0.0756921313	a billion
0.0756912761	approach consists of
0.0756884118	the order of
0.0756867302	the fluency
0.0756738826	use of language
0.0756722079	for dialogue response
0.0756678975	each module
0.0756669366	system performs
0.0756667556	application of
0.0756643437	the clause
0.0756365095	these observations
0.0756361307	the highest accuracy
0.0756312808	different pre trained
0.0756239380	the exploration
0.0756081121	language modeling for
0.0756000801	particular context
0.0755976726	to support
0.0755933564	the decoder and
0.0755872532	than 100
0.0755808901	such datasets
0.0755795950	an effective approach
0.0755658184	a multi label
0.0755656292	tasks such as named
0.0755630514	a contrastive
0.0755613229	this loss
0.0755548747	a new sequence
0.0755535194	the annotator
0.0755533326	on three real world
0.0755498372	the quality of machine
0.0755476793	recognized as
0.0755256541	the chosen
0.0755008399	the primary
0.0754686002	this method
0.0754652375	between people
0.0754632349	on two real world
0.0754391638	the organization
0.0754354828	the inferred
0.0754317607	beginning of
0.0754118217	two types of
0.0753959389	than ever
0.0753826051	the query
0.0753755309	a chatbot
0.0753614869	helpful for
0.0753596602	to leverage
0.0753454370	the wer
0.0753394962	to optimize
0.0753276213	this design
0.0753272904	a human in
0.0753162884	compared to other
0.0753155837	particular language
0.0753045500	same domain
0.0752973852	the synthesized speech
0.0752944109	help of
0.0752819658	into pre trained
0.0752793121	directly from
0.0752755578	the softmax layer
0.0752712817	accordance with
0.0752589928	accounts for
0.0752571645	this case
0.0752514101	the second
0.0752356971	the main focus
0.0752269751	the best performing
0.0752222805	a mutual
0.0752004358	future research in
0.0751923372	the number of documents
0.0751917446	volumes of
0.0751884118	the extraction of
0.0751757887	the whole sentence
0.0751431012	on english to
0.0751279904	in order to facilitate
0.0751144954	various lexical
0.0751082093	a third
0.0751025954	average accuracy of
0.0750988652	the expertise
0.0750980321	the industry
0.0750960496	two language pairs
0.0750944303	range of language
0.0750862701	a relation
0.0750847043	learns to
0.0750802794	extraction based on
0.0750648392	targeted at
0.0750575287	made with
0.0750339544	these latent
0.0750258464	to feed
0.0750191567	by visualizing
0.0750054085	dependency between
0.0750024400	the published
0.0749954725	the use of natural
0.0749933097	the representational
0.0749891317	points on
0.0749865512	a self attention
0.0749689257	the spread
0.0749628373	on raw text
0.0749573828	benchmark dataset show
0.0749419104	both semantic
0.0749292526	the robustness
0.0749245513	the scientific literature
0.0749206200	transferred from
0.0749182064	by looking at
0.0749147748	to disentangle
0.0749046576	to simulate
0.0749037880	using sequence to sequence
0.0748945453	a text to
0.0748914582	this simple
0.0748846233	the blanks
0.0748840685	per sentence
0.0748773249	the speaker
0.0748773214	self attention to
0.0748767276	the evaluation results
0.0748757451	computational models for
0.0748736243	the attention model
0.0748727214	the mode
0.0748589703	well suited to
0.0748462746	a scene
0.0748289206	but also in
0.0748143728	the semeval
0.0748120528	well as to
0.0748021918	in two different
0.0747952775	finetuned on
0.0747943110	at least one
0.0747912614	the timit
0.0747889966	a beam search
0.0747849797	accuracy of
0.0747828703	a new training
0.0747607985	to modify
0.0747559638	a commercial
0.0747471228	a new attention
0.0747468445	an error rate
0.0747330117	hidden states of
0.0747312416	the severe
0.0747231750	more likely
0.0747150106	retrieved by
0.0747031675	two real world
0.0746976681	more closely
0.0746904002	an increasing number of
0.0746826607	the zero resource
0.0746722003	with part of speech
0.0746601351	the existing works
0.0746578016	new approach
0.0746565127	a production
0.0746461683	this feature
0.0746393669	more robust against
0.0746353033	the assistant
0.0746344601	the model's performance
0.0746328025	a spectrum
0.0746320495	the execution
0.0746302119	on two public datasets
0.0746287416	the english to
0.0746281623	a given topic
0.0746135854	the lack
0.0745837671	research area in
0.0745811174	the system in
0.0745811174	and then to
0.0745406567	an ad
0.0745377508	the acoustic model
0.0745300266	the preceding
0.0745208744	the sampling
0.0745206989	a pipelined
0.0745180381	of health related
0.0744995814	and out of vocabulary
0.0744864612	self attention with
0.0744801577	comparison with other
0.0744798856	an in depth analysis of
0.0744665361	each instance
0.0744573972	a statistical machine translation
0.0744262187	incorporation of
0.0744102189	interest to
0.0744001879	in terms of word
0.0743995788	in academia
0.0743837777	and significantly
0.0743690280	the tremendous
0.0743650560	the amount of data
0.0743472061	a degree
0.0743285625	keywords from
0.0743191255	the audience
0.0743072121	to enrich
0.0742956982	each query
0.0742894952	to segment
0.0742877257	the increased
0.0742849992	and highly
0.0742793883	language interface to
0.0742685985	the maximum
0.0742677487	very difficult
0.0742672902	an individual
0.0742619111	some words
0.0742528880	the false
0.0742169350	combined with other
0.0742014727	different machine learning
0.0741922296	an existing
0.0741869001	language model with
0.0741854857	a positive
0.0741844249	the large vocabulary
0.0741785422	algorithms such as
0.0741758417	to decrease
0.0741690967	too large
0.0741622471	the fake news
0.0741492271	such as google
0.0741192393	also discussed
0.0740946768	to produce high
0.0740920922	to learn complex
0.0740810216	to promote
0.0740791626	a remarkable
0.0740788652	the preferred
0.0740747626	the opposite
0.0740652039	listen to
0.0740484740	a crowdsourcing
0.0740415152	the error rate
0.0740321981	reinforcement learning with
0.0740312659	an accuracy
0.0740256620	these approaches
0.0740245500	same sentence
0.0740217643	removed from
0.0740026526	an important problem
0.0739941224	architectures like
0.0739930407	q & a
0.0739902800	the ranked
0.0739765683	to suppress
0.0739492786	translated from
0.0739282541	to report
0.0739235100	the data distribution
0.0739218762	the application
0.0739174747	this solution
0.0739162822	on two language
0.0739152637	compiled from
0.0739141770	an autoregressive
0.0739136878	the diagnosis
0.0739033965	the ideal
0.0738941661	the backward
0.0738852227	different characteristics
0.0738742650	relationship between two
0.0738492833	this particular
0.0738240885	mainly in
0.0738112175	the projection
0.0738007186	to reconstruct
0.0737965858	primarily due to
0.0737867950	representations for
0.0737791152	a bag
0.0737706322	the creation
0.0737570038	the instruction
0.0737507478	a position
0.0737500957	to improve performance
0.0737367298	the cascade
0.0737335367	to inject
0.0737190402	based representations of
0.0737024304	these features
0.0737023187	the real time
0.0736992980	the expectation
0.0736942738	the minimum
0.0736875411	the benefit
0.0736708699	two large scale
0.0736630425	a variational autoencoder
0.0736507657	conditions under
0.0736436437	development of neural
0.0736362883	learning models for
0.0736317772	error rate of
0.0736237175	the healthcare
0.0736129297	and sometimes
0.0736113422	both context
0.0735997262	this principle
0.0735955932	out of domain test
0.0735942537	constraints into
0.0735880706	a recursive
0.0735795447	for recognizing
0.0735770244	of speech data
0.0735754674	part of speech information
0.0735718469	an approach
0.0735688995	natural language understanding in
0.0735645243	effective use of
0.0735558411	extraction aims to
0.0735463848	a seed
0.0735417918	novel neural
0.0735411475	the beginning
0.0735381429	than state of
0.0735290044	also introduce
0.0735285871	from wikipedia
0.0735274383	new insights
0.0735126064	the job
0.0735037194	the out of
0.0735032788	little or
0.0734958068	the author's
0.0734790139	truthfulness of
0.0734755936	to navigate
0.0734701502	effective technique to
0.0734636174	developed by
0.0734491045	significant improvement of
0.0734376160	the model of
0.0734351209	the inverse
0.0734278214	the multi domain
0.0734269916	speech corpus for
0.0734252156	tweets into
0.0734251094	the corpus to
0.0734175461	proposed model on
0.0734163169	to catch
0.0734130514	a simplified
0.0733924449	response selection in
0.0733896639	system development
0.0733888892	a predicate
0.0733852556	after training
0.0733739464	the influence of
0.0733725111	often use
0.0733577900	both training
0.0733465662	some interesting
0.0733425098	different parts
0.0733343961	a public
0.0733250690	evolution of
0.0733189120	a user's
0.0733150809	five languages
0.0733098486	available dataset
0.0733057580	question whether
0.0733023070	able to identify
0.0732982120	the retriever
0.0732887372	potential applications in
0.0732826338	stem from
0.0732822115	challenging due to
0.0732796826	as yet
0.0732764640	objects in
0.0732633049	used to create
0.0732416322	a crucial task
0.0732249359	2017 task 4
0.0732248767	a shift
0.0732180764	results on two
0.0732011379	a fresh
0.0731757316	the root
0.0731741946	model in terms
0.0731634168	an expression
0.0731622228	most similar
0.0731544720	the bible
0.0731525184	a training corpus
0.0731431165	any word
0.0731431142	out perform
0.0731407616	a query and
0.0731346658	the central
0.0731298906	possibility of
0.0731271961	a cnn based
0.0731269453	and accurately
0.0731184405	the classification accuracy
0.0731173122	end to end with
0.0731153678	similar to
0.0731093066	for assessing
0.0730985888	product of
0.0730968879	the world in
0.0730959398	the phonological
0.0730946814	attributes such as
0.0730903372	the predefined
0.0730843243	system achieves
0.0730821412	previously used
0.0730714057	the user's
0.0730693831	this pipeline
0.0730583121	the dimensionality
0.0730548309	or only
0.0730545615	2016 us
0.0730463475	starts with
0.0730442415	the changes in
0.0730390887	an adaptation
0.0730277628	the art in
0.0730035797	a null
0.0730007789	the pretraining
0.0729714672	first show
0.0729705072	the necessary
0.0729704189	the leaderboard
0.0729624647	introduction to
0.0729616855	previous work on
0.0729582453	the current state
0.0729578394	to stimulate
0.0729471411	but also on
0.0729466932	the derivation
0.0729408050	a child
0.0729393422	the e commerce
0.0729240191	very strong
0.0729232870	all layers
0.0729217922	towards end to end
0.0729197127	to constrain
0.0729054606	a fundamental
0.0728990108	the pandemic
0.0728968693	this problem in
0.0728932404	study aims to
0.0728925516	to strengthen
0.0728822390	the actual
0.0728782083	the parallel corpus
0.0728736996	to learn embeddings
0.0728564084	the recent advancements
0.0728491575	the original model
0.0728397552	the principle
0.0728362490	dependence on
0.0728192593	based model with
0.0728082052	these datasets
0.0727995543	to return
0.0727951826	seek to
0.0727937296	a complex network
0.0727889316	different configurations
0.0727677112	relating to
0.0727633390	each text
0.0727574579	matching between
0.0727441496	2020 task
0.0727090456	a natural way
0.0726879194	an operational
0.0726877598	a graph
0.0726858233	by minimizing
0.0726767353	the magnitude of
0.0726709449	the fake
0.0726618726	the previous methods
0.0726541051	these scores
0.0726529330	to clarify
0.0726443876	a simple but effective
0.0726397257	limited amount
0.0726209464	such errors
0.0726195997	to save
0.0726072916	to automatically discover
0.0726064765	a specific
0.0725773236	these clusters
0.0725753076	the key
0.0725728349	the above
0.0725728297	empirical evaluation on
0.0725716890	better performance
0.0725609206	formation of
0.0725571660	the factual
0.0725557646	the success of
0.0725535856	a poor
0.0725362400	in low data
0.0725342427	sides of
0.0725313081	and also
0.0725172702	f1 on
0.0725146841	several text classification
0.0725130279	the private
0.0725104075	quality compared to
0.0725019126	significant improvement on
0.0724960452	the raw text
0.0724895577	a reconstruction
0.0724835380	text generation with
0.0724827943	to cross lingual
0.0724790191	numbers of
0.0724763644	and time consuming
0.0724707093	a mean
0.0724684360	novel transformer based
0.0724673852	to remove
0.0724626052	language like
0.0724588399	rich source of
0.0724523250	to use
0.0724445623	each step
0.0724373538	a decoder
0.0724373101	term goal of
0.0724286203	to extract features
0.0724050697	designed for
0.0724039281	various downstream
0.0724001909	consistency of
0.0723987013	proxies for
0.0723921289	the sequence level
0.0723888359	extended to
0.0723799424	forms of
0.0723783506	the default
0.0723767915	machine translation between
0.0723638322	aligned with
0.0723615631	involved in
0.0723602041	art methods on
0.0723551208	effective approach for
0.0723279058	to keep
0.0723246119	2019 challenge
0.0723230648	a common representation
0.0723122989	the tensor
0.0723008177	the lack of large
0.0722565945	the top of
0.0722565369	to separate
0.0722359705	indication of
0.0722212018	ideas from
0.0722151105	test sets for
0.0722095167	a broader range of
0.0722091485	the university
0.0722082268	a novel task
0.0722052787	to suit
0.0722046913	by maximizing
0.0722010633	the task of text
0.0722001045	the generated questions
0.0721976325	the number
0.0721939721	the training procedure
0.0721914595	translation model with
0.0721908907	a key role
0.0721872143	able to discover
0.0721833482	two classes
0.0721755277	the punctuation
0.0721727199	three parts
0.0721700335	to generate natural
0.0721694327	a generic
0.0721484131	to express
0.0721355491	these differences
0.0721240758	a whole
0.0721197109	all language pairs
0.0720985774	the transformed
0.0720983197	a particular language
0.0720911644	a novel reinforcement
0.0720721202	the rise
0.0720688963	a reading comprehension
0.0720529564	proposed method uses
0.0720517369	the genre
0.0720442547	a low level
0.0720433097	the trigger
0.0720104453	for resource poor
0.0719929959	the linguistic structure
0.0719853960	dimension of
0.0719797587	further improve performance
0.0719744134	uncertainty in
0.0719637513	result in
0.0719469846	different properties
0.0719465572	the enrichment
0.0719318829	those words
0.0719297168	experimental results on five
0.0719260421	novel sentences
0.0719252476	these modules
0.0719225337	this pattern
0.0719204078	a version
0.0719195525	the generated
0.0719147748	to accommodate
0.0719075637	this shared task
0.0719071786	general enough
0.0718982126	two public datasets
0.0718981534	the most difficult
0.0718979045	the sentence and
0.0718845034	explanations for
0.0718607416	exploration of
0.0718544013	pairs such as
0.0718534409	the problem of identifying
0.0718524062	the device
0.0718515986	one way to
0.0718459108	a dialogue agent
0.0718438038	for robust speech
0.0718208924	to close
0.0718188849	practical use
0.0718081233	bert model on
0.0718068475	a fundamental task
0.0717963053	a copy
0.0717950740	facts from
0.0717932654	useful to
0.0717760220	questions into
0.0717750839	used to improve
0.0717690069	a large set
0.0717659147	recipes for
0.0717631182	approaches to
0.0717602802	this application
0.0717571195	different people
0.0717361792	a wide range of nlp
0.0717341784	test set for
0.0717237079	morphological analysis of
0.0717188133	and diverse responses
0.0717046913	by aggregating
0.0716940819	modeled with
0.0716894049	as well as to
0.0716891562	given document
0.0716846899	seen as
0.0716817726	to augment
0.0716756861	the unexpected
0.0716734741	the decoder
0.0716626213	an opinion
0.0716512928	first encodes
0.0716414350	the linguistic knowledge
0.0716389222	typology of
0.0716363931	a word by
0.0716333689	the classifier
0.0716332627	not sufficient
0.0716327078	a given document
0.0716288431	the art baselines in
0.0716244270	tuned to
0.0716171046	a criterion
0.0716069122	most popular
0.0716001642	the learnt
0.0715940898	over previously
0.0715935149	three new
0.0715884988	the presence
0.0715859281	the third
0.0715829100	to generate questions
0.0715386282	half of
0.0715367046	the subjective
0.0715317260	the system submitted
0.0715262118	failed to
0.0715203005	a movie
0.0715089420	semantic representations of
0.0715082733	to guarantee
0.0715056454	a sequence tagging
0.0715027142	to identify important
0.0715024872	words in
0.0714964750	corresponding to
0.0714946007	rise in
0.0714898815	and semantically
0.0714785525	recent success of
0.0714732059	theories of
0.0714642297	the vocabulary
0.0714637440	representations from
0.0714597639	empirical study of
0.0714586182	a mixture
0.0714549258	the characteristic
0.0714528000	the most similar
0.0714472787	the outbreak
0.0714470224	a bootstrapping
0.0714374512	an average improvement
0.0714057556	datasets show
0.0714049728	different classes
0.0714022813	both in domain and
0.0713916454	the pivot
0.0713903456	each line
0.0713901251	attention mechanism to
0.0713750981	the winner
0.0713594965	a non
0.0713562739	an extractive
0.0713489253	the principal
0.0713473464	to open domain
0.0713273660	the task of visual
0.0713249128	depicted in
0.0713214331	performed by
0.0713182653	the viability of
0.0713114146	to jointly learn
0.0713102576	the variational autoencoder
0.0713063402	for text independent
0.0713044758	a native
0.0713020109	for classifying
0.0713007352	different stages
0.0712977464	a long history
0.0712954597	interest in
0.0712926885	each dataset
0.0712869036	often suffer
0.0712800732	a question in
0.0712763118	more promising
0.0712724928	the model takes
0.0712667162	the first shared
0.0712604535	a plan
0.0712500003	a graphical
0.0712468038	the first end to end
0.0712435536	to protect
0.0712371403	into two parts
0.0712256460	in different ways
0.0712218906	an input
0.0712150460	as well as in
0.0712133710	well as
0.0712062308	the associated
0.0712024480	the author
0.0711898051	role in
0.0711617363	entities into
0.0711518545	a feature
0.0711473204	the virus
0.0711429322	keyphrases from
0.0711406607	the training dataset
0.0711307006	the official
0.0711266733	the problem of generating
0.0711168279	each other and
0.0710983892	the intuition
0.0710956347	english to german and
0.0710854175	empirical analysis of
0.0710810216	to embed
0.0710795040	an algorithm
0.0710732692	large set of
0.0710697213	pros and
0.0710571015	the child
0.0710544001	name of
0.0710534633	both sentence
0.0710464860	the discovery
0.0710464832	the sparsity
0.0710444670	to prevent
0.0710269307	thorough evaluation
0.0710173723	the art performance in
0.0710075324	a symbol
0.0710009578	the company
0.0709950008	lists of
0.0709757192	generative model for
0.0709754381	the association
0.0709652811	the context of natural
0.0709507282	the information flow
0.0709477228	as described
0.0709429572	more training data
0.0709425160	a need to
0.0709353769	de identification of
0.0709337472	sarcasm in
0.0709222408	such as user
0.0709195370	gist of
0.0709189120	a proper
0.0709166901	compared with state of
0.0709150575	both single
0.0709112677	the formula
0.0709031562	a valuable source of
0.0708967923	a valuable resource
0.0708959315	to begin
0.0708896398	classification with
0.0708883502	the possibility
0.0708860981	the dependency tree
0.0708791588	learned in
0.0708751535	to read
0.0708741530	a wide
0.0708660994	new event
0.0708639423	transcripts from
0.0708566430	the spectrum
0.0708517587	the submission
0.0708467359	compared with other
0.0708170220	the verbal
0.0708044703	for further research
0.0708018858	dependency parsing as
0.0707928990	not fully
0.0707757259	in order to increase
0.0707741923	significant amount of
0.0707698381	computer systems
0.0707630956	the system achieves
0.0707485954	bleu score of
0.0707371768	the upper
0.0707347642	the project
0.0707317516	for different languages
0.0707293720	a finite
0.0707021224	outputs from
0.0706954823	the feature extraction
0.0706820140	two directions
0.0706704784	for text categorization
0.0706645711	a document and
0.0706595761	increase in
0.0706576967	the global
0.0706471016	lost in
0.0706393924	the strength
0.0706390613	robustness in
0.0706382105	to make predictions
0.0706372812	nlg system
0.0706368196	each task
0.0706365057	not only on
0.0706170748	by extending
0.0706153760	to reason
0.0706115322	the existing literature
0.0706068524	an object
0.0705956643	best lists
0.0705888893	a broad
0.0705869494	the command
0.0705864819	drawing on
0.0705690898	by domain experts
0.0705656366	the code
0.0705600290	of speech in
0.0705382333	the neural model
0.0705380004	two machine learning
0.0705271041	a classification task
0.0705208292	a precise
0.0705093403	mainly for
0.0705060686	the encoder to
0.0704940885	the layout
0.0704933494	to link
0.0704849794	the art methods on
0.0704764834	topic models with
0.0704764264	system without
0.0704726485	this kind
0.0704709807	a gaussian mixture
0.0704477687	do not always
0.0704370828	a novel knowledge
0.0704351376	efficient method to
0.0704338652	less effective
0.0704254445	proposed model uses
0.0704131695	the art results by
0.0704063778	ordering of
0.0703922088	of bias in
0.0703884118	in comparison to
0.0703883581	a window
0.0703861771	model in terms of
0.0703846891	a meeting
0.0703766994	novel end to end
0.0703633719	the proposed techniques
0.0703582753	the trade off between
0.0703337153	most important
0.0703262682	outside of
0.0703113740	a restaurant
0.0703062824	the patent
0.0703031321	well developed
0.0702793666	than previously
0.0702760856	the training stage
0.0702726436	either by
0.0702588634	events such as
0.0702587090	problems with
0.0702438426	very simple
0.0702394193	and extrinsic evaluations
0.0702310303	an english
0.0702300906	better able
0.0702185910	f score for
0.0701941800	the wake
0.0701790418	the internal
0.0701782242	experimental results on three
0.0701672669	flow of
0.0701397001	both english
0.0701326583	the tail
0.0701244486	removal of
0.0700889504	aspect of
0.0700872165	able to extract
0.0700742434	language model as
0.0700726913	increased by
0.0700637606	seen in training
0.0700613008	members of
0.0700564083	quality of
0.0700519699	the gate
0.0700470689	a method to
0.0700447923	extension of
0.0700388940	network architecture for
0.0700387164	polarity of
0.0700334398	the e2e
0.0700302706	hope to
0.0700294983	the existing state
0.0700035857	the actor
0.0699993831	this reason
0.0699918932	a chain
0.0699840636	a novel generative
0.0699740347	last few
0.0699723741	this observation
0.0699680676	to boost
0.0699674019	different granularity
0.0699673307	the common
0.0699550673	a recall
0.0699513436	description of
0.0699484678	to answer
0.0699331164	an electronic
0.0699138109	the semantic representation
0.0699132145	the task to
0.0699057058	many research
0.0699026204	systems trained on
0.0699011301	the dialogue state
0.0698968949	and fully
0.0698918537	to access
0.0698784540	the same corpus
0.0698686177	advancement in
0.0698659213	agents with
0.0698569228	some state of
0.0698452644	a system to
0.0698444705	the semantic relations
0.0698425786	the recipe
0.0698282370	conduct experiments with
0.0698234676	from transformers
0.0698151755	and consistently
0.0697872822	new entities
0.0697863573	a default
0.0697834736	trained and tested on
0.0697829617	increasing interest in
0.0697727122	response generation for
0.0697726436	or very
0.0697436198	the objective function
0.0697406529	areas such as
0.0697193597	parser for
0.0697171380	such as sentiment
0.0697126265	by human annotators
0.0697064135	able to detect
0.0697055388	and syntactically
0.0697049258	the center
0.0697019511	shortage of
0.0696906137	the linguistic features
0.0696875328	an unlabeled
0.0696861607	model on
0.0696675041	inventory of
0.0696489856	a smooth
0.0696488961	image captioning with
0.0696467645	deep neural network for
0.0696363730	two experiments
0.0696354835	different neural architectures
0.0696276759	source language to
0.0696270573	proposed method on
0.0696191720	a dialogue
0.0696174055	the quantity
0.0696107362	no direct
0.0696083726	this additional
0.0696070825	a consequence
0.0696025476	case study for
0.0695987846	an attention
0.0695942279	in order to reduce
0.0695886687	a natural language inference
0.0695869494	the anchor
0.0695777281	little research
0.0695694999	the reader
0.0695669312	argument structure of
0.0695598420	neural network model to
0.0695562967	reading time
0.0695530078	each topic
0.0695507352	more frequently
0.0695425552	useful for
0.0695423429	an objective function
0.0695361419	the timing
0.0695273630	a comprehensive analysis
0.0695264170	new type
0.0695115994	answers from
0.0694900817	amount of human
0.0694839230	evaluation method for
0.0694812378	the text in
0.0694699632	the best systems
0.0694631595	a novel dual
0.0694601688	the failure
0.0694581164	corpus of
0.0694525056	provided in
0.0694407884	n gram language
0.0694361781	the standard approach
0.0694275080	\ accuracy
0.0694258946	for speech enhancement
0.0694227087	different domain
0.0694180810	much room for
0.0694177703	a wide range of tasks
0.0694172742	important source of
0.0694050697	annotated with
0.0693803362	given aspect
0.0693641599	a summary
0.0693622878	each new
0.0693530885	six benchmark
0.0693359993	the art performance with
0.0693171113	a greedy
0.0693081784	released as
0.0692887643	referred as
0.0692839612	interest from
0.0692694891	the intensity
0.0692665015	to expand
0.0692613385	co occurrence of
0.0692594450	by manipulating
0.0692562640	to stop
0.0692470689	a method for
0.0692465898	help improve
0.0692406337	three kinds of
0.0692278432	the existing
0.0692210493	a machine
0.0692022410	the organizers
0.0691833824	two versions
0.0691831553	the past
0.0691807707	well as for
0.0691778059	the f
0.0691596015	three different
0.0691536771	various kinds of
0.0691355519	the public
0.0691337615	successful at
0.0691316557	to attack
0.0691224027	language pairs from
0.0691150649	growth in
0.0691104709	to sequence generation
0.0691087956	the problem
0.0690939664	the system performance
0.0690932606	new challenge
0.0690906120	sentences based on
0.0690828219	the referential
0.0690769038	complementary to
0.0690730807	amount of available
0.0690647837	the composition
0.0690587416	this novel
0.0690423652	near state of
0.0690423540	based on reinforcement
0.0690344310	the artificial
0.0690212875	the attention mechanisms
0.0690207428	the art methods in
0.0690194213	such as images
0.0690078600	a given sentence
0.0690060548	the workshop
0.0689941217	a new domain
0.0689889467	adapt to
0.0689622297	reduced by
0.0689615025	this mapping
0.0689356891	often requires
0.0689325819	ability of
0.0689302211	than 5
0.0689163121	large dataset of
0.0689073195	fields such as
0.0688990296	important information in
0.0688961284	release of
0.0688924131	variation of
0.0688879762	the case
0.0688825456	with human judgment
0.0688820495	the restaurant
0.0688793553	representation into
0.0688711072	widely used in natural
0.0688643929	from youtube
0.0688613437	semantic structure of
0.0688534899	the emergent
0.0688378300	a common approach
0.0688291344	come from
0.0688179435	those features
0.0688150283	equal to
0.0688053484	concatenation of
0.0688046470	the recent
0.0687969071	the use of external
0.0687954001	these results
0.0687943930	a weakly
0.0687825808	information in order to
0.0687713644	able to effectively
0.0687701617	model augmented with
0.0687655444	of out of vocabulary
0.0687618806	significant interest
0.0687547373	a static
0.0687530517	a critical role
0.0687523663	the best single
0.0687453134	the minority
0.0687249970	very close
0.0687207616	the content and
0.0687144525	the stock
0.0687066469	often limited
0.0686980168	manages to
0.0686889310	between adjacent
0.0686874142	success rate of
0.0686820123	an exponential
0.0686813570	a plethora
0.0686771290	more than one
0.0686737078	a key
0.0686734214	an integral part of
0.0686675209	specificity of
0.0686659151	generalise to
0.0686332181	a new loss function
0.0686313800	the persona
0.0686301317	models fail to
0.0686192464	comparable results with
0.0686065010	existing work on
0.0685843558	types of models
0.0685841100	to monitor
0.0685780444	the door
0.0685729636	a token
0.0685693765	a micro
0.0685594802	foundation for
0.0685538148	and possibly
0.0685439344	the original data
0.0685425786	the headline
0.0685414656	the chain
0.0685222668	to mimic
0.0685032955	the attack
0.0684997373	a specialized
0.0684938426	first build
0.0684926171	also observe
0.0684904388	selected by
0.0684849794	the art models on
0.0684805201	the man
0.0684675018	to engage
0.0684662883	difficult due
0.0684657486	the hidden state
0.0684516464	score of
0.0684515604	a special
0.0684508529	to get
0.0684467451	to think
0.0684449797	errors made
0.0684337507	many of
0.0684247805	a viable
0.0684241492	and vice
0.0684197905	few years
0.0683905694	breakthroughs in
0.0683898971	show promising results
0.0683745820	for large vocabulary
0.0683721596	a variant
0.0683701399	recall by
0.0683635843	source domain to
0.0683567116	both tasks
0.0683465069	five datasets
0.0683400568	classification accuracy of
0.0683348798	a state of
0.0683332936	the discussion
0.0683313628	to comprehend
0.0683226712	of sequence to sequence models
0.0683161631	occurrences of
0.0683142230	used to measure
0.0683101736	done to
0.0683016183	network approach to
0.0682834469	to zero
0.0682693210	the emergence
0.0682651680	derived by
0.0682496765	attributes from
0.0682417675	performance gains on
0.0682397157	for vision and language
0.0682348538	the intra
0.0682214026	a detailed
0.0681990192	the hidden layer
0.0681959546	an emotion
0.0681881611	works on
0.0681868232	the partition
0.0681822094	a controlled natural
0.0681805084	recall of
0.0681714878	able to recognize
0.0681711172	f score of
0.0681595525	a well trained
0.0681593231	system design
0.0681589879	the model in
0.0681541610	over state of
0.0681377965	new architecture
0.0681350259	realized by
0.0681274120	improvement of
0.0681078694	existing methods on
0.0681004476	the relevant
0.0680968879	the network to
0.0680774939	attempting to
0.0680760162	deep learning for
0.0680662549	a piece of text
0.0680656147	the art performance in terms of
0.0680533913	burden on
0.0680511822	not seen
0.0680358371	for text generation
0.0680245168	an nmt model
0.0680227405	the true
0.0680111214	on chinese english and
0.0680109703	become more
0.0679984448	the benchmark dataset
0.0679964508	each question
0.0679832832	calculated by
0.0679794666	an intent
0.0679764982	generated from
0.0679757352	by observing
0.0679695088	ease of
0.0679589023	time by
0.0679565280	the shortest
0.0679550090	used in different
0.0679221744	the semantic meaning
0.0679183478	each level
0.0679158977	a new class
0.0679077768	the nearest
0.0679061284	in order to improve
0.0678869028	at test
0.0678820993	these components
0.0678778889	lead to better
0.0678640190	to enforce
0.0678560782	than 20
0.0678464721	competitive performance with
0.0678427603	experimental results on four
0.0678381131	translated to
0.0678329080	to go
0.0678324852	attention models for
0.0678072121	useful information
0.0678069207	the picture
0.0678059610	used to compute
0.0678040482	the stream
0.0677918517	new evaluation
0.0677903144	the result shows
0.0677627004	extension to
0.0677497410	a time
0.0677420342	reason for
0.0677399587	annotated dataset for
0.0677327166	prevalence of
0.0677313612	information regarding
0.0677178849	on three public datasets
0.0677146324	adopted by
0.0677099856	the link prediction
0.0677069313	the part
0.0677024872	models for
0.0676978590	significant improvements for
0.0676931022	start of
0.0676868232	the fairness
0.0676700102	baseline models for
0.0676616350	different metrics
0.0676609769	comparable performance with
0.0676566430	the government
0.0676520834	a neural language
0.0676511284	point to
0.0676443589	conjunction with
0.0676440737	the semantic similarity
0.0676389747	speech recognition by
0.0676383135	the performance improvement
0.0676367440	the greedy
0.0676363504	improve performance on
0.0676354583	the other
0.0676278394	text such as
0.0676255179	the converted
0.0676150633	communicating with
0.0676143508	given query
0.0676109712	on 6
0.0676088172	the overfitting
0.0676057075	to discriminate
0.0676018238	in many natural language processing
0.0676001057	used to select
0.0675937435	methods fail to
0.0675922324	to decompose
0.0675839841	the faithfulness
0.0675744184	a well studied
0.0675676549	convenient to
0.0675649507	opinions from
0.0675573738	manual work
0.0675567959	conduct several
0.0675469242	the publication
0.0675312595	grouping of
0.0675308853	the motivation
0.0675307573	or not to
0.0675223372	the system's
0.0675168618	to judge
0.0675167788	with negligible
0.0675141551	more suitable
0.0675114657	space models of
0.0675088208	the core
0.0674922402	such annotations
0.0674772912	networks trained on
0.0674734741	a sequence
0.0674730475	each domain
0.0674677079	labeled data for
0.0674614806	to show
0.0674602053	a decrease
0.0674596844	a small dataset
0.0674465319	problems such as
0.0674159133	such as wikipedia
0.0674124233	the scope
0.0674059392	a bi
0.0674039044	other factors
0.0674019779	to render
0.0673990171	or phrases
0.0673971021	the empirical results
0.0673922602	to accelerate
0.0673910372	a trade off between
0.0673908422	the offline
0.0673763136	the training objective
0.0673762083	contextual embeddings for
0.0673750580	the listener
0.0673659708	time consuming and
0.0673645882	scarcity of
0.0673587777	existing datasets for
0.0673568409	in three different
0.0673540145	the art system
0.0673535610	an lstm
0.0673394962	to align
0.0673295509	challenges associated with
0.0673267727	best hypotheses
0.0673050149	each character
0.0672992952	pool of
0.0672916514	well as on
0.0672902709	this question
0.0672858864	show through
0.0672771786	needed to
0.0672739984	the promise
0.0672736634	a set of candidate
0.0672715832	not so
0.0672601757	memory network for
0.0672532977	each input
0.0672428335	the most accurate
0.0672340538	and text features
0.0672328403	the evolution of language
0.0672304165	the question to
0.0672196236	the information encoded
0.0672193391	drop in
0.0672151387	research work
0.0672069028	more meaningful
0.0671925446	a tensor
0.0671854581	proposed methods for
0.0671728530	the development
0.0671727070	occurrence of
0.0671720348	a shallow
0.0671691545	a macro
0.0671676244	existing approaches on
0.0671613306	the hidden layers
0.0671499499	answered by
0.0671364810	a central
0.0671280186	the baseline systems
0.0671180093	translation models with
0.0671155231	the art models in
0.0671109527	the assessment
0.0671063968	a neural sequence
0.0670929430	by injecting
0.0670903469	a supervised learning
0.0670884036	to act
0.0670844450	three ways
0.0670835037	the task of classifying
0.0670725402	further gains
0.0670664428	high accuracy on
0.0670630263	able to automatically
0.0670397409	dataset of
0.0670273857	such as topic
0.0670183797	of individual words
0.0670173723	the art results in
0.0670102391	the named entities
0.0670096242	interpretation of
0.0670060767	the sense
0.0669913630	these techniques
0.0669818155	a positive effect
0.0669745473	both language
0.0669679831	extensive experiments on three
0.0669660075	data set for
0.0669574283	the input of
0.0669550175	art approaches for
0.0669511244	the translation performance
0.0669461387	the instability
0.0669451923	a toolkit
0.0669395569	other methods
0.0669387547	methods focus on
0.0669276359	feasibility of
0.0669268437	or not
0.0669251232	data from
0.0669174796	the linguistic properties
0.0668922602	to adjust
0.0668902172	some light on
0.0668654651	another one
0.0668625402	a story
0.0668615646	acoustic modeling for
0.0668492645	a modest
0.0668427310	training method for
0.0668297048	used to identify
0.0668273505	gain from
0.0668202159	the price
0.0668196055	annotated by
0.0668165750	a regression
0.0668134270	the same image
0.0668121646	to fill in
0.0668060480	and yet
0.0668045879	compositional generalization in
0.0668032021	leads to state of
0.0667843955	the expected
0.0667835050	in many languages
0.0667691597	to unify
0.0667679707	parallel data for
0.0667667450	and less
0.0667643847	problem into
0.0667547122	several baselines
0.0667525100	to receive
0.0667471783	converted to
0.0667469096	learning to
0.0667461336	2020 shared
0.0667341816	different sources
0.0667286464	able to use
0.0667204249	documents based on
0.0667139128	implemented as
0.0667084006	a seq2seq
0.0667064076	performance in
0.0666976870	measurement of
0.0666892384	richness of
0.0666881864	amount of parallel
0.0666862604	the reasoning process
0.0666812943	translation quality of
0.0666742117	to converge
0.0666647850	the different types of
0.0666637145	the victim
0.0666606979	effort to
0.0666574098	the semeval 2016
0.0666402540	the cache
0.0666382153	the attitude
0.0666382052	the amount
0.0666381345	this joint
0.0666373816	the quantum
0.0666356252	the dynamic nature
0.0666213652	very similar
0.0666094857	effective approach to
0.0666078484	the art on
0.0665839005	large corpus of
0.0665814339	barrier to
0.0665724491	novel deep
0.0665677090	a large amount
0.0665662631	many efforts
0.0665622842	to analyze
0.0665557646	the length of
0.0665417893	a meaningful
0.0665329325	the mutual information
0.0665177228	able to outperform
0.0665162547	a classification problem
0.0665136762	the acquisition
0.0665128293	strong baselines for
0.0665068960	a self attention based
0.0664996101	also conduct
0.0664973117	each other in
0.0664965818	the resulted
0.0664915795	disparity in
0.0664913388	the source document
0.0664868824	adversarial training with
0.0664849794	the art results for
0.0664845520	attempted to
0.0664836113	type of
0.0664825751	mismatch between training and
0.0664784262	with out of
0.0664747020	the lemma
0.0664702828	the morpheme
0.0664690008	the selected
0.0664654167	the collaborative
0.0664621722	part of speech tagging for
0.0664402698	the expanded
0.0664298721	use of large
0.0664296943	wer on
0.0664281389	the dialogue context
0.0664272262	fundamental task in
0.0664229650	platform for
0.0664150693	the text to
0.0664107218	feature representations for
0.0664006650	the proposed model to
0.0663950937	an average improvement of
0.0663840685	more strongly
0.0663809361	in order to predict
0.0663799570	two benchmarks
0.0663790667	a novel mechanism
0.0663743964	assessment of
0.0663702531	a paragraph
0.0663692201	few shot text
0.0663682328	to automatically extract
0.0663668725	even without
0.0663662631	three aspects
0.0663636689	unseen during
0.0663636613	the gender bias
0.0663466523	in order to assess
0.0663326056	applied with
0.0663301461	source code of
0.0663246821	many fields
0.0663190319	hardness of
0.0663088208	the initial
0.0663052133	network for
0.0662937754	to distill
0.0662926309	a person
0.0662922996	such cases
0.0662842030	the total
0.0662806840	the worst
0.0662751679	do not scale
0.0662701179	the visual features
0.0662671938	approaches focus on
0.0662487535	the challenge
0.0662423484	tweets from
0.0662417771	the context of natural language
0.0662307423	one layer
0.0662295174	mostly due
0.0662213987	each example
0.0662058505	based system for
0.0662012846	a system
0.0661975962	using less than
0.0661924102	a vital role in
0.0661921505	the attentional
0.0661755020	conclude by
0.0661714862	this model to
0.0661679215	to deal
0.0661507539	the gradient
0.0661394961	a certain
0.0661282248	tasks such as machine
0.0661271173	to allow
0.0661265591	baseline model for
0.0661233135	the cosine
0.0661226666	syntactic information in
0.0661187439	experiences with
0.0661020017	a topic
0.0660894829	an arbitrary
0.0660874947	important problem in
0.0660841455	experimental results on two
0.0660728807	a succinct
0.0660692620	the body
0.0660672724	restrictions on
0.0660634676	constraints on
0.0660500369	matches or
0.0660296998	helps to
0.0660229273	several benchmark datasets
0.0660201399	games with
0.0660093022	f measure of
0.0660001593	make decisions
0.0659997921	portability of
0.0659813456	based methods for
0.0659808358	the classification performance
0.0659787408	promising results for
0.0659728698	hold for
0.0659690071	an appropriate
0.0659682447	the generated sentences
0.0659538992	more effectively
0.0659452108	work in
0.0659444670	to recover
0.0659416573	to condition
0.0659326237	in doing so
0.0659308801	the standard
0.0659265966	translated by
0.0659237862	best system
0.0659207041	many applications
0.0659195525	the output
0.0659180803	the n gram
0.0659173001	a normal
0.0659118065	probabilities from
0.0658956187	the workflow
0.0658947178	a bidirectional long
0.0658924591	limited by
0.0658916325	the downstream tasks
0.0658900845	performed well
0.0658866411	the enormous
0.0658585766	the first dataset
0.0658580122	effective at
0.0658536456	language model into
0.0658518220	human evaluations show
0.0658509116	from multiple languages
0.0658497468	exploited by
0.0658430550	different degrees of
0.0658428854	engage in
0.0658406327	rare words in
0.0658190409	any input
0.0658139221	the resultant
0.0658108817	the increase of
0.0658073388	the local context
0.0658047874	used to build
0.0658047874	used to extract
0.0658026099	abstractive summarization of
0.0658008693	conclude with
0.0657956875	a human like
0.0657867747	interface to
0.0657765278	and computationally
0.0657760208	a novel attention
0.0657714009	handful of
0.0657588814	the majority
0.0657521369	an active
0.0657513436	importance of
0.0657458253	the insertion
0.0657365378	aiming to
0.0657220021	these learned
0.0657214026	a unique
0.0657194282	each part
0.0657182948	the collective
0.0657048900	invariant to
0.0656942738	the intended
0.0656787490	information between
0.0656512319	understood by
0.0656430883	the latent representation
0.0656405569	the word sense
0.0656381338	while at
0.0656233441	the task of learning
0.0656206382	end to end using
0.0655945913	convolutional network for
0.0655843435	the experiment results
0.0655832556	prediction based on
0.0655803458	such as twitter
0.0655793666	by assigning
0.0655738859	the ner task
0.0655613633	already available
0.0655523181	a well known
0.0655483602	the assignment
0.0655474601	such as summarization
0.0655433533	a left to right
0.0655323772	to rate
0.0655167258	mechanism to
0.0655155319	the dependence
0.0655101474	of language evolution
0.0654956555	the new
0.0654914030	the glue
0.0654876875	a diverse set
0.0654864886	to approximate
0.0654825652	all such
0.0654811784	still suffers from
0.0654801017	data set from
0.0654754057	piece of
0.0654727132	use of
0.0654673192	a probability distribution
0.0654603013	a masked
0.0654595657	an internal
0.0654537318	the general
0.0654451160	system output
0.0654429415	opportunity for
0.0654383139	the case study
0.0654376359	result of
0.0654358941	approach for
0.0654307079	a year
0.0654293632	included in
0.0654253047	further present
0.0654250983	the best overall
0.0654243135	the situation
0.0654113328	to automatically classify
0.0653970831	this task in
0.0653958834	to differentiate
0.0653902540	the rationale
0.0653889222	minimization of
0.0653844597	does not depend on
0.0653816025	a significant
0.0653590857	learning approach for
0.0653552990	the art for
0.0653526012	a subsequent
0.0653524119	an f1
0.0653499032	much attention
0.0653379821	to find
0.0653209471	such as word2vec
0.0653198935	robustness to
0.0653013583	conducted in
0.0652920824	an encoder
0.0652898325	neural approach for
0.0652895392	without relying
0.0652864273	a great deal of
0.0652720831	a sentence in
0.0652671113	a primary
0.0652633463	2016 task
0.0652632234	assumptions on
0.0652564607	the amount of
0.0652548576	the social sciences
0.0652543874	the requirement
0.0652406714	at once
0.0652384025	different levels of
0.0652311858	to prove
0.0652304165	a word to
0.0652296263	semantic information of
0.0652291207	sampling from
0.0652197057	an associated
0.0652186024	memory networks for
0.0652111336	to communicate
0.0652109209	a shared
0.0652028064	all domains
0.0651973405	the sketch
0.0651905192	two datasets
0.0651903392	performance than
0.0651857871	the country
0.0651786773	knowledge bases such as
0.0651629370	take full
0.0651327103	an effective method
0.0651251055	the raw data
0.0651071770	the inherent
0.0651063332	the field of natural language
0.0650986471	a grammar
0.0650927901	an abstraction
0.0650909314	influence on
0.0650905460	of 75
0.0650899542	even more challenging
0.0650795215	comparison of
0.0650751673	a given image
0.0650746833	to further improve
0.0650735190	more easily
0.0650670689	two parts
0.0650657852	the world's
0.0650650868	faster training and
0.0650607208	a bottleneck
0.0650541603	the recent advances
0.0650478313	events from
0.0650450722	at \ url
0.0650375111	a program
0.0650304518	setup for
0.0650259337	not only improves
0.0650087063	three contributions
0.0649724928	policies for
0.0649707758	emotions from
0.0649563355	the synthetic
0.0649532504	this prediction
0.0649381371	to meet
0.0649329298	each mention
0.0649314785	a series of experiments
0.0649178632	the support
0.0649103102	a new technique
0.0648982466	the intersection
0.0648813354	between humans
0.0648671744	the quality of machine translation
0.0648660631	capacity of
0.0648656490	a subset
0.0648581547	competitive results for
0.0648519165	mainly due to
0.0648516577	sensitivity of
0.0648449028	unsupervised approach to
0.0648260715	a natural
0.0648095416	offensive language in
0.0647917675	new ideas
0.0647868774	the given
0.0647660297	major challenge for
0.0647497848	a guide
0.0647479974	each aspect
0.0647464804	the generator
0.0647399246	a wide variety of tasks
0.0647371049	the two main
0.0647349914	1 bleu
0.0647335367	to deliver
0.0647255936	to gather
0.0647250679	trend in
0.0647224530	the world wide
0.0647197564	the existence
0.0647168183	and then fine tune
0.0647135511	knowledge from
0.0646981920	the individual
0.0646916541	the respective
0.0646879713	better quality
0.0646772833	the decoder of
0.0646725598	of interest in
0.0646700774	the switchboard
0.0646693592	such as part of speech
0.0646639503	a user
0.0646607554	to verify
0.0646594713	the functionality
0.0646522299	to pay
0.0646499331	a trained model
0.0646386127	used with
0.0646369493	an extensive analysis
0.0646330802	inferred by
0.0646139102	documents containing
0.0646038211	the tool
0.0646001824	the number of words
0.0645948012	overall performance
0.0645887610	this framework
0.0645854265	a valuable source
0.0645785413	requires less
0.0645753076	the correct
0.0645707564	the new method
0.0645677713	a named
0.0645660400	a standard dataset
0.0645637498	the dataset to
0.0645588185	comparison with
0.0645564335	model to
0.0645493955	possible to
0.0645485115	nmt system
0.0645377600	also find
0.0645374435	the exact
0.0645365583	to weight
0.0645322047	a factor
0.0645199670	a support
0.0645141783	existing models for
0.0645094355	calculation of
0.0645066410	coupling of
0.0645005741	beneficial to
0.0644996101	by adapting
0.0644989817	self attention for
0.0644964489	word embeddings into
0.0644908911	from text corpora
0.0644893948	new dataset called
0.0644855540	or simply
0.0644836131	the point of view
0.0644818007	the dimension
0.0644731930	the entire model
0.0644713066	development of new
0.0644683248	the antecedent
0.0644653747	most relevant
0.0644502353	all baselines
0.0644388921	a well established
0.0644375865	to correct
0.0644357278	each term
0.0644340226	at different
0.0644328108	a proof
0.0644326328	the generative model
0.0644291587	the fused
0.0644251980	two other
0.0644251094	the results with
0.0644160184	the web and
0.0644026372	incompleteness of
0.0644017597	models trained in
0.0643946545	the extrinsic
0.0643935178	classifier using
0.0643915899	the emoji
0.0643913333	improvement on
0.0643868252	most existing models
0.0643814287	available to
0.0643787397	of interest from
0.0643701447	the heavy
0.0643659489	the stability
0.0643570989	method performs better
0.0643568346	the full text
0.0643550520	limitation of
0.0643550318	between objects
0.0643378775	reduction on
0.0643275037	a representative
0.0643190895	to eliminate
0.0643140190	a solid
0.0643079298	by modifying
0.0643078934	tweets with
0.0642975630	the temporal
0.0642928892	a short text
0.0642663422	the art performance for
0.0642632337	a time consuming
0.0642631986	any text
0.0642597385	the structural information
0.0642483358	the mined
0.0642470689	the ability to
0.0642434143	and out of
0.0642355496	on held out
0.0642080781	ner system
0.0642030551	the global context
0.0641966741	not observed
0.0641912448	in practical applications
0.0641908111	ratings from
0.0641889310	provides insights
0.0641878586	volume of
0.0641754081	the advent of
0.0641674190	human evaluation of
0.0641629232	performance improvements on
0.0641623729	four different
0.0641583046	system with
0.0641566240	the simplest
0.0641538024	an interaction
0.0641209769	from unlabeled text
0.0641128161	important words in
0.0641071564	learning method for
0.0640962403	the paraphrase
0.0640926402	level of
0.0640843115	the advantage
0.0640755783	to refer
0.0640470689	the ability of
0.0640422044	learning method to
0.0640416719	abilities of
0.0640367877	this way
0.0640366130	different authors
0.0640360249	new terms
0.0640286840	means to
0.0640273664	functionality of
0.0640182393	the maximum likelihood
0.0640050785	a small amount of
0.0640018822	embedding models for
0.0640001593	either manually
0.0639923935	a very low
0.0639851107	mostly on
0.0639727132	system for
0.0639713933	text generation from
0.0639706655	engage with
0.0639664964	matching network for
0.0639487896	labeled by
0.0639316577	repository of
0.0639289636	and significantly outperforms
0.0639286282	sequences into
0.0639262151	the subject and
0.0639255581	sequence to sequence model to
0.0639148599	the dialog state
0.0639135511	algorithm for
0.0638961468	range from
0.0638948743	effectiveness and efficiency of
0.0638916106	the art performances on
0.0638869962	the first steps
0.0638748354	all training
0.0638645556	trend of
0.0638622393	specification of
0.0638580133	platforms such as
0.0638542717	the shape
0.0638449119	day to
0.0638303181	a statistical language
0.0638110892	machine comprehension of
0.0638091466	the task's
0.0638050785	the degree to
0.0637981465	supervision for
0.0637716277	the goodness
0.0637672556	generate better
0.0637587615	produces more
0.0637557646	an approach to
0.0637480362	used to estimate
0.0637457353	bert model to
0.0637448684	the new task
0.0637392216	generalizes well to
0.0637154489	the time
0.0637024233	able to provide
0.0636955679	unlikely to
0.0636888136	new concepts
0.0636877159	a set
0.0636790594	top of
0.0636693425	attention heads in
0.0636641717	art results for
0.0636531430	a human evaluation
0.0636525861	parallel corpus of
0.0636515244	allow users
0.0636421492	benefit many
0.0636403338	lstm model with
0.0636373623	on ten
0.0636061495	to decode
0.0636051802	the spoken language
0.0635958981	tested with
0.0635945034	sum of
0.0635926309	the art systems on
0.0635905315	biases in
0.0635887187	tailored for
0.0635880706	a discriminator
0.0635728267	the inference process
0.0635727937	the cloud
0.0635703685	to cope with
0.0635683811	each answer
0.0635642708	a given domain
0.0635638698	well as other
0.0635606347	to reflect
0.0635557646	the design of
0.0635557646	a list of
0.0635557646	the difficulty of
0.0635554456	organization of
0.0635537686	different aspects
0.0635512527	able to answer
0.0635506726	by taking
0.0635461152	faithful to
0.0635440533	the applicability
0.0635407714	different scripts
0.0635401853	most salient
0.0635186880	two benchmark datasets
0.0635173413	a fragment
0.0635162551	on pairs of
0.0635140156	to extend
0.0635123674	the labeled data
0.0635077966	for enriching
0.0635024872	model with
0.0634976298	to pursue
0.0634882772	the generalization ability
0.0634805451	two state of
0.0634744675	survey on
0.0634675238	several state of
0.0634652325	the complicated
0.0634546415	concepts such as
0.0634523861	the seq2seq model
0.0634467062	extensive experiments with
0.0634421353	the greatest
0.0634419167	also used
0.0634389919	mode of
0.0634325183	duration of
0.0634270030	compilation of
0.0634179797	knowledge into
0.0634081998	many people
0.0633954633	results on three
0.0633922602	to compress
0.0633769393	many studies
0.0633768101	but also to
0.0633724892	2016 shared
0.0633688704	the art approaches to
0.0633672771	words within
0.0633648550	representation based on
0.0633633973	required for
0.0633629047	such as wordnet
0.0633411315	mapping from
0.0633390076	scenario with
0.0633100020	occur in
0.0633074716	partition of
0.0633065181	to associate
0.0633054957	this relationship
0.0633010364	reinforcement learning to
0.0632937754	to drive
0.0632902362	often require
0.0632865671	output of
0.0632827438	very well
0.0632816312	the results for
0.0632799030	for supporting
0.0632752325	more than one language
0.0632671941	suggestions for
0.0632634652	the 3rd
0.0632603174	the quality of generated
0.0632589150	this extended
0.0632587382	evaluation results on
0.0632520786	the data sets
0.0632444005	explosion of
0.0632434649	and social sciences
0.0632316175	to recommend
0.0632297244	the required
0.0632269347	appear in
0.0632220095	a political
0.0632115874	sequence model with
0.0631948393	only once
0.0631761112	evaluations show
0.0631747446	information through
0.0631686135	idea of
0.0631677285	papers from
0.0631593127	self training for
0.0631534533	annotated corpora for
0.0631418502	an isolated
0.0631372121	performance of state of
0.0631348262	a pre processing
0.0631194634	still far
0.0631083232	to measure
0.0631063863	for sentence classification
0.0631017334	a training set
0.0630935378	training set of
0.0630905914	three different datasets
0.0630865012	this discrepancy
0.0630817254	the graph structure
0.0630664765	to form
0.0630659198	the model as
0.0630642852	to generalize
0.0630609831	computational model of
0.0630559706	to seek
0.0630537686	by selecting
0.0630529176	cues from
0.0630523351	a surface
0.0630498350	between two words
0.0630470689	the notion of
0.0630413537	demonstrated through
0.0630407016	the superiority
0.0630322637	performs as well
0.0630221267	the benchmark datasets
0.0630145009	the returned
0.0630116212	in order to understand
0.0629985142	given question
0.0629962949	the appropriate
0.0629928725	to word order
0.0629917377	with up to
0.0629872377	5 languages
0.0629600757	an f score
0.0629486203	model to focus on
0.0629405495	easy for
0.0629373858	task of
0.0629365273	a teacher
0.0629361558	the biological
0.0629351173	a widespread
0.0629316622	for inducing
0.0629307769	learnt from
0.0629111765	the latent
0.0629054430	by separating
0.0628907249	to mention
0.0628869459	a huge
0.0628855861	the generated texts
0.0628846612	translation quality for
0.0628764887	contains more than
0.0628676632	a necessary
0.0628674188	recognition based on
0.0628662631	three popular
0.0628656181	a link
0.0628379762	the learned
0.0628363420	to carry
0.0628218814	recent works in
0.0628161426	a phenomenon
0.0628081697	used to construct
0.0628076357	one token
0.0628004863	valuable resource for
0.0627881287	these two
0.0627648781	this technique
0.0627648781	a powerful
0.0627554039	an exploratory
0.0627465898	available resources
0.0627406503	summarization with
0.0627396834	the shift
0.0627203882	an easy
0.0627166028	in detail and
0.0627082584	art on
0.0627007849	the training and test
0.0626987971	by allowing
0.0626874292	opportunity to
0.0626866488	increases with
0.0626804828	the problem of automatically
0.0626768187	a realistic
0.0626751099	on large corpora
0.0626697061	an extensive set of
0.0626442325	models used in
0.0626426741	a novel training
0.0626271918	the semantic content
0.0626051802	the sentiment classification
0.0626009466	the hypothesis
0.0626004674	the programmer
0.0625984438	competitive results in
0.0625973675	inherent to
0.0625911626	used to determine
0.0625771327	further analyses
0.0625691012	certain types of
0.0625672488	score compared to
0.0625619719	collected through
0.0625578995	different forms
0.0625463527	the use case
0.0625428524	only improves
0.0625380951	much better than
0.0625252326	to transfer
0.0625220430	rate by
0.0625003730	to limit
0.0624992207	hindi to
0.0624937453	the latent semantic
0.0624796275	a pointer
0.0624755936	to locate
0.0624743631	error analysis of
0.0624740681	does not use
0.0624648777	the training examples
0.0624644404	a tree structure
0.0624563463	to pose
0.0624499635	in line with
0.0624481403	mostly in
0.0624463492	either as
0.0624454659	the beam search
0.0624420790	especially for long
0.0624273308	contrasts with
0.0624083902	a verb
0.0624056753	improvements on
0.0624047686	work shows
0.0624026651	possibilities for
0.0623998980	important task in
0.0623885832	learning algorithm for
0.0623719490	the vision
0.0623654539	token in
0.0623652560	this problem with
0.0623605599	in order to extract
0.0623545526	a new measure
0.0623536720	more complex models
0.0623532072	and ter
0.0623398492	other sources
0.0623381058	across time
0.0623350898	provided as
0.0623072794	generalize to
0.0623034166	to learn representations
0.0623021063	progress in
0.0622904998	recent work on
0.0622901649	the source and target languages
0.0622892478	joint model for
0.0622888686	accurate than
0.0622653219	in order to produce
0.0622579735	this challenging task
0.0622522736	a new metric
0.0622499621	the simulated
0.0622328872	such as tweets
0.0622072794	expected to
0.0622017422	all aspects
0.0621971157	novel method
0.0621915785	the front
0.0621865221	to prepare
0.0621812985	encoded as
0.0621679503	challenging task as
0.0621660863	different speakers
0.0621640722	rise to
0.0621563320	step in
0.0621561970	a great
0.0621508880	three types of
0.0621485315	concepts from
0.0621448925	the reference
0.0621438712	important step in
0.0621391818	sentence representations for
0.0621351648	the unlabeled data
0.0621268187	a minimal
0.0621187045	a cross entropy
0.0621149366	by performing
0.0621133667	branch of
0.0621088090	any language
0.0621046564	models like
0.0620940463	the iemocap
0.0620804273	two new datasets
0.0620680952	messages from
0.0620667212	framework to
0.0620666704	the time of writing
0.0620649402	back to
0.0620560319	only on
0.0620554803	each entity
0.0620490786	available as
0.0620244387	these systems
0.0620168229	to stay
0.0620154042	each dialogue
0.0619994636	carried by
0.0619973079	to adversarial attacks
0.0619917108	attention mechanism with
0.0619763342	positive or
0.0619756660	these measures
0.0619512787	structuring of
0.0619486827	the manual annotation
0.0619470350	each image
0.0619415225	based approach for
0.0619372276	a vector representation
0.0619293054	as fast
0.0619287603	the rumour
0.0619285552	in order to identify
0.0619282493	start by
0.0619256660	different layers
0.0619247393	frequencies of
0.0619132924	to store
0.0619113140	yet effective method
0.0619071817	a decision
0.0619044355	shifts in
0.0619017263	confirmed by
0.0618986222	a partial
0.0618885045	a soft
0.0618853289	unique to
0.0618741048	less parameters
0.0618634855	as per
0.0618603322	faced in
0.0618285118	external knowledge to
0.0618281202	a convolutional neural
0.0618261896	still suffer from
0.0618245238	a visual question
0.0618224884	a qualitative analysis
0.0618224204	the expressive power of
0.0617984310	a joint training
0.0617976317	performances on
0.0617788667	further reduce
0.0617774860	on social networks
0.0617666338	the polish
0.0617599204	a ranked list of
0.0617525053	a speaker
0.0617331470	model using
0.0617316068	reduction in
0.0617260761	a rough
0.0617252578	the scalability
0.0617229626	any domain
0.0617222339	not work
0.0617176535	entailment between
0.0617020843	amount of training data
0.0616717863	the performance in
0.0616622147	research topic in
0.0616603322	on several benchmark
0.0616557646	the creation of
0.0616313443	the final model
0.0616238101	the conversation context
0.0616149366	to annotate
0.0616146065	for relation classification
0.0616011116	three key
0.0615922705	to imitate
0.0615860415	a pair of words
0.0615834430	the major challenges
0.0615586893	the first to
0.0615557646	the concept of
0.0615552494	the amount of information
0.0615532182	the task of answering
0.0615498751	unlike many
0.0615469215	demonstrated in
0.0615355018	justification for
0.0615341717	a subtask
0.0615272652	new features
0.0615229242	a pilot
0.0615219047	applicable to other
0.0615103165	improved performance in
0.0615022619	a supervised
0.0614920246	a given input
0.0614836113	identification of
0.0614809368	a fundamental problem
0.0614760841	different datasets
0.0614755762	often suffers from
0.0614561660	methods like
0.0614520223	the babi
0.0614473766	institute for
0.0614278267	a novel way
0.0614274605	to further
0.0614266457	study of
0.0614263361	the well
0.0614081556	to give
0.0614078151	learning methods for
0.0614064530	calls for
0.0614021936	an annotation
0.0613993756	a mix of
0.0613947659	deep understanding of
0.0613900520	a different
0.0613864415	detection of
0.0613826741	the second part
0.0613789821	a classic
0.0613744785	or otherwise
0.0613674515	choice of
0.0613652327	error analysis for
0.0613644739	change in
0.0613640591	in different contexts
0.0613382922	even more
0.0613309578	for visual question
0.0613299280	to converse
0.0613276071	the observation
0.0613256545	the problem in
0.0613078310	the completeness
0.0613015272	a word or
0.0612890296	the syntactic structure
0.0612826808	a computational approach
0.0612822110	models perform well
0.0612692177	by example
0.0612598507	a successful
0.0612548202	a binary
0.0612527440	unaware of
0.0612496101	an unknown
0.0612473807	best list
0.0612446428	the new model
0.0612394739	difficulty of
0.0612378048	to preserve
0.0612335289	topology of
0.0612309313	a new model
0.0612248100	of crossings
0.0612222362	a deep reinforcement
0.0612174658	to unseen data
0.0612173212	use of social media
0.0612167467	influence of
0.0612097573	a complementary
0.0611942440	under certain
0.0611941086	a fixed number
0.0611899933	a central role
0.0611899762	the ongoing
0.0611876689	the estimation
0.0611843481	acoustic to
0.0611800231	an article
0.0611723693	written in
0.0611596533	a lexicon
0.0611403539	the few
0.0611343068	problem of
0.0611162403	the resolution
0.0610984131	across five
0.0610928938	identification with
0.0610926052	provided to
0.0610823075	implication of
0.0610747830	learning from
0.0610704627	predict if
0.0610647202	performs on
0.0610643266	existing approaches for
0.0610596323	any other
0.0610564132	also propose
0.0610508504	no external
0.0610484527	the compositionality
0.0610384025	different parts of
0.0610329300	translation with
0.0610284277	challenging due
0.0610121316	the main contribution
0.0610062405	baseline for
0.0610059191	this work introduces
0.0610051024	the art accuracy on
0.0609892069	better interpretability
0.0609858793	well as in
0.0609848443	a hierarchical recurrent
0.0609846201	then test
0.0609758567	in language change
0.0609723397	system to generate
0.0609498751	relationships across
0.0609493353	neural model with
0.0609119625	intensity of
0.0609079466	to replicate
0.0609057404	some simple
0.0609020455	a target
0.0609014683	to study
0.0608934203	simple method for
0.0608855029	direction of
0.0608740123	to hold
0.0608708049	weaknesses of
0.0608630266	hate speech in
0.0608618770	an efficient method
0.0608526093	a medium
0.0608518645	other tasks
0.0608375769	the link
0.0608313869	developed on
0.0608258703	the task of automatic
0.0608233399	in order to capture
0.0608199942	to apply
0.0608168778	experimentally show
0.0607904112	the shortest dependency
0.0607659922	to reveal
0.0607634365	a response
0.0607600782	a novel deep learning
0.0607595098	an acoustic
0.0607579644	restricted to
0.0607437979	developed for
0.0607432164	1 best
0.0607094906	data from other
0.0607015179	of different sizes
0.0606728569	a semantic similarity
0.0606596578	able to understand
0.0606532439	a limited
0.0606484992	questions from
0.0606390297	and then
0.0606336014	review on
0.0606268040	region of
0.0606202385	by looking
0.0606193260	example of
0.0606054148	words such as
0.0606049331	for constructing
0.0606048355	domain knowledge to
0.0606021637	the most crucial
0.0606003830	a range
0.0605898417	an observed
0.0605889227	the same performance
0.0605881881	requires further
0.0605877240	and instead
0.0605763372	trained with different
0.0605682431	for zero resource
0.0605568094	to uncover
0.0605541476	implemented with
0.0605405620	to english and
0.0605384920	sentences from
0.0605126074	limited to
0.0605113411	two public
0.0605084478	on two publicly
0.0605083482	these factors
0.0605069570	the model by
0.0605062654	value of
0.0605031745	foundations of
0.0604924619	corresponding word
0.0604829014	a word sense
0.0604798544	to improve machine
0.0604791385	from reddit
0.0604754136	well as with
0.0604748858	thorough analysis
0.0604681183	a predefined
0.0604628922	the same word
0.0604608034	in many real world
0.0604388599	several machine learning
0.0604374005	the largest dataset
0.0604359770	a novel framework
0.0604135888	$ score of
0.0604044734	a new benchmark
0.0604030500	used to represent
0.0603991594	the english and
0.0603991004	the prior work
0.0603879696	the network structure
0.0603789465	track of
0.0603709968	for english and
0.0603701191	the effectiveness
0.0603643276	this property
0.0603616917	to model long
0.0603582376	other words
0.0603531780	needed for
0.0603347718	a compelling
0.0603329949	dataset for
0.0603305609	zero shot learning for
0.0603304276	the first attempt
0.0603216265	the sharing of
0.0603214654	rare or
0.0603154361	a wide range of natural
0.0603129609	a reverse
0.0602857694	a standard
0.0602836505	baselines in terms of
0.0602784724	in order to generate
0.0602648093	the duration
0.0602584861	the summary
0.0602547122	most current
0.0602474591	the problem of learning
0.0602470689	the potential of
0.0602248494	need for more
0.0602243025	learned with
0.0602220095	the music
0.0602209904	context information in
0.0602158011	required by
0.0602136730	design and implementation of
0.0602134155	library for
0.0602082461	advocate for
0.0601980716	the oracle
0.0601937763	questions like
0.0601906603	relatively high
0.0601891406	less sensitive to
0.0601756010	natural language into
0.0601653114	a speaker's
0.0601630248	information during
0.0601612489	for developing
0.0601597418	a toy
0.0601570730	held in
0.0601550692	an extension to
0.0601537790	by clustering
0.0601433769	combined into
0.0601343068	research in
0.0601174406	the downstream task
0.0601168033	central to
0.0600990271	generation with
0.0600861954	method to
0.0600837112	deployed to
0.0600764467	the light
0.0600610803	the market
0.0600513763	second best
0.0600503794	a popular
0.0600470689	the effects of
0.0600423202	modalities such as
0.0600389409	the predicted
0.0600291598	the optimal
0.0600278620	propose to
0.0600223820	to complement
0.0600127541	the information in
0.0600123541	by showing
0.0600064434	the way
0.0600029974	the full
0.0599969153	downstream task of
0.0599942399	a way
0.0599885983	information in
0.0599854361	parallel corpus for
0.0599816594	a key role in
0.0599813642	a claim
0.0599794687	and so on
0.0599649227	method uses
0.0599547935	the tendency
0.0599526381	a situation
0.0599501396	the superior performance
0.0599485660	sentence representations from
0.0599421268	similarity between two
0.0599393129	the lack of data
0.0599368164	overlap with
0.0599350802	dependence of
0.0599254094	necessary to
0.0599073453	the same data
0.0599004837	in terms of f1
0.0598948371	more recent
0.0598910970	the problem of finding
0.0598745476	a concept
0.0598739464	a function of
0.0598702311	no data
0.0598613102	a given task
0.0598583350	emerge from
0.0598546564	the city
0.0598532720	as input features
0.0598461667	input text to
0.0598450304	on three publicly
0.0598425237	to reproduce
0.0598357978	the performance by
0.0598261825	on different types of
0.0598248782	such as language modeling
0.0598245622	existing models on
0.0598220277	the last few
0.0598211959	this space
0.0598136714	by doing so
0.0598102457	a coherent
0.0598100435	corpus containing
0.0598084920	largely due to
0.0598001009	many advantages
0.0597987576	or more
0.0597972292	not explicitly
0.0597965389	the art models for
0.0597709041	different settings
0.0597667310	accessible to
0.0597656079	a semi
0.0597583747	the widespread
0.0597527376	centered on
0.0597302649	informativeness of
0.0597296982	a simple yet effective approach
0.0597283964	also explore
0.0597230536	strongly on
0.0597157544	to ask
0.0597152822	human evaluation on
0.0597057421	search space for
0.0596984316	a suitable
0.0596973259	to abstract
0.0596969726	but not
0.0596934600	experiments on six
0.0596867632	an output
0.0596811940	presented to
0.0596790624	available in
0.0596711584	of varying
0.0596706643	mainly focus
0.0596610068	a ground
0.0596525289	the general public
0.0596389153	for many natural language processing
0.0596387592	the first problem
0.0596341638	for summarizing
0.0596313117	extensive evaluation of
0.0596308365	an investigation into
0.0596090877	for designing
0.0596049675	introduce two new
0.0596011554	a pre specified
0.0595886311	a fine
0.0595831017	of text documents
0.0595692835	two kinds
0.0595626917	by calculating
0.0595532705	experiments on five
0.0595426634	competitively with
0.0595279031	capabilities of
0.0595240489	an out of
0.0595156227	for explaining
0.0594998206	in doing
0.0594911413	to impose
0.0594899101	this review
0.0594897116	parsing with
0.0594896208	best score
0.0594867677	both entity
0.0594779324	over four
0.0594733356	a given language
0.0594622149	performance of different
0.0594621300	two questions
0.0594614969	this topic
0.0594564357	validity of
0.0594482165	obtained on
0.0594476870	requirements for
0.0594460016	adapted to other
0.0594459828	the available data
0.0594406116	reasons for
0.0594378499	the sequence to
0.0594299930	the existing state of
0.0594254518	and larger
0.0594175735	some light
0.0593943429	this intuition
0.0593818726	sentiment classification of
0.0593776715	a general method
0.0593649475	of vocabulary words
0.0593356684	of research on
0.0593252020	and meanwhile
0.0593190973	achieves good
0.0593151830	a suite of
0.0593141985	and report results
0.0593120969	transfer between
0.0593112110	the major
0.0592949100	to assume
0.0592948588	this information
0.0592776325	a variety of nlp tasks
0.0592684404	from left to right
0.0592628403	cer of
0.0592615693	the agent
0.0592396132	the given text
0.0592393968	a given context
0.0592361766	two shortcomings
0.0592330633	hypothesis for
0.0592263341	14 english
0.0592250027	significant attention in
0.0592209117	door to
0.0592171587	as ground truth
0.0591979730	translation quality by
0.0591672407	submitted to
0.0591646756	a semantic space
0.0591643536	patients with
0.0591589504	to confirm
0.0591549641	the treatment
0.0591423935	the attention layer
0.0591258524	an important and challenging
0.0591258524	on several text classification
0.0591164458	the chance
0.0591083386	significance of
0.0590994425	less training
0.0590880350	variants of
0.0590807293	an end to end system
0.0590767341	a collection of documents
0.0590741559	less sensitive
0.0590723488	a much
0.0590699346	a rapid
0.0590660066	mainly on
0.0590658531	or even
0.0590495131	made use of
0.0590169217	external knowledge in
0.0590144625	network model for
0.0590142977	crucial role in
0.0590130073	the rare
0.0590115301	position in
0.0590031372	a risk
0.0590027468	generate more
0.0589877027	takes into
0.0589860005	the same domain
0.0589841435	the participation
0.0589760988	features from
0.0589609752	just as
0.0589539499	pretrained on
0.0589511326	independence of
0.0589451191	the goal
0.0589442139	agree with
0.0589386687	place in
0.0589369302	a critical role in
0.0589351141	semantic information in
0.0589315291	sentiments from
0.0589258112	the parser
0.0589248759	the scaling
0.0589248245	aligns with
0.0589228162	this situation
0.0589182113	the collected data
0.0589132924	to synthesize
0.0589119706	hierarchical structure of
0.0589048290	for en
0.0589046338	to sequence tasks
0.0589025711	effect of
0.0588904678	way to improve
0.0588848556	the same topic
0.0588801834	a literature
0.0588756705	two different
0.0588720420	domains such as
0.0588699463	also examine
0.0588696105	to revise
0.0588657113	a novel graph
0.0588633592	most frequently
0.0588557814	a given target
0.0588441796	training on
0.0588334286	detection in
0.0588332506	to retain
0.0588331005	rise of
0.0588174615	a unification
0.0588145636	the submitted
0.0588116681	decrease in
0.0587921279	spectrum of
0.0587867316	^ \
0.0587812051	reconstruction of
0.0587783973	the bidirectional encoder
0.0587779239	able to train
0.0587774001	emotion from
0.0587733234	a novel data augmentation
0.0587728162	to advance
0.0587701511	more popular
0.0587607420	in dealing with
0.0587457541	benchmarks show
0.0587376781	a novel corpus
0.0587364326	this claim
0.0587177507	the task of determining
0.0587149011	by merging
0.0587116681	implementations of
0.0587116681	characteristic of
0.0587115471	challenging problem in
0.0587050597	to lead
0.0586683689	main contribution of
0.0586592146	the news domain
0.0586491513	failing to
0.0586301179	of up to
0.0586199667	trained and evaluated on
0.0586152807	the textual content
0.0586130850	ambiguity in
0.0586092612	a method to learn
0.0586062694	used to obtain
0.0585948732	such as machine
0.0585929967	working on
0.0585862487	to collect
0.0585826915	a vital
0.0585800121	done with
0.0585765891	the community
0.0585763698	not need to
0.0585628928	the ultimate
0.0585603470	a trivial
0.0585568094	to refine
0.0585536391	a categorical
0.0585482960	a refinement
0.0585482468	system outperforms
0.0585455433	techniques for
0.0585428087	a new objective
0.0585415801	many tasks
0.0585327708	the art methods for
0.0585309335	become more and
0.0585267099	well as two
0.0585235735	rate of
0.0585226870	variability in
0.0585224720	the inability
0.0585219752	a clear
0.0585209168	the traditional
0.0585182267	allows users
0.0584803790	a functional
0.0584791344	for example in
0.0584692540	the necessity
0.0584672961	by masking
0.0584640219	supervised learning for
0.0584587822	a large collection of
0.0584542138	semantic content of
0.0584493882	significant margin on
0.0584460644	on 9
0.0584242427	values from
0.0584203585	both linguistic
0.0584133745	the art results on three
0.0584132122	a maximum
0.0584078968	new research
0.0584060155	course of
0.0584033344	to counter
0.0584020649	this behavior
0.0584005911	a naive
0.0583918610	various text
0.0583782210	likelihood of
0.0583753765	generation from
0.0583726330	the annotated corpus
0.0583724892	2017 shared
0.0583693720	same language
0.0583622677	in terms of precision
0.0583559358	propagation through
0.0583547614	the introduction
0.0583506235	this hypothesis
0.0583452800	to calculate
0.0583266832	the formation
0.0583256418	a semantic representation
0.0583233896	a sound
0.0583186876	performed over
0.0583174780	sources such as
0.0583061116	the data scarcity
0.0583000923	a variety
0.0582945027	a weighted
0.0582748512	this metric
0.0582654688	the automatic evaluation
0.0582477547	knowledge graph with
0.0582449381	system produces
0.0582364671	from unlabeled
0.0582248179	languages other than
0.0581986049	approach uses
0.0581648874	even in
0.0581543647	used at
0.0581420777	a base
0.0581402967	much interest
0.0581277139	these extensions
0.0581258901	the first two
0.0581176672	participating in
0.0581168873	a bidirectional
0.0581125865	a solution
0.0581036386	challenging since
0.0581024451	dataset as well as
0.0580995041	achieved on
0.0580953769	a phrase
0.0580940717	many works
0.0580788667	an orthogonal
0.0580788494	complexity of
0.0580642412	the cosine similarity
0.0580619662	do well
0.0580613704	relative to
0.0580530045	to characterize
0.0580500823	the same sentence
0.0580475182	the linked
0.0580470689	the generation of
0.0580414789	an ensemble of
0.0580259325	art systems on
0.0580254570	a separate model
0.0580130037	strategies for
0.0580111049	by breaking
0.0580095385	profile of
0.0580080958	with respect
0.0579957554	a survey of
0.0579944963	two extensions
0.0579890134	of available training data
0.0579856052	all over
0.0579807893	to manipulate
0.0579571541	need more
0.0579535335	to noise
0.0579533326	these advances
0.0579431151	the surprising
0.0579394510	a wide range of natural language
0.0579381298	expressive power of
0.0579351173	a narrow
0.0579323767	the task of automatically
0.0579254281	learned via
0.0579189627	less accurate
0.0579179823	stemming from
0.0579090056	the volume
0.0579085884	search space of
0.0579050793	winner of
0.0579008536	for identifying
0.0578918863	work on
0.0578890240	associated to
0.0578845275	often contain
0.0578525460	of view of
0.0578399228	an extension
0.0578382119	co occurrences of
0.0578187469	required in
0.0578164465	principle of
0.0578127543	to fill
0.0577709088	inspection of
0.0577696341	developments in
0.0577694662	effects of
0.0577490245	the performance gap
0.0577261130	bert model for
0.0577216195	aim of
0.0577177688	obtained with
0.0577166066	a custom
0.0577012810	recent research in
0.0577002715	of words by
0.0576958881	a small set
0.0576957488	does not depend
0.0576892414	this capability
0.0576859922	compositionality in
0.0576716093	systems need to
0.0576590920	of pairs of
0.0576470610	taken from
0.0576429914	dissemination of
0.0576418813	a syntax
0.0576415259	for automatically detecting
0.0576272561	discuss several
0.0576238652	a word in
0.0576207234	a standardized
0.0576174214	a crucial
0.0576066538	for evaluating
0.0576061495	to compose
0.0575934877	further use
0.0575891920	a speech recognition
0.0575653736	formalization of
0.0575580775	trained using
0.0575414350	the relevant information
0.0575252311	the performance of state of
0.0575200011	a live
0.0575190481	the poor
0.0575115122	the art across
0.0575091935	f score on
0.0575084892	report on
0.0575072681	an out
0.0575055792	a regularizer
0.0575025266	a majority
0.0574980742	context of
0.0574858374	to convey
0.0574834593	extended to other
0.0574747675	new way to
0.0574657750	instance of
0.0574626741	the best system
0.0574489938	to explain
0.0574487229	with real users
0.0574478466	sentiment towards
0.0574378593	a parser
0.0574306087	interpretations of
0.0574101565	all kinds
0.0574099005	training examples for
0.0574057107	tied to
0.0573966055	reports from
0.0573937338	the obvious
0.0573782636	going to
0.0573764166	the same level
0.0573754060	come at
0.0573668650	the model to learn
0.0573548839	automated system
0.0573466843	difficulty in
0.0573425747	the performance of two
0.0573423327	different categories
0.0573412693	competitive results with
0.0573361116	of user reviews
0.0573357403	the efficacy
0.0573333050	a method for learning
0.0573282601	the best accuracy
0.0573273643	the art result on
0.0573246182	over existing
0.0573244883	this approach with
0.0573012515	final system
0.0572982970	absolute improvement of
0.0572951588	demonstrated by
0.0572944204	the qa task
0.0572823659	to other tasks
0.0572710251	an adequate
0.0572615399	a new algorithm
0.0572607088	the network's
0.0572590007	valuable source of
0.0572578327	the most recent
0.0572523574	also include
0.0572498032	leveraged by
0.0572417684	processing tasks such as
0.0572408456	the basic
0.0572368203	the regression
0.0572267710	the same space
0.0572193080	a combinatorial
0.0572169255	languages within
0.0572015260	learning model for
0.0572002207	factors such as
0.0571970770	annotated data for
0.0571951191	the literature
0.0571816910	the task of multi
0.0571781764	the potential
0.0571772974	advantages of
0.0571693234	with little
0.0571688209	constructed from
0.0571649307	the discrepancy
0.0571540927	features like
0.0571533369	linearly with
0.0571470197	a flat
0.0571440498	the corpus and
0.0571414391	the bidirectional lstm
0.0571398726	categorization of
0.0571385078	function of
0.0571341203	domain adaptation of
0.0571308293	engaging in
0.0571114182	collected in
0.0570966944	the support vector
0.0570950226	summaries from
0.0570917195	the most promising
0.0570844926	in accordance
0.0570789743	japanese to
0.0570730811	recover from
0.0570665657	texts into
0.0570573250	the conversation
0.0570549771	a long time
0.0570530045	to summarize
0.0570470151	continue to
0.0570407610	not available
0.0570405423	experimental results on several
0.0570346775	each sub
0.0570226851	applied to other
0.0570176705	of words and phrases
0.0570127507	by varying
0.0569855460	to simplify
0.0569853474	the problem of predicting
0.0569691493	the art baselines on
0.0569650056	an effect
0.0569644717	the effect
0.0569638958	also conducted
0.0569567803	the perspective
0.0569540387	the development of deep
0.0569516432	done by
0.0569489528	a set of experiments
0.0569479544	of words as
0.0569447382	the calculation
0.0569388704	and english to
0.0569308961	further used to
0.0569269364	the usability
0.0569128875	the top
0.0569034384	hybrid approach to
0.0568975149	benchmark dataset of
0.0568974184	both textual
0.0568866681	mentioned in
0.0568738745	the next
0.0568734470	the same meaning
0.0568653302	used in natural language processing
0.0568617110	competitively on
0.0568599413	enough to
0.0568511687	expression of
0.0568509432	several datasets
0.0568348479	contextual information for
0.0568244883	to train on
0.0568197659	the gap
0.0568141226	to regularize
0.0568133690	system developed
0.0568092228	problem as
0.0567956993	introduction of
0.0567956148	implemented by
0.0567817304	of human intelligence
0.0567683153	a part of speech tagger
0.0567557646	the possibility of
0.0567510988	the three tasks
0.0567463581	provide more
0.0567448797	the model with
0.0567419778	to explore
0.0567397480	the web
0.0567387406	two key
0.0567363631	a significant performance
0.0567330104	show significant improvement
0.0567290168	this tool
0.0567283964	further research
0.0567230536	attack on
0.0566986072	production of
0.0566901407	the article
0.0566812079	system built
0.0566800417	an action
0.0566773350	to better
0.0566706020	increases as
0.0566668687	specified in
0.0566634227	the meaning of words
0.0566538051	merits of
0.0566527026	prior work on
0.0566520455	this corpus
0.0566499073	well as different
0.0566483933	more important
0.0566434094	approach on two
0.0566419498	a small amount
0.0566393212	a binary classification
0.0566238994	of language change
0.0566214284	up to now
0.0566139257	enhanced by
0.0566073843	translate between
0.0566036333	a kernel
0.0566026894	scheme for
0.0565978704	many state of
0.0565958538	on two benchmark
0.0565932502	to turn
0.0565876358	built for
0.0565796515	made possible
0.0565764194	for discovering
0.0565693528	for training neural
0.0565682859	the feature space
0.0565612365	property of
0.0565568094	to visualize
0.0565508036	to arrive
0.0565405195	to sub optimal
0.0565357733	learning framework to
0.0565343300	sub tasks of
0.0565330387	calibration of
0.0565170014	to check
0.0565140317	the lexicon
0.0565129893	do not incorporate
0.0565106167	the knowledge in
0.0565094355	awareness of
0.0565072681	the d
0.0564963523	any pair of
0.0564890914	an error
0.0564889360	opinions on
0.0564877077	two languages
0.0564864347	few as
0.0564848123	first use
0.0564758355	model without
0.0564758133	the truthfulness
0.0564699899	no single
0.0564638958	several studies
0.0564593299	the recent success
0.0564550850	the un
0.0564298181	possible by
0.0564230710	a sequence to
0.0564207186	further propose
0.0564000414	believed to
0.0563884874	the first approach
0.0563699867	the recognition accuracy
0.0563654838	to match
0.0563653200	on wmt14
0.0563579956	evaluation on
0.0563529514	ones for
0.0563527528	a global
0.0563421371	translation systems for
0.0563411276	inclusion of
0.0563197878	rate on
0.0563038969	automatic method for
0.0563035448	much of
0.0562988118	only one
0.0562922296	the information about
0.0562922294	relevance between
0.0562862796	model capable of
0.0562675304	organized as
0.0562403254	results than
0.0562244445	construction of
0.0562198059	to generate sentences
0.0562092345	tagger for
0.0561941882	sample of
0.0561934722	component of
0.0561826213	by around
0.0561823710	key challenge in
0.0561813413	dream of
0.0561659100	relation between two
0.0561504533	a trade off
0.0561489304	introduced into
0.0561401204	a negligible
0.0561371545	but more
0.0561218068	referents of
0.0561115244	several interesting
0.0561111202	but often
0.0561079916	on large datasets
0.0561028467	to demonstrate
0.0560983819	a new deep
0.0560980804	these metrics
0.0560898432	to satisfy
0.0560808851	an inductive
0.0560743716	of vision and
0.0560632172	approach against
0.0560580619	expense of
0.0560503544	a sentence by
0.0560368132	improved by
0.0560357267	model results in
0.0560294514	recognition with
0.0560265105	on top
0.0560259769	inference over
0.0560167764	robust to
0.0560136828	appearance of
0.0560135277	many methods
0.0560087063	without introducing
0.0559805087	supervision from
0.0559734690	used to enhance
0.0559701187	dataset containing
0.0559660713	the two datasets
0.0559605530	to mine
0.0559537208	more comprehensive
0.0559530593	in order to obtain
0.0559473774	indicator of
0.0559457588	the top performing
0.0559414386	generative model of
0.0559263461	tokens from
0.0559037487	by doing
0.0558993070	tried to
0.0558956287	most successful
0.0558932986	generalize well to
0.0558783393	the fidelity
0.0558691658	the most efficient
0.0558671390	able to improve
0.0558633592	most representative
0.0558574067	parsers with
0.0558561406	published in
0.0558511326	period of
0.0558489007	novel neural architecture
0.0558254431	this system
0.0558254245	several variants
0.0558251103	the art methods by
0.0558184369	a set of rules
0.0558113072	spoken by
0.0558083112	the canonical
0.0558080923	eer of
0.0558012527	able to perform
0.0557989169	used to learn
0.0557983381	tasks like
0.0557962824	do not consider
0.0557935516	enough information
0.0557888359	queries over
0.0557887031	essential part of
0.0557847656	the field of natural
0.0557845584	a large and diverse
0.0557774989	the next sentence
0.0557658756	comparable or
0.0557639204	also includes
0.0557557646	a new approach to
0.0557531895	like bert
0.0557375229	claimed to
0.0557346222	a novel algorithm
0.0557109755	to aid
0.0557029284	descriptions of
0.0556977998	baseline models on
0.0556954899	two different languages
0.0556911132	novel task
0.0556892176	for further analysis
0.0556728776	diffusion of
0.0556655569	this report
0.0556648096	the relative
0.0556587319	the future
0.0556571916	represented in
0.0556547876	to extract information
0.0556394615	in many natural language processing tasks
0.0556388623	the same dataset
0.0556377541	the information of
0.0556339498	the overall
0.0556281202	to formulate
0.0556132684	by asking
0.0556022167	to automatically learn
0.0555973144	to highlight
0.0555971630	the learnability
0.0555964380	the inner product
0.0555914412	items from
0.0555891450	all three
0.0555825039	able to accurately
0.0555813811	on wsj
0.0555731978	the hebrew
0.0555689779	to automatically predict
0.0555608764	first step towards
0.0555397850	in accuracy on
0.0555372880	models in terms of
0.0555192358	history of
0.0554989063	on standard datasets
0.0554963075	propose methods for
0.0554923995	by offering
0.0554781451	the most likely
0.0554778348	widely used in
0.0554555092	ignored in
0.0554439594	able to obtain
0.0554438552	both modalities
0.0554425583	the space of
0.0554393846	a family
0.0554368921	generation via
0.0554282845	a seq2seq model
0.0554214878	this work explores
0.0554199091	a timely
0.0554119085	the reconstruction
0.0554098361	any kind of
0.0554080228	applied as
0.0554059724	to progress
0.0554020961	extracted by
0.0553926517	on two standard
0.0553886467	made at
0.0553877128	a new way
0.0553794099	learning methods on
0.0553752839	the source side
0.0553706287	an evolutionary
0.0553705265	technique for
0.0553504321	in order to achieve
0.0553492646	a basic
0.0553306742	external knowledge for
0.0553145523	capability of
0.0553106354	time span
0.0553065676	given as
0.0553063164	further improved by
0.0553039245	a heuristic
0.0553006994	an automatic speech
0.0552960616	applicability to
0.0552934146	public use
0.0552902362	while performing
0.0552839267	evaluated by
0.0552555408	the usage
0.0552512109	the two tasks
0.0552504566	just by
0.0552496738	far as
0.0552474058	do not capture
0.0552468937	the same type
0.0552457941	such issues
0.0552424536	each position
0.0552398007	language processing tasks such as
0.0552316796	embeddings for
0.0552257577	at least as
0.0552157565	the field
0.0552152922	in many natural language
0.0552136729	in real applications
0.0552070238	the conventional
0.0551920788	an increasing interest in
0.0551894121	computing with
0.0551839569	an established
0.0551773833	to fix
0.0551756672	addition to
0.0551712033	a second
0.0551603349	described in
0.0551561355	a novel embedding
0.0551488824	the same input
0.0551462540	except for
0.0551350325	the high cost
0.0551344544	a conditional
0.0551335343	the semeval 2020
0.0551312095	by searching
0.0551229355	due to limited
0.0551149944	a feasible
0.0551119564	a variety of natural language
0.0550943301	measures such as
0.0550939467	information within
0.0550908747	simple but
0.0550836188	this dataset and
0.0550818849	an unstructured
0.0550735529	models on two
0.0550547617	mainly rely on
0.0550481602	contextual information in
0.0550466444	extensive experiments on two
0.0550435348	information such as
0.0550417894	the need of
0.0550404285	results in better
0.0550346790	the same task
0.0550338609	variance in
0.0550316470	across time and
0.0550284084	to organize
0.0550277560	information need
0.0550046567	game with
0.0550039447	a tiny
0.0549967698	a video
0.0549961315	a large scale dataset of
0.0549937813	the synthesized
0.0549927256	utility for
0.0549914291	important tasks in
0.0549902094	pointing to
0.0549879940	solely from
0.0549838955	higher level of
0.0549775659	a modified
0.0549555182	between two sentences
0.0549491370	the second approach
0.0549478429	only used
0.0549449093	this difference
0.0549389702	system achieved
0.0549348260	many multilingual
0.0549297233	four types of
0.0549204081	pairs from
0.0549183264	the art approaches for
0.0549134669	a new approach
0.0549102733	generalization ability of
0.0549019920	purpose of
0.0548993629	the evolutionary
0.0548921902	the lexical and
0.0548887480	most state of
0.0548788464	several advantages
0.0548654241	model consisting of
0.0548604002	downstream tasks such as
0.0548593848	the surrounding
0.0548550749	a computer
0.0548484861	a difficult
0.0548374401	sentences into
0.0548320590	mappings from
0.0548260715	a classifier
0.0548199837	a long
0.0548191472	a simulated
0.0548152280	show experimentally
0.0548094674	factuality of
0.0547989022	to probe
0.0547939994	such as social media
0.0547939755	the first work
0.0547915732	study based on
0.0547888603	progress on
0.0547806833	such as machine translation
0.0547747996	to aggregate
0.0547698335	informative than
0.0547654805	resources like
0.0547557646	a novel approach to
0.0547525053	the base
0.0547490865	both automatic
0.0547398926	used to produce
0.0547329257	trained over
0.0547131608	hierarchy of
0.0547108370	a lack of
0.0547081166	adaptive to
0.0546910248	present in
0.0546810864	the lack of sufficient
0.0546796703	methodology for
0.0546772826	by querying
0.0546646978	this manner
0.0546557646	the emergence of
0.0546556071	a notable
0.0546500226	lines of
0.0546459352	of input words
0.0546450876	does not make
0.0546376950	on three benchmark datasets
0.0546275787	an end to
0.0546250134	different tasks
0.0546193032	tendency to
0.0546171390	a novel technique
0.0546151274	a capsule
0.0546134246	strong performance in
0.0546047828	the vast
0.0545993273	consisting of three
0.0545965542	consideration of
0.0545837347	developed to
0.0545820453	the view of
0.0545819837	taking into
0.0545798797	a notion
0.0545673355	a sophisticated
0.0545570536	either require
0.0545526315	quite different
0.0545471798	a dedicated
0.0545448463	annotated according to
0.0545428325	a valid
0.0545405203	mechanics of
0.0545368132	possible to train
0.0545334099	couple of
0.0545285200	to fail
0.0545277591	own language
0.0545201805	a deep understanding
0.0545168893	paper proposes to
0.0545168607	a convenient
0.0545118715	architecture allows
0.0545102841	to learn to
0.0544980742	generation of
0.0544865670	better at
0.0544848008	dataset contains
0.0544818247	identified by
0.0544736049	requires only
0.0544648628	an automatic evaluation
0.0544581883	moving from
0.0544581472	both english and chinese
0.0544580625	of existing models
0.0544545144	a tendency
0.0544538432	asked to
0.0544393047	of word meaning
0.0544322045	allows for efficient
0.0544229549	testbed for
0.0544185978	an exact
0.0544101675	with high precision
0.0544042381	improve over
0.0543959352	samples from
0.0543894556	by defining
0.0543862666	used in text
0.0543777594	three modules
0.0543723160	and perhaps
0.0543660414	an external language
0.0543566175	new task
0.0543496211	the source and target
0.0543469921	to document
0.0543455131	an appealing
0.0543383996	poses new
0.0543234126	novel approach
0.0543219289	group of
0.0543187272	and hence
0.0543006165	the corresponding
0.0542987531	the prevalent
0.0542959528	respectively for
0.0542931051	a raw
0.0542743597	quality than
0.0542686784	to more complex
0.0542681028	work investigates
0.0542632913	better than state of
0.0542619359	to emphasize
0.0542562923	desirable to
0.0542555788	other researchers
0.0542519192	a new multi
0.0542507453	work presents
0.0542201734	the data from
0.0542151193	a left
0.0542138031	the context in
0.0542077584	a technique
0.0542062297	gold standard for
0.0542047446	the arc
0.0541965081	problem by
0.0541875800	also report
0.0541874655	the prominent
0.0541809199	done in
0.0541799198	scope of
0.0541752340	words into
0.0541749362	such as question answering
0.0541734498	correlated to
0.0541583471	to derive
0.0541537709	a given user
0.0541521261	a lot of research
0.0541513095	pretraining for
0.0541466898	an absolute
0.0541422807	for learning word
0.0541409151	and hard to
0.0541385134	a statistical machine
0.0541380535	results in terms of
0.0541268932	a valuable
0.0541268391	to automatically detect
0.0541174866	the inductive
0.0541088761	challenging task in
0.0541086741	introduced for
0.0541048989	series of
0.0541011356	to operate
0.0540981242	of work in
0.0540948545	a threshold
0.0540946753	approach provides
0.0540873478	important part of
0.0540812547	to balance
0.0540691239	reduction over
0.0540684437	come to
0.0540652280	these concerns
0.0540597380	speech from
0.0540541506	the vanilla
0.0540371110	scores than
0.0540349018	the success of deep learning
0.0540230682	the limitation
0.0540203389	novel ways
0.0540199760	the interpretation of
0.0540063754	a given corpus
0.0539988288	coverage of
0.0539949297	format for
0.0539837019	a coarse
0.0539822321	view of
0.0539755911	a concrete
0.0539669344	a model to predict
0.0539658011	method provides
0.0539602553	described by
0.0539451623	a hard
0.0539433399	in terms of accuracy
0.0539343506	not seem
0.0539171980	large datasets of
0.0539153786	the method to
0.0538974539	to maintain
0.0538960564	aimed to
0.0538909744	sequence of
0.0538885981	the prior knowledge
0.0538880681	such as glove
0.0538732988	by considering
0.0538725041	by inferring
0.0538591682	on several benchmark datasets
0.0538548425	a challenging
0.0538531752	to track
0.0538481705	for converting
0.0538462145	to supplement
0.0538445507	abstraction of
0.0538416492	the age
0.0538376747	supervised training of
0.0538356684	a complex and
0.0537986755	perception of
0.0537948491	meanings of
0.0537947553	a careful
0.0537892813	domains like
0.0537868824	core of
0.0537863299	different degrees
0.0537691542	of many natural
0.0537525509	first generates
0.0537409590	the first study
0.0537234690	used to define
0.0536979283	cast as
0.0536877159	a source
0.0536736858	suggested by
0.0536720383	other state of
0.0536705986	use of machine learning
0.0536663819	perform as well
0.0536608205	more often
0.0536548964	a machine reading
0.0536510455	verified by
0.0536451536	implications of
0.0536423107	the speaker's
0.0536359521	the model outperforms
0.0536299152	applied at
0.0535963581	provide further
0.0535951476	the training time
0.0535809281	multiple sources of
0.0535805773	based architecture for
0.0535793203	to replace
0.0535775653	the incorporation
0.0535756030	show significant improvements
0.0535643743	problems like
0.0535522833	the framework to
0.0535414412	trees from
0.0535341873	while others
0.0535320590	detected by
0.0535281127	an interface
0.0535248875	entities across
0.0535139628	for implicit discourse
0.0535108370	the domain of
0.0535006912	heavily on
0.0534893797	trained on one
0.0534590170	the bag of
0.0534571646	translation quality on
0.0534563923	need to develop
0.0534562547	a vast
0.0534379687	a fully automated
0.0534298620	this resource
0.0534298322	thus providing
0.0534286385	the clickbait
0.0534258101	system combines
0.0534090309	subject of
0.0534048290	the ptb
0.0534037142	the base model
0.0533927042	by discovering
0.0533888155	case study of
0.0533868715	robust across
0.0533864240	the south
0.0533748367	the f1 score of
0.0533626624	a model on
0.0533617261	to sub
0.0533394307	manifested in
0.0533250138	to reach
0.0533214743	for measuring
0.0533181868	preserved in
0.0533092975	the art approaches on
0.0533032947	theory of
0.0532865772	a traditional
0.0532826794	especially if
0.0532823432	consists of over
0.0532818882	to attain
0.0532792764	a given text
0.0532777869	often only
0.0532768659	abstracts from
0.0532710881	the closest
0.0532705581	more useful
0.0532689755	a novel joint
0.0532679208	typically use
0.0532555478	increasingly used
0.0532488336	extended with
0.0532485827	framework allows
0.0532428905	a company
0.0532362706	the two modalities
0.0532224316	hard for
0.0532221877	used to guide
0.0532203221	not covered
0.0532199784	in order to solve
0.0532102829	used to assess
0.0532067845	the amount of training data
0.0531974298	computational complexity of
0.0531925687	the performance of several
0.0531875155	both acoustic
0.0531818450	for research on
0.0531810362	relation extraction from
0.0531780860	stability of
0.0531567202	a biased
0.0531561940	recognition from
0.0531455181	the source and
0.0531403613	by producing
0.0531353489	by optimizing
0.0531312033	occurring in
0.0531308274	documents into
0.0531264321	developed with
0.0531106167	to apply in
0.0531084407	in order to address
0.0531055490	uses only
0.0531040326	intended to
0.0530972398	the local
0.0530895311	such as cross
0.0530863812	the resulting system
0.0530854361	competitive performance in
0.0530785836	a new open
0.0530682307	attacks on
0.0530643743	compare various
0.0530597809	by copying
0.0530555478	performed better
0.0530520185	prototype system
0.0530470689	the idea of
0.0530459551	attempt at
0.0530349057	and then apply
0.0530066820	embedded in
0.0530061942	more and more attention
0.0530006651	a novel methodology
0.0529796703	critical for
0.0529740900	crucial to
0.0529730677	this perspective
0.0529466774	a simple data
0.0529361649	and implementation of
0.0529295231	important task for
0.0529207165	good as
0.0529164555	method does not
0.0529157632	on standard benchmarks
0.0529134056	any manual
0.0529124510	data in order to
0.0529099983	for detecting
0.0528880631	rooted in
0.0528849864	attention mechanism on
0.0528840858	information than
0.0528528421	superiority of
0.0528477613	a posterior
0.0528439629	a transparent
0.0528268140	an intermediate
0.0528196545	system at
0.0528119651	as follows
0.0528049903	the model consists
0.0528021672	the heart
0.0527942375	domains without
0.0527856549	ratio of
0.0527725552	words per
0.0527647530	discovery from
0.0527614500	signal for
0.0527566901	this process
0.0527301846	procedure for
0.0527283964	also investigate
0.0527283964	also compare
0.0527170446	allowed to
0.0526980685	this algorithm
0.0526911981	a direct
0.0526887851	a given question
0.0526862381	single system
0.0526854768	a connection
0.0526829453	quantification of
0.0526802138	by attending
0.0526759876	some experiments
0.0526680680	task aims to
0.0526659655	more robust than
0.0526646605	to automatically construct
0.0526609831	creation of
0.0526569058	a vanilla
0.0526507743	by sharing
0.0526413796	the veracity
0.0526243268	a novel method for
0.0526243257	years due to
0.0526215720	performance against
0.0526188147	generation model with
0.0526106436	the notion
0.0526093749	by investigating
0.0525995192	the recent progress
0.0525983723	for resolving
0.0525777825	for model training
0.0525708555	a pipeline
0.0525686669	to work with
0.0525662631	each pair
0.0525644234	trained via
0.0525539540	the first system
0.0525507863	process of
0.0525461073	documents such as
0.0525425060	to memorize
0.0525398369	dataset from
0.0525343025	a model's
0.0525277057	the artificial intelligence
0.0525260697	this setting
0.0525244626	the micro
0.0525222046	much as
0.0525108370	the sentiment of
0.0525078685	two kinds of
0.0525055287	as well as other
0.0525005389	limited due
0.0524755665	and visually
0.0524701893	versatility of
0.0524666915	to narrow
0.0524618255	translating from
0.0524604660	two complementary
0.0524577256	established by
0.0524569642	the back
0.0524493775	for addressing
0.0524452045	result on
0.0524432674	guidance on
0.0524415356	selected from
0.0524329140	aspects such as
0.0524298322	usually require
0.0524297805	recommendations for
0.0524237981	better results
0.0524231875	to describe
0.0524195577	to involve
0.0524185813	also proposed
0.0524155319	the input to
0.0523992503	visualization of
0.0523978112	question about
0.0523928990	information from different
0.0523893924	a general approach
0.0523852697	the realization
0.0523843555	performs well on
0.0523562212	probability distribution of
0.0523499116	prediction from
0.0523486922	provide better
0.0523446604	the word's
0.0523387640	no such
0.0523328546	researches on
0.0523284469	challenging task for
0.0523121945	such as information retrieval
0.0523042120	probabilistic model of
0.0523028498	used to detect
0.0523018440	by randomly
0.0522994909	or comparable
0.0522951588	identified as
0.0522921296	modeled in
0.0522841203	by composing
0.0522821778	explosion in
0.0522788103	to pre
0.0522667223	system for extracting
0.0522651352	seems to
0.0522637839	metric for
0.0522596933	differently from
0.0522587797	models learn to
0.0522566808	a probability distribution over
0.0522500579	the electronic
0.0522322471	to attend
0.0522304099	a curriculum
0.0522291378	concatenated with
0.0522102015	knowledge base for
0.0522058562	to rely
0.0522055342	approaches do not
0.0522030087	three languages
0.0521988026	to share
0.0521987270	a simple neural
0.0521906982	protocols for
0.0521895054	because of
0.0521888895	to filter
0.0521864415	terms of
0.0521857519	to sign
0.0521850697	system obtains
0.0521819609	promise for
0.0521769073	done on
0.0521717724	a heavy
0.0521655288	challenging to
0.0521594209	the ner
0.0521442061	such problems
0.0521399610	also perform
0.0521345536	a learnable
0.0521338686	dependencies within
0.0521321472	a new large scale
0.0521302936	of thousands
0.0521203235	created from
0.0521156071	to break
0.0521145605	a canonical
0.0521085181	instantiation of
0.0520918309	the automatic identification
0.0520862847	work explores
0.0520857097	even after
0.0520854024	training time by
0.0520801006	three groups
0.0520746207	the result
0.0520700011	to draw
0.0520684997	word segmentation for
0.0520658232	no loss
0.0520528421	presentation of
0.0520426518	a conversation
0.0520346157	produced from
0.0520337832	evaluate whether
0.0520237860	the best baseline
0.0520147160	several distinct
0.0520140129	failures of
0.0520111136	a considerable
0.0520096769	well on
0.0519919026	such as word embeddings
0.0519860473	to keep track
0.0519784070	informed by
0.0519774244	into coherent
0.0519735554	a novel attention mechanism
0.0519579265	of sentence length
0.0519517805	variations in
0.0519504439	translations from
0.0519500041	to devise
0.0519421807	useful tool
0.0519409955	a big
0.0519377929	some initial
0.0519357582	dynamics of
0.0519291719	body of
0.0519248418	success in
0.0519219458	those based on
0.0519082023	posts from
0.0518953637	helpful in
0.0518902144	survey of
0.0518864208	by approximately
0.0518729941	acquisition of
0.0518699564	an area
0.0518698222	to smooth
0.0518608689	not very
0.0518516164	several tasks
0.0518469921	the addition
0.0518431164	all existing
0.0518400041	to inform
0.0518167769	assignment of
0.0518147832	built with
0.0518093403	on many nlp tasks
0.0518050028	acoustics to
0.0518034483	redundancy in
0.0517989169	used to evaluate
0.0517981801	with human users
0.0517981478	representation for
0.0517908170	a latent
0.0517819757	performance gains in
0.0517769971	a spoken dialogue
0.0517705159	to test
0.0517625142	issue by
0.0517525509	some potential
0.0517501061	as well as two
0.0517454419	evaluation on three
0.0517437441	for retrieving
0.0517404340	different techniques
0.0517403937	a single neural
0.0517383346	across different domains
0.0517304059	always available
0.0517292895	these four
0.0517042622	widely used as
0.0516982648	the dialog system
0.0516972433	bleu over
0.0516944609	demand for
0.0516862843	augmented by
0.0516796155	to ease
0.0516793933	condition on
0.0516718692	in two languages
0.0516609066	to do
0.0516573654	to figure
0.0516536502	a key component
0.0516535015	the labelled
0.0516451258	an important source
0.0516212902	crucial in
0.0516196756	the need to
0.0516173986	the relationship between
0.0516160360	various data
0.0516149626	graph into
0.0515969481	these algorithms
0.0515843356	does not need to
0.0515837620	to validate
0.0515827603	for future
0.0515795694	perspective on
0.0515788667	an approximation
0.0515746254	propose two new
0.0515737043	algorithm uses
0.0515707615	important for
0.0515666050	trained without
0.0515653786	the grammar and
0.0515613317	the fundamental
0.0515600290	for training of
0.0515562615	learning techniques for
0.0515555788	by transferring
0.0515543686	in real time
0.0515428563	often suffer from
0.0515393412	network architectures for
0.0515365610	differ in
0.0515260407	this paper uses
0.0515145037	the longest
0.0515144756	network models for
0.0515108370	the choice of
0.0515073856	an effort
0.0515038464	many areas
0.0515031946	the direction
0.0514994207	the results indicate
0.0514972871	to investigate
0.0514863547	both traditional
0.0514687119	the visual and
0.0514640219	word order of
0.0514517794	certain types
0.0514493461	help people
0.0514435289	a directed
0.0514384150	novelty of
0.0514383652	more positive
0.0514300894	amount of data
0.0514217374	able to better
0.0514201178	a number of tasks
0.0514167734	made on
0.0514132266	to come
0.0514004816	the visual question
0.0513968091	this technical
0.0513967856	to better performance
0.0513789465	discussion of
0.0513758293	the art results on several
0.0513743777	the dominant
0.0513667934	to parse
0.0513592975	the art results on two
0.0513586064	appropriate to
0.0513572384	posted on
0.0513567098	this improvement
0.0513537535	language used in
0.0513438822	a typical
0.0513418056	performance in many
0.0513414862	the number of parameters
0.0513335266	designed as
0.0513287355	works well for
0.0513250138	to establish
0.0513214012	member of
0.0513174533	performance across
0.0513172553	the experimental results on
0.0513132870	also confirm
0.0513115693	the extracted
0.0513088446	level performance on
0.0513053793	to sequence model for
0.0513046501	mitigate such
0.0512985697	a substantial
0.0512861610	these linguistic
0.0512702590	passed to
0.0512463511	to control
0.0512386135	the assumption
0.0512210474	learning approaches for
0.0512023469	this paper aims
0.0511791365	this result
0.0511783347	shortcomings of
0.0511765786	results on five
0.0511753417	to talk
0.0511610577	overall quality
0.0511572945	the wmt 2014
0.0511515822	nlp applications such
0.0511507078	these two tasks
0.0511381898	the presentation
0.0511241565	by converting
0.0511183793	list of
0.0511031764	the popular
0.0510927256	irrelevant to
0.0510768565	alternative to
0.0510749873	the other one
0.0510749159	a customized
0.0510713069	conclusions from
0.0510640246	using crowdsourcing
0.0510541506	the classic
0.0510485630	as expected
0.0510472995	a novel metric
0.0510422870	produce more
0.0510422455	further improved
0.0510356001	for joint entity
0.0510285180	fail on
0.0510260154	various domains
0.0510258128	query into
0.0510231274	the structure of
0.0510176672	vital for
0.0510089321	sparsity of
0.0510083747	and eventually
0.0510055225	helpful to
0.0509980653	the wall
0.0509902772	new data set
0.0509879663	by keeping
0.0509837909	on two tasks
0.0509819934	approach on
0.0509784919	to automatically identify
0.0509644799	the mainstream
0.0509562613	a means
0.0509524125	training example
0.0509436983	not available for
0.0509420790	study focuses on
0.0509301688	the first publicly
0.0509281456	or higher
0.0509275159	the second one
0.0509132231	selection for
0.0508966203	more details
0.0508924489	a simple and effective
0.0508891014	performance over
0.0508725402	other hand
0.0508560581	obtained using
0.0508504267	percentage of
0.0508419445	a baseline
0.0508339781	resources for
0.0508243961	to put
0.0508228909	align with
0.0508222373	well at
0.0508157215	translate from
0.0508151114	syntactic structure of
0.0508031006	a vector
0.0507989415	meaning of
0.0507972273	a final
0.0507931083	tasks show
0.0507921110	used by
0.0507824524	previous work by
0.0507811013	trends in
0.0507684871	more informative and
0.0507574996	a variety of natural language processing
0.0507525509	also reveal
0.0507476541	in order to provide
0.0507402084	an end
0.0507386513	given input
0.0507386513	given topic
0.0507364563	in computer vision
0.0507347602	methods such as
0.0507300458	line of
0.0507263842	data augmentation with
0.0507245898	the hierarchical structure
0.0507211113	strong baseline for
0.0507199221	the exposure
0.0507173526	efficiency of
0.0507153588	accuracy than
0.0507110259	right to
0.0507070970	the improvements in
0.0507034874	the art by
0.0506962995	a statistical
0.0506842868	a prominent
0.0506822321	power of
0.0506818408	subject to
0.0506788044	such as text classification
0.0506767234	by extracting
0.0506653233	very different
0.0506650485	usually requires
0.0506640441	adoption of
0.0506626317	the semi
0.0506516971	in order to create
0.0506472312	of different models
0.0506463038	model achieves new
0.0506452466	freely available for
0.0506334151	to compile
0.0506318767	sentence into
0.0506242017	a strict
0.0506222399	further introduce
0.0506221829	large amount
0.0506166365	to gain
0.0506161963	by reporting
0.0506161963	by removing
0.0506100284	the speaker and
0.0506052219	to expose
0.0505822656	the availability of
0.0505810876	pretraining on
0.0505760201	different classifiers
0.0505746963	different applications
0.0505631552	each relation
0.0505559412	the vocal
0.0505471428	flexible to
0.0505460355	fragments from
0.0505377231	sent to
0.0505376256	in order to perform
0.0505197626	many problems
0.0505180936	not hold
0.0505130110	a proxy
0.0505108370	the complexity of
0.0504974436	various state of
0.0504902715	often leads
0.0504782045	encountered in
0.0504767868	in depth analysis of
0.0504382389	novel methods
0.0504368460	any training
0.0504302636	outperform several
0.0504192731	embedding models on
0.0504143172	the granularity
0.0504116681	growth of
0.0504104097	especially in
0.0504060233	gained from
0.0504010697	to define
0.0503993629	the reverse
0.0503927870	several features
0.0503842126	such documents
0.0503782033	text into
0.0503724739	of source sentences
0.0503708890	a useful resource
0.0503507320	model in order to
0.0503403480	at identifying
0.0503288099	used to describe
0.0503285428	on seven
0.0503280153	iteration of
0.0503223453	a linear
0.0503128103	publicly available for
0.0503075608	part of speech tagger for
0.0503004742	the text into
0.0502997149	methods in terms of
0.0502741565	already existing
0.0502363263	the first shared task on
0.0502241948	many datasets
0.0502164916	outperforms several
0.0502131529	the question as
0.0502117807	tools such as
0.0501977941	this paper deals
0.0501911951	compare three
0.0501906629	sequence models with
0.0501668466	a user to
0.0501653448	the opportunity
0.0501650201	a method based
0.0501524125	based on two
0.0501455181	the image and
0.0501356147	a novel way to
0.0501287023	the work of
0.0501252094	studied for
0.0501208890	the way humans
0.0501203235	signals from
0.0501195693	strengths and weaknesses of
0.0501135029	and interpretation of
0.0501057991	on two
0.0501019094	explore various
0.0500874608	such as elmo
0.0500709134	by studying
0.0500666730	a structured
0.0500626715	to implement
0.0500600922	available on
0.0500556018	way of
0.0500553389	the best known
0.0500552293	the fourth
0.0500494523	point for
0.0500460015	the energy
0.0500405263	battery of
0.0500368072	of research in
0.0500349886	the number of topics
0.0500199023	analysis on
0.0500059384	and simultaneously
0.0500020156	then apply
0.0500016152	on different datasets
0.0500013966	to discern
0.0499999621	content of
0.0499970156	connected to
0.0499959193	by encouraging
0.0499848885	engaged in
0.0499669979	system needs to
0.0499660781	paper reports on
0.0499646915	two orders
0.0499633619	baselines on
0.0499514876	the glottal
0.0499437003	published by
0.0499411841	useful in
0.0499379726	first approach
0.0499377929	some limitations
0.0499333012	classified by
0.0499311452	outperform other
0.0499296047	the reliability
0.0499266416	of interest to
0.0499244907	by averaging
0.0499203538	the viewpoint
0.0499146503	a part
0.0499049211	a residual
0.0499044668	seen during
0.0499017451	least one
0.0498968569	typically used
0.0498962982	widely used for
0.0498873244	models rely on
0.0498826379	paper provides
0.0498786425	produce better
0.0498668949	different approaches
0.0498659958	explore several
0.0498616564	behind human
0.0498480045	much better
0.0498388785	paper gives
0.0498331545	about products
0.0498317796	the positional
0.0498317259	whether two
0.0498316610	a protocol
0.0498310882	whole sequence
0.0498281209	shared between
0.0498257133	shared by
0.0498004404	in today's
0.0497904827	a demonstration
0.0497899741	findings show
0.0497824704	a variety of tasks
0.0497817275	the dop
0.0497776204	for many nlp tasks
0.0497752797	the indus
0.0497667223	on two public
0.0497646670	an extrinsic
0.0497646499	to achieve state of
0.0497618591	an array of
0.0497604930	a separate
0.0497421642	taken by
0.0497410316	in two ways
0.0497370275	the evolution of
0.0497365780	for specific tasks
0.0497361268	search for
0.0497274605	to obtain high
0.0497251424	to process
0.0497236939	leads to more
0.0497222415	estimation of
0.0497143747	learning algorithms for
0.0497110716	the transformer model
0.0497104660	three components
0.0496959279	a simple and efficient
0.0496922191	perform as well as
0.0496879663	by deriving
0.0496840301	for other languages
0.0496791834	the amount of available
0.0496769639	while providing
0.0496659633	not come
0.0496606938	datasets indicate
0.0496589176	features such as
0.0496505798	need for
0.0496451948	text containing
0.0496379075	the given sentence
0.0496284083	case of
0.0496273754	the semeval 2020 task
0.0496137472	translation system to
0.0496105062	an important source of
0.0495987079	by testing
0.0495971691	credibility of
0.0495950059	the demonstration
0.0495926181	help understand
0.0495921516	outperformed by
0.0495919373	under noisy
0.0495779526	challenge for
0.0495641828	the variety
0.0495631626	data selection for
0.0495621153	further demonstrate
0.0495599419	considered for
0.0495598932	a rich
0.0495588350	lead to more
0.0495557646	the issue of
0.0495555788	an extra
0.0495534781	the specification
0.0495527573	such as image
0.0495526346	an idea
0.0495498297	used to infer
0.0495488988	usage of
0.0495263140	achieved with
0.0495111721	across different tasks
0.0495108370	the efficiency of
0.0495059556	a novel feature
0.0494996110	probability of
0.0494942246	mixed with
0.0494911834	the effectiveness of using
0.0494822087	models capable of
0.0494709798	the example of
0.0494671863	most informative
0.0494600813	problem because
0.0494532045	similarly to
0.0494513406	task due to
0.0494460144	transfer via
0.0494395205	defined on
0.0494394234	accuracy across
0.0494288293	contributed to
0.0494173860	usefulness of
0.0494145990	from unannotated
0.0494070377	a remedy
0.0493996189	as inputs
0.0493960223	while translating
0.0493949108	to jointly model
0.0493945855	and subsequently
0.0493811741	by changing
0.0493684746	an architecture
0.0493576981	to fine
0.0493489286	for calculating
0.0493409012	a noticeable
0.0493401936	many questions
0.0493322767	study on
0.0493317271	both intrinsic
0.0493203055	phenomenon in
0.0493195327	the existence of
0.0493180751	given context
0.0493158232	without increasing
0.0493057650	results against
0.0493033899	achieved through
0.0493023661	used for evaluation
0.0493017992	new light on
0.0492997740	a variety of natural
0.0492681799	an f
0.0492632003	a random
0.0492570637	and many other
0.0492470689	the majority of
0.0492435595	success in many
0.0492225404	both source and target
0.0492131317	fundamental to
0.0492120472	presented by
0.0492094374	proportions of
0.0492088568	the approach on
0.0492085128	the whole model
0.0492082098	to complete
0.0491960571	datasets used for
0.0491810572	novel way of
0.0491800911	especially for
0.0491759358	the needs of
0.0491739901	does not rely
0.0491657746	annotated corpus for
0.0491536195	start from
0.0491494164	the expressiveness
0.0491488121	estimated on
0.0491483446	extent to
0.0491483089	a small amount of data
0.0491422550	a core
0.0491377541	the source of
0.0491341335	to take into account
0.0491340903	the art on two
0.0491195327	the similarity of
0.0491183535	the most challenging
0.0491115845	a novel self
0.0491065518	a reference
0.0491006155	baselines by
0.0490995803	model uses
0.0490929232	future work on
0.0490917957	sequence model for
0.0490898856	augmentation method for
0.0490858382	the art results across
0.0490814353	this ability
0.0490783163	compare several
0.0490647332	a case study on
0.0490596684	typically only
0.0490595083	help predict
0.0490460015	the copy
0.0490431160	a novel sequence
0.0490423663	building on
0.0490255968	gain on
0.0490249344	effectively use
0.0490172944	to treat
0.0490118715	parameters across
0.0490095415	therefore propose
0.0490081589	each speech
0.0490033644	a generative model to
0.0489953538	the foundation
0.0489946897	the increase in
0.0489607961	more about
0.0489606526	different from
0.0489530906	an overall
0.0489464707	different components
0.0489430844	a connectionist
0.0489305564	use of external
0.0489164365	to machine
0.0488998887	in detail
0.0488987482	the target side
0.0488929232	evaluated on two
0.0488911091	step by
0.0488867792	age of
0.0488861788	then use
0.0488744735	for improving
0.0488738652	the understanding of
0.0488645558	to yield
0.0488617261	to top
0.0488580118	a genetic
0.0488557695	phenomena such as
0.0488510796	beyond simple
0.0488504575	a low
0.0488411105	demonstrated to
0.0488362780	in computer vision and
0.0488320510	performed at
0.0488275626	the sum
0.0488131651	a new perspective
0.0488098974	localization of
0.0487926343	to capture long
0.0487924972	correlation with
0.0487884378	metrics like
0.0487831732	success on
0.0487812947	by classifying
0.0487799419	the two types of
0.0487777117	not require
0.0487753219	improvement in terms of
0.0487684384	and naturally
0.0487671209	aids in
0.0487652437	the first part
0.0487615626	the phone
0.0487557646	the feasibility of
0.0487473150	effects of various
0.0487468127	model on two
0.0487464958	analyses show
0.0487452336	studies show
0.0487420342	toolkit for
0.0487370275	the end of
0.0487311043	used across
0.0487279884	experiment on
0.0487273021	other fields
0.0487251193	in order to learn
0.0487108370	the benefits of
0.0487075959	published on
0.0487066042	practicality of
0.0487063901	system used
0.0487008567	and especially
0.0487005581	difficult to use
0.0486988405	well with
0.0486942023	not even
0.0486903857	the user to
0.0486665359	methods do
0.0486661954	contextual information to
0.0486552846	approaches for
0.0486521799	the features of
0.0486382752	the utterance and
0.0486371140	difference in
0.0486222149	and then train
0.0486168504	with limited data
0.0486153074	efficient use of
0.0486029018	different corpora
0.0486016138	annotated dataset of
0.0485794942	sources of
0.0485791741	the realm of
0.0485737352	known to
0.0485674409	by sampling
0.0485635957	these architectures
0.0485598480	developed in
0.0485568513	and more efficient
0.0485487387	such as amazon
0.0485401333	labeled as
0.0485377662	a consensus
0.0485312237	the expense
0.0485154949	models against
0.0485108370	the degree of
0.0485103039	unsupervised way
0.0485090086	also presented
0.0484940298	the feasibility
0.0484837136	to illustrate
0.0484785615	achieves new
0.0484776262	the proposed method on
0.0484682891	by discussing
0.0484564792	this aim
0.0484510671	conference on
0.0484498098	to copy
0.0484473403	this connection
0.0484358305	the point of
0.0484350601	best model
0.0484340642	a backward
0.0484143181	a novel deep
0.0484113212	the informativeness
0.0484112803	with very few
0.0484094006	the ubiquitous
0.0483955486	leading to better
0.0483881960	a light
0.0483846897	systems do not
0.0483775114	models do not
0.0483714483	approaches in terms of
0.0483603183	important for many
0.0483541152	a lack
0.0483489511	a moderate
0.0483440883	better and more
0.0483380424	difficult due to
0.0483365001	considered by
0.0483352536	from others
0.0483331958	issues such as
0.0483296391	novel architecture
0.0483174602	the encoder and decoder
0.0483140061	capacity for
0.0483110332	the rate of
0.0483087430	these characteristics
0.0483064570	the baseline on
0.0483037706	link prediction in
0.0483015375	the fine
0.0483010756	from massive
0.0482973443	and even
0.0482930880	constraint on
0.0482918761	annotated at
0.0482874888	utilized for
0.0482864618	a recurrent
0.0482848671	the predominant
0.0482791991	released for
0.0482713038	systems use
0.0482693034	helps in
0.0482670027	the prior state of
0.0482605016	allow to
0.0482593209	any human
0.0482517478	such as sentiment analysis
0.0482494933	for many natural language
0.0482481090	a novel unsupervised
0.0482442761	different aspects of
0.0482351208	experiment with two
0.0482298067	extraction system
0.0482248186	to fit
0.0482211094	such approaches
0.0482045363	particular focus
0.0482012109	a few words
0.0481953164	an offline
0.0481900708	found in
0.0481838568	a classifier to
0.0481685496	impacts on
0.0481681846	the given context
0.0481599233	investigation on
0.0481237613	different scenarios
0.0481237119	a benchmark of
0.0480960616	working with
0.0480899485	the overall quality
0.0480848421	previous work in
0.0480765479	the meanings of words
0.0480722753	the lrl
0.0480678943	perplexity by
0.0480635864	implemented in
0.0480563422	achieves better
0.0480530045	to analyse
0.0480523801	those produced by
0.0480519621	both real
0.0480516638	conversations between
0.0480456951	performance with
0.0480421468	design of
0.0480420897	techniques like
0.0480419642	a novel data
0.0480380235	does so
0.0480331578	snippets from
0.0480231274	the distribution of
0.0480224720	the merits
0.0480195327	the frequency of
0.0480067832	exploited to
0.0479974283	the semantic structure
0.0479947412	non trivial to
0.0479837009	prevalent in
0.0479809547	only monolingual
0.0479778033	trained to
0.0479684641	enriched with
0.0479672233	architecture for
0.0479624579	2018 shared
0.0479492524	central role in
0.0479390589	first train
0.0479168466	a level of
0.0479114086	improving over
0.0478968969	also study
0.0478929280	evidence from
0.0478899610	this project
0.0478847464	used in speech
0.0478777052	architectures for
0.0478766030	length of
0.0478650042	performance on two
0.0478582712	also achieve
0.0478553855	measure of
0.0478469719	the relationships among
0.0478392508	both sources
0.0478382093	all three datasets
0.0478352895	a reasonable
0.0478303894	this effect
0.0478298598	many natural
0.0478177518	for enhancing
0.0478168396	work done on
0.0478144360	also shown
0.0478133565	while still
0.0478098974	comprising of
0.0478025509	more expensive
0.0478008960	to move
0.0477971798	extends to
0.0477970461	the task at hand
0.0477969369	the level of
0.0477928886	to grasp
0.0477738194	these two problems
0.0477648890	into four
0.0477501992	the most useful
0.0477474506	a refined
0.0477426866	the left and
0.0477384383	comments from
0.0477223162	lie in
0.0477171328	to label
0.0477161080	the portuguese
0.0477147942	guaranteed to
0.0477100774	from different modalities
0.0477091218	forced to
0.0477079825	process by
0.0477039986	in terms of rouge
0.0477025711	impact of
0.0476943335	all pairs
0.0476910699	some examples
0.0476834564	a competitive
0.0476750458	aligned to
0.0476749225	a promising
0.0476702360	do not provide
0.0476677408	potential to
0.0476669234	sequence learning with
0.0476621822	a machine learning approach to
0.0476536502	an extensive set
0.0476512240	to jointly train
0.0476497745	optimized with
0.0476492354	information across
0.0476427993	and therefore
0.0476398145	important to
0.0476348329	efficacy of
0.0476233926	but even
0.0475951405	the acoustic and
0.0475942953	the text of
0.0475941038	such scenarios
0.0475908395	orders of
0.0475769307	reported by
0.0475658232	many practical
0.0475652803	for many nlp
0.0475649278	a model to
0.0475630445	a use case
0.0475578403	a neural network to
0.0475556369	this methodology
0.0475467511	datasets contain
0.0475436407	other baselines
0.0475309229	learn to
0.0475128789	unclear to
0.0475092250	certain words
0.0474933816	become ubiquitous in
0.0474918363	against state of
0.0474897015	trained models for
0.0474855012	approaches such as
0.0474841816	several metrics
0.0474831451	necessary for
0.0474769630	inspired from
0.0474748904	various contexts
0.0474728969	to compare
0.0474670012	for studying
0.0474610613	first construct
0.0474567390	yet efficient
0.0474563372	not able
0.0474302295	both quantitative
0.0474251849	with varying
0.0474222448	performance under
0.0474212568	left to
0.0474021536	the proposed system
0.0473827937	possible to achieve
0.0473736213	from text to
0.0473703778	one reason
0.0473570270	to differentiate between
0.0473560529	a novel hybrid
0.0473475093	this setup
0.0473296391	novel unsupervised
0.0473282089	the grounding
0.0473194228	resources available
0.0473158232	provides additional
0.0473083872	generation model to
0.0473039414	a methodology
0.0472982148	and dynamically
0.0472912165	algorithms for
0.0472878608	issue of
0.0472842152	for estimating
0.0472621626	adaptable to
0.0472607761	problem due to
0.0472074899	by treating
0.0471889702	by reducing
0.0471650418	overall system
0.0471593309	proof of
0.0471494626	to go beyond
0.0471390756	the average
0.0471185359	for different tasks
0.0471146803	selected for
0.0471136751	a stream
0.0471104373	required to
0.0470924529	use of natural language
0.0470854268	way to
0.0470799318	an actual
0.0470741565	by conditioning
0.0470575075	and still
0.0470521799	a better understanding of
0.0470331220	corpus for
0.0470327380	in different languages
0.0470222718	the gated
0.0470217624	get better
0.0470208028	engagement with
0.0470124283	the community to
0.0470095782	model gives
0.0470023271	by exploring
0.0469983607	the same way
0.0469939547	achieves very
0.0469824695	metrics such as
0.0469816594	a key component of
0.0469757840	used to automatically
0.0469627638	this form
0.0469609120	experiments on various
0.0469575742	these models on
0.0469493249	the most widely used
0.0469456156	resource for
0.0469443665	the change in
0.0469320226	the new dataset
0.0469318619	three other
0.0469179355	10 languages
0.0469159874	optimized for
0.0469047404	resolution system
0.0468994526	average over
0.0468962982	commonly used in
0.0468958043	also very
0.0468902422	these aspects
0.0468881343	the statistical properties
0.0468870151	the time of
0.0468839650	utilized to
0.0468838561	problem in
0.0468819505	different variants
0.0468756869	the best performing models
0.0468750082	advancement of
0.0468680788	by collecting
0.0468675149	such biases
0.0468664304	amount of unlabeled
0.0468639410	taken into
0.0468570524	the method of
0.0468525692	relevance of
0.0468518369	weighted by
0.0468452567	inherent in
0.0468440555	means of
0.0468386677	written in different
0.0468362536	effort on
0.0468350802	adopted to
0.0468325213	thus propose
0.0468243665	this model on
0.0468201536	formulation of
0.0468171342	but semantically
0.0467903917	the considerable
0.0467735232	to hindi
0.0467663514	two approaches
0.0467430336	by solving
0.0467234943	opportunities for
0.0467195327	the strength of
0.0467121181	given sentence
0.0467042499	an explosion of
0.0467020163	to ask questions
0.0467000486	score on
0.0466996491	few labeled
0.0466959999	a gated recurrent
0.0466901079	class of
0.0466880788	further develop
0.0466857122	this contribution
0.0466839569	also suggest
0.0466811223	any type
0.0466724901	more coherent
0.0466695562	models such as
0.0466690588	trained by
0.0466684167	in two steps
0.0466680705	both structured
0.0466653786	the content in
0.0466561014	present here
0.0466342037	also contains
0.0466308741	to shed light
0.0466287060	a framework to
0.0466161893	used for training
0.0466144419	a focus on
0.0466117785	from large text
0.0466100061	latent variable to
0.0465983366	in combination with
0.0465979279	in such cases
0.0465933833	technique to
0.0465631675	order to
0.0465565185	scale analysis of
0.0465518172	a real
0.0465489450	aid in
0.0465466476	tuned for
0.0465411128	built by
0.0465285734	to paraphrase
0.0465195327	the rest of
0.0465108370	the content of
0.0465108370	the prediction of
0.0464950414	the word error
0.0464906071	the first one
0.0464895781	inconsistent with
0.0464839611	to update
0.0464797790	field of
0.0464774448	requirement in
0.0464730315	and deeper
0.0464717052	limited set of
0.0464620356	the value of
0.0464493295	with other state of
0.0464408295	gain more
0.0464280089	the new state of
0.0463942601	of three different
0.0463681832	most commonly
0.0463612753	often difficult
0.0463566068	areas of
0.0463545378	tend to use
0.0463513351	content within
0.0463424351	difficult to find
0.0463376050	directly on
0.0463340286	different variations
0.0463269384	a local
0.0463108217	the most effective
0.0463083872	based method to
0.0462939171	a variety of domains
0.0462931660	used as input
0.0462926933	the geometry
0.0462871968	by as much as
0.0462795597	vital to
0.0462775356	usually not
0.0462709153	standards for
0.0462613057	some specific
0.0462476934	to fool
0.0462396171	queries from
0.0462261754	task compared to
0.0462257036	the attention mechanism to
0.0462118576	objective function to
0.0461888231	datasets used in
0.0461865989	2019 shared task on
0.0461826140	the diversity and
0.0461802252	syntax and semantics of
0.0461698416	not necessary
0.0461618944	first study
0.0461612777	several key
0.0461602204	near state
0.0461572737	the meanings of
0.0461398223	do not contain
0.0461393641	three classes
0.0461336091	this paradigm
0.0461334151	to isolate
0.0461334151	to alter
0.0461226127	same text
0.0461170342	deployment of
0.0461164899	generalizes to
0.0461095083	more reasonable
0.0461092804	strategy for
0.0460998540	the vector representation
0.0460943165	family of
0.0460929957	ubiquitous in
0.0460922963	an optimization
0.0460709614	a knowledge base of
0.0460626917	very popular
0.0460608818	different contexts
0.0460542099	designed with
0.0460453918	on two corpora
0.0460380797	relatively large
0.0460363452	by enabling
0.0460195383	by translating
0.0460171342	then conduct
0.0460061620	dataset with
0.0460018312	order to make
0.0459962443	in order to detect
0.0459939547	works better
0.0459938168	allowing for
0.0459914833	first work
0.0459880048	the research in
0.0459854369	this problem as
0.0459838568	and efficiency of
0.0459828054	not effective
0.0459716677	and then fine
0.0459611566	work suggests
0.0459585427	created for
0.0459561097	as prior
0.0459524872	into three
0.0459478002	light of
0.0459434329	many state
0.0459346039	in other words
0.0459298321	the whole input
0.0459171832	speech system
0.0459129957	not applicable
0.0459108369	success at
0.0459102778	measured on
0.0459074899	two views
0.0458861655	optimized by
0.0458851702	collected by
0.0458719411	a variety of nlp
0.0458610894	consisting of over
0.0458595171	the two components
0.0458561374	proposed for
0.0458471739	study whether
0.0458407554	the second best
0.0458301969	the automatic detection
0.0458240703	comparing with
0.0458201536	collections of
0.0458086157	speech without
0.0458071863	several downstream
0.0458070133	performance in terms of
0.0458068132	prove to
0.0457914463	system learns
0.0457749131	respectively in
0.0457680788	while reducing
0.0457661147	event detection in
0.0457507056	a part of
0.0457343155	demonstrated on
0.0457283336	only unlabeled
0.0457209229	different input
0.0457175112	efficiently with
0.0457136314	by measuring
0.0457107353	to include
0.0457097464	a multitude of
0.0457059364	not well
0.0456872323	to new domains
0.0456815515	semantic meaning of
0.0456777841	a similar
0.0456634705	estimated using
0.0456609863	a preprocessing
0.0456586626	discussion on
0.0456550861	considered in
0.0456526191	mood of
0.0456509172	the absence
0.0456497262	a dramatic
0.0456454224	shift from
0.0456346396	the backbone
0.0456252577	the advancement
0.0456240970	improvements across
0.0456098958	commonly used for
0.0455890527	just one
0.0455806803	some important
0.0455680788	also investigated
0.0455411494	all text
0.0455394203	the code for
0.0455389937	different topics
0.0455286271	between two languages
0.0455253834	these two models
0.0455078685	as measured by
0.0455060960	placed on
0.0454929686	interpretability of
0.0454807089	tools for
0.0454771266	data collection for
0.0454737691	various features
0.0454673183	most predictive
0.0454635258	a very high
0.0454466741	stream of
0.0454458897	work addresses
0.0454283393	the landscape
0.0454251273	summary by
0.0454240418	leading to more
0.0454189941	widespread use of
0.0454163263	the same way as
0.0454062540	guidance from
0.0454021020	refer to as
0.0453939187	better utilize
0.0453839718	languages do not
0.0453811014	approach allows
0.0453761533	various methods
0.0453737782	words used in
0.0453733755	to take advantage
0.0453540965	obtain better
0.0453503602	does not provide
0.0453492385	this formulation
0.0453407021	move from
0.0453381058	used during
0.0453306478	this fact
0.0453280153	completeness of
0.0453175399	first present
0.0453159831	three modalities
0.0453120620	same number of
0.0452997578	directions for
0.0452977197	good at
0.0452844725	a statistical analysis
0.0452819170	of different modalities
0.0452636091	two agents
0.0452631412	bleu on
0.0452623190	to new languages
0.0452531037	presented at
0.0452406946	a ranked
0.0452401484	often required
0.0452379833	interface for
0.0452358937	first time
0.0452347455	further design
0.0452297467	the generator to
0.0452195876	conducted to
0.0452168275	data used for
0.0452141219	self attention mechanism to
0.0452094374	weakness of
0.0452035225	and systematically
0.0451973698	the significance
0.0451836808	a fixed number of
0.0451806113	the code and
0.0451702734	self attention network for
0.0451690431	become very
0.0451599195	than existing
0.0451584900	down to
0.0451582765	schema for
0.0451347668	also help
0.0451297641	constrained to
0.0451281387	given by
0.0451254164	most often
0.0451195707	to diagnose
0.0451173460	an improvement
0.0451144740	the model using
0.0451139410	gives better
0.0451087095	research into
0.0451081130	such studies
0.0450909870	basis of
0.0450869076	properties such as
0.0450685819	the automation
0.0450669378	tested in
0.0450660423	a couple of
0.0450608757	tagged with
0.0450452336	method allows
0.0450438584	the investigation of
0.0450402052	system over
0.0450389059	a wider range of
0.0450370551	almost all
0.0450297098	the black
0.0450077070	translation without
0.0450066256	used to capture
0.0450061524	between words
0.0449880048	the encoder and
0.0449862706	a very simple
0.0449707419	reduced to
0.0449697748	stages of
0.0449685442	and thus
0.0449679666	of words with
0.0449678683	a suite
0.0449621050	a prerequisite
0.0449580405	as measured
0.0449361566	an inverse
0.0449302727	to rely on
0.0449192116	rest of
0.0449161276	preservation of
0.0449161008	contribution of
0.0449089244	agreement on
0.0449083196	for text understanding
0.0449017662	other state
0.0448956711	this success
0.0448834634	then discuss
0.0448812173	a third of
0.0448769463	size of
0.0448648847	correlations with
0.0448603969	variability of
0.0448600369	the syntax and
0.0448590430	the versatility
0.0448513203	language models such as
0.0448432674	thought to
0.0448396803	performed with
0.0448344979	the last two
0.0448308568	presented in
0.0448299216	more detail
0.0448278840	work well in
0.0448268258	the popularity
0.0448257131	to other languages
0.0448203515	taken as
0.0448203217	of words from
0.0448148881	the variety of
0.0447967845	the question and
0.0447911079	learns from
0.0447803012	fit for
0.0447716499	recognized by
0.0447649942	made in
0.0447648890	but less
0.0447478924	for reasoning over
0.0447423489	the similarities between
0.0447386113	make use
0.0447302675	addressed in
0.0447298844	model leads to
0.0447229018	three datasets
0.0447204733	this year
0.0447198872	automatically by
0.0446858851	to perceive
0.0446792770	a system for
0.0446721696	in many real
0.0446704000	both approaches
0.0446687318	the study of
0.0446684438	of word embeddings for
0.0446587722	an upper
0.0446531552	among words
0.0446521799	a factor of
0.0446465767	bags of
0.0446351318	two speakers
0.0446340286	more direct
0.0446339662	question into
0.0446294458	understanding system
0.0446264462	optimization of
0.0446183517	only provides
0.0446145054	the best performance in
0.0446106018	geometry of
0.0446078078	conducted on two
0.0446049434	generality of
0.0445975782	place for
0.0445968383	on three datasets
0.0445897419	flaws in
0.0445889924	conducted by
0.0445859280	applicability of
0.0445741940	the best possible
0.0445600731	explored for
0.0445595083	better performances
0.0445521799	in light of
0.0445516451	accuracy over
0.0445416719	emerge in
0.0445357699	the best reported results
0.0445331941	new corpus of
0.0445161559	far from
0.0445074171	these explanations
0.0444774248	of questions and answers
0.0444729379	a word from
0.0444719309	computed from
0.0444634946	the previous work
0.0444556566	most natural language
0.0444547578	basis for
0.0444494005	also improved
0.0444493015	both natural language
0.0444454511	not exist
0.0444443248	proposed by
0.0444340293	a first attempt
0.0444181111	that most
0.0444146314	to work well
0.0444064501	enhanced with
0.0444012054	employed as
0.0443883403	achieved by using
0.0443851391	to shallow
0.0443805384	the availability of large
0.0443793569	a very challenging
0.0443741382	the automatic generation
0.0443726947	a hot
0.0443686945	at \ url https
0.0443629531	to predict whether
0.0443584281	plans for
0.0443578223	this means
0.0443461206	learn from
0.0443337009	deployed in
0.0443336091	all previous
0.0443323819	labeled with
0.0443282089	and publicly
0.0443251273	summary from
0.0443250980	a new dataset for
0.0443215904	the ace
0.0443164116	a max
0.0442976659	these properties
0.0442912764	also achieves
0.0442837009	captured in
0.0442701469	to make full use
0.0442550936	insufficient for
0.0442453805	presented as
0.0442440791	performs at
0.0442408705	than conventional
0.0442400560	in various natural language
0.0442384297	also evaluate
0.0442369481	to lower
0.0442245117	plan to
0.0442242499	a host of
0.0442191585	comes to
0.0442164656	a novel way of
0.0442110385	benchmark for
0.0442058338	these predictions
0.0442008743	accuracy at
0.0441976734	notes from
0.0441935639	models need to
0.0441904223	an increase
0.0441878384	only requires
0.0441875800	more recently
0.0441844779	novel features
0.0441815606	only few
0.0441653726	the optimization of
0.0441626575	formulation for
0.0441569250	essential to
0.0441564315	tendency of
0.0441494609	generalizing to
0.0441414408	baselines on two
0.0441413159	from two different
0.0441410539	set of possible
0.0441400547	presented with
0.0441342294	to specify
0.0441015549	the nist
0.0441013925	all code
0.0440975724	by stacking
0.0440967553	to direct
0.0440909596	the proposed approach on
0.0440906201	methods on two
0.0440849048	predicted from
0.0440808848	encoded with
0.0440780193	look for
0.0440639050	the beginning of
0.0440591420	simplicity of
0.0440436759	inadequate for
0.0440436407	two popular
0.0440415727	increasing attention in
0.0440406463	allow for
0.0440188356	between concepts
0.0440166525	without access
0.0440159712	trained under
0.0440145197	a complex
0.0440077526	results on several
0.0439995223	same meaning
0.0439975767	a validation
0.0439952277	system called
0.0439889395	the inner workings of
0.0439828447	a top
0.0439772574	tuned by
0.0439755808	regard to
0.0439631887	of different approaches
0.0439611566	work demonstrates
0.0439440094	released to
0.0439327717	also make
0.0439300189	calculus for
0.0439157304	consequence of
0.0439101262	the research on
0.0439056038	located in
0.0439037171	to exist
0.0439003835	experience with
0.0438964720	many potential
0.0438928008	helpfulness of
0.0438895074	also demonstrates
0.0438869605	the collection of
0.0438698232	quantities of
0.0438689241	overcome by
0.0438592992	on several datasets
0.0438374088	identified in
0.0438353267	a theoretical
0.0438298470	a demo
0.0438221510	matter of
0.0438210770	only uses
0.0438159431	of magnitude faster
0.0438096819	specialized for
0.0438048001	a total
0.0437997621	several competitive
0.0437997621	many downstream
0.0437988038	both natural
0.0437962824	even better
0.0437914463	four tasks
0.0437839396	effective than
0.0437839086	an ability
0.0437777423	scores over
0.0437764689	a consistent
0.0437715640	explanation for
0.0437708555	a tool
0.0437613596	the trade off
0.0437613447	for text to
0.0437543526	very first
0.0437409847	the brown
0.0437370275	the result of
0.0437202283	to craft
0.0437124393	also do
0.0437108370	the cost of
0.0437091263	the most widely
0.0437057941	in many downstream
0.0437042961	risk of
0.0437030153	generalisation of
0.0436973204	draw on
0.0436966503	three categories
0.0436877788	various applications
0.0436865887	challenges such as
0.0436855626	total of
0.0436765086	available for training
0.0436669665	entities and relations in
0.0436438400	a new annotated
0.0436425458	decomposition of
0.0436347054	two groups
0.0436320404	a k
0.0436247833	a commonsense
0.0436236145	degradation of
0.0436201311	for predicting
0.0436069849	of sentences from
0.0436050060	for many applications
0.0435964278	resulting from
0.0435928772	every word
0.0435884493	becomes more
0.0435860172	applies to
0.0435597898	in reality
0.0435573146	most recent
0.0435538732	the most successful
0.0435502128	a turing
0.0435458501	the compositionality of
0.0435448445	novel framework
0.0435420342	guidelines for
0.0435401482	the task of mapping
0.0435391971	possible to build
0.0435387373	different architectures
0.0435329591	input into
0.0435305844	for training and evaluating
0.0435304398	both in terms
0.0435267448	challenging task of
0.0435234658	for creating
0.0435202161	to other domains
0.0435201436	two variants
0.0435195327	the validity of
0.0435171393	submission for
0.0435061203	some existing
0.0435052104	connected by
0.0434979648	the same amount of
0.0434881382	the literature on
0.0434880184	in various nlp
0.0434828521	only small
0.0434702229	new dataset of
0.0434648893	feedback from
0.0434626178	utterances into
0.0434267234	by evaluating
0.0434185930	alternatives for
0.0434174552	the adequacy
0.0434173010	plausibility of
0.0434089580	transfer across
0.0434069221	into sub
0.0434051024	the interest of
0.0433962200	to rank
0.0433956233	ranked by
0.0433930749	a pivotal
0.0433620871	do not use
0.0433514566	for solving
0.0433458501	the granularity of
0.0433408005	those produced
0.0433396575	well with human
0.0433394829	an adapted
0.0433280153	expressiveness of
0.0433223823	several languages
0.0433212949	restriction of
0.0433203830	network model with
0.0433131127	stated in
0.0433093588	work together
0.0433025939	integration with
0.0433012639	to lead to
0.0432974808	and back
0.0432966079	both high
0.0432962415	an inherent
0.0432909613	a mixture of
0.0432878812	best results
0.0432877925	five benchmark
0.0432873756	any given
0.0432841282	the distinction
0.0432791827	change over
0.0432763457	sense disambiguation using
0.0432737473	works with
0.0432701162	adapts to
0.0432696606	to suffer
0.0432640588	a word error rate of
0.0432635422	the rapid development
0.0432595657	model needs to
0.0432592479	controlled by
0.0432554040	a challenging task in
0.0432521707	an empirical investigation of
0.0432473643	the name of
0.0432472813	to face
0.0432451433	a novel multi
0.0432321970	compared with several
0.0432195327	the absence of
0.0432074744	introduced in
0.0431971624	start with
0.0431954445	the contrary
0.0431910622	used in nlp
0.0431909847	the connectionist
0.0431875800	also shows
0.0431851970	used to develop
0.0431849222	a novel architecture
0.0431820092	embedding model for
0.0431783134	new benchmark
0.0431673183	four types
0.0431645800	this document
0.0431632359	without using
0.0431595083	not suitable
0.0431562923	minutes of
0.0431488988	limitations of
0.0431320332	various components
0.0431186110	for different types of
0.0431164900	not know
0.0431016035	and limitations of
0.0431002139	the sensitivity
0.0430945560	the application of
0.0430858160	origin of
0.0430851085	this finding
0.0430737996	bound on
0.0430696538	humans use
0.0430680788	these ideas
0.0430623378	best performance
0.0430531807	cost of
0.0430521501	also improves
0.0430471071	several approaches
0.0430421109	accurately than
0.0430339322	two types
0.0430254603	or absence
0.0430244022	a high degree of
0.0430240969	this basis
0.0430231274	the similarity between
0.0430028044	error rate on
0.0429880173	the automatic detection of
0.0429851334	entry to
0.0429824781	a sentence as
0.0429741643	space of
0.0429721321	observed in
0.0429603278	unit for
0.0429557097	both user
0.0429541554	sufficient for
0.0429485618	overall results
0.0429475965	score over
0.0429466650	a new large
0.0429429806	evaluated with
0.0429250177	inconsistencies in
0.0429223763	both visual
0.0429189581	comes with
0.0429131053	do not rely
0.0429088522	for english to
0.0429054040	the list of
0.0428962454	this work describes
0.0428883510	studied in
0.0428834634	find evidence
0.0428830298	produced in
0.0428736091	two problems
0.0428701220	such as named entity
0.0428669018	and achieve new state of
0.0428632752	the subset of
0.0428622198	employed to
0.0428464745	compare with
0.0428444909	to take into
0.0428316057	primarily on
0.0428286642	entities such as
0.0428148881	the knowledge from
0.0428060610	to justify
0.0428038598	by annotating
0.0427858240	the mel
0.0427822656	the usage of
0.0427809074	results in many
0.0427771679	then present
0.0427488121	modification to
0.0427229948	a simple method to
0.0427176560	modification of
0.0427160734	the pointer
0.0427146538	not designed
0.0427030711	the experimental results of
0.0427010879	adopted as
0.0426957126	to examine
0.0426919795	formed from
0.0426914463	also identify
0.0426906566	facets of
0.0426906566	varieties of
0.0426861788	than other
0.0426767918	robustness of
0.0426761266	more efficient and
0.0426734213	benefits of
0.0426698931	a corpus for
0.0426654578	most natural
0.0426580354	to recognise
0.0426565423	framework provides
0.0426556604	two domains
0.0426515336	used to make
0.0426478441	intersection of
0.0426365772	an insight
0.0426340143	a strategy to
0.0426338305	the other two
0.0426287785	architectures such as
0.0426245714	a system based
0.0426217182	and carefully
0.0426074899	still require
0.0426065407	potential of
0.0425975637	applicable for
0.0425885628	many cases
0.0425859641	strengths of
0.0425835251	especially important
0.0425742415	different datasets and
0.0425710159	sample from
0.0425670916	prior work by
0.0425614843	do not fully
0.0425589157	candidates from
0.0425581075	ability to generalize to
0.0425569142	system for generating
0.0425462542	best performing system
0.0425402758	between entities
0.0425362194	the aim to
0.0425338946	the input and output
0.0425251668	then learn
0.0425206890	for generating
0.0425180788	while simultaneously
0.0425088765	the supporting
0.0425073565	then propose
0.0425061473	a multitude
0.0425040947	running on
0.0425021803	to talk about
0.0424995271	implemented on
0.0424970342	requirement of
0.0424942803	the lead
0.0424883476	the coarse
0.0424820309	principles of
0.0424788768	particular attention
0.0424761749	even higher
0.0424760067	this strategy
0.0424669730	mechanism by
0.0424623294	at best
0.0424580715	introduced to
0.0424430285	the null
0.0424406443	art performance in
0.0424357625	yet challenging
0.0424301186	diversity of
0.0424159117	models used for
0.0424050387	specified by
0.0424017277	often used to
0.0423962186	readings of
0.0423903976	other algorithms
0.0423797165	empirical study to
0.0423780193	on eight
0.0423736091	this procedure
0.0423710057	to contribute
0.0423683960	the asr system
0.0423668852	proves to
0.0423607122	by capturing
0.0423581309	or no
0.0423555568	the adoption
0.0423514138	performed in
0.0423501949	information among
0.0423496874	unavailable for
0.0423431946	used to analyze
0.0423414899	validated on
0.0423366651	current best
0.0423358729	a case study in
0.0423331221	model performs better
0.0423316061	to surpass
0.0423201345	the reduction of
0.0423035264	managed to
0.0422951803	the internal structure
0.0422856263	perplexity on
0.0422750242	these extracted
0.0422720262	not scalable
0.0422719300	abstractive summarization with
0.0422697990	employed by
0.0422696691	released with
0.0422658647	novel system
0.0422583000	each agent
0.0422562923	matched to
0.0422396570	generally more
0.0422214807	ignored by
0.0422089197	scores across
0.0422084315	an advantage
0.0422041837	this context
0.0422029897	resources such as
0.0422026975	the previous best
0.0421956467	regularities in
0.0421878384	two categories
0.0421791353	limited due to
0.0421786599	certain tasks
0.0421785141	automatically from
0.0421775563	the guidance
0.0421680697	performance with respect to
0.0421679363	to design
0.0421613358	codes from
0.0421611717	by simply
0.0421571266	not perform well on
0.0421546533	vary in
0.0421492227	and easy to
0.0421460101	these biases
0.0421382757	goal of
0.0421348865	vary from
0.0421234353	in many areas
0.0421186705	between two entities
0.0421154060	both standard
0.0421132253	following questions
0.0421108973	to take
0.0421066160	an element
0.0420987553	both question
0.0420972602	languages such as
0.0420872673	experiment with different
0.0420820461	appropriate for
0.0420723683	major challenge in
0.0420682892	various tasks
0.0420617829	deployed on
0.0420510916	a variety of languages
0.0420479590	comment on
0.0420384155	optimize for
0.0420261735	generation tasks such as
0.0420134953	these requirements
0.0420129141	combination with
0.0420087047	this work demonstrates
0.0419894793	facilitate further
0.0419888502	challenging task due to
0.0419836951	preparation of
0.0419834567	thorough evaluation of
0.0419827857	in contrast to existing
0.0419806783	a large corpus of
0.0419463521	system for automatic
0.0419404353	feasible for
0.0419371545	optimizing for
0.0419334359	for building
0.0419329836	tedious and
0.0419259195	argue for
0.0419222461	new metrics
0.0419209851	two perspectives
0.0419205656	also apply
0.0419161276	drawback of
0.0419158232	more complete
0.0419112173	tuned on
0.0419078484	the way to
0.0419051750	the property of
0.0418977991	also presents
0.0418898583	various languages
0.0418871030	on three public
0.0418838166	demonstration of
0.0418822708	build on
0.0418807320	several important
0.0418736341	improvements in
0.0418645550	research community to
0.0418575041	time cost
0.0418528023	approach does not
0.0418440555	ensemble of
0.0418435164	recipe for
0.0418432143	host of
0.0418428521	novel interactive
0.0418381347	new framework
0.0418318650	use of data
0.0418261439	by half
0.0418215867	a principled way
0.0418208465	such as gpt
0.0418020087	then demonstrate
0.0417961945	two tasks
0.0417943946	various sources
0.0417938308	a method based on
0.0417899185	the expressive power
0.0417857036	successful in
0.0417688626	also called
0.0417605681	much work
0.0417581566	impossible to
0.0417546646	parsing over
0.0417397804	both syntactic
0.0417161785	utilized as
0.0417146097	in other tasks
0.0417133852	the cause of
0.0417131588	various baselines
0.0417019086	the attention of
0.0416969812	learning architecture for
0.0416888390	both local
0.0416752327	limited in
0.0416752096	the computer vision
0.0416742415	of thousands of
0.0416695971	employed for
0.0416676950	all metrics
0.0416643742	an ambiguous
0.0416629072	able to achieve state
0.0416621379	future work in
0.0416556604	many approaches
0.0416512506	jointly with
0.0416492181	in various natural
0.0416466240	to affect
0.0416406527	also applied
0.0416398441	many others
0.0416251004	in four languages
0.0416174184	already in
0.0416174011	between source and target
0.0416142495	studied by
0.0416042264	two different datasets
0.0416020130	a new model called
0.0415838677	component in
0.0415831838	two layers
0.0415822656	the area of
0.0415711886	corpus of over
0.0415497352	decoder model to
0.0415476880	in various tasks
0.0415319335	these phenomena
0.0415018460	part due
0.0414958148	the task at
0.0414917551	the shared task on
0.0414886324	most previous
0.0414794465	by way of
0.0414638188	element in
0.0414506062	study provides
0.0414505570	the automatic generation of
0.0414414973	proposals for
0.0414402490	parameters than
0.0414383851	the problem by
0.0414382928	concept of
0.0414342124	generalized to
0.0414334486	also compared
0.0414128697	first uses
0.0414097455	by implementing
0.0414007949	only at
0.0413933840	more specific
0.0413917174	refinement of
0.0413916026	allows users to
0.0413906851	into five
0.0413895074	through extensive
0.0413889633	useful resource
0.0413823343	further analysis
0.0413778655	missing or
0.0413753987	not depend
0.0413629947	to find relevant
0.0413615321	the vast majority of
0.0413526741	these studies
0.0413481100	the information from
0.0413361190	this new task
0.0413334524	difficulties in
0.0413261624	relations between two
0.0413203778	no prior
0.0413129532	outbreak of
0.0413111197	a held out
0.0413103909	built using
0.0413095548	such as bleu
0.0413089141	transformed to
0.0413034919	between words in
0.0412963420	an unseen
0.0412936314	an informative
0.0412462392	six different
0.0412461354	available datasets
0.0412347455	several challenges
0.0412281552	each speaker
0.0412222014	correctness of
0.0412192928	proposed system
0.0412182953	various metrics
0.0412045072	the entropy of
0.0412023706	many techniques
0.0411913381	no existing
0.0411810712	representations via
0.0411782480	capability for
0.0411703196	for extracting
0.0411679355	two modalities
0.0411617385	by constructing
0.0411612563	named as
0.0411568882	the intersection of
0.0411378560	the model to
0.0411355131	reported on
0.0411320850	or neutral
0.0411301179	the analysis of
0.0411280701	manage to
0.0411243940	and generally
0.0411200477	one particular
0.0411147269	defined in
0.0411071756	the set of
0.0411062191	evidence for
0.0410983213	requirement for
0.0410958259	a range of nlp
0.0410945560	the combination of
0.0410931761	while taking
0.0410864464	an increased
0.0410811741	provides evidence
0.0410702277	other techniques
0.0410698173	these changes
0.0410615437	several methods
0.0410578472	baseline by
0.0410549220	bound to
0.0410546850	reliability of
0.0410420831	then describe
0.0410412334	this task as
0.0410410250	novel attention
0.0410394203	a role in
0.0410392362	on two widely
0.0410280579	preference for
0.0410182023	struggle to
0.0410054040	an issue
0.0409977593	used in natural language
0.0409855405	a gain of
0.0409827857	a method for automatically
0.0409810059	both human
0.0409638679	an understanding
0.0409582413	by designing
0.0409559771	available data and
0.0409550410	art methods by
0.0409382767	iterations of
0.0409228057	these two approaches
0.0409219211	to struggle
0.0409205656	also analyze
0.0409142807	work introduces
0.0409135857	a word error
0.0409111780	applied to new
0.0409089322	each target
0.0409043737	only needs
0.0408967913	the best result
0.0408940621	these fields
0.0408823819	provided for
0.0408801869	expertise of
0.0408783648	the scan
0.0408742196	evaluation results of
0.0408736091	this scenario
0.0408695047	problematic for
0.0408634888	paradigm for
0.0408598688	place on
0.0408585433	decrease of
0.0408537603	art models by
0.0408465302	fed with
0.0408448241	vectors into
0.0408358637	these efforts
0.0408287023	a need for
0.0408219658	not directly
0.0408196596	the adequacy of
0.0408180788	two modules
0.0408082282	costly to
0.0408033026	discussed in
0.0408013666	two different models
0.0407914788	contain various
0.0407849162	extent of
0.0407688626	two challenges
0.0407632307	utilized in
0.0407427212	through time
0.0407412232	by highlighting
0.0407412182	the first shared task
0.0407313392	against several
0.0407294866	by making
0.0407195327	a survey on
0.0407132029	the depth of
0.0407130269	of different kinds
0.0407108370	the advantages of
0.0407083067	a useful
0.0407062263	although several
0.0407027617	by controlling
0.0407025303	further analyze
0.0407009013	not limited to
0.0406875800	both cases
0.0406839408	such as squad
0.0406772559	adapted from
0.0406672577	arguments from
0.0406662552	context into
0.0406594203	the grammar of
0.0406557316	needed in
0.0406466063	both global
0.0406442187	pipeline for
0.0406429833	spread of
0.0406427081	also known
0.0406399707	take as
0.0406369605	the differences in
0.0406298460	provided with
0.0406296815	sufficient to
0.0406269263	agreement with
0.0406196200	used to solve
0.0406175080	a costly
0.0406103947	grounded in
0.0405969772	proposed to
0.0405957756	a better understanding
0.0405943197	to cover
0.0405927786	behavior of
0.0405888956	necessity of
0.0405557140	clearly show
0.0405513925	several applications
0.0405481097	amount of labeled
0.0405317267	various techniques
0.0405265410	more focused
0.0405225637	controlling for
0.0405223163	allows for
0.0405173079	developed as
0.0405114093	questions such as
0.0405074171	all settings
0.0405005910	segments from
0.0404941702	model does not
0.0404862255	the model on
0.0404696934	of millions of
0.0404648374	datasets such as
0.0404370904	to participate
0.0404360265	to discriminate between
0.0404329561	this line of research
0.0404303675	drawbacks of
0.0404268007	in many nlp applications
0.0404244687	a drop in
0.0404032999	natural way to
0.0404032718	usefulness for
0.0403896954	for many languages
0.0403855133	the same level of
0.0403846641	in many nlp
0.0403711488	do not appear in
0.0403537600	elements from
0.0403451149	obtained for
0.0403391161	contrast to
0.0403309809	often do
0.0403292919	do not appear
0.0403280153	curation of
0.0403223009	various forms
0.0403168616	task because
0.0402971522	explainability of
0.0402958408	the origins of
0.0402848075	a method to automatically
0.0402783360	the percentage
0.0402773788	also design
0.0402716366	a first step towards
0.0402705433	with other methods
0.0402646721	these insights
0.0402634219	and weakly
0.0402360620	ensemble system
0.0402320539	other approaches
0.0402270618	not allow
0.0402093228	the necessity of
0.0401919481	the growth
0.0401911788	on four datasets
0.0401815606	less time
0.0401773513	the second method
0.0401596513	exploited for
0.0401557006	two widely used
0.0401460743	nuances of
0.0401406527	also build
0.0401404223	more traditional
0.0401390457	foundation of
0.0401356227	for various nlp tasks
0.0401332632	released by
0.0401295415	further processing
0.0401269029	pace of
0.0401267448	while making
0.0401256928	the distances between
0.0401240573	a model with
0.0401198046	an easy to
0.0401186110	for tasks such as
0.0401161377	utility of
0.0401140965	the treatment of
0.0401112356	further development
0.0401112356	good accuracy
0.0401091827	the comparison of
0.0401017265	framework uses
0.0401004512	analyzed by
0.0400954981	a support vector
0.0400822670	same amount of
0.0400769263	formalism for
0.0400721900	to domain shift
0.0400682207	different machine
0.0400667563	novel dataset
0.0400612738	both visual and textual
0.0400609900	between two
0.0400547969	and successfully
0.0400534402	the existing methods for
0.0400526718	mechanisms for
0.0400421109	powerful than
0.0400412248	further show
0.0400368944	only recently
0.0400338903	a topic of
0.0400268713	the macro
0.0400231274	the framework of
0.0400195327	the focus of
0.0400060825	tagging for
0.0400050233	second part
0.0399998716	exist in
0.0399967006	success of
0.0399706500	most used
0.0399669425	by computing
0.0399609755	but also improves
0.0399609755	the most prominent
0.0399608317	popularity of
0.0399594467	two reasons
0.0399584437	a way of
0.0399542124	comparable with
0.0399525449	periods of
0.0399517631	then introduce
0.0399439480	to occur
0.0399377594	by generating
0.0399333422	reported in
0.0399316547	guidance for
0.0399301690	this approach in
0.0399236085	the guidance of
0.0399211485	previous best
0.0398928655	benefit of
0.0398742628	interaction with
0.0398631731	area of
0.0398611141	that end
0.0398341030	of language use
0.0398339414	all relevant
0.0398255227	best knowledge
0.0398234809	all reviews
0.0398153794	on five
0.0398084561	the relation between
0.0398071213	to arrive at
0.0397938748	recent work in
0.0397852597	approximation of
0.0397815636	to consider
0.0397737556	these works
0.0397701131	unrelated to
0.0397597668	only very
0.0397590579	a number of different
0.0397529465	to train and test
0.0397370275	the style of
0.0397346850	leveraged for
0.0397291058	observed to
0.0397283391	the two approaches
0.0397273788	any existing
0.0397193463	developed at
0.0397054040	an implicit
0.0396968850	possibility to
0.0396919539	a kind
0.0396914899	suite of
0.0396882614	performs well in
0.0396861690	large margin on
0.0396827717	available through
0.0396815065	consensus on
0.0396811324	other similar
0.0396803526	used to explore
0.0396795351	on four
0.0396793270	for conducting
0.0396764887	mostly rely on
0.0396749084	from different domains
0.0396694812	behaviour of
0.0396552263	these benchmarks
0.0396550022	a new way of
0.0396249044	burden of
0.0396153206	several factors
0.0396128021	a new type of
0.0396109508	most suitable
0.0396074544	several aspects
0.0396025687	the same entity
0.0396000288	searches for
0.0395992222	other than
0.0395928772	most common
0.0395898899	4 different
0.0395819086	this method to
0.0395764887	in accordance with
0.0395701729	an emphasis on
0.0395689937	each feature
0.0395662999	further explore
0.0395644921	then uses
0.0395526287	a representation of
0.0395523756	viability of
0.0395499959	the second part of
0.0395413953	while improving
0.0395284052	then train
0.0395231916	by achieving
0.0395132796	proxy for
0.0395062012	found to
0.0394922036	submitted by
0.0394889777	difficult for
0.0394845421	released on
0.0394838843	only provide
0.0394785791	to conduct
0.0394756839	attractive for
0.0394541220	a version of
0.0394518357	a novel hierarchical
0.0394498232	sort of
0.0394470618	not take
0.0394286202	a few examples
0.0394260271	results in various
0.0394133287	attention due to
0.0394001309	to make use of
0.0393860129	improved over
0.0393782329	at inference
0.0393768300	use case of
0.0393601474	the novel task
0.0393508382	missing from
0.0393501153	more common
0.0393413929	system outputs
0.0393358716	available for
0.0393322888	leveraged to
0.0393312230	of gender bias in
0.0393216085	the research of
0.0393213927	a high level of
0.0393198251	necessary information
0.0393113136	for various nlp
0.0393046272	flexibility of
0.0392918643	still challenging
0.0392886022	also able
0.0392743847	than just
0.0392612074	while generating
0.0392594467	two fundamental
0.0392488458	picture of
0.0392445350	the correspondence
0.0392438573	scales to
0.0392381036	presented for
0.0392358136	both speech
0.0392331567	struggle on
0.0392126309	the way for
0.0392108370	a new method for
0.0392044979	good performance on
0.0391736079	such as latent dirichlet
0.0391611717	show empirically
0.0391603182	a challenge in
0.0391560857	both language and
0.0391481023	maintenance of
0.0391460743	era of
0.0391287567	the vector representations
0.0391256684	a variety of applications
0.0391150348	focus on two
0.0391128565	investigated for
0.0391060118	onset of
0.0390941206	on three benchmark
0.0390892836	by representing
0.0390885628	then learns
0.0390872673	research interest in
0.0390865087	but only
0.0390839244	readers with
0.0390774239	to contribute to
0.0390712093	by running
0.0390696648	computational cost of
0.0390591420	variance of
0.0390538990	by creating
0.0390503265	better performance in
0.0390488458	reuse of
0.0390363223	three corpora
0.0390196081	reason with
0.0390195423	both short
0.0390195175	by interacting
0.0390186147	various factors
0.0390109032	art performance for
0.0390084561	the behavior of
0.0389949930	restricted by
0.0389921863	on three
0.0389902222	most work
0.0389829467	pretrained with
0.0389793020	many different
0.0389716166	the performance of different
0.0389633777	all language
0.0389617691	novel approaches
0.0389605083	better use of
0.0389603769	for controlling
0.0389561759	each single
0.0389486413	and rapidly
0.0389382767	enrichment of
0.0389381584	much recent
0.0389318660	effective in
0.0389304101	applied to many
0.0389240838	to see
0.0389197798	an advanced
0.0389113233	in many tasks
0.0389100389	all three tasks
0.0389047755	improvement of more than
0.0388801869	mismatch in
0.0388721342	simple way
0.0388586951	complexities of
0.0388412612	during fine
0.0388389458	both datasets
0.0388330291	better than other
0.0388287023	to allow for
0.0388154626	in terms of bleu
0.0388122665	instead propose
0.0388084561	the challenge of
0.0388084561	a comparison of
0.0388065310	both word and
0.0387825667	point of view of
0.0387716366	in conjunction with
0.0387617482	most discriminative
0.0387567966	best overall
0.0387522725	various topics
0.0387506304	for sentiment analysis in
0.0387370275	the relevance of
0.0387311588	used within
0.0387277343	autoencoders for
0.0387199517	in many domains
0.0387195327	this type of
0.0387164899	poorly on
0.0387083743	performance at
0.0387081382	the output from
0.0386917100	several machine
0.0386880181	each stage
0.0386857739	instability of
0.0386854251	the relative importance of
0.0386650648	orientation of
0.0386633305	consistency with
0.0386540058	the credibility
0.0386463560	an original
0.0386452050	this paper focuses on
0.0386404505	effective way to
0.0386388829	valuable for
0.0386288300	to reason about
0.0386282264	this dataset to
0.0386204772	applied for
0.0386203778	also establish
0.0386095480	experience in
0.0385972279	utilized by
0.0385916079	system architecture
0.0385847997	division of
0.0385822656	the scope of
0.0385759503	several techniques
0.0385679278	identification from
0.0385401482	in order to support
0.0385343190	the connectionist temporal
0.0385289186	techniques such as
0.0385286401	instead of relying on
0.0385262737	first introduce
0.0385239111	different from existing
0.0385208487	types such as
0.0385161050	first describe
0.0385152510	the best f1
0.0385071657	a deeper understanding of
0.0385013573	several simple
0.0384894777	architecture with
0.0384739860	useful features
0.0384731769	scale well
0.0384691047	different combinations
0.0384674434	a problem of
0.0384546516	several problems
0.0384541364	converge to
0.0384520909	representations through
0.0384434077	the dialogue system
0.0384366412	only achieve
0.0384346907	a fast and accurate
0.0384331323	a rich source of
0.0384122235	no more
0.0384037876	also obtain
0.0384022050	a f1 score of
0.0383983604	two typical
0.0383792321	possible to learn
0.0383684674	second part of
0.0383664281	employed in
0.0383658127	shortcoming of
0.0383628605	information into
0.0383613913	interfaces for
0.0383569831	these feature
0.0383525460	of learning from
0.0383487385	the system uses
0.0383457246	comparison to
0.0383356033	the most commonly used
0.0383267552	each review
0.0383192565	all five
0.0383171245	many machine
0.0383131671	different levels
0.0383127721	by predicting
0.0383097699	make publicly available
0.0383093701	information beyond
0.0383054579	model on three
0.0382996132	applied to various
0.0382983213	motivation for
0.0382950184	gaps in
0.0382945560	the advantage of
0.0382906593	motivation of
0.0382880098	critical to
0.0382737687	scope for
0.0382680788	first define
0.0382656820	an empirical comparison of
0.0382605656	by developing
0.0382510284	also enables
0.0382409847	a log
0.0382296584	the answer from
0.0382191094	also use
0.0382019086	the estimation of
0.0381978426	an artificial
0.0381964369	explored in
0.0381925613	designed by
0.0381904223	much information
0.0381882410	the categorical compositional
0.0381829526	the origin of
0.0381810621	a treatment
0.0381808439	while increasing
0.0381796201	contains over
0.0381728133	easier for
0.0381650600	these tools
0.0381649266	factor in
0.0381572737	the type of
0.0381565869	a bag of
0.0381486001	then analyze
0.0381389894	to distinguish between
0.0381341892	more sensitive to
0.0381181106	with access to
0.0381161488	effective for
0.0381026148	successes in
0.0381004972	the consistency of
0.0380987912	novel corpus
0.0380901313	generalizability of
0.0380811667	the gap between
0.0380801599	also performs
0.0380781663	for various tasks
0.0380675406	then show
0.0380639050	the position of
0.0380576883	more consistent
0.0380528716	two scenarios
0.0380511799	relaxation of
0.0380511799	acceptance of
0.0380501729	more challenging than
0.0380468356	the best performing system
0.0380398070	no parallel
0.0380397174	four benchmark
0.0380373430	implemented for
0.0380339322	no attention
0.0380265609	a plethora of
0.0380247796	between events
0.0380105905	metric to
0.0380101179	this approach to
0.0379966662	entirely on
0.0379915479	this work addresses
0.0379817721	current time
0.0379781584	even outperforms
0.0379755808	started to
0.0379732800	made significant
0.0379678252	generalizable to
0.0379654891	in conjunction
0.0379645130	the areas of
0.0379531131	each label
0.0379499213	an approach based
0.0379443665	the ensemble of
0.0379284860	the means
0.0379284860	to experiment
0.0379280186	only consider
0.0379118340	on two large
0.0379066865	found at
0.0379045501	the overall performance
0.0378846044	reproducibility of
0.0378729941	still requires
0.0378678485	show improvements over
0.0378629957	second step
0.0378441202	made by
0.0378407400	used to provide
0.0378302575	first propose
0.0378082780	comparisons with
0.0378049605	currently used
0.0377974552	the plausibility
0.0377967845	the representations of
0.0377909087	evaluated in
0.0377689898	different communities
0.0377586397	retrieval from
0.0377510284	four datasets
0.0377450301	able to significantly
0.0377337556	little data
0.0377320981	prior work in
0.0377155746	representations of words in
0.0377135202	publication of
0.0376873788	better understanding
0.0376873788	several baseline
0.0376854251	the main goal of
0.0376743163	novel multi
0.0376631253	true for
0.0376617885	across three
0.0376564627	even with
0.0376472464	various forms of
0.0376409115	the first large
0.0376366486	the same vector
0.0376361762	and many others
0.0376274343	first place in
0.0376261993	dimensions of
0.0376260352	also train
0.0376012515	some language
0.0375962095	for new domains
0.0375674591	not found
0.0375661969	in contrast to previous
0.0375642289	a conditional random
0.0375440803	the evaluation of
0.0375401476	various problems
0.0375313847	the point of view of
0.0375287794	do not take
0.0375181814	annotated for
0.0375167820	two levels
0.0375167615	the detection and
0.0375159810	this note
0.0375138685	faithfulness of
0.0374927322	nlp tasks such as
0.0374768058	the formulation
0.0374734590	consider only
0.0374657470	the results from
0.0374643245	a new framework
0.0374586554	the united
0.0374575928	both chinese
0.0374546516	many domains
0.0374481580	a simple approach to
0.0374404223	more relevant
0.0374377908	successfully used
0.0374325236	one major
0.0374280150	adapted for
0.0374263947	some other
0.0374221289	bed for
0.0374182669	on three tasks
0.0374061628	or at least
0.0374049084	two different approaches
0.0373895009	a deep understanding of
0.0373827870	amount of text
0.0373696820	promise in
0.0373658817	a matter
0.0373642304	derivation of
0.0373470303	this task by
0.0373455025	to focus on
0.0373434851	a model of
0.0373302504	on two benchmarks
0.0373171535	in many fields
0.0373151568	the impact of different
0.0373147556	also significantly
0.0373137924	same image
0.0372987200	from other languages
0.0372967845	the query and
0.0372956628	still not
0.0372950184	artifacts in
0.0372912642	performance on several
0.0372893740	a general framework for
0.0372884133	tasks due to
0.0372733604	2 bleu
0.0372638527	three levels
0.0372611797	a challenging task for
0.0372539150	directly at
0.0372436185	the vulnerability
0.0372427212	all available
0.0372368536	the semantic meaning of
0.0372325070	available datasets for
0.0372308971	comprises of
0.0372306642	or better than
0.0372251251	on two widely used
0.0372225096	the suitability of
0.0372045072	the light of
0.0372042575	promise of
0.0372012587	several benchmark
0.0372002781	across several
0.0371929977	methods on three
0.0371783410	different sets
0.0371779978	kernels for
0.0371752130	novel metric
0.0371669468	exists for
0.0371503260	the performance of various
0.0371490900	much like
0.0371440455	first learn
0.0371400057	implemented system
0.0371371324	high accuracy of
0.0371359343	modes of
0.0371192450	to production
0.0371091431	the rise of
0.0371089714	different combinations of
0.0370952277	further evaluate
0.0370905929	the origin
0.0370844740	exists in
0.0370831793	gain in
0.0370814810	formulated in
0.0370798974	status of
0.0370748231	system relies
0.0370706416	the past few
0.0370634519	pair of
0.0370443665	a key to
0.0370262220	the prevalence
0.0370255598	approach on three
0.0370212131	appear to
0.0370040453	1 score
0.0370015973	performance on various
0.0369998871	a new method to
0.0369966829	automatic way
0.0369965850	although many
0.0369919581	also serve
0.0369860389	two advantages
0.0369781584	most prior
0.0369743072	of available data
0.0369729379	the decoder to
0.0369581875	phenomena such
0.0369489381	an agent to
0.0369414894	very close to
0.0369376398	and easy to use
0.0369284860	the reduction
0.0369220212	the total number of
0.0369214355	for analysing
0.0369209851	several times
0.0369199679	the interest
0.0369083469	more than two
0.0368929541	medium to
0.0368876957	two issues
0.0368845402	a model based on
0.0368807904	package for
0.0368779626	in order to build
0.0368670226	the performance on
0.0368374260	without external
0.0368323906	the dataset and
0.0368216214	both automatic and human
0.0368150145	adopted for
0.0368105870	augmentation for
0.0368094212	connection with
0.0367895009	to transfer knowledge from
0.0367858240	a bilstm
0.0367821756	the results show
0.0367771728	a broad set of
0.0367390400	new form of
0.0367370275	the objective of
0.0367360298	about whether
0.0367357683	origins of
0.0367284792	both semantic and
0.0367216845	further investigate
0.0367024125	various levels
0.0367012587	between characters
0.0366911857	trained from
0.0366909190	improved with
0.0366898583	several experiments
0.0366845102	used to perform
0.0366741163	the main contribution of
0.0366730200	considering only
0.0366653230	not take into account
0.0366566901	best baseline
0.0366559503	other applications
0.0366548920	thus improving
0.0366501581	new insights into
0.0366495597	no labeled
0.0366392612	induction from
0.0366389747	and other related
0.0366376957	more similar
0.0366343157	an empirical analysis of
0.0366340173	other works
0.0366340173	also introduced
0.0366223009	not easy
0.0366210523	and currently
0.0366123862	a tradeoff
0.0365995476	runs in
0.0365689937	each user
0.0365448352	exist for
0.0365431549	these new
0.0365428789	the relations among
0.0365360259	any labeled
0.0365283311	an nlg
0.0365266901	both monolingual
0.0364925197	prerequisite for
0.0364730436	first step
0.0364729941	also tested
0.0364729379	a score of
0.0364596850	strength of
0.0364593685	some research
0.0364559503	also develop
0.0364546516	several domains
0.0364539033	team for
0.0364496858	do so by
0.0364336739	the weight of
0.0364290310	in order to make
0.0364261133	many aspects
0.0364180348	used to support
0.0364131261	show improvements
0.0364117885	among other
0.0364063157	a valuable resource for
0.0363954562	for learning representations
0.0363860324	both training and inference
0.0363815065	incorporated with
0.0363727573	cells in
0.0363641301	new set of
0.0363587382	a range of tasks
0.0363429618	first pre
0.0363363420	an objective
0.0363357470	the implementation of
0.0363334134	adopted in
0.0363274518	begin to
0.0363077661	on six
0.0363041427	by means
0.0362977998	various natural
0.0362970123	not known
0.0362945560	a framework for
0.0362859088	to determine whether
0.0362766901	both positive
0.0362762220	the tradeoff
0.0362634219	the batch
0.0362618491	social media such as
0.0362459437	the nature of
0.0362442939	on four different
0.0362435664	most widely
0.0362387603	applicability in
0.0362356488	new type of
0.0362176170	both automatic and
0.0362114307	used in many
0.0362087062	also found
0.0362009296	so as
0.0361918999	operates in
0.0361917174	outcome of
0.0361856376	a potential
0.0361745151	the training of
0.0361742238	composed of two
0.0361572737	the translation of
0.0361512995	potential for
0.0361497990	the resolution of
0.0361495566	also explain
0.0361457908	a neural machine translation system
0.0361353336	a precision of
0.0361291724	method on three
0.0361287469	at least two
0.0361222745	on four benchmark
0.0361148899	contains more
0.0361021799	a bleu score of
0.0360968750	this approach on
0.0360915671	novel methodology
0.0360828587	a tool to
0.0360795372	overall performance of
0.0360695779	role of
0.0360685231	applied on
0.0360684213	appears in
0.0360583174	enable further
0.0360507962	text via
0.0360477991	novel techniques
0.0360350259	those used
0.0360197445	on two popular
0.0360167985	interest in recent
0.0360084561	a variant of
0.0360072335	investigated in
0.0360070359	baselines across
0.0360029193	an accurate and
0.0360020074	and relatively
0.0359996710	the second problem
0.0359709407	a piece
0.0359701136	often do not
0.0359623294	often used
0.0359527468	new method for
0.0359522725	some researchers
0.0359475028	also describe
0.0359362475	automation of
0.0359271748	of great interest
0.0359224308	the dependency of
0.0359209851	also prove
0.0359127243	solution for
0.0359098220	as judged by
0.0359065496	most appropriate
0.0359050818	criterion for
0.0358861731	also give
0.0358782217	a detailed description of
0.0358779626	the problem of detecting
0.0358744037	off between
0.0358711739	the extent of
0.0358684925	the identity
0.0358642607	severity of
0.0358321585	whereas in
0.0358184486	an assumption
0.0358122805	and human evaluations show
0.0357965556	considered to
0.0357940621	without explicitly
0.0357894240	mix of
0.0357731429	the severity of
0.0357667683	especially on
0.0357638527	then combined
0.0357492405	well across
0.0357454505	a nearest
0.0357452821	novel problem
0.0357405050	values of
0.0357370275	a study of
0.0357367330	array of
0.0357155104	in many natural
0.0357149880	percent of
0.0357107983	degradation in
0.0357082618	first learns
0.0357009367	in many nlp tasks
0.0356897648	transferability of
0.0356864065	representative of
0.0356798337	various types
0.0356727270	two factors
0.0356628459	advancements in
0.0356205465	an update
0.0356148056	same architecture
0.0356059688	and diversity of
0.0356006339	systems in terms of
0.0355957168	directly used
0.0355945532	in various nlp tasks
0.0355691256	used to compare
0.0355632234	the dimensions of
0.0355607252	intended for
0.0355572235	distinct from
0.0355375931	then used to
0.0355090506	the semantic structure of
0.0355061203	then provide
0.0355059503	two components
0.0355012535	not consider
0.0354962392	across four
0.0354750858	some previous
0.0354593685	some linguistic
0.0354546516	among multiple
0.0354425112	contextual information of
0.0354403996	mostly due to
0.0354338529	the availability
0.0354332230	different strategies
0.0354326392	better results in
0.0354249757	conducted with
0.0354216286	used as features
0.0354175665	performance of several
0.0354134082	effectiveness and robustness of
0.0354132259	if not
0.0353736085	the effectiveness and robustness of
0.0353533257	the model needs
0.0353521799	this results in
0.0353458501	the identity of
0.0353325236	many modern
0.0353282329	then evaluate
0.0353042499	a prototype system
0.0352945560	the power of
0.0352894852	the learning process of
0.0352834217	and tools for
0.0352769263	flexibility in
0.0352596763	by relying
0.0352538300	mentions in
0.0352510857	the cloze
0.0352462865	work focuses on
0.0352415160	take into
0.0352264456	variation in
0.0352240989	come with
0.0352079653	mean of
0.0352070442	popularity in
0.0352045072	the processing of
0.0351956586	the held out
0.0351838059	adequate for
0.0351798337	each case
0.0351642267	this paper gives
0.0351638527	any prior
0.0351608745	in tasks such as
0.0351606030	perform well in
0.0351572737	a theory of
0.0351481671	to serve as
0.0351443722	possible future
0.0351290377	obstacle to
0.0351215587	several popular
0.0351078965	allow users to
0.0350738756	not appear in
0.0350623902	used in previous
0.0350619089	still under
0.0350608871	shown by
0.0350547316	ideal for
0.0350538120	useful for many
0.0350527449	amount of training
0.0350403450	this information to
0.0350372927	significantly over
0.0350372273	to make full
0.0350290686	the dynamics of
0.0350290686	the history of
0.0350219401	a period
0.0350168955	natural way
0.0350167571	performance of two
0.0349758404	than previous
0.0349756412	environment for
0.0349687363	organized in
0.0349462375	system uses
0.0349258690	reduction of
0.0349209851	also highlight
0.0349209851	also reveals
0.0349134681	each time
0.0349105049	further discuss
0.0349088821	results on four
0.0348977806	system submitted
0.0348974239	the family of
0.0348972787	the novel task of
0.0348957919	in many ways
0.0348853989	particularly well
0.0348672101	efficient way to
0.0348657160	various pre
0.0348650712	on three publicly available
0.0348555678	if so
0.0348464523	full use of
0.0348394233	the translation quality of
0.0348358637	also extend
0.0348298702	or even better
0.0348253560	the question of
0.0348136847	reduces to
0.0348084561	the contribution of
0.0348084561	the diversity of
0.0348037617	still very
0.0347962580	the curse of
0.0347769263	exploited in
0.0347611405	the emotion of
0.0347555985	supervised way
0.0347346314	a simple method for
0.0347325767	contain only
0.0347301179	of words in
0.0347271582	performance while
0.0347193840	and expensive to
0.0346942652	while most
0.0346892319	in contrast to prior
0.0346865867	with very limited
0.0346854251	the compositional structure of
0.0346656869	better translations
0.0346331784	the model learns to
0.0346092845	the words in
0.0345996189	even for
0.0345966154	to correlate
0.0345936968	the ordering of
0.0345898584	of words into
0.0345739583	for doing
0.0345720730	people use
0.0345485502	every time
0.0345470058	usability of
0.0345372203	experiments on several
0.0345272525	incorporated in
0.0345249791	information available in
0.0345225101	for dealing with
0.0345202999	other than english
0.0345167820	also observed
0.0345135218	contains many
0.0345054093	more attention to
0.0345020089	at producing
0.0344981580	the semantic content of
0.0344875985	both unsupervised
0.0344716366	to compensate for
0.0344527804	in order to evaluate
0.0344418250	structures such as
0.0344287023	the limitations of
0.0344172787	the propagation of
0.0343886847	feedback for
0.0343769359	created with
0.0343763297	the international
0.0343622132	then generates
0.0343333680	by taking into account
0.0343315608	good performance in
0.0343263280	reported to
0.0343059840	potentials of
0.0343040053	often make
0.0343022525	operate in
0.0343004960	contain more
0.0342984720	for evaluation of
0.0342811738	for named entity recognition and
0.0342781507	described as
0.0342759358	the speed of
0.0342747355	a pool
0.0342732440	or better performance
0.0342634981	amount of time
0.0342606592	this new
0.0342569220	first publicly available
0.0342543623	first investigate
0.0342494899	between speakers
0.0342481580	the semantic similarity of
0.0342367196	takes as
0.0342294100	the simplicity
0.0342244072	both text
0.0342060869	a new dataset with
0.0342045072	the syntax of
0.0341873219	other competitive
0.0341792528	two strategies
0.0341744483	other types of
0.0341596613	expensive to
0.0341578795	data used in
0.0341481023	predictors of
0.0341462375	also allows
0.0341352022	the rapid growth of
0.0341025558	on many natural language processing
0.0340895391	the notions of
0.0340851762	the suitability
0.0340815422	capability to
0.0340787878	in other languages
0.0340720730	not possible
0.0340632285	the attributes of
0.0340579863	also benefit
0.0340528716	only require
0.0340443665	the agent to
0.0340368284	the control of
0.0340350903	many other
0.0340208288	the experiment results show
0.0340194127	replacement for
0.0339959195	various linguistic
0.0339846314	a period of
0.0339620723	factor of
0.0339577686	the sentiment polarity of
0.0339540527	also improve
0.0339516591	on three different
0.0339443665	the average of
0.0339369746	same question
0.0339336739	the adaptation of
0.0339322541	important part
0.0339245151	the annotation of
0.0339145513	both in terms of
0.0339081606	training framework to
0.0339045299	sets from
0.0338998875	by relying on
0.0338976225	performance of many
0.0338821809	optimized to
0.0338789599	advance in
0.0338716207	to train and evaluate
0.0338711739	the recognition of
0.0338443722	most modern
0.0338242499	an introduction to
0.0338175123	arise in
0.0338084561	a group of
0.0337966423	an image with
0.0337919613	new problem
0.0337870641	not reflect
0.0337846314	the dimensionality of
0.0337846314	a stream of
0.0337812217	all other
0.0337515577	but instead
0.0337371631	and effective method
0.0337346314	a comparative analysis of
0.0337255511	amount of information
0.0337231922	then explore
0.0337189898	contains multiple
0.0337132029	the progress of
0.0337067980	qualities of
0.0336989769	time consuming to
0.0336852813	no significant
0.0336842004	live in
0.0336790031	new perspective
0.0336646682	more robust and
0.0336461815	training process of
0.0336448350	for many natural
0.0336432373	possible with
0.0336249080	also incorporate
0.0336133216	among different
0.0335770928	the research community to
0.0335436987	very useful in
0.0335432029	the method used
0.0335431461	and then use
0.0335379023	talk to
0.0335167820	each type
0.0335144746	but little
0.0335088335	hypothesis by
0.0334974370	learning across
0.0334972290	a proxy for
0.0334942940	robust than
0.0334802781	also consider
0.0334800880	better than existing
0.0334794076	coefficient of
0.0334709851	also explored
0.0334697737	the sense of
0.0334670769	some aspects
0.0334543812	the veracity of
0.0334523880	further studies
0.0334435930	detected in
0.0334425583	learning process of
0.0334340173	then applied
0.0334336739	the stance of
0.0333836488	by at least
0.0333828810	beneficial in
0.0333815125	do not perform
0.0333679254	often perform
0.0333357470	the description of
0.0333350326	best known
0.0333329632	expertise in
0.0333305623	two variants of
0.0333202287	all four
0.0333004972	the core of
0.0332988723	cascade of
0.0332965265	three diverse
0.0332903430	disadvantages of
0.0332825645	at capturing
0.0332792155	the winning
0.0332747730	an expressive
0.0332716600	for sentiment analysis of
0.0332639723	the correspondence between
0.0332588839	the exploration of
0.0332462695	a fast and
0.0332445134	second approach
0.0332389182	the penn treebank and
0.0332376464	the function of
0.0332312130	same way
0.0332287023	the polarity of
0.0332287023	the identification of
0.0332209851	two core
0.0331879433	new resource
0.0331659686	methods used in
0.0331634452	available online at
0.0331529462	key to
0.0331414955	two years
0.0331199788	relatively new
0.0331186110	as input for
0.0331148247	a comparative study of
0.0331081382	the formulation of
0.0330945560	the construction of
0.0330914568	under various
0.0330750088	particular task
0.0330720936	posted in
0.0330645812	also provides
0.0330552894	an off
0.0330497730	very specific
0.0330376016	several different
0.0330314059	with less than
0.0330084561	a novel approach for
0.0330084561	the perspective of
0.0330067297	the success of deep
0.0329988458	means for
0.0329912957	new objective
0.0329871881	identification system
0.0329863054	the law of
0.0329818244	used to find
0.0329733083	the baseline by
0.0329606919	theme of
0.0329443665	the vocabulary of
0.0329426024	the precision of
0.0329426024	the outcome of
0.0329210806	two sources
0.0329116567	model to make
0.0329072737	the product of
0.0328875882	lot of attention in
0.0328789078	a comprehensive survey of
0.0328783377	method does
0.0328657506	between training and testing
0.0328370124	the prevalence of
0.0328287023	an example of
0.0328208742	studies on
0.0328084561	the introduction of
0.0328084561	the benefit of
0.0328084561	a measure of
0.0327861375	last two
0.0327822656	an algorithm for
0.0327801567	less well
0.0327704047	entity recognition on
0.0327690629	on two types of
0.0327605081	still rely on
0.0327418745	given set of
0.0327323199	the original one
0.0327312479	but still
0.0327164686	to help users
0.0326822532	the answer to
0.0326615306	provides useful
0.0326512981	the sparsity of
0.0326503982	the shortage
0.0326365571	the correlations between
0.0326322831	then develop
0.0326290031	most promising
0.0326288971	the scarcity
0.0326261674	mechanism for
0.0326068229	suitability of
0.0326036582	a relevance
0.0325979351	reductions in
0.0325798009	new class
0.0325726789	the capability to
0.0325693335	also makes
0.0325570095	with little or
0.0325507199	the deployment
0.0325354507	the semantic information of
0.0325317559	most useful
0.0325171517	the strengths of
0.0325158175	measured in
0.0325138616	several benchmarks
0.0325103308	not yield
0.0324963403	block for
0.0324963403	law for
0.0324893740	a detailed analysis of
0.0324773247	the downstream task of
0.0324597964	and disadvantages of
0.0324552277	several linguistic
0.0324546516	then compare
0.0324519294	several evaluation
0.0324323393	far more
0.0324287023	the integration of
0.0324247730	an e2e
0.0324100659	since most
0.0323865166	five different
0.0323736085	a rich set of
0.0323555678	contain many
0.0323273015	a novel framework to
0.0323255759	applied in
0.0323125585	second method
0.0323109715	that training on
0.0322976980	a corpus of over
0.0322759358	the gender of
0.0322711930	compared to several
0.0322677129	not suffer
0.0322623264	any parallel
0.0322612897	in many applications
0.0322466364	on two different
0.0322409484	of two modules
0.0322300896	both synthetic
0.0322180174	the recent work
0.0321908469	innovations in
0.0321829929	this line of
0.0321745151	the detection of
0.0321572737	the topic of
0.0321522057	also learns
0.0321287923	on two real
0.0321249796	landscape of
0.0321232331	the author of
0.0321145615	third of
0.0321137483	given set
0.0320995566	better leverage
0.0320962110	the source code and
0.0320649785	from scratch on
0.0320616579	on large amounts of
0.0320376016	at most
0.0320259476	often not
0.0320189981	for implementing
0.0320146314	an empirical evaluation of
0.0319943335	then perform
0.0319850811	for future research in
0.0319826307	2020 shared task on
0.0319805803	2015 task
0.0319546516	only limited
0.0319539990	effort in
0.0319426024	the age of
0.0319383268	calls to
0.0319349340	three benchmarks
0.0319291700	provides better
0.0319288724	do not rely on
0.0319143937	three domains
0.0319138417	the development of new
0.0319136482	trained on two
0.0319093978	the large number of
0.0318936592	effect of different
0.0318846314	the shape of
0.0318816636	code at
0.0318765702	a generalization of
0.0318753729	any new
0.0318716026	an asr system
0.0318711739	the stability of
0.0318677311	a novel method of
0.0318607122	also evaluated
0.0318523788	other related
0.0318448846	novel evaluation
0.0318376518	several works
0.0318084561	the probability of
0.0317953972	dialogue system for
0.0317894160	widely used to
0.0317810936	any hand
0.0317593334	proposed approach on
0.0317465195	realm of
0.0317326195	accuracy while
0.0317197203	often rely on
0.0316802917	between nodes
0.0316782648	the dataset used
0.0316754439	least two
0.0316737678	two approaches to
0.0316693335	system generates
0.0316615306	give better
0.0316451606	result from
0.0316172142	useful in many
0.0316171517	a discussion of
0.0316068793	the syntactic structure of
0.0316008243	in many different
0.0315945103	used in various
0.0315936630	often need
0.0315908766	capacity to
0.0315875137	process into
0.0315799895	supervised learning with
0.0315778836	flow from
0.0315764554	the sensitivity of
0.0315698634	shows better
0.0315525395	two ways
0.0315478097	a crucial role in
0.0315422862	the model's ability to
0.0315402517	on various natural
0.0315395648	tagger using
0.0315381071	available for research
0.0315223488	on many nlp
0.0315171517	to search for
0.0315028468	not capture
0.0314803918	two novel
0.0314802781	across various
0.0314542683	these three
0.0314501216	different ways of
0.0314385684	most challenging
0.0314385684	most efficient
0.0314382164	several types of
0.0314327932	three novel
0.0314307980	usually rely on
0.0314270180	decision on
0.0314193337	this approach by
0.0314176902	same accuracy
0.0314162635	dataset of over
0.0314100659	available under
0.0313990320	the distance between
0.0313948676	several aspects of
0.0313942233	such as question answering and
0.0313894504	no human
0.0313502994	to date on
0.0313322235	often than
0.0313276725	this paper reports on
0.0313095985	queries into
0.0312989171	different sets of
0.0312851993	only focus
0.0312791683	place with
0.0312731385	comparably to
0.0312716366	more suitable for
0.0312692605	on many natural language
0.0312309138	the hierarchical structure of
0.0312230787	more computationally
0.0312035575	and weaknesses of
0.0311858725	this problem by
0.0311775512	most prior work
0.0311774586	with three different
0.0311467373	a model trained on
0.0311426024	a family of
0.0311253729	several novel
0.0311082570	the coherence of
0.0310976212	rate over
0.0310974344	technique using
0.0310846314	the modeling of
0.0310809239	this position
0.0310634452	an important yet
0.0310566315	scratch on
0.0310443376	not include
0.0310409671	a new way to
0.0310368926	combination of several
0.0310328463	in comparison with
0.0310148402	the connection between
0.0310148402	the difference between
0.0310067297	work in natural
0.0310003095	and robustness of
0.0309983680	the empirical results show
0.0309793664	some aspects of
0.0309787714	the recent success of
0.0309708762	without access to
0.0309686297	a generative model of
0.0309595925	an essential role in
0.0309470816	most related
0.0309426024	the discovery of
0.0309370124	the burden of
0.0309279886	from prior work
0.0309233127	integrate with
0.0309202184	better capture
0.0309144938	all kinds of
0.0309108519	the automatic extraction of
0.0309050104	most effective
0.0308932326	to capture more
0.0308876647	also able to
0.0308851719	novel dynamic
0.0308485644	the magnitude
0.0308376518	better represent
0.0308250427	on various nlp
0.0308202634	on several tasks
0.0308200530	then used
0.0308057842	well even
0.0308048130	not depend on
0.0308040926	number of parameters in
0.0308039886	novel technique
0.0308013803	work studies
0.0307721085	new measure
0.0307711664	best performance on
0.0307624354	measured using
0.0307316266	integrated in
0.0307259358	the same set of
0.0307193444	knowledge such as
0.0307185569	occurs in
0.0307178998	with different levels
0.0307169902	to generate more
0.0307146516	not easily
0.0307087062	not much
0.0307078965	to keep track of
0.0306950804	used together
0.0306536818	the class of
0.0306536818	the knowledge of
0.0306512981	the risk of
0.0306443332	the recent advances in
0.0306415420	to account for
0.0306272715	a thorough analysis of
0.0306133216	under different
0.0305970958	the model uses
0.0305863422	the representation of
0.0305773259	impacts of
0.0305408491	novel feature
0.0305260180	to make full use of
0.0305171517	the promise of
0.0304982226	a significant impact on
0.0304926157	a methodology to
0.0304908828	the transferability
0.0304874814	an increase in
0.0304852813	certain linguistic
0.0304759358	the subject of
0.0304708919	given target
0.0304680447	new approach for
0.0304590328	to provide better
0.0304572737	an evaluation of
0.0304536818	the task as
0.0304477269	novel concept
0.0304469092	a thorough analysis
0.0304435930	recorded in
0.0304300425	the vast amount of
0.0304287023	a dataset for
0.0304168284	the best performance on
0.0304050104	many important
0.0303981955	released in
0.0303941783	even further
0.0303907951	a lexicon of
0.0303873264	via adversarial
0.0303847837	with millions of
0.0303629070	both local and
0.0303523788	several recent
0.0303334192	given entity
0.0303316328	the mismatch between
0.0303171517	the selection of
0.0302967845	the properties of
0.0302881413	the semantic similarity between
0.0302812237	a mismatch
0.0302808280	this architecture
0.0302759358	the occurrence of
0.0302759358	the formation of
0.0302668193	both lexical
0.0302623264	most cases
0.0302591125	better search
0.0302229750	the experimental results show
0.0302197737	the duration of
0.0302165519	important than
0.0302076677	usually trained
0.0301831063	more practical
0.0301781735	the association between
0.0301703457	the tendency of
0.0301224851	an empirical study on
0.0301117401	to obtain better
0.0300985021	a beam
0.0300910011	not previously
0.0300768137	system designed
0.0300757845	constructed for
0.0300693843	structure into
0.0300623607	not enough to
0.0300620978	best answer
0.0300550355	a quantitative analysis of
0.0300499052	performance in various
0.0300480244	new ones
0.0300368284	the order in
0.0300328463	a new approach for
0.0300315285	measure between
0.0300216278	the expense of
0.0300184486	several strong
0.0300139453	a significant role in
0.0299791058	conducted for
0.0299467145	any linguistic
0.0299446793	suffer from two
0.0299426024	the likelihood of
0.0299426024	the basis for
0.0299426024	the flow of
0.0299410772	the resulting embeddings
0.0299071756	the range of
0.0298863204	of one or more
0.0298822527	few words
0.0298668421	best practices for
0.0298623238	work provides
0.0298325690	the number of parameters in
0.0298306884	a single system
0.0298239071	entry in
0.0298150220	not generalize
0.0297952184	most studies
0.0297929001	the difficulty in
0.0297882693	different senses of
0.0297437728	between candidate
0.0297258404	first identify
0.0297238036	such as word2vec and
0.0297203113	to generalize to new
0.0296979285	new technique
0.0296693376	some form
0.0296632741	a daily
0.0296604182	commonly used to
0.0296597040	learnability of
0.0296536818	the sequence of
0.0296375585	given test
0.0296361731	against other
0.0296231221	these types of
0.0295632285	the voice of
0.0295569749	the system consists of
0.0295327389	fragment of
0.0295171517	a basis for
0.0295146938	known in
0.0295038512	both single and
0.0294902215	the plausibility of
0.0294891884	like sentiment
0.0294287023	a review of
0.0294265820	mechanism into
0.0294251250	an increasing interest
0.0294197798	also considered
0.0294197798	also provided
0.0294163211	veracity of
0.0294082281	approaches usually
0.0294079692	the proposed model on
0.0293989726	gap by
0.0293861466	an extensive analysis of
0.0293769961	such as bert and
0.0293536174	order to find
0.0293007693	system trained on
0.0292998852	evaluation across
0.0292895320	overall accuracy of
0.0292759358	the behaviour of
0.0292697737	an understanding of
0.0292347326	a proof of
0.0292287023	the interpretability of
0.0292266321	interact in
0.0292025427	the final system
0.0291977022	a central role in
0.0291909108	efficient than
0.0291527655	first approach to
0.0291488525	a degree of
0.0291426024	a class of
0.0291364404	a description of
0.0291290579	task since
0.0291201958	system consists of
0.0291141892	other forms of
0.0291082570	the scenario of
0.0291048708	made available to
0.0290930766	these results show
0.0290796516	many challenges
0.0290720545	place of
0.0290648475	the compositional structure
0.0290592691	a large dataset of
0.0290590124	this work focuses on
0.0290397966	the search for
0.0290084561	the applicability of
0.0290021078	training via
0.0289863054	the composition of
0.0289736245	very different from
0.0289557677	solution by
0.0289162817	performance without
0.0289041619	results on various
0.0288909209	translation system for
0.0288757883	problem by using
0.0288360861	novel task of
0.0288068793	the generalization ability of
0.0287949817	created using
0.0287647741	a computational model of
0.0287490615	with large amounts of
0.0287459437	the potential for
0.0287435326	appropriate word
0.0287413968	speed by
0.0287339297	a relative improvement of
0.0286874395	by focusing
0.0286745151	the design and
0.0286643814	more widely
0.0286512981	the setting of
0.0286432029	new types of
0.0286414925	a novel task of
0.0286364404	the architecture of
0.0286179023	neighborhood of
0.0286068793	a promising approach to
0.0286016439	policy for
0.0285973643	the values of
0.0285956628	across many
0.0285899647	the rapid development of
0.0285807479	a challenging problem in
0.0285769443	a case study for
0.0285750041	only needs to
0.0285497730	new metric
0.0285449700	novel mechanism
0.0285330635	approach does
0.0284681659	the training process of
0.0284572737	the problem as
0.0284409520	an attempt to
0.0283956254	conversion using
0.0283787858	more robust to
0.0283731123	best accuracy
0.0283680470	extracted using
0.0283111401	a battery of
0.0282864301	the evaluation results show
0.0282802397	the mapping from
0.0282759358	the production of
0.0282565251	an algorithm to
0.0282459437	the characteristics of
0.0282447872	and quantitatively
0.0282427736	to decide whether
0.0282376464	a vector of
0.0282346314	the capacity to
0.0282328081	work proposes
0.0282287023	a form of
0.0282168171	art methods in
0.0281723457	then used as
0.0281426024	the release of
0.0281303008	to significant improvements in
0.0281252836	the connections between
0.0281201062	a test set of
0.0281064203	reported for
0.0280998591	the generalizability
0.0280982098	the proportion of
0.0280641219	the concatenation
0.0280592691	an important task for
0.0280216600	the noise in
0.0280142544	all aspects of
0.0279731751	a bottom
0.0279713322	the dependencies between
0.0279557677	signal from
0.0279549611	an exploration of
0.0279514864	mostly based on
0.0279468839	the large amount of
0.0279426024	the potential to
0.0279373103	the burden
0.0279231919	no performance
0.0279051000	the data used
0.0278954920	method on two
0.0278521632	the different types
0.0278325690	the complexity and
0.0278322350	a system based on
0.0278286429	and experimentally
0.0278224344	manner by
0.0278103206	three types
0.0278086412	constructed using
0.0277746867	in view of
0.0277459437	the coverage of
0.0277311957	the distinction between
0.0277044730	studied on
0.0276966778	the conditions under
0.0276902907	on various tasks
0.0276349772	sampling for
0.0276321052	reasonable to
0.0276227739	realization of
0.0276220361	publicly available to
0.0276176464	a challenge to
0.0276053620	the same number of
0.0275854285	for research in
0.0275764554	a case for
0.0275637072	novel framework for
0.0275582087	an important task in
0.0275342690	attention due
0.0275337985	than prior work
0.0275199610	for many tasks
0.0275181955	overfitting to
0.0275073259	shape of
0.0274855463	the dimension of
0.0274518522	heart of
0.0274431935	different forms of
0.0274287023	an approach for
0.0274149137	a model for
0.0273962148	assist in
0.0273879001	the title and
0.0273522391	written to
0.0273401155	contribution from
0.0273287021	performance on many
0.0273057810	experimentation on
0.0272922609	yields more
0.0272759358	by training on
0.0272737924	same space
0.0272586916	the overall quality of
0.0272547459	to apply to
0.0272425774	most previous work
0.0272383570	new model for
0.0271986258	a drop
0.0271930312	the door for
0.0271831063	not considered
0.0271787719	bottleneck for
0.0271786131	the perplexity of
0.0271577504	performance on three
0.0271536323	to respond to
0.0271426024	the inclusion of
0.0271426024	a study on
0.0271363269	an impact
0.0271327156	the first step of
0.0271201958	more difficult to
0.0271091399	through extensive experiments on
0.0270958146	languages as well
0.0270868353	first work to
0.0270807360	as compared with
0.0270693843	document into
0.0270672044	task without
0.0270544730	works by
0.0270473988	work aims to
0.0270291971	the statistical properties of
0.0270220925	a significant portion of
0.0270084561	the scarcity of
0.0269994799	the challenging task of
0.0269731429	a fragment of
0.0269697737	a database of
0.0269612503	this study focuses on
0.0269595925	a small fraction of
0.0269426024	the correctness of
0.0269336739	the correlation of
0.0269116693	the severity
0.0268974552	the appearance
0.0268809271	work well
0.0268788492	trust in
0.0268669536	a balance
0.0268295624	essence of
0.0268216278	to attend to
0.0268099946	classification tasks such as
0.0267941373	problem through
0.0267807359	other aspects
0.0267514999	of training data for
0.0267505793	problem of learning to
0.0267459437	a task of
0.0267275231	both monolingual and
0.0267245643	useful information from
0.0267124132	the inclusion
0.0266942159	the reasons for
0.0266860600	a selection of
0.0266782217	a special case of
0.0266728005	as judged
0.0266486034	a step toward
0.0266259265	the strengths and weaknesses of
0.0265905947	to determine if
0.0265389966	the merits of
0.0265263408	this work aims to
0.0265153787	an attention mechanism to
0.0265036324	little training
0.0265015481	work describes
0.0264927983	the definition of
0.0264913449	not suitable for
0.0264855463	a recall of
0.0264854874	not able to
0.0264831323	by making use of
0.0264442431	the discrepancy between
0.0264336739	the novelty of
0.0264336739	the scale of
0.0264312981	a computational approach to
0.0264283167	an implementation of
0.0264209871	a new task of
0.0264116850	an interesting and
0.0264062063	data from different
0.0263998247	the inside
0.0263994281	other types
0.0263989803	to improve over
0.0263974025	much research
0.0263849772	constructed in
0.0263732331	the volume of
0.0263684579	the production
0.0263513322	to benefit from
0.0263509171	a wealth of
0.0263473643	the sum of
0.0263473643	a mechanism to
0.0263353336	the question of whether
0.0263296741	the next word in
0.0263232470	to learn from
0.0262997897	an investigation of
0.0262902780	the efficiency and
0.0262759358	the weaknesses of
0.0262671294	limitation by
0.0262646314	a given set of
0.0262478336	both supervised and
0.0262412023	a source of
0.0262078249	to speed
0.0261968368	novel model for
0.0261961041	counterpart in
0.0261914850	tuning on
0.0261745151	a method of
0.0261426024	the spread of
0.0261426024	the flexibility of
0.0261373947	all words in
0.0261347886	deployment in
0.0261051331	benefit of using
0.0260958146	effects of different
0.0260939622	methods do not
0.0260939445	development of such
0.0260854285	the web for
0.0260828132	a new class of
0.0260804792	the hidden states of
0.0260773643	both text and
0.0260720936	operating in
0.0260488204	capable to
0.0260412023	the implications of
0.0260243608	2010 task
0.0260219603	engine for
0.0260105772	instead of using
0.0260051934	available parallel
0.0259722691	also leads to
0.0259697737	the alignment of
0.0259648288	successful for
0.0259479987	not present in
0.0259426024	the reliability of
0.0259403695	the dataset consists of
0.0259335523	the m
0.0259265820	common use
0.0258849611	the advancement of
0.0258831063	even simple
0.0258462357	extensively in
0.0258410659	all source
0.0258340143	a lot of attention in
0.0258222310	through experiments on
0.0258156810	the resulting models
0.0258092424	interactive system
0.0258074834	performed to
0.0257792357	in order for
0.0257745151	a dataset with
0.0257710446	an opportunity to
0.0257577929	the shortcomings of
0.0257512981	a hierarchy of
0.0257491397	the behaviour
0.0257337502	the model consists of
0.0257287540	new way of
0.0257258983	combination of different
0.0257225773	a comparison with
0.0257183106	selected to
0.0257176330	the correctness
0.0257041174	to achieve better
0.0256780097	between humans and
0.0256668839	a summary of
0.0256503457	the transferability of
0.0256386916	and scalability of
0.0256239392	new class of
0.0256134013	one type of
0.0256106457	many types of
0.0255769704	an empirical study of
0.0255764554	the calculation of
0.0255679648	an important problem in
0.0255503457	a modification to
0.0255328213	several variants of
0.0255189391	any feature
0.0255159288	both unsupervised and
0.0255052591	the direction of
0.0255051024	the practice of
0.0255004739	three benchmark
0.0254915327	at different levels of
0.0254338305	a new set of
0.0254169927	a significant improvement in
0.0254146221	a surge of
0.0254130826	the problem into
0.0254066773	overall quality of
0.0253968139	novel method of
0.0253879001	the deployment of
0.0253831063	most works
0.0253794960	a neural model for
0.0253735739	the mutual information between
0.0253655886	to generalize to
0.0253582725	interpretation for
0.0253434119	performance of various
0.0253427410	system based on
0.0253345668	the reason for
0.0253295690	analyzed in
0.0253056883	an approach based on
0.0252925931	to interact with
0.0252902215	both types of
0.0252853553	to suffer from
0.0252635120	gap with
0.0252595848	the uncertainty of
0.0252467812	the exploitation of
0.0251908492	execution of
0.0251858452	the baseline system
0.0251805684	this paper deals with
0.0251793780	novel graph
0.0251787719	chosen to
0.0251787023	this paper aims to
0.0251697654	most accurate
0.0251644081	this line
0.0251572737	to adapt to
0.0251506267	new concept
0.0251444438	modeled using
0.0251426024	a means of
0.0251360907	computed in
0.0251359741	many applications in
0.0251082570	the transfer of
0.0250752639	each token in
0.0250642789	often fail to
0.0250590648	a reduction of
0.0250590648	the measurement of
0.0250416501	the generality of
0.0250412023	a process of
0.0250347747	the paucity
0.0250196054	to shed light on
0.0250136679	to aid in
0.0250119986	an effort to
0.0249762035	several orders of
0.0249759358	by learning from
0.0249725546	two new
0.0249599053	the generalization of
0.0249578690	a technique to
0.0249578690	the category of
0.0249562513	various aspects of
0.0249487385	in order to better
0.0249426024	a sample of
0.0249371014	effectively than
0.0249344198	while previous work
0.0249245151	the relation of
0.0249230599	new dataset for
0.0249166447	often used in
0.0248873476	the first step in
0.0248840035	all types of
0.0248748753	a significant amount of
0.0248718637	better understanding of
0.0248718142	the kurdish
0.0248552936	factor for
0.0248475877	good results on
0.0248454553	a machine translation system
0.0248273643	the overall performance of
0.0248045690	bottleneck in
0.0247756883	a novel framework for
0.0247746232	scalability of
0.0247416600	the expression of
0.0247294839	effectively used
0.0247132157	to answer questions about
0.0246953563	extraction via
0.0246673629	the correlation between
0.0246478840	the reconstruction of
0.0246462654	different layers of
0.0246334642	the requirement of
0.0246172787	the scalability of
0.0246103468	a problem for
0.0246068793	an annotated corpus of
0.0245990320	the interaction between
0.0245593513	an effective way to
0.0245497737	an improvement in
0.0245485592	dataset as well
0.0245412023	the computation of
0.0245349611	the era of
0.0245347767	methodologies for
0.0245274511	a bottleneck in
0.0245212023	a pipeline of
0.0245173723	each word in
0.0245119880	viewpoint of
0.0245087258	a growing interest in
0.0244950148	to improve performance on
0.0244790486	and then uses
0.0244713322	more diverse and
0.0244548369	more similar to
0.0244287023	a solution to
0.0244287023	a technique for
0.0244287023	the capability of
0.0244287023	a kind of
0.0244073912	to translate from
0.0243703457	a limited amount of
0.0243473643	the loss of
0.0243346571	the new task of
0.0243286401	by sampling from
0.0243122953	system relies on
0.0243004972	an average of
0.0242943397	a new perspective on
0.0242676083	the essence of
0.0242675838	some form of
0.0242476940	a pool of
0.0242440249	an important component of
0.0242260611	each component of
0.0242098584	the organization of
0.0242056883	an effective method for
0.0242056883	a comprehensive analysis of
0.0242056883	an extensive evaluation of
0.0241835385	the reliance
0.0241829929	the widespread use of
0.0241754739	several types
0.0241748962	new application
0.0241669927	a challenge for
0.0241426024	a benchmark for
0.0241260070	a novel model to
0.0241082570	the intent of
0.0241004972	the acquisition of
0.0240832941	little attention in
0.0240830786	the credibility of
0.0240797408	a major challenge in
0.0240792430	model as well
0.0240584437	the key to
0.0240554335	language as well
0.0240412023	a resource for
0.0240216278	a fraction of
0.0239914801	the shortage of
0.0239863054	a population of
0.0239745894	a simple but
0.0239676940	the nuances of
0.0239143021	this framework to
0.0239115208	also experiment with
0.0239071756	the improvement of
0.0238900684	the utilization of
0.0238542948	an order of
0.0238199817	manner using
0.0238010054	an essential component of
0.0237973643	the automation of
0.0237933510	to learn more
0.0237690353	optimized using
0.0237229509	an important aspect of
0.0237229509	a large portion of
0.0237229509	the sequential nature of
0.0237104384	not capable of
0.0236832024	a limited amount
0.0236715666	time complexity of
0.0236634513	the embedding of
0.0236330786	the expressiveness of
0.0236068793	the results obtained by
0.0235703339	as compared to
0.0235646402	to produce more
0.0235473643	the simplicity of
0.0235051024	the possibility to
0.0234965389	as input to
0.0234672337	wizard of
0.0234415686	the effect of different
0.0234216751	best results on
0.0234084482	a significant amount
0.0233458428	the source code of
0.0233353336	the first attempt to
0.0233346571	the benefit of using
0.0233004972	a methodology for
0.0233001309	a notion of
0.0232946756	a result of
0.0232926449	to extract information from
0.0232835279	better performance on
0.0232804181	the location of
0.0232708750	the execution of
0.0232506538	most significant
0.0232336739	to provide more
0.0232287023	the popularity of
0.0232251103	the capacity of
0.0232034626	constructed to
0.0231468454	bound for
0.0231411591	only focus on
0.0231207224	salience of
0.0231053590	on data from
0.0231052714	a novel approach based on
0.0230942268	a case study of
0.0230806400	technique used
0.0230663054	the effects of different
0.0230577340	the variability of
0.0230422521	not lead to
0.0230084929	each stage of
0.0229963979	to lack of
0.0229731288	in order to find
0.0229697737	the principle of
0.0229564382	strongly with
0.0229280637	different components of
0.0229247438	most widely used
0.0229222493	a distinction
0.0229168087	not rely on
0.0229121452	this issue by
0.0228948956	occur with
0.0228544540	novel approach of
0.0228467486	different sources of
0.0227997931	novel application of
0.0227816478	contrast with
0.0227405005	a key challenge in
0.0227229509	a huge number of
0.0226860857	a novel application of
0.0226454381	two sets of
0.0226194923	models on three
0.0225983452	the incorporation of
0.0225976315	with thousands of
0.0225910659	no task
0.0225769166	the appearance of
0.0225516892	other kinds of
0.0224890124	to converge to
0.0224879228	two levels of
0.0224839297	a systematic study of
0.0224593513	two classes of
0.0224446616	to refer to
0.0224378690	a novel dataset of
0.0224274203	the percentage of
0.0224238295	date on
0.0224152161	the large amount
0.0224126888	very difficult to
0.0224117481	thought of
0.0224011365	an emphasis
0.0223706341	a host
0.0223629972	the geometry of
0.0223558935	the kind of
0.0223412101	new methods for
0.0223291626	a new benchmark for
0.0222940059	a solution for
0.0222925931	to take advantage of
0.0222890217	due to lack of
0.0222646314	the first dataset for
0.0222577340	to scale to
0.0222289721	first attempt to
0.0222162105	from left to
0.0221817206	new approach to
0.0221415686	a baseline system
0.0221347002	better performance compared to
0.0220889130	performing system
0.0220869749	a parallel corpus of
0.0220826718	the corpus contains
0.0220632862	the naturalness
0.0220165056	and qualitatively
0.0220073046	the richness of
0.0220068836	a mismatch between
0.0220036416	the gap with
0.0219916515	the performance gap between
0.0219754553	an answer to
0.0219479553	machine translation system for
0.0219460180	the proliferation of
0.0219403695	a dataset consisting of
0.0219248358	mainly based on
0.0219164850	agent with
0.0218989419	not generalize to
0.0218937293	still challenging to
0.0218273643	an unsupervised method for
0.0218046022	control for
0.0217893559	a mapping from
0.0217870643	very similar to
0.0217658015	a first step in
0.0217587023	a mechanism for
0.0217553895	both training and
0.0217366832	then applied to
0.0217251103	the theory of
0.0217061824	this paper proposes to
0.0217051024	the phenomenon of
0.0216745151	in case of
0.0216726024	a new algorithm for
0.0216379023	dimensionality of
0.0216259265	a distribution over
0.0215986151	website for
0.0215947720	two groups of
0.0215769166	a consequence of
0.0215768147	an important step in
0.0215749640	not perform well
0.0215278840	two approaches for
0.0215122875	two orders of
0.0214890124	first step in
0.0214851024	the conversion of
0.0214431935	three different types of
0.0214287023	a new dataset of
0.0214274203	the difference in
0.0214117481	choose to
0.0213914400	novel approach for
0.0213629972	a taxonomy of
0.0213238386	various tasks such as
0.0213056883	a fundamental task in
0.0212890217	a challenging task due to
0.0212812870	the specification of
0.0212662972	good results for
0.0212610746	classification via
0.0212604251	to perform well on
0.0212584437	a baseline for
0.0212412023	the adoption of
0.0212251103	a type of
0.0212192203	a couple
0.0211903037	a major challenge for
0.0211790877	a fraction
0.0211429353	a large body of
0.0211429353	a growing number of
0.0211274867	more coherent and
0.0211140118	new framework for
0.0210994885	a dataset containing
0.0210913656	recognition via
0.0210617882	a tool for
0.0210222463	detection via
0.0209999817	maps from
0.0209875172	a plug
0.0209852591	a novel model for
0.0209787023	a dictionary of
0.0209616713	both visual and
0.0209615432	a battery
0.0209555198	a trade
0.0209370124	an instance of
0.0209068648	replacement of
0.0209054321	the abundance of
0.0209014627	fail in
0.0208791415	impact of using
0.0208695259	novel approach to
0.0208555569	both synthetic and
0.0208495730	same set of
0.0208273643	the generalizability of
0.0208265310	the variance of
0.0207734676	this paper attempts to
0.0207658015	a platform for
0.0207408167	a toolkit for
0.0207303759	a dialog system
0.0207152073	serves to
0.0207017155	a large margin on
0.0206955155	a strong baseline for
0.0206863644	the principles of
0.0206802127	first step to
0.0206451219	most commonly used
0.0206247761	by taking into
0.0206234222	an alternative to
0.0206148769	a discussion on
0.0205858915	whole model
0.0205787331	the motivation for
0.0205770925	a foundation
0.0205549093	different approaches for
0.0205354259	the viability
0.0205121728	the link between
0.0205065720	by learning to
0.0204947139	communicate in
0.0204757168	a new framework for
0.0204574203	the limitation of
0.0204446616	the concatenation of
0.0203890666	a pressing
0.0203502113	this limitation by
0.0203412867	new dataset with
0.0203330362	a significant improvement over
0.0203272391	the past two
0.0203255558	devised to
0.0203240275	and consistency of
0.0203227948	an effective approach to
0.0202196373	both labeled
0.0202133964	these problems by
0.0201967084	in place of
0.0201686391	new perspective on
0.0201349565	the current work
0.0200782294	new task of
0.0200780544	to achieve good
0.0200700530	a prerequisite for
0.0200645280	helping to
0.0200427877	both source and
0.0200412023	the significance of
0.0200330543	these issues by
0.0200184213	a qualitative analysis of
0.0200145966	very effective in
0.0199940310	a fundamental problem in
0.0199745643	three levels of
0.0199251103	the capabilities of
0.0198653713	feasible to
0.0198213809	various levels of
0.0197902200	an important part
0.0197893559	the backbone of
0.0197739913	better suited to
0.0197559358	the impacts of
0.0197051986	the dependence on
0.0196959929	by attending to
0.0196939061	a black
0.0196146571	the gap by
0.0196027523	the computational cost of
0.0195898550	a foundation for
0.0195569749	with different levels of
0.0195569749	a procedure for
0.0195294885	next generation of
0.0195203249	the variability
0.0195141747	an nmt system
0.0195064441	much attention in
0.0195003260	an application to
0.0194581948	system submitted to
0.0194451158	also propose to
0.0194040079	this question by
0.0193754364	a modification of
0.0193344328	system capable of
0.0193122453	a piece of
0.0192960123	tail of
0.0192670961	new benchmark for
0.0192196373	both source
0.0192043780	not sufficient to
0.0191860857	the strengths and
0.0191599365	favor of
0.0191589152	the corpus consists of
0.0191460464	useful information for
0.0191396076	two ways of
0.0191163656	mechanism over
0.0191108358	the dataset contains
0.0191009110	this paper provides
0.0190309119	the inability to
0.0190271959	to translate between
0.0190219671	novel method for
0.0190083374	also introduce two
0.0189152435	insufficient to
0.0188885180	to perform well
0.0188334642	a new model for
0.0188301309	the interaction of
0.0187723457	many aspects of
0.0187542882	two methods for
0.0187261428	make sense of
0.0186848179	calculus with
0.0186280423	not account for
0.0186271426	the computational complexity of
0.0185903400	useful resource for
0.0184832758	purely on
0.0184327937	the informativeness of
0.0182820379	hold in
0.0182786496	the current best
0.0182547142	often difficult to
0.0182358382	an effective approach for
0.0182341710	challenging than
0.0182329494	find evidence for
0.0182261428	with emphasis on
0.0181989133	to perform better
0.0180117115	the proportion
0.0179564805	focus only
0.0179132321	novel method to
0.0178988601	novel dataset of
0.0178069166	the demand for
0.0177974068	both human and
0.0177849611	two sources of
0.0177542882	using data from
0.0177112309	the lens
0.0176649762	a working
0.0176328820	a means to
0.0175427461	to discover new
0.0175220988	different categories of
0.0175048858	both lexical and
0.0174644257	a new corpus of
0.0174604951	the realm
0.0173966196	the measurement
0.0173856319	a learning to
0.0172547142	to mitigate such
0.0172306552	these challenges by
0.0171946616	the growth of
0.0171282870	proposal for
0.0170773643	both english and
0.0170679267	to learn better
0.0169914801	a comparison between
0.0169743456	processed to
0.0169246179	further research in
0.0169188302	combined to
0.0168327031	the generality
0.0168327031	a modification
0.0167542882	an application of
0.0167229409	to communicate with
0.0166757075	for future work
0.0164104409	each pair of
0.0164001089	an improvement over
0.0162273550	the relevance between
0.0162074203	a majority of
0.0161635022	to reason over
0.0161612594	by focusing on
0.0160926309	a novel method to
0.0160700608	an architecture for
0.0159743456	fit to
0.0159331267	the richness
0.0158298723	the case for
0.0157977337	also propose two
0.0157208106	further research on
0.0156860857	a means for
0.0156569166	this challenge by
0.0156434907	these two types of
0.0155639885	very effective for
0.0155565310	an adaptation of
0.0155262003	each step of
0.0152301890	to engage in
0.0152144257	different approaches to
0.0150962977	best results for
0.0148237046	backbone of
0.0144843356	to recover from
0.0142273550	as evaluated by
0.0141904870	many tasks in
0.0136777911	on average over
0.0135802264	lens of
0.0132691622	by referring to
0.0127486724	in contrast with
0.0119498746	trivial to
0.0119254844	tuned with
