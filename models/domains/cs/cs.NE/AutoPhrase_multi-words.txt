0.9632699482	collaborative filtering
0.9625887407	speech recognition
0.9613729585	anomaly detection
0.9608527829	alzheimer's disease
0.9590322424	optical flow
0.9578695712	synaptic plasticity
0.9575284662	maximum likelihood
0.9570323650	natural language processing
0.9568276449	partial differential equations
0.9562119906	swarm intelligence
0.9557666828	principal component analysis
0.9554844748	sentiment analysis
0.9551039876	simulated annealing
0.9550010847	handwriting recognition
0.9549920745	polyphonic music
0.9547930674	reading comprehension
0.9546991063	machine translation
0.9544063171	dynamical systems
0.9542646584	drug discovery
0.9539356610	stochastic gradient descent
0.9536416137	artificial intelligence
0.9534916286	dimensionality reduction
0.9527314329	multilayer perceptron
0.9526212502	magnetic resonance
0.9517458835	resource allocation
0.9517294656	gene expression
0.9516877261	message passing
0.9515510983	von neumann
0.9512194781	support vector machines
0.9511225792	stock market
0.9508561835	fuzzy logic
0.9505588843	gravitational wave
0.9504548899	computed tomography
0.9502770759	associative memories
0.9499699356	turing machine
0.9497199063	partially observable
0.9488369178	symbolic regression
0.9486063371	artificial life
0.9478920704	particle swarm
0.9478140951	markov chain
0.9474712196	question answering
0.9471536459	kalman filter
0.9470770015	chemical reaction
0.9470716998	load dispatch
0.9469756547	ordinary differential equations
0.9464411871	intrusion detection
0.9464237938	wind farm
0.9461817850	multilayer perceptrons
0.9457676464	fitness landscapes
0.9457448270	density estimation
0.9456777068	reinforcement learning
0.9456175043	dew point
0.9455041655	wind turbines
0.9454372234	feature selection
0.9453600586	shortest path
0.9452984787	collision avoidance
0.9450994677	markov chains
0.9450756240	breast cancer
0.9450621269	statistical mechanics
0.9449585156	convex hull
0.9446306146	decision tree
0.9445375894	constraint satisfaction
0.9443302430	differential equations
0.9442120477	cellular automata
0.9438981106	autonomous driving
0.9437522024	tabu search
0.9435036703	reservoir computing
0.9434132594	decision trees
0.9433680736	medical diagnosis
0.9430135169	inverted pendulum
0.9428003174	turing machines
0.9427858955	logistic regression
0.9426480099	point clouds
0.9423838951	community detection
0.9415332925	covariance matrix
0.9412873742	genetic algorithm
0.9412010701	genetic programming
0.9410983089	bayesian inference
0.9409995184	hyperbolic tangent
0.9409522195	beam search
0.9408519250	fourier transform
0.9408442566	variational autoencoder
0.9408261966	neural network
0.9407248388	facial expression
0.9406157185	nearest neighbour
0.9404329975	remote sensing
0.9402653183	neuromorphic computing
0.9401553697	massively parallel
0.9401167172	differential evolution
0.9393351508	soft computing
0.9392736298	mountain car
0.9389923221	software engineering
0.9386619253	feature extraction
0.9383597412	speaker recognition
0.9382720397	batch normalization
0.9381288636	fault tolerance
0.9380283504	denoising autoencoder
0.9379337125	spanning tree
0.9378504925	ant colony
0.9373127612	supply chain
0.9372554281	particle swarm optimization
0.9372478280	credit assignment
0.9368505688	boltzmann machines
0.9368493874	reservoir computers
0.9361253589	hexapod robot
0.9359132218	object recognition
0.9358978953	nearest neighbors
0.9357466276	piecewise linear
0.9356741310	tournament selection
0.9356060147	evolutionary algorithms
0.9354348123	gibbs sampling
0.9352678685	superconducting optoelectronic
0.9351802694	graph coloring
0.9351503697	stock price
0.9349417554	finite element
0.9346664946	fractional order
0.9346634905	radial basis
0.9346210395	inductive bias
0.9341284387	ad hoc
0.9340981675	knowledge base
0.9338140466	pareto fronts
0.9337733620	adversarial perturbations
0.9333793509	evolutionary computation
0.9332782439	associative memory
0.9331992722	local search
0.9331128259	support vector machine
0.9331112723	evolutionary robotics
0.9330464349	temporal coding
0.9327332303	universal approximation
0.9322081061	edit distance
0.9320774689	prisoner's dilemma
0.9320673113	nurse scheduling
0.9319999138	nearest neighbor
0.9312124392	monte carlo
0.9311631709	cellular automaton
0.9311237664	matrix factorization
0.9311199391	artificial neural networks
0.9304284518	smart grid
0.9303953881	health care
0.9303820264	adversarial attacks
0.9302354945	image segmentation
0.9300829249	belief propagation
0.9300616273	genetic algorithms
0.9300458395	graphics processing units
0.9296842060	machine learning
0.9295614082	web page
0.9291794970	free energy
0.9290782921	cuckoo search
0.9283508628	tail bounds
0.9278684672	random walk
0.9278030995	gaussian process
0.9277973322	dendritic cells
0.9277867975	speech enhancement
0.9277669002	autonomous vehicles
0.9275289693	radial basis function
0.9271654857	boltzmann machine
0.9268014274	saliency maps
0.9267332816	sparse coding
0.9267071539	social media
0.9266288070	source separation
0.9263363539	vector quantization
0.9262939605	knowledge transfer
0.9262030551	rectified linear units
0.9261711851	matrix multiplication
0.9259837434	automatic speech recognition
0.9258371763	knowledge distillation
0.9257927386	eye movement
0.9255458418	diffractive optical
0.9254843376	predictive coding
0.9253914706	natural language
0.9253887919	speech synthesis
0.9250946214	test suite
0.9249835242	steady state
0.9247279665	text mining
0.9247215142	external archive
0.9245893884	spin glass
0.9242829295	policy gradient
0.9238510921	electronic health records
0.9237884227	neuromorphic hardware
0.9237619356	adversarial robustness
0.9235531575	feedback alignment
0.9232394285	linear regression
0.9229397765	gesture recognition
0.9225695335	spike timing
0.9224939047	dialogue act
0.9224526628	adversarial examples
0.9220490672	restricted boltzmann machine
0.9218296384	data mining
0.9217450772	keyword spotting
0.9216975497	humanoid robot
0.9214967956	nk landscapes
0.9214339676	gradient descent
0.9213449988	membrane potential
0.9212925083	beetle antennae search
0.9211714345	novelty search
0.9210934892	cognitive radio
0.9210934183	random forest
0.9208706096	variational autoencoders
0.9205606227	restricted boltzmann machines
0.9204511219	working memory
0.9202512547	selective attention
0.9200813502	rolling horizon
0.9198725326	spiking neurons
0.9194863497	negatively correlated
0.9194572479	artificial bee colony
0.9194097755	neural networks
0.9186799713	source code
0.9185883354	deep learning
0.9181032419	visual question answering
0.9179236017	wearable devices
0.9176609611	wind power
0.9176113335	runtime analysis
0.9175667718	recommender systems
0.9174371724	speaker verification
0.9168757703	pose estimation
0.9167068112	character recognition
0.9166875472	gaussian mixture
0.9166573710	experience replay
0.9166147981	random walks
0.9164176776	crossbar array
0.9163631893	gaussian processes
0.9163624111	multiobjective optimization
0.9157605200	eeg signals
0.9153813101	receptive field
0.9152514509	neuromorphic processor
0.9152247227	gene expression programming
0.9150543185	error correction
0.9150292406	emotion recognition
0.9148451003	computational intelligence
0.9148248738	fitness landscape
0.9147783316	population sizing
0.9146731333	floating point
0.9144413905	traveling salesperson problem
0.9143453730	minimum spanning tree
0.9143387708	wave energy
0.9142086939	implicit regularization
0.9140096992	domain adaptation
0.9139581758	shortest paths
0.9136735148	digital ecosystem
0.9136448438	forward pass
0.9134593619	cooperative coevolution
0.9133784506	contrastive divergence
0.9132596321	echo state
0.9132094297	continual learning
0.9131284940	arithmetic circuits
0.9130052475	technical report
0.9129594821	multiobjective optimisation
0.9127414909	object detection
0.9127329197	local optima
0.9127304412	crossover operators
0.9126820802	lottery ticket hypothesis
0.9126275405	euclidean space
0.9126149213	upper bounds
0.9125413077	sequence labeling
0.9124578188	renewable energy
0.9124506491	short term memory
0.9123404487	mixture density
0.9122412852	cocktail party
0.9122128236	style transfer
0.9121923188	penn treebank
0.9120453809	mixed integer
0.9118463126	hyperparameter tuning
0.9116871733	transfer learning
0.9116482735	concept drift
0.9112341582	semantic segmentation
0.9111766562	filter banks
0.9111318950	linear algebra
0.9111001970	motor imagery
0.9108687522	program synthesis
0.9104937761	sparsely connected
0.9104881724	class imbalance
0.9102959800	atari games
0.9101671226	vehicle routing
0.9101603056	bin packing
0.9101458967	expectation maximization
0.9101308486	weight decay
0.9101211341	collective intelligence
0.9099538863	image registration
0.9099105417	compressed sensing
0.9096663347	face recognition
0.9096586519	wavelet transform
0.9094948219	gated recurrent unit
0.9094585406	job shop
0.9093536675	stepping stones
0.9092091905	activation function
0.9090966469	parameter choices
0.9088964030	news articles
0.9082694960	traveling salesman
0.9082169035	central pattern generators
0.9081872951	probability density
0.9081399837	test suites
0.9081007418	parameter tuning
0.9079725306	lexicase selection
0.9079463556	backward pass
0.9079414776	vocabulary speech recognition
0.9074910292	maximum likelihood estimation
0.9074745247	variational inference
0.9074724588	receptive fields
0.9074351436	rectified linear unit
0.9069998786	structural plasticity
0.9068643957	openai gym
0.9067680223	biologically motivated
0.9064392906	gated recurrent units
0.9062551038	latent variable
0.9056904466	weight sharing
0.9055722185	weakly supervised
0.9053560552	password authentication
0.9053196360	bounding box
0.9052422058	graph partitioning
0.9051162704	traveling salesman problem
0.9046931195	robot navigation
0.9046686813	rule extraction
0.9046578384	clonal selection
0.9045416005	denoising autoencoders
0.9044910927	nature inspired
0.9040403525	low precision
0.9040260137	drift analysis
0.9039343513	hill climbing
0.9035789582	cloud computing
0.9033983436	subset selection
0.9033005471	quantum computers
0.9028659921	tensor decomposition
0.9028505958	ablation studies
0.9026100881	np complete
0.9025803563	knapsack problem
0.9024286094	motor cortex
0.9023930087	feature extractor
0.9021762247	genetic drift
0.9017253255	robotic arm
0.9015681662	local minima
0.9014540208	liquid state
0.9014317765	sufficient conditions
0.9013651668	particle filter
0.9011604182	dynamic programming
0.9011327248	machine comprehension
0.9005188903	correlation coefficient
0.9003226104	phase change
0.9002221534	cross entropy
0.8999788828	satellite images
0.8999560382	monte carlo tree search
0.8999302493	constraint handling
0.8999288600	surrogate assisted
0.8997295701	resting state
0.8996954254	dependent plasticity
0.8996953124	neighborhood search
0.8995004260	spiking neural networks
0.8994768718	dataflow matrix machines
0.8993228203	travelling salesman problem
0.8992760561	mutation rates
0.8987586414	randomized search heuristics
0.8980227876	flower pollination
0.8979994754	literature review
0.8979669439	ant colony optimization
0.8978919907	field programmable gate
0.8974273478	connectionist temporal classification
0.8972549486	mutual information
0.8972356005	hebbian plasticity
0.8972219353	big data
0.8969878040	handwritten digit
0.8963150675	inception v3
0.8963118677	crossbar arrays
0.8963023072	firing rates
0.8962986142	iot devices
0.8962298286	covariance matrix adaptation
0.8958367076	equilibrium propagation
0.8956959444	fish school
0.8953643224	load forecasting
0.8953625433	pattern recognition
0.8952072656	supervised learning
0.8951744949	biological plausibility
0.8951188628	swarm robotics
0.8950909981	encoder decoder
0.8944608399	real coded
0.8944336101	domain wall
0.8944272362	order statistics
0.8944239850	activation functions
0.8943179356	evolutionary synthesis
0.8939231525	travelling salesman
0.8937274243	feedback loop
0.8936585990	oriented dialogue
0.8934952993	locally connected
0.8934854033	global optimum
0.8934709636	grey wolf
0.8933509131	building block
0.8932275921	geometric semantic
0.8930477204	convolutional neural networks
0.8929486929	digital ecosystems
0.8929204787	neural nets
0.8928579556	benchmark suite
0.8927949563	signal processing
0.8925412271	skip connections
0.8920890825	procedural content generation
0.8920159448	temporal dependencies
0.8917541396	information retrieval
0.8916904556	quadratic assignment problem
0.8916369216	feature extractors
0.8913851029	hand written
0.8909600539	adversarial attack
0.8907184700	integer programming
0.8905140497	malware detection
0.8901243916	visual cortex
0.8898252453	black box optimization
0.8895349464	rectified linear
0.8895257754	bat algorithm
0.8895054214	double descent
0.8893546784	convolutional neural network
0.8893069961	energy efficient
0.8890889002	quantum mechanics
0.8890152979	reactive power
0.8888242957	np hard
0.8886485988	univariate marginal distribution algorithm
0.8885175429	generative adversarial
0.8883786861	combinatorial optimization
0.8883051307	sleep stages
0.8883043168	maximum entropy
0.8882601521	named entity recognition
0.8882222284	activity recognition
0.8880072115	gene regulatory
0.8879572251	firefly algorithm
0.8876748819	pascal voc
0.8876343929	rough set
0.8875510604	memetic algorithm
0.8875353292	randomised search
0.8874794668	spoken language
0.8874617295	mutation operators
0.8872057925	fault tolerant
0.8871995387	handwritten digit recognition
0.8871644594	particle swarm optimisation
0.8869789292	physics informed
0.8869289706	lifelong learning
0.8866343917	long short term memory
0.8863408412	spike timing dependent plasticity
0.8860533134	recurrent neural networks
0.8858310662	global optimization
0.8857237505	medical imaging
0.8856996649	water distribution
0.8856262178	neural tangent kernel
0.8854839763	importance sampling
0.8854620932	artificial immune systems
0.8854199204	event driven
0.8853024892	bidirectional lstm
0.8849962024	living organisms
0.8849061163	hyperparameter optimization
0.8848697423	artificial neural network
0.8847819399	euclidean distance
0.8847134471	actor critic
0.8845683231	cerebral cortex
0.8843785344	harmony search
0.8843777295	sensitivity analysis
0.8843764011	leaky integrate
0.8842529997	selection pressure
0.8840428115	feature maps
0.8835240510	tensor product
0.8833065633	grid cells
0.8830997032	point cloud
0.8829984438	shot nas
0.8827691207	extreme learning machines
0.8827466619	support vector
0.8826982474	cartesian genetic programming
0.8825771723	resource constrained
0.8824986960	building blocks
0.8821939864	ordinary differential
0.8819940018	ultra low power
0.8819719355	restricted boltzmann
0.8817627073	semi supervised
0.8817070713	recurrent neural network
0.8816976206	vehicle routing problem
0.8816405253	image classification
0.8816194548	language modeling
0.8812588404	mixed signal
0.8810883850	magnetic resonance imaging
0.8804574424	computationally intensive
0.8804554253	bi objective
0.8804437084	video games
0.8803962981	offspring population
0.8803585965	weight initialization
0.8802727218	auto encoder
0.8801862589	frequency domain
0.8800394976	traffic signal
0.8799503669	similarity matching
0.8799103493	evolutionary algorithm
0.8797040048	cell assemblies
0.8795766752	extreme learning machine
0.8795446669	bee colony
0.8794408734	mixed precision
0.8788886248	parent selection
0.8788567754	shortcut connections
0.8786659024	parameter sharing
0.8786082976	mixed initiative
0.8785723610	short term
0.8784472174	hidden markov model
0.8784045692	evolutionary multitasking
0.8780105703	image caption
0.8776973244	unsupervised learning
0.8776851216	hopfield network
0.8774705458	mutation operator
0.8773056726	grammatical evolution
0.8772625210	goal directed
0.8772179912	variance reduction
0.8770814858	pure strategy
0.8769932532	multi objective
0.8768740727	phase diagram
0.8767420364	loss function
0.8767366204	spiking neuron
0.8765482713	acoustic emission
0.8764672214	video game
0.8764487247	test case
0.8763925351	faster convergence
0.8763670035	super mario
0.8759671130	vertex cover problem
0.8759089616	upper bound
0.8758878758	singular values
0.8758175682	distance metric
0.8753468410	mutation rate
0.8753355519	crossover operator
0.8753047455	higher order
0.8750674756	global minima
0.8749584576	spiking neural network
0.8746691343	uncertainty quantification
0.8744646621	anti hebbian
0.8739873586	structured sparsity
0.8739344764	mixed strategy
0.8738954227	quality diversity
0.8738907075	random access memory
0.8738670278	bi directional
0.8725618948	lower bound
0.8722407461	variable length
0.8721839467	spike train
0.8721545994	context aware
0.8721504950	traffic flow
0.8720282284	deep neural networks
0.8715181079	forward propagation
0.8713978156	similarity measure
0.8711877261	artificial immune
0.8709793154	probability distribution
0.8709089883	job shop scheduling
0.8708084851	quantum inspired
0.8706123102	covariance matrix adaptation evolution strategy
0.8705718891	average pooling
0.8704360236	spike timing dependent
0.8701904438	dimension reduction
0.8700933251	global optima
0.8700790593	neuro fuzzy
0.8700001013	reduced precision
0.8699683987	skip thought
0.8699438680	visual saliency
0.8696257233	state transition
0.8693271543	phoneme recognition
0.8692783039	vanishing gradients
0.8689220874	open source
0.8688297250	association rule
0.8687304611	black box
0.8685990711	posterior distribution
0.8684476361	power law
0.8684122889	directed acyclic
0.8676456755	decision making
0.8671943328	chance constrained
0.8671579676	pseudo boolean
0.8668699826	bleu points
0.8667121625	unitary matrices
0.8665864919	bacterial foraging optimization
0.8663134950	weighted sum
0.8662032579	phase transition
0.8659652120	auto encoders
0.8658313530	data augmentation
0.8656805427	max pooling
0.8656774659	false positive
0.8651888890	spike trains
0.8649543064	multi layer perceptron
0.8648867984	design space exploration
0.8648639461	mobile robot
0.8646173233	conflicting objectives
0.8643499138	early stopping
0.8635739292	facial expressions
0.8629186356	extremal optimization
0.8627441261	mobile devices
0.8626214162	drift theorem
0.8625939791	decision makers
0.8623723654	bilevel optimization
0.8623708322	secondary structure
0.8622539832	machine reading
0.8621274045	search spaces
0.8620595126	echo state networks
0.8619694032	word embedding
0.8618199630	event triggered
0.8617866963	spectral clustering
0.8616638813	stochastic gradient
0.8614675596	digital circuits
0.8612568192	case study
0.8609613355	biologically realistic
0.8607026055	derivative free
0.8607010060	beetle antennae
0.8605138416	fitness function
0.8598424070	multi scale
0.8596443005	evolutionary strategies
0.8594966658	correctly classified
0.8593693880	image patches
0.8593645409	vision sensor
0.8591298465	logic gates
0.8590176232	saddle points
0.8589066461	brain inspired
0.8588283652	special cases
0.8587372847	monotone functions
0.8587362884	multi layer
0.8584092573	randomly initialized
0.8584067612	takes place
0.8583928368	probabilistic graphical
0.8583240200	vertex cover
0.8581784716	light weight
0.8579360724	fine grained
0.8576993868	permutation invariant
0.8575986104	closed form
0.8575818814	random forests
0.8574223191	ground truth
0.8570093694	visual perception
0.8568730159	probability distributions
0.8567954648	mid level
0.8567395974	population diversity
0.8567252616	lower bounds
0.8566988690	mammalian brain
0.8566728556	divergent search
0.8564086205	residual networks
0.8559461937	fixed budget
0.8556762218	language processing
0.8556019919	job shop scheduling problem
0.8555073124	context sensitive
0.8554195485	multi agent
0.8554077140	evolution strategies
0.8553495671	poorly understood
0.8550249211	long term
0.8548963989	hyper heuristic
0.8546099419	generative adversarial networks
0.8544349736	image recognition
0.8543947797	hessian free
0.8543495295	information processing
0.8543303082	memristor crossbar
0.8543174927	fully convolutional
0.8542071875	starting point
0.8541505010	hidden nodes
0.8540638718	neuromorphic chips
0.8539110459	dempster shafer clustering
0.8538628962	low latency
0.8538219796	function approximation
0.8537662939	error rate
0.8536218568	compression ratio
0.8535295657	fading memory
0.8533525277	convolutional filters
0.8530536732	word embeddings
0.8527502960	hidden units
0.8526529297	parameter space
0.8523884467	unbiased black box
0.8522855483	attention mechanism
0.8522408673	approximation ratio
0.8521469740	high fidelity
0.8516528710	memory footprint
0.8515963992	firing rate
0.8513837130	complex valued
0.8513683829	neuro evolution
0.8510137939	greedy search
0.8508954715	handwritten digits
0.8507833685	spatio temporal
0.8506514514	universal approximators
0.8503454858	hill valley
0.8502577510	decision maker
0.8501829153	open questions
0.8500644218	convergence rate
0.8500617433	analog neuromorphic
0.8497998277	remains unclear
0.8497435537	standard deviation
0.8496379979	human intervention
0.8495708588	empirical evaluation
0.8495136928	high throughput
0.8493125987	scheduling problem
0.8491487350	object tracking
0.8490584382	long short term
0.8489511180	worst case
0.8488045614	large scale
0.8487923911	super resolution
0.8486688690	hidden layers
0.8483868732	image synthesis
0.8482321980	low power
0.8479935068	embedded systems
0.8477705014	baldwin effect
0.8477503674	fisher information
0.8476806888	dempster shafer theory
0.8473799901	general video game
0.8473224153	target propagation
0.8472300003	high speed
0.8470842173	open ended evolution
0.8468881275	average convergence rate
0.8466188554	population size
0.8465287726	map elites
0.8464180216	gravitational search
0.8464052475	dictionary learning
0.8463704493	accurately predict
0.8462971284	statistical tests
0.8462761851	encoding scheme
0.8460265819	human brain
0.8460137863	linear programming
0.8459501837	extensively studied
0.8458430148	widely adopted
0.8456864470	gradient ascent
0.8454651584	fashion mnist
0.8450925938	electricity consumption
0.8450407914	social interactions
0.8450370252	high resolution
0.8450222229	bio inspired
0.8449650479	biologically implausible
0.8448687583	differential equation
0.8448127413	feature engineering
0.8445398817	context dependent
0.8444653210	cost sensitive
0.8444054609	hardware accelerator
0.8443856909	natural gradient
0.8443806004	aerial vehicles
0.8441633747	ultra high
0.8437254558	max min
0.8436358813	years ago
0.8435452716	previously unseen
0.8433604895	critical points
0.8431385736	named entity
0.8430237111	globally optimal
0.8429687139	increasingly popular
0.8428494705	neural architecture search
0.8425065741	widely accepted
0.8423497915	convolution kernels
0.8421239514	bio plausible
0.8418883920	episodic memory
0.8416239168	nodule detection
0.8416120418	multi agent systems
0.8413922976	hardware friendly
0.8413198666	weight pruning
0.8412512642	fixed point
0.8411246449	disentangled representations
0.8411233379	gradient boosting
0.8407975891	grows exponentially
0.8406187656	evaluation metrics
0.8405793100	response generation
0.8405606388	association rules
0.8404783877	whale optimization
0.8404396096	single shot
0.8403723322	path length
0.8403251700	cognitive science
0.8400520728	context free
0.8397818213	active inference
0.8396474827	previously published
0.8395673017	virtual machine
0.8395471790	environmental conditions
0.8395183466	missing data
0.8395029248	character level
0.8393636773	biological ecosystems
0.8391670378	knowledge graph
0.8388113382	mini batch
0.8384557786	neuromorphic processors
0.8384254171	blind source separation
0.8383661418	catastrophic forgetting
0.8383248082	computationally expensive
0.8383012466	reference point
0.8381259100	generative models
0.8380049215	main contribution
0.8379187199	bacterial foraging
0.8377065247	weight updates
0.8376791205	memory access
0.8376614684	mackey glass
0.8376143956	symbolic rules
0.8374362893	convex optimization
0.8372460401	logical relations
0.8371735683	programming language
0.8371481988	memory consolidation
0.8370912175	social interaction
0.8370829365	evolution strategy
0.8370206233	structured pruning
0.8368661225	exhaustive search
0.8366556117	memory accesses
0.8365416481	premature convergence
0.8364989961	immune inspired
0.8361382947	processing units
0.8361264420	text classification
0.8360865761	state space
0.8360177436	scales linearly
0.8359658222	weight update
0.8356901756	open ended
0.8355949822	gold standard
0.8354744442	membrane potentials
0.8353364917	residual blocks
0.8352401611	computational complexity
0.8350441156	nsga iii
0.8350163972	island model
0.8349294310	unlabeled data
0.8348418822	neural turing machines
0.8347378303	` `
0.8344057478	black boxes
0.8341125568	embedded devices
0.8341095728	liquid state machine
0.8339547710	thief problem
0.8338865098	fuzzy clustering
0.8335148154	power dissipation
0.8334124273	safety critical
0.8332559092	haploid diploid
0.8331614684	dempster shafer
0.8330618825	adaptive structural learning
0.8326093064	edge devices
0.8325624637	recurrent unit
0.8323930495	pareto optimal
0.8322530060	initial conditions
0.8321116633	empirical evidence
0.8319277852	recent advances
0.8318979625	success rule
0.8317780754	game theory
0.8317657828	edge detection
0.8317604094	sequence modeling
0.8316950288	deep rl
0.8313866500	surrogate model
0.8313539907	covariance matrix adaptation evolution
0.8310822111	missing values
0.8310775781	physical systems
0.8308362107	domain knowledge
0.8307827354	imitation learning
0.8306836812	penalty term
0.8306803334	solution quality
0.8304969508	optimal control
0.8302312829	loss surface
0.8298965667	intelligent systems
0.8296620563	capsule networks
0.8295027672	log likelihood
0.8294758877	function approximator
0.8293551950	gating mechanism
0.8291721363	multifactorial evolutionary
0.8288729308	adversarial samples
0.8287679679	transfer function
0.8286229315	low rank
0.8285962242	tsp instances
0.8285401578	random access
0.8284270292	user interface
0.8282300046	convergence speed
0.8281071172	empirical study
0.8279219978	power consumption
0.8278016435	quantum computation
0.8275792275	great success
0.8274306803	data streams
0.8273674833	bayesian optimization
0.8273223974	learned optimizers
0.8272343558	latent space
0.8270545269	transferring knowledge
0.8270301492	general purpose
0.8269346369	meta heuristics
0.8267865811	class selectivity
0.8267306870	read write
0.8265423595	open domain
0.8263695834	intrusion detection systems
0.8263190213	immune systems
0.8262680319	decision support
0.8262414586	coarse grained
0.8261360897	wireless networks
0.8260734867	curriculum learning
0.8260317400	multi layer perceptrons
0.8258525700	pre synaptic
0.8257510989	computationally efficient
0.8256149886	convolutional layers
0.8256043371	dynamic environments
0.8253468891	jump functions
0.8252643593	strong baseline
0.8251107360	multi objective optimization
0.8250235765	meta learner
0.8249279319	convolutional auto encoder
0.8249021653	object oriented
0.8248592697	object classification
0.8248213588	hardware accelerators
0.8247733279	image compression
0.8247348463	linear threshold
0.8247286307	randomly generated
0.8246673125	loss functions
0.8245682765	memristive devices
0.8243430259	weakly labeled
0.8243203816	gaussian noise
0.8242443909	cross validation
0.8240423983	deep belief networks
0.8239824953	long range
0.8238594479	real valued
0.8234624117	neuro inspired
0.8233259560	traveling salesperson
0.8231324725	univariate marginal distribution
0.8230580462	manually designed
0.8229843234	multi tasking
0.8229275147	low dimensional
0.8228903637	hyper parameter
0.8223164268	previously reported
0.8220142697	surrogate models
0.8219326191	salesman problem
0.8218099138	image generation
0.8217959591	optimization problems
0.8217676931	relative improvement
0.8217141259	weight matrices
0.8216290158	biologically inspired
0.8213648544	uniformly distributed
0.8211418935	external memory
0.8211262985	single neuron
0.8210660706	update rule
0.8210159103	constrained optimization
0.8208315028	latent variables
0.8207430098	memory augmented
0.8206812621	intelligent agents
0.8205643561	labeled data
0.8204690574	energy consumption
0.8201878354	small world
0.8199792321	speech separation
0.8199543241	small fraction
0.8197551085	low cost
0.8196925938	vital role
0.8196841582	response function
0.8196249019	fold cross validation
0.8196241597	network morphism
0.8194383259	high level
0.8191907048	os elm
0.8191683230	fuzzy inference
0.8190969764	approximation error
0.8190091494	fixed points
0.8187191896	data science
0.8187096671	unsupervised feature learning
0.8186239711	neural net
0.8176203814	event based
0.8176128466	cma es
0.8175606930	membership functions
0.8170235496	generalization error
0.8170025391	sentence representation
0.8169889959	low resolution
0.8169674051	cutting edge
0.8168791612	recurrent neural
0.8167713315	spiking neural
0.8161625304	artificial neuron
0.8158903326	high quality
0.8158251905	social networks
0.8157650481	language modelling
0.8153059160	heavy tailed
0.8151975602	multi objective optimisation
0.8150791334	image denoising
0.8150637381	hierarchical bayesian
0.8149767282	domain specific
0.8149604665	test bed
0.8149273927	quantitative analysis
0.8148274165	adversarial images
0.8146562464	visual attention
0.8145668919	spectrum disorder
0.8141584319	heuristic search
0.8137462296	nonlinear dynamics
0.8137095716	generative modeling
0.8134481104	multi modal
0.8134286905	programmable gate array
0.8133930347	expressive power
0.8132207559	generative adversarial network
0.8129024638	deep neural network
0.8128644827	edge computing
0.8126200188	pixel wise
0.8125260986	search heuristics
0.8123664161	recurrent units
0.8123186543	widely studied
0.8121651308	multi armed bandit
0.8119919877	nelder mead
0.8119091150	black box complexity
0.8118014093	prior knowledge
0.8117689202	information theoretic
0.8115527980	text generation
0.8115479101	adaptive walks
0.8114867597	data driven
0.8113520819	information extraction
0.8113064734	hebbian learning
0.8110893635	snp systems
0.8110007456	recent successes
0.8109988463	initial population
0.8109919877	gauss seidel
0.8108380021	comprehensive review
0.8108292056	parameter control
0.8108043021	genetic operators
0.8108030796	statistical physics
0.8107905940	sars cov
0.8106725930	winograd domain
0.8104760954	action spaces
0.8104550178	bayesian optimisation
0.8104130047	vice versa
0.8103189179	lottery ticket
0.8102231333	fully connected
0.8099960283	dnn weight pruning
0.8098346905	meta learning
0.8094800947	neuromorphic systems
0.8093006238	population sizes
0.8091821499	element wise
0.8091724742	continuous control
0.8091399273	exploding gradients
0.8091371586	early stage
0.8088324007	hierarchical temporal memory
0.8085075101	memetic algorithms
0.8084681531	hidden layer
0.8084055927	fitness functions
0.8081565873	graph bert
0.8080973372	user friendly
0.8079391796	numerical optimization
0.8079198072	logic programming
0.8077008964	input output
0.8075816014	error backpropagation
0.8075411932	experimental evaluation
0.8071889026	single objective
0.8069081583	adversarial training
0.8065982605	echo state network
0.8065824846	population based
0.8064889217	wall clock
0.8063391105	multimodal optimization
0.8057995210	bit width
0.8055804819	timing dependent plasticity
0.8055763740	sensory motor
0.8053792595	single pass
0.8053741441	f1 score
0.8052969866	large vocabulary
0.8052883602	salesperson problem
0.8048879193	quantum computing
0.8048604180	federated learning
0.8047806528	fine tuned
0.8047355847	sot mram
0.8047042345	multi core
0.8047030753	finite state
0.8041187901	readout layer
0.8040772213	fine tune
0.8040539452	graph embeddings
0.8034448874	open endedness
0.8033901610	deep reinforcement learning
0.8032633322	hand crafted
0.8030714268	limited memory
0.8030645632	incremental learning
0.8029343260	complex systems
0.8028823914	graph structured data
0.8027674979	hodgkin huxley
0.8024889525	brain computer interface
0.8024473950	neural machine translation
0.8022924004	success rate
0.8022373274	long standing
0.8020570593	multi class
0.8020125810	weight matrix
0.8019915647	hidden states
0.8018365738	surrogate gradient
0.8018322140	multi label classification
0.8017508868	step size
0.8017329498	sensory inputs
0.8015751566	hopfield networks
0.8015191194	convolutional autoencoder
0.8009897772	main contributions
0.8008733465	diverse sets
0.8006023077	action recognition
0.8005399176	convolutional networks
0.8005149670	cognitive architecture
0.8003334898	parameter adaptation
0.8003025165	long term memory
0.8002447573	weight transport
0.8001434619	mobile robots
0.8000585986	arithmetic operations
0.8000324295	knowledge bases
0.7998843059	optimal solution
0.7998806958	expression programming
0.7998611822	task agnostic
0.7998449130	critical point
0.7995464490	svm classifier
0.7994491561	jump size
0.7994035677	stationary points
0.7993861146	multiobjective evolutionary algorithms
0.7993371744	collaborative design
0.7993246455	analog circuits
0.7992479211	biologically plausible
0.7991959118	evolutionary optimisation
0.7991820312	test cases
0.7991615370	search history
0.7990305625	face detection
0.7989732722	matrix adaptation evolution strategy
0.7988487897	emerging field
0.7987582415	long term dependencies
0.7986294007	evolutionary programming
0.7982580152	component analysis
0.7980381669	limited precision
0.7979044666	real life
0.7972643776	floating point operations
0.7972221322	multi view
0.7970004835	deep neural
0.7964180020	design methodology
0.7963773843	hardware design
0.7957580478	word level
0.7956084187	natural selection
0.7955934655	low complexity
0.7954472970	image captioning
0.7952738502	synthetic gradients
0.7952136922	neural odes
0.7951141940	learning rates
0.7950135643	design exploration
0.7948194115	spatial temporal
0.7948107253	diversity mechanisms
0.7946864584	variation operators
0.7944443694	network embedding
0.7941961097	predictive control
0.7940497442	feature construction
0.7940172953	policy search
0.7938986025	data stream
0.7936596187	problem solving
0.7935277369	music generation
0.7933797499	structural bias
0.7929276509	single trial
0.7927949803	fine tuning
0.7927118489	design space
0.7924791033	feedforward neural networks
0.7924194747	future directions
0.7924184066	embedded platforms
0.7923041055	quantum circuits
0.7922172062	combinatorial optimisation
0.7921170738	similarity measures
0.7920318083	sensory stimuli
0.7918152886	pareto set
0.7917952318	label noise
0.7917044649	post synaptic
0.7916480519	population dynamics
0.7915048295	memristor based
0.7914093757	high frequency
0.7913654130	optimisation problems
0.7912937400	feed forward
0.7912746579	random mutation
0.7911751102	combinatorial optimization problems
0.7911563132	swarm optimization
0.7910871517	parallel computing
0.7910549411	decision variables
0.7907610967	sparse connectivity
0.7905855004	research topic
0.7904575213	sat problem
0.7904109761	white box
0.7899556870	artificial neural
0.7899298718	memory cell
0.7899065284	multi label
0.7898071407	learned representations
0.7897720685	feedforward networks
0.7897044178	minimum spanning
0.7896212483	pair wise
0.7895579764	high performance computing
0.7893879788	evolutionary strategy
0.7893345469	sequence prediction
0.7891010427	objective function
0.7890299337	softmax output
0.7890081030	session based
0.7888175489	pattern classification
0.7888132071	information flow
0.7887258641	variable selection
0.7886578063	semi supervised learning
0.7886276118	artificial evolution
0.7885471700	recombination operator
0.7884840846	binary weight
0.7881884868	local optimum
0.7881356808	numerical simulations
0.7879002076	deep belief network
0.7872916078	speed ups
0.7872418068	recent studies
0.7872287827	meta heuristic
0.7869544737	pseudo boolean functions
0.7865941131	candidate solutions
0.7864786064	task specific
0.7860406809	hand crafting
0.7859948063	bayesian network
0.7859101027	cross modal
0.7858500531	microarray data
0.7858068228	reasoning tasks
0.7855871240	gpu hours
0.7854731891	metaheuristic optimization
0.7854210962	cartesian genetic
0.7848803325	empirical studies
0.7845890148	active learning
0.7845473151	boolean networks
0.7844823710	closed loop
0.7843832345	natural language inference
0.7841420601	video frames
0.7841286607	self organizing map
0.7840579482	multi gene
0.7840348589	feature space
0.7839497882	linear equations
0.7838180174	relation classification
0.7837312261	deep convolutional neural networks
0.7832725648	medical image
0.7832652599	rigorous runtime analysis
0.7831658395	unsupervised clustering
0.7831315349	vector regression
0.7830860230	hardware implementations
0.7830404060	autism spectrum
0.7829877761	reconstruction error
0.7829405768	greatly reduced
0.7829320085	recurrent connections
0.7829221680	field programmable
0.7828666529	parameter estimation
0.7826658262	sparse dnn
0.7825812154	kt ram
0.7824383922	fixed length
0.7824014917	metaheuristic algorithms
0.7819731709	higher dimensional
0.7819567511	genotype phenotype
0.7818387139	rapid development
0.7817381226	energy efficiency
0.7816412067	update rules
0.7812959267	architecture search
0.7810932588	wireless sensor
0.7810795283	discriminant analysis
0.7810208643	gated recurrent
0.7809685468	adaptive neuro
0.7809333866	fully connected layers
0.7805013248	dimensional space
0.7804010533	adaptive control
0.7801420547	hardware acceleration
0.7800612851	optimal solutions
0.7799882753	post processing
0.7799447016	convergence analysis
0.7798479560	target motion
0.7797865913	game levels
0.7797764935	dendritic cell algorithm
0.7797348524	memetic search
0.7796072034	performance evaluation
0.7789828227	search space
0.7785995067	feature detection
0.7784991966	linear regions
0.7783157064	word error rate
0.7781531898	multi layered
0.7781469590	memory cells
0.7781157725	carefully designed
0.7778296683	motion detection
0.7774860981	regularization methods
0.7774359654	distributed computing
0.7771963307	natural images
0.7771804405	multivariate time series
0.7770737138	data sets
0.7768921254	character level language
0.7768248251	critical role
0.7767665602	nature inspired algorithms
0.7766064056	artificial neurons
0.7764920149	onemax problem
0.7763467792	processing unit
0.7763163817	benchmark functions
0.7762580118	stochastic computing
0.7762331688	broad class
0.7762196909	neuromorphic architecture
0.7761702523	dynamic routing
0.7757960054	image processing
0.7757392589	online learning
0.7756563787	adaptation evolution strategy
0.7756089248	functional connectivity
0.7755052214	threshold functions
0.7754378378	fixed size
0.7754112486	accuracy degradation
0.7753500861	change detection
0.7751312859	computational cost
0.7751286216	learning rate
0.7748884463	computing systems
0.7748472583	goal oriented
0.7747703381	data set
0.7747514545	deep nets
0.7745859785	epsilon lexicase
0.7745650775	multi channel
0.7741732919	gan training
0.7739722502	higher level
0.7737629670	sufficiently large
0.7734064451	high dimensional
0.7733942343	physics based
0.7731805856	grid search
0.7728120710	error rates
0.7727950252	optimal design
0.7727147196	stochastic search
0.7723108167	hierarchical structure
0.7721277422	pre processing
0.7721163303	conceptual framework
0.7717757071	low level
0.7717307719	high confidence
0.7715224760	routing problem
0.7714503102	nonconvex optimization
0.7711310814	colony optimization
0.7710741406	case studies
0.7710618100	sequential decision making
0.7710385317	tree structured
0.7704792927	convolutional neural
0.7703828837	sparse rewards
0.7703780561	motor learning
0.7703619421	neural turing machine
0.7702636889	principal component
0.7702553135	class conditional
0.7702453025	visual recognition
0.7701086826	analog hardware
0.7700831017	deep networks
0.7700605016	visual feature
0.7697232106	trade offs
0.7696823353	lstm rnns
0.7696760399	robotic control
0.7696643895	hardware software
0.7696368702	vector space
0.7695069668	user defined
0.7693746124	residual network
0.7690953981	latent representation
0.7690087438	online sequential
0.7689171727	expression recognition
0.7688833669	design choices
0.7688044735	cross entropy loss
0.7685419767	multi turn
0.7684513176	generalization ability
0.7683288711	memory capacity
0.7682693233	human activity
0.7678414769	deep convolutional networks
0.7678106278	visual tracking
0.7675952045	rate coding
0.7675553203	dendritic cell
0.7673909870	basic idea
0.7671934463	dynamic evaluation
0.7670768250	standard pso
0.7667712957	previous works
0.7666785540	fuzzy systems
0.7666210533	electronic health
0.7665000745	synaptic weights
0.7664975763	hyper parameters
0.7664424187	faster training
0.7663689841	game playing
0.7660025065	central pattern
0.7655825712	self organizing maps
0.7649765436	representation learning
0.7648750892	medical image analysis
0.7648373341	pre trained
0.7646671376	public datasets
0.7646157713	hidden unit
0.7645993010	small scale
0.7645775798	artificial bee
0.7645136037	inverse problems
0.7643719662	intelligent transportation
0.7643340331	significantly outperform
0.7642643527	memory augmented neural networks
0.7641382226	feed forward neural network
0.7641178810	circuit design
0.7641043457	visual object recognition
0.7639357423	population coding
0.7634637367	sequence learning
0.7633780312	evolutionary multi objective optimization
0.7633153185	comprehensive study
0.7631771860	layer wise
0.7631168549	hidden state
0.7630479141	network quantization
0.7629893982	evolutionary search
0.7628457852	evolutionary computing
0.7628164911	image transition
0.7626616021	opposition based
0.7626244124	fewer parameters
0.7625954386	scheduling problems
0.7624940794	multi step
0.7624279026	selection schemes
0.7624003450	solution sets
0.7622578914	neuromorphic circuits
0.7621421555	hopfield neural network
0.7620648516	continuous optimization
0.7619326237	main goal
0.7619066551	ensemble learning
0.7615361305	deep generative
0.7615141612	learning classifier systems
0.7614642888	bit wise
0.7613665342	inverse design
0.7612987001	tangent kernel
0.7611886712	multi objective evolutionary algorithm
0.7611407159	autonomous agents
0.7611118874	relative error
0.7610766940	dnn accelerators
0.7610687188	squared error
0.7610156973	visual stimuli
0.7609614208	reference vectors
0.7608285021	translation task
0.7607471826	artificial agents
0.7607108246	alternating direction
0.7606709535	high performing
0.7605749278	probabilistic model
0.7604188176	hidden neurons
0.7603365187	weight storage
0.7600835411	diversity optimization
0.7600550455	parameter optimization
0.7599257848	early detection
0.7598715692	consistently outperforms
0.7598196447	distributed representations
0.7594393289	theoretical properties
0.7593135617	success rates
0.7592665981	recurrent networks
0.7591196460	language models
0.7591181977	automated machine learning
0.7590831536	minimization problem
0.7590164324	hybrid evolutionary algorithm
0.7590157708	stream data
0.7587412172	shop scheduling
0.7580215238	combinatorial problems
0.7578993872	future research directions
0.7577610685	memory bandwidth
0.7577600427	manipulation tasks
0.7575273036	control policies
0.7574600069	long short
0.7572401107	connection weights
0.7571340481	recent years
0.7570605918	text representation
0.7569612168	recent developments
0.7567954463	non dominated sorting
0.7567268180	cost function
0.7566885089	scoring function
0.7565783999	visual representations
0.7565636922	weight quantization
0.7564280182	computational costs
0.7561217133	probabilistic inference
0.7561129683	antennae search
0.7560751667	high density
0.7560556446	optimization problem
0.7558002986	performance gains
0.7555699588	function evaluations
0.7554796054	neuroimaging data
0.7551328244	nervous systems
0.7550783058	lstm rnn
0.7550711577	quadratic assignment
0.7547687993	landscape analysis
0.7547133189	neural controller
0.7546595580	network topologies
0.7543484408	hopfield model
0.7542759181	dialogue systems
0.7542574437	function approximators
0.7542322413	long range dependencies
0.7537530295	test error
0.7536035386	multitask learning
0.7534967535	routing problems
0.7532240459	approximate computing
0.7531006030	learning rules
0.7528017312	hidden representations
0.7525606438	probabilistic models
0.7525425917	stochastic optimization
0.7523589329	adaptive behavior
0.7522295542	spatial information
0.7521609686	conditional probability
0.7521570564	continuous function
0.7521333897	statistically significant
0.7520918063	assignment problem
0.7519219596	markov decision
0.7518909588	random weight
0.7518039099	lif neurons
0.7517976718	loihi neuromorphic
0.7517838874	detection rate
0.7517484451	brain inspired computing
0.7517175314	power generation
0.7517136762	deep belief
0.7516365590	plasticity rule
0.7515331102	open source software
0.7512048941	data analytics
0.7507131539	engineering applications
0.7506735975	link prediction
0.7506258080	graph clustering
0.7505390409	multi dimensional
0.7503269375	objective functions
0.7502997990	saacm es
0.7502838431	convergence properties
0.7502130505	pre training
0.7500247988	snn conversion
0.7486045440	chain monte carlo
0.7484723322	audio visual
0.7483200614	computational resources
0.7478711476	global search
0.7477105141	random search
0.7476637825	data analysis
0.7474920572	x ray
0.7474802062	local minimum
0.7474521695	mnist handwritten digits
0.7474386386	efficient exploration
0.7473288835	bi atsp
0.7472711175	ultra low
0.7470472969	feature map
0.7466816709	neural coding
0.7466369295	image analysis
0.7463148087	sample complexity
0.7463058099	digit recognition
0.7460131358	multi robot
0.7459304272	joint training
0.7459292765	single objective optimization
0.7458797724	real world
0.7458690146	model based policy
0.7457753262	economic load
0.7457385574	neuromorphic engineering
0.7455419209	lower dimensional
0.7451274087	streaming data
0.7449738778	continuous valued
0.7449608342	stability analysis
0.7449123602	operating conditions
0.7442706998	sequential data
0.7441482337	stdp based
0.7441071137	human machine
0.7440275336	hidden markov
0.7439673280	multi level
0.7439498358	neuromorphic chip
0.7437452766	differentiable neural
0.7436287268	mathematical analysis
0.7431606555	optimization algorithms
0.7431054156	parallel genetic algorithm
0.7431003455	previous studies
0.7430373460	visual features
0.7428809865	significantly improved
0.7428166042	highway networks
0.7421136641	benchmark test functions
0.7419147688	temporal information
0.7418135647	agent based
0.7418114037	recently attracted
0.7415443493	comparative study
0.7412111684	pareto optimal solutions
0.7408756420	fully automated
0.7408451817	self organization
0.7408395022	benchmark problems
0.7408353561	binary synapses
0.7407347647	statistical analysis
0.7407007564	numerical experiments
0.7404887570	bit mutation
0.7404029701	long distance
0.7403835742	preliminary results
0.7403782662	sparse distributed
0.7403092010	extensive experiments
0.7401889132	synthetic data
0.7401866214	experimental analysis
0.7400618704	pso variants
0.7400330931	rigorous runtime
0.7400304485	pre defined
0.7397486747	collective decision
0.7397073474	state feedback
0.7392597411	cnn architectures
0.7390360892	differential evolution algorithm
0.7389351865	joint distribution
0.7388058427	search strategy
0.7386352405	extensive evaluation
0.7381924615	combinatorial optimization problem
0.7379348265	language understanding
0.7376723942	deep cnns
0.7376381966	knowledge extraction
0.7376082603	constrained optimization problems
0.7375828699	information contained
0.7374298076	multi swarm
0.7373490548	optimization algorithm
0.7370221475	randomly chosen
0.7369640921	speech signals
0.7367193336	theoretical framework
0.7366858061	efficient inference
0.7366087028	activation maximization
0.7365162973	neuronal activity
0.7363494983	open problems
0.7363446719	synthetic images
0.7361134524	significantly improves
0.7360895759	parameter settings
0.7358611376	evolutionary design
0.7357603824	convolutional layer
0.7356919254	elitist evolutionary
0.7356866626	bayesian deep learning
0.7356650074	sample efficient
0.7355496545	recently introduced
0.7354345404	recent works
0.7353303934	mean square error
0.7352069505	regulatory networks
0.7350286440	linear transformations
0.7346218937	gaussian distribution
0.7345618389	dynamic systems
0.7343053636	binary classification
0.7342468275	evolutionary processes
0.7340566994	gradient based
0.7337543076	swarm algorithm
0.7336506826	mixture model
0.7335776644	noisy data
0.7335128455	visual representation
0.7334164215	neuronal networks
0.7331191892	problem instance
0.7331016977	convolutional architectures
0.7330829616	linear combination
0.7324857234	expected runtime
0.7322763868	decision space
0.7322019849	feature representations
0.7321510156	fitness evaluation
0.7320744442	sparse representations
0.7320641330	high order
0.7320134475	firing patterns
0.7318625620	significantly outperforms
0.7318529737	long short term memory networks
0.7315740486	biological brains
0.7315138875	compact genetic algorithm
0.7315084650	information fusion
0.7314351832	hand designed
0.7314271464	engineering design
0.7312695528	mirroring neural
0.7311980140	main result
0.7311605830	vector representation
0.7306876500	action space
0.7304537193	feature detectors
0.7303776181	binarized neural networks
0.7300788611	high efficiency
0.7297147040	evolutionary optimization
0.7293874511	human pose
0.7293414079	basis function
0.7293358566	learning rule
0.7292213271	fully connected layer
0.7291244266	relu activation
0.7281593646	random variables
0.7281258590	large margin
0.7279199217	no free lunch
0.7278755940	adaptive parameter
0.7277750477	reinforcement learning agents
0.7275285186	single hidden layer feedforward
0.7274706352	unified framework
0.7273493763	classification accuracies
0.7272512699	optimisation problem
0.7272340927	cell types
0.7271030405	handcrafted features
0.7268198013	network topology
0.7263037950	generalization capability
0.7262088102	input signal
0.7261562811	non volatile memory
0.7260572975	resource efficient
0.7257179629	fully supervised
0.7256646296	heuristic method
0.7255362622	synaptic connections
0.7254610886	acoustic models
0.7252311211	class labels
0.7249561887	brain activity
0.7248873637	mnist dataset
0.7243762107	computational capabilities
0.7242902263	parameter selection
0.7242548647	power efficient
0.7241480128	significantly improve
0.7239244573	neural circuits
0.7236359373	layer wise relevance propagation
0.7235600251	long short term memory recurrent
0.7235193166	input variables
0.7233312510	hierarchical recurrent
0.7230288794	coding scheme
0.7229176202	cognitive functions
0.7228937373	benchmark datasets
0.7227873999	random fields
0.7226945242	deep convolutional
0.7226486407	model parallelism
0.7226014887	information content
0.7223378237	biological neurons
0.7222013210	pooling operations
0.7220457088	multiple optima
0.7220252857	external world
0.7218778490	vector representations
0.7218764532	language model
0.7217432280	general intelligence
0.7216041090	biological evolution
0.7212068095	validation set
0.7211383203	relevance propagation
0.7211296889	labeled examples
0.7209368292	neuromorphic architectures
0.7207566537	feature subset
0.7206133474	dynamic environment
0.7204864650	feedforward neural network
0.7201777896	model free
0.7200935843	regularization method
0.7200581656	spike patterns
0.7200068412	optimisation algorithms
0.7196630944	hardware implementation
0.7195969601	deep reinforcement
0.7195064993	high accuracy
0.7192849857	discrete optimization
0.7189364029	deep neural nets
0.7187806315	shallow networks
0.7187264382	low resource
0.7186895438	interaction network
0.7184479105	continuous space
0.7184266391	bidirectional recurrent
0.7183220501	theoretical analyses
0.7182889265	convolutional network
0.7177971130	information theory
0.7176942280	feed forward networks
0.7176765356	fixed target
0.7176628019	computational neuroscience
0.7176606772	pooling layer
0.7175732540	sentiment classification
0.7175728628	kernel based
0.7175138027	test set
0.7173377558	input samples
0.7173072747	artificial bee colony algorithm
0.7170220867	self organized
0.7170155105	theoretical study
0.7168376020	output layer
0.7162925374	variational auto
0.7160730928	recursive neural networks
0.7160486188	linear functions
0.7160070512	zero shot
0.7159953845	training speed
0.7159214693	fuzzy rules
0.7158109536	stochastic processes
0.7155672673	attractor dynamics
0.7152564671	low energy
0.7151958658	self assembly
0.7151470938	rl agent
0.7150337289	ct images
0.7149787853	important features
0.7148275834	classification accuracy
0.7147067490	cognitive tasks
0.7145089687	final population
0.7144581308	multi task
0.7144054277	vanishing gradient
0.7142717107	nonlinear activation functions
0.7140102255	neural tangent
0.7138852399	neural architecture
0.7133020201	random number
0.7132660692	deep feedforward
0.7130274779	high precision
0.7129404870	fuzzy rule
0.7128811136	pooling layers
0.7124831995	von neumann architecture
0.7124560734	self adjusting
0.7124450522	test functions
0.7122105116	error signal
0.7121158924	competitive results
0.7118240904	single output
0.7115597398	distance measure
0.7114316680	random initial
0.7112734700	visual question
0.7111063761	cost functions
0.7109867748	spike based
0.7107860993	agent populations
0.7106345700	principal components
0.7104104798	multi stage
0.7104090544	tree based
0.7100081226	memory usage
0.7099793034	multiple scales
0.7099035525	recurrent attention
0.7098542862	promising solutions
0.7096261371	real world problems
0.7095465341	increasing complexity
0.7095025088	downstream tasks
0.7091489096	hand crafted features
0.7089569112	predictive models
0.7087878595	single image
0.7086225730	high capacity
0.7085591116	input vector
0.7083533681	performance metrics
0.7083061153	multiobjective evolutionary
0.7082779186	memory requirements
0.7082683879	graph neural networks
0.7082461942	k means
0.7081896597	increasingly important
0.7078537926	spiking activity
0.7077147715	multi objective evolutionary algorithms
0.7076927026	language generation
0.7074839244	bayesian optimization algorithm
0.7072763775	lower level
0.7072550875	quantum circuit
0.7069952821	linear units
0.7069790201	excellent performance
0.7069535279	input sequence
0.7068671414	self organising
0.7067412965	basis functions
0.7066809711	discriminative features
0.7065867421	boolean functions
0.7065737676	particle swarm optimization algorithm
0.7058227204	self organising maps
0.7058198779	offspring solutions
0.7057623732	feature learning
0.7054579272	image retrieval
0.7051651137	optimum solutions
0.7051599626	neural architectures
0.7047528652	distance based
0.7045527081	manifold learning
0.7043720049	eeg data
0.7042169331	design principles
0.7041235334	transfer functions
0.7038240797	robot control
0.7037932382	hybrid approach
0.7036104801	weight vectors
0.7035791357	deep architectures
0.7034886333	visual processing
0.7032060795	empirical success
0.7032031947	field theory
0.7030683957	hybrid algorithm
0.7028139731	gradient free
0.7027718675	training samples
0.7027062676	high performance
0.7021297342	gradient flow
0.7017269699	temporal dynamics
0.7014898696	training sets
0.7010915111	unsupervised feature
0.7008675842	image quality
0.7007987584	term memory
0.7007906254	square error
0.7004676094	increasingly complex
0.7004443203	visual scene
0.7004139828	recently proposed
0.7003856424	local learning rules
0.7000509505	l bfgs
0.7000302846	solving combinatorial
0.7000242585	great potential
0.6999222466	text to speech
0.6998579833	output sequence
0.6997957515	sorting genetic algorithm
0.6996025792	artificial systems
0.6992183774	multi objective particle swarm
0.6986441875	experimental results
0.6985362430	initial results
0.6985266507	root mean square
0.6984726385	ranking based
0.6983205011	spiking networks
0.6981486992	human intelligence
0.6979770471	inference latency
0.6978389464	theoretical analysis
0.6975159179	fully trained
0.6974432417	training times
0.6973528112	single layer
0.6973179824	high quality solutions
0.6972876491	contextual information
0.6969109589	input signals
0.6965570875	decomposition based
0.6965369065	numerical examples
0.6963698551	robust optimization
0.6962115146	abc algorithm
0.6961216709	motor control
0.6960726740	convolution layers
0.6960489960	problem classes
0.6960045738	human motion
0.6959071140	visual input
0.6958489900	higher layers
0.6956381797	training examples
0.6955010027	random features
0.6951216536	iterative optimization
0.6949688343	connectionist models
0.6947210853	computational overhead
0.6947012801	reward function
0.6945724777	derivative free optimization
0.6944050259	theoretical basis
0.6943160532	mathematical model
0.6941997084	pruning rate
0.6941608022	times faster
0.6941577823	distributed evolutionary
0.6941370474	high performing solutions
0.6941321180	multi objective evolutionary
0.6940465136	parameter setting
0.6937060203	array based
0.6935414255	nonlinear systems
0.6935195865	non dominated sorting genetic algorithm
0.6933297988	multiple types
0.6933047366	individual neurons
0.6931823591	black box models
0.6930390123	threshold values
0.6929832650	u net
0.6929675286	visual information
0.6926807472	neural language models
0.6925782962	parameter values
0.6922144014	preference information
0.6921752792	impressive results
0.6918375092	polynomial neural networks
0.6916131397	fundamental problem
0.6915566049	small target
0.6914417979	output variables
0.6913645141	improving performance
0.6908582985	improved performance
0.6906789175	action selection
0.6905448868	generated images
0.6905250974	running time analysis
0.6902014907	unseen data
0.6902005923	closely related
0.6900532848	relu activations
0.6899699041	recurrent layers
0.6899384170	human experts
0.6896823075	multi task learning
0.6894559315	reward functions
0.6893730616	control parameters
0.6893560637	scale free
0.6885414748	inference accuracy
0.6884686884	test instances
0.6880987958	single machine
0.6880534268	traffic data
0.6879497589	spatial domain
0.6878986490	deep recurrent
0.6878752198	sample efficiency
0.6878646974	spike events
0.6877566613	tree search
0.6877416695	vanishing gradient problem
0.6876170341	complex behaviors
0.6873877921	linear unit
0.6873415010	linear transformation
0.6873118531	information capacity
0.6872597662	design problem
0.6872438595	weight values
0.6869346868	feedforward neural
0.6869066544	accuracy loss
0.6866261189	parallel training
0.6863519242	feedback weights
0.6861548927	network structure
0.6859202085	input weights
0.6858792517	point crossover
0.6858254026	evolutionary multi objective
0.6856814006	results suggest
0.6856453574	organizing map
0.6856427109	component based
0.6855838374	sample size
0.6854772398	specialized hardware
0.6853301909	significant role
0.6852584036	learned features
0.6850139511	temporal correlations
0.6850019596	biological brain
0.6848469688	engineered features
0.6846227612	paper presents
0.6845685836	output neurons
0.6843001650	computational efficiency
0.6842610207	evaluation function
0.6840096400	phase space
0.6839489773	attention mechanisms
0.6839389673	supervised classification
0.6839323944	neuro evolutionary
0.6838094243	self organizing
0.6836174878	accurate predictions
0.6831455993	image pixels
0.6828161779	self replicating
0.6827080486	predictive model
0.6826511971	embedding space
0.6826146215	hierarchical representations
0.6825295352	heuristic algorithms
0.6824660108	model compression
0.6824320763	software framework
0.6824165057	artificial immune system
0.6823808443	real numbers
0.6816360108	performance improvement
0.6814861240	continuous speech
0.6814439613	weight vector
0.6814245618	conductance based
0.6813496100	np hard combinatorial
0.6813439703	promising results
0.6812947644	human perception
0.6811565985	output neuron
0.6807959954	higher accuracy
0.6807612507	generative model
0.6807274812	optimum solution
0.6806714711	high dimensionality
0.6803023157	neural plasticity
0.6801940654	greedy algorithm
0.6800598922	neural network architectures
0.6799379235	pruning methods
0.6798717088	meta heuristic optimization
0.6797452216	selection mechanism
0.6796716396	internal structure
0.6795937095	pso algorithm
0.6793618792	free lunch
0.6793533669	direct training
0.6791580077	numerical values
0.6791407364	multidimensional data
0.6791051992	world models
0.6790590971	mnist database
0.6786390923	empirical results
0.6785168625	expert knowledge
0.6783626633	significantly reduces
0.6774193785	least squares
0.6773285570	prediction error
0.6772741871	pruning techniques
0.6772113682	optimisation process
0.6771482497	historical data
0.6770133175	multiple objectives
0.6769829860	ant colony optimization algorithm
0.6768782134	plasticity rules
0.6768461505	linear combinations
0.6764433295	superior performance
0.6764078114	time variant adaptive
0.6761995943	improved accuracy
0.6761741765	evolving agent
0.6759210588	deep q network
0.6759176753	speech signal
0.6757941610	neuron model
0.6757043983	knowledge representation
0.6754512051	visual object
0.6753987413	salesman problems
0.6753804893	recurrent network
0.6753318612	associative learning
0.6753310034	learning machines
0.6751297725	backpropagation algorithm
0.6750221664	mutation probability
0.6748200205	onemax function
0.6746272006	non elitist
0.6745633475	adversarial networks
0.6744275864	power efficiency
0.6742969999	mnist benchmark
0.6741902702	social network
0.6740986450	synaptic weight
0.6739645593	procedural content
0.6738434023	gene expression data
0.6738391917	convergence rates
0.6735921710	exploitation phase
0.6735195841	raw data
0.6734672718	computational effort
0.6730954157	article presents
0.6730269461	gate array
0.6729889078	matrix vector
0.6727745942	quantum states
0.6723653036	global convergence
0.6723525289	recently developed
0.6723230081	significant improvements
0.6722091144	promising alternative
0.6720881408	problem instances
0.6720736831	computing paradigm
0.6719641402	multi modal multi objective optimization
0.6718509695	regression tasks
0.6717181071	chaotic systems
0.6714720001	indicator based
0.6713385113	theoretical findings
0.6711506349	proven effective
0.6704638432	unsupervised pre training
0.6702224238	regression problems
0.6701216293	bidirectional long
0.6700997695	problem sizes
0.6700330521	interesting features
0.6699944300	highly efficient
0.6697761014	deep convolutional neural network
0.6696236780	state machines
0.6691547668	absolute error
0.6690574369	network design
0.6685257109	spike times
0.6684548688	language tasks
0.6684278974	rule mining
0.6684153088	internal states
0.6678919268	conduct experiments
0.6678275354	memory intensive
0.6678107435	biological systems
0.6678094103	performance improvements
0.6678021045	target domain
0.6676644324	resource limited
0.6673855280	vector matrix
0.6673412167	random sampling
0.6672185534	solving constrained
0.6671914640	layer feedforward
0.6671363451	neural circuit
0.6671290604	single neurons
0.6668429734	hardware platforms
0.6667791164	classification tasks
0.6667260346	compression techniques
0.6667188206	regularization techniques
0.6667054186	predictive performance
0.6663322787	data structures
0.6660782609	deep learning models
0.6658505163	student network
0.6657705623	automatic speech
0.6657574989	hardware efficiency
0.6655980877	low power consumption
0.6653672891	active research
0.6650342082	easily implemented
0.6647982890	complexity theory
0.6647681620	statistical methods
0.6646970749	search algorithm
0.6646403166	qualitative analysis
0.6646282050	direct feedback
0.6645411566	smooth functions
0.6642804959	control mechanism
0.6641930277	rule based
0.6641004457	term dependencies
0.6640818356	machine learning models
0.6636399742	linear models
0.6634785954	newly proposed
0.6632143229	research directions
0.6631865856	highly nonlinear
0.6630780265	specifically designed
0.6626853897	multi objective genetic
0.6626798128	previously unknown
0.6624774822	taking into account
0.6622647690	current research
0.6621962907	deep linear
0.6621598332	multi objective genetic programming
0.6617862180	design decisions
0.6614462890	memory blocks
0.6612545752	hardware architecture
0.6607549731	training set
0.6604693292	non negative matrix
0.6604405602	soft attention
0.6604217890	linear classifier
0.6603738661	recognition accuracy
0.6602983159	distributed systems
0.6602235694	attention based
0.6601548943	training procedure
0.6601129821	training epochs
0.6600383165	dnn based
0.6600028843	jointly trained
0.6599590091	multi class classification
0.6597765732	class specific
0.6593821928	memory devices
0.6590914848	relevant information
0.6587985124	feed forward neural networks
0.6586643663	successful application
0.6581792031	selection scheme
0.6580469358	basins of attraction
0.6572154703	computational power
0.6571616565	practical applications
0.6571512854	parallel genetic
0.6568095525	shop scheduling problem
0.6567798874	accuracy drop
0.6565481528	energy function
0.6564005353	signal to noise ratio
0.6563004843	test function
0.6562350978	sentence level
0.6561588619	highly scalable
0.6558696152	self organisation
0.6558519466	feature based
0.6557855944	network architecture
0.6556891342	reward based
0.6555832763	evolutionary reinforcement learning
0.6553953116	previously proposed
0.6553090876	regularization term
0.6551200746	paper proposes
0.6548669267	similarity based
0.6548595321	baseline methods
0.6547631895	analysis reveals
0.6544989226	highly complex
0.6542587174	experimental result
0.6539121640	winner take
0.6536708370	probability model
0.6533772946	combinatorial problem
0.6533337171	preliminary experiments
0.6532448435	initial set
0.6531737846	biological neural networks
0.6530995128	numerical results
0.6530766569	hopfield neural
0.6530743136	takes into account
0.6529892374	multiple levels
0.6528220398	valuable information
0.6523649717	computing platform
0.6522924658	predictive accuracy
0.6521895626	competitive performance
0.6520223407	study presents
0.6519614831	theoretical results
0.6519229052	challenging task
0.6518030698	brain function
0.6515822378	benchmark dataset
0.6514000358	nas methods
0.6512695886	web based
0.6511628852	least square
0.6508782953	energy based
0.6508604628	empirical performance
0.6508115852	state dependent
0.6507532217	weight memory
0.6507234425	student model
0.6507210860	power systems
0.6506162347	computational budget
0.6499089493	regression models
0.6496322991	output space
0.6496313117	achieve higher
0.6494993144	computational models
0.6492856953	computational load
0.6490357420	highly competitive
0.6489998265	generalized linear
0.6488424588	solution space
0.6488202419	nlp tasks
0.6485014024	key elements
0.6480575844	solving optimization problems
0.6480571189	prior works
0.6479776655	lstm networks
0.6477907066	hybrid method
0.6476020608	data points
0.6475452806	prior information
0.6473120646	optimal point
0.6470314612	back propagation
0.6469755550	feature spaces
0.6469211489	rnn architectures
0.6467119573	exploration ability
0.6465721030	mathematical models
0.6464296052	comparable results
0.6463930178	model based control
0.6463486207	computer aided
0.6462991393	local interactions
0.6457493439	previous approaches
0.6456082302	research works
0.6453772689	multiple times
0.6453525379	increasing demand
0.6451488718	deep spiking neural networks
0.6451112414	test accuracy
0.6448752319	evolutionary deep intelligence
0.6446031543	content generation
0.6445770717	additional information
0.6445559563	recurrent layer
0.6445517187	acquisition function
0.6444396697	batch size
0.6440914025	problem specific knowledge
0.6440264630	meta heuristic algorithms
0.6439744223	achieving high
0.6434328377	input values
0.6432331851	recurrent spiking
0.6431624708	model complexity
0.6431585358	fitness evaluations
0.6426149583	ant based
0.6425522799	shown great
0.6422694369	similar accuracy
0.6419881048	vanishing and exploding gradients
0.6419719276	higher quality
0.6418597317	internal representations
0.6418595145	significant advantages
0.6418134226	highly parallel
0.6416347491	single run
0.6412459300	graph convolutional networks
0.6411590735	rnn models
0.6409723744	main challenges
0.6406734194	few shot
0.6404356459	remains challenging
0.6404265130	input vectors
0.6403564650	comparable accuracy
0.6400557916	protein structure
0.6399473789	pso based
0.6397898484	relevant features
0.6396773437	recurrent models
0.6396668591	network pruning
0.6395440277	fast convergence
0.6393399900	lenet 5
0.6389299804	attention weights
0.6387094844	sensory input
0.6386926892	traditional methods
0.6386641934	cnn architecture
0.6386411844	state variables
0.6380448041	single task
0.6380215497	single hidden layer
0.6379592016	swarm intelligence based
0.6376482242	important issue
0.6374857275	outstanding performance
0.6373970517	estimation of distribution algorithms
0.6372717283	pareto front
0.6371978048	clustering methods
0.6371309768	reduction technique
0.6371231435	continuous functions
0.6370562229	recurrent connectivity
0.6370270852	chip memory
0.6369567835	structural properties
0.6369072462	lower power
0.6368729512	deep relu
0.6368133691	spiking neuromorphic
0.6364444984	significantly reduce
0.6363598112	future research
0.6363493127	study shows
0.6361575304	engineering problems
0.6358182507	real data
0.6356877068	emo algorithm
0.6356630648	multimodal functions
0.6355722268	computational resource
0.6355381182	existing approaches
0.6355275740	high computational
0.6353509807	evolutionary dynamics
0.6352704319	results reported
0.6350380364	research field
0.6349580656	related tasks
0.6348892657	test problems
0.6347668666	matrix adaptation
0.6346186667	achieve competitive
0.6345145779	metaheuristic algorithm
0.6344291124	synthetic datasets
0.6343523467	sorting genetic algorithm ii
0.6341847961	power output
0.6341389383	image classification benchmarks
0.6339744021	provide insights
0.6339360318	difficult task
0.6336963611	mining techniques
0.6335011469	global optimization problems
0.6334599905	xor problem
0.6334377398	recent progress
0.6334179189	lstm based
0.6331404143	typically require
0.6330832848	extensive experimental
0.6326526651	degrees of freedom
0.6324495210	experiments reveal
0.6317001238	internal state
0.6315605824	significant progress
0.6313387423	intelligent behavior
0.6312772927	multimodal problem
0.6312693266	high energy efficiency
0.6311811232	unlike previous
0.6308582867	time series forecasting
0.6305811232	global minimum
0.6303413150	standard benchmarks
0.6302629677	labeled training
0.6301216628	existing methods
0.6300420965	non stationary
0.6299718016	neural network's
0.6299673347	rl algorithms
0.6299489279	alternative approaches
0.6297559067	main idea
0.6297519174	application domains
0.6296228567	solving problems
0.6295990241	provide evidence
0.6295792348	the univariate marginal
0.6293197315	paper describes
0.6290618315	selection strategy
0.6288043190	multi armed
0.6287363802	classification benchmarks
0.6287345871	acoustic features
0.6286249362	computation cost
0.6285935676	extensive empirical
0.6284629269	local search algorithm
0.6283837117	cognitive systems
0.6283518499	research community
0.6282241293	long sequences
0.6281940566	linear function
0.6281306560	partial differential
0.6280888474	trainable parameters
0.6280505220	training data
0.6280207587	machine learning techniques
0.6278295962	computing platforms
0.6276166188	search capability
0.6274473987	prediction model
0.6274188934	latent variable models
0.6273893176	neuronal network
0.6273830524	ticket hypothesis
0.6269638494	recurrent neural network architectures
0.6268412854	evolutionary multiobjective
0.6267651710	cost effective
0.6264055597	internal representation
0.6261176044	input space
0.6260900620	output signals
0.6259588599	significantly smaller
0.6258871782	neuromorphic devices
0.6258014797	computing resources
0.6257382953	mnist handwritten
0.6257195825	results showed
0.6252813065	competitive learning
0.6252338521	practical problems
0.6249374925	detailed analysis
0.6248727440	stochastic runtime
0.6248380497	fuzzy inference system
0.6247794764	inference speed
0.6245779613	hard problem
0.6244749166	widely applied
0.6244389194	recent research
0.6240126658	outperforms existing
0.6238453829	target function
0.6236145079	result shows
0.6235483571	graph structured
0.6234576063	task oriented
0.6231776029	experimental studies
0.6230952813	report results
0.6228624989	highly effective
0.6227855024	key feature
0.6227719460	feature representation
0.6227513849	structured weight
0.6227276207	mean absolute
0.6226798538	neural network based
0.6226325768	previous methods
0.6225952780	input patterns
0.6225072800	mnist data set
0.6224936358	model selection
0.6224343490	human level
0.6219478715	machine learning approaches
0.6214524796	pre train
0.6214184844	dynamic optimization
0.6213373379	research area
0.6210265018	network intrusion detection
0.6209387688	unlike existing
0.6209059605	challenging tasks
0.6209012894	vocabulary speech
0.6208870697	divide and conquer
0.6205950797	internet of things
0.6205912819	average case
0.6205555452	empirically demonstrate
0.6204864407	sigmoid activation
0.6204585040	neural activity
0.6201102155	graphical models
0.6197548849	neural dynamics
0.6197278240	feedback connections
0.6196047822	human designed
0.6195412963	error signals
0.6195106371	competitive accuracy
0.6194874780	analysis shows
0.6194157975	hard combinatorial
0.6193694167	moea d
0.6192856904	synthetic dataset
0.6191548999	shallow network
0.6191201434	crucial step
0.6191018760	vision tasks
0.6190956528	resnet 18
0.6190692435	constrained multi objective
0.6188374474	real world applications
0.6185049423	n gram
0.6184995169	optimization strategies
0.6184989137	emo algorithms
0.6182087906	multiple layers
0.6181724051	model based
0.6179743868	paper introduces
0.6179197539	brain regions
0.6176767575	nsga ii
0.6176204965	challenging problem
0.6174489315	deep snns
0.6173286096	complex domain
0.6171629628	algorithm selection
0.6170548952	real datasets
0.6170295191	superior results
0.6169148544	ai systems
0.6167423148	hard problems
0.6163229641	optimization techniques
0.6163050978	machine learning algorithms
0.6161709024	dynamical properties
0.6161661414	optimal values
0.6161069107	high dimensional space
0.6157855587	data clustering
0.6157708713	vector machine
0.6157486402	tracking algorithm
0.6157153244	pre existing
0.6156534405	previous research
0.6156508809	experimental data
0.6156122375	deep q learning
0.6155616298	technique called
0.6154928011	local information
0.6153791354	satisfactory results
0.6151763695	data driven approach
0.6151502309	paper discusses
0.6149666113	graph structure
0.6146265439	study proposes
0.6140785161	generalization performance
0.6140665392	significantly faster
0.6140412499	improves performance
0.6140313099	significant improvement
0.6140110231	specific tasks
0.6138803379	results highlight
0.6136479706	experimental results demonstrate
0.6135006937	synaptic connectivity
0.6131302860	important issues
0.6131291919	approach outperforms
0.6128987154	complex environments
0.6125903367	final performance
0.6119136952	sensor data
0.6118991693	real time strategy
0.6118276517	current approaches
0.6117919693	training deep neural networks
0.6114412796	vgg 16
0.6113509503	physical world
0.6108244269	recently shown
0.6107651799	rate based
0.6106497334	remarkable performance
0.6104033594	existing nas
0.6103227934	inner product
0.6102697112	benchmark instances
0.6098752746	imagenet dataset
0.6098438460	supervised setting
0.6095740054	results obtained
0.6095620962	computer chess
0.6093356322	results reveal
0.6092483319	approach combines
0.6091986126	memory efficient
0.6089615378	performance gain
0.6087068468	large data sets
0.6086980497	deep artificial neural networks
0.6085820628	search heuristic
0.6085456199	training phase
0.6084370405	experimental study
0.6081416623	continuous optimization problems
0.6081298543	processing steps
0.6079868785	natural evolution
0.6078445535	simulation results
0.6077604708	extracted features
0.6075100252	non parametric
0.6074940484	swarm optimization algorithm
0.6074082142	dimensional spaces
0.6073897138	benchmark tasks
0.6072677339	classification error
0.6072539458	data processing
0.6071311554	computational properties
0.6068828810	genetic algorithm based
0.6068593817	high complexity
0.6067898360	application areas
0.6065476741	connectionist temporal
0.6064214518	temporal domain
0.6063098125	efficiently solve
0.6062454969	problem dimension
0.6062084677	data sources
0.6060007295	benchmark function
0.6057737399	e commerce
0.6056689618	clustering algorithms
0.6054875925	fully connected networks
0.6054813687	small datasets
0.6054188644	previously learned
0.6051350143	paper investigates
0.6051106783	existing studies
0.6049632696	efficient implementation
0.6048733708	features extracted
0.6047972910	special case
0.6045858266	highly accurate
0.6043158613	rgb d
0.6042294484	natural image
0.6041692105	pieces of evidence
0.6041271979	computing power
0.6041260747	promising performance
0.6039461600	resnet 50
0.6034843111	binary weights
0.6033655399	significant reduction
0.6031340836	approximate solutions
0.6031270004	quality diversity algorithms
0.6030854574	previous tasks
0.6029937035	easy to implement
0.6029643772	accelerating deep
0.6029591697	experimentally demonstrate
0.6029387452	image datasets
0.6027132421	modern machine learning
0.6023766691	metric learning
0.6021549984	empirical analysis
0.6020732027	feature vector
0.6018670179	real world datasets
0.6015420351	stl 10
0.6014577609	multi objective optimization problems
0.6013146066	sensory data
0.6012117476	fitness values
0.6012110657	deep learning methods
0.6009445846	neural network models
0.6009318739	selection method
0.6009271082	vector machines
0.6009029647	structured data
0.6008438703	caltech 101
0.6005991844	experiments conducted
0.6005772814	swarm based
0.6000293447	evaluation shows
0.5998997472	selection methods
0.5995448431	classification problems
0.5992632570	existing works
0.5992493621	training instances
0.5991339284	traditional approaches
0.5990907836	proposed hybrid
0.5989555397	comparable performance
0.5989230098	nonlinear dynamical
0.5988662788	prediction accuracy
0.5984244077	experiments suggest
0.5981156434	standard backpropagation
0.5981080122	powerful tool
0.5979663423	learning experiences
0.5978533852	achieved great
0.5978012547	teacher model
0.5976875198	simulation environment
0.5975221215	proof of concept
0.5974873391	real images
0.5973646602	network learns
0.5973563186	data samples
0.5973303680	data representation
0.5972507229	search operators
0.5970882039	complex networks
0.5970834004	significantly lower
0.5965519469	test data
0.5965494939	sensory information
0.5965241219	current methods
0.5961696899	extremely high
0.5960354193	deep network
0.5960092011	application scenarios
0.5959794141	mean squared error
0.5959063432	classification rules
0.5958065077	standard benchmark
0.5956502756	regression problem
0.5956330224	method outperforms
0.5955612598	results confirm
0.5953375408	clustering algorithm
0.5950586020	\ ~ ao
0.5949156847	network configurations
0.5947499078	deep learning framework
0.5947168603	hybrid evolutionary
0.5944150564	state action
0.5938822531	neural network language models
0.5937437961	selection process
0.5937332661	complex tasks
0.5934595006	activation patterns
0.5932772761	neural network architecture
0.5931687711	frame based
0.5930989376	function class
0.5928515288	increasing attention
0.5928425449	complex interactions
0.5927109523	direction method of multipliers
0.5924797361	quality solutions
0.5923929515	key idea
0.5920886090	computing devices
0.5919780167	open problem
0.5919506135	proposed scheme
0.5917035544	conventional methods
0.5914470043	neural network structures
0.5913206894	problems involving
0.5912337857	network architectures
0.5910006533	gpu based
0.5908484171	significantly higher
0.5908066408	problem domains
0.5905084617	sum of max
0.5905073184	optimization strategy
0.5903387190	randomized search
0.5903262064	important role
0.5902437088	meta heuristic algorithm
0.5894573566	successfully applied
0.5894042325	limited data
0.5893934722	cifar 100
0.5889600821	feature vectors
0.5888488790	significantly reduced
0.5883730306	deep neural network architectures
0.5883712572	computational requirements
0.5883685607	recognition task
0.5882671425	bidirectional recurrent neural
0.5881129564	classification task
0.5878444957	recent approaches
0.5877045671	term dependency
0.5871224253	mean field
0.5870736891	ga based
0.5870187475	optical neural networks
0.5869854175	convolution neural network
0.5865330852	recognition tasks
0.5863849351	graph based
0.5859611955	existing techniques
0.5858501914	computer science
0.5856596949	deep learning techniques
0.5850265502	np hard problem
0.5848818647	intrusion detection system
0.5846586036	network dynamics
0.5846023960	algorithmic framework
0.5845170102	mutation strategy
0.5843378390	supervised training
0.5842397344	extreme learning
0.5839105957	search behavior
0.5838939122	non volatile
0.5838633565	model learns
0.5835798518	experiments demonstrate
0.5831681136	modified version
0.5831648715	computational experiments
0.5829340064	highly efficient deep neural
0.5827672194	significant performance
0.5824968703	deep convolution
0.5820003219	finding optimal
0.5819543029	many objective optimization
0.5804303267	theoretical studies
0.5800569989	problem specific
0.5792910977	strengths and weaknesses
0.5789079469	control tasks
0.5788320229	graph convolutional
0.5783400335	experimental comparison
0.5782571566	processing tasks
0.5782501551	critical applications
0.5781854341	estimation of distribution algorithm
0.5777652080	recurrent architectures
0.5776159187	architecture design
0.5775831878	image data
0.5771436511	search based
0.5762862231	evolutionary approaches
0.5760885825	benchmark data sets
0.5759146525	textual data
0.5755345345	evolution process
0.5755021400	performance loss
0.5755012079	training loss
0.5748966951	large datasets
0.5746867368	classification problem
0.5744994087	learning paradigm
0.5742052970	input data
0.5739053083	algorithm called
0.5737967985	extensive experimental results
0.5736333350	vulnerable to adversarial
0.5735951466	high level features
0.5734930972	representational power
0.5733895088	weight parameters
0.5731350920	branch and bound
0.5724163036	singular value
0.5718791507	spiking network
0.5717134461	parallel implementation
0.5715913817	open question
0.5715714846	neural computation
0.5714677208	inspired computing
0.5714499148	successful applications
0.5711236148	one shot
0.5710022837	challenging problems
0.5709981663	in memory computing
0.5709646440	neural turing
0.5708275565	performance comparison
0.5705723745	search algorithms
0.5704086729	central role
0.5703104623	particle swarm optimization based
0.5698319011	hybrid model
0.5693396071	\ url https
0.5691928214	error function
0.5688568105	grow and prune
0.5680969392	memory networks
0.5679519362	data types
0.5675444106	similar performance
0.5670548080	feed forward neural
0.5669776535	np hard problems
0.5669450020	competitive algorithm
0.5669201489	synthetic and real world
0.5666694765	pareto optimal front
0.5666230156	mutation based
0.5665112213	input features
0.5663139734	inference process
0.5650269115	resulting model
0.5645779427	great importance
0.5639776163	rnn based
0.5639611826	design process
0.5635433732	learning algorithms
0.5627874397	self supervised
0.5623795240	convex optimization problems
0.5622376004	visual tasks
0.5622352923	quantum systems
0.5620280207	excitatory and inhibitory
0.5619485711	input images
0.5617192434	approximation property
0.5613262046	comparative analysis
0.5602370286	entity recognition
0.5598283560	deep representations
0.5598055404	previous results
0.5597927593	deep neural network architecture
0.5595439520	automatically learn
0.5594584190	field of view
0.5593642542	learning procedure
0.5585739778	edge of chaos
0.5578441233	k nn
0.5577708536	crucial role
0.5575785746	achieve comparable
0.5573635992	nas algorithms
0.5573560478	dominated solutions
0.5565799347	method achieves
0.5565412917	existing algorithms
0.5563865196	key component
0.5560485857	level design
0.5557738766	network structures
0.5557238641	popular approach
0.5549262726	ann model
0.5544977122	policy learning
0.5544578009	random weights
0.5543589923	hierarchical temporal
0.5540886747	large scale problems
0.5540293009	deep learning based
0.5540014667	spiking neuron model
0.5538768200	layer wise relevance
0.5537301567	sparse neural networks
0.5535402505	deep convolutional neural
0.5530766575	few shot learning
0.5530534522	markov model
0.5523187858	neural network training
0.5520793950	mean square
0.5515582291	shown promising results
0.5514346269	adversarial learning
0.5512889850	biologically plausible learning
0.5508510885	method enables
0.5507923369	time series prediction
0.5506795674	covid 19
0.5502865346	orders of magnitude
0.5501987726	optimization tasks
0.5498097823	hitting time
0.5491540022	past few years
0.5491404807	imaging data
0.5490508464	application specific
0.5484555558	labeled and unlabeled
0.5474918191	training sample
0.5474559657	objective space
0.5472490284	learning process
0.5468866716	8 bit
0.5465725821	learning algorithm
0.5464707745	gradient information
0.5463215403	paper examines
0.5461228319	problem size
0.5457797919	higher performance
0.5453964661	end to end
0.5453514854	results demonstrate
0.5446176321	model achieves
0.5445897869	colony algorithm
0.5444239641	based approaches
0.5443701787	standard lstm
0.5438787842	deep learning architectures
0.5435532320	step by step
0.5428996036	feedforward network
0.5425844407	learning speed
0.5417469406	vanishing and exploding
0.5417452230	sampling based
0.5416228514	vision systems
0.5416190141	simulation model
0.5412604846	gradient based methods
0.5412438631	single input
0.5408817940	convolutional architecture
0.5406279019	evolutionary process
0.5404341277	deep spiking
0.5403791698	heuristic based
0.5402424259	perform inference
0.5400197922	forward and backward
0.5394298296	recently achieved
0.5393742930	large scale datasets
0.5390141706	image representation
0.5387754932	deep recurrent neural networks
0.5383319856	multi agent system
0.5377334832	automatic feature
0.5372266347	published results
0.5371490858	reservoir computer
0.5370967917	belief network
0.5370562106	proposed method
0.5357576758	cover problem
0.5357075743	neuroevolution of augmenting
0.5353570111	binary neural networks
0.5353462599	foraging optimization
0.5353372908	brainscales 2
0.5351484079	cifar 10
0.5348492536	nonlinear activation
0.5348476715	non linearity
0.5347876509	deep residual
0.5345606913	taking advantage
0.5341090987	automated machine
0.5341082936	optimization technique
0.5339077300	optimization method
0.5335897911	control problems
0.5335561683	high dimensional data
0.5328208202	classifier systems
0.5323804769	trial and error
0.5321367764	inspired genetic
0.5319441077	training process
0.5317892125	model outperforms
0.5317001796	based approach
0.5316851661	graphics processing
0.5314239013	classification performance
0.5312292051	sequence to sequence
0.5307627862	mnist datasets
0.5307532407	heuristic methods
0.5307444474	large scale optimization
0.5306081595	number of fitness evaluations
0.5302164365	gradient methods
0.5299163865	rnn architecture
0.5297702907	relu networks
0.5296859261	proposed framework
0.5295493748	network size
0.5295438755	forward model
0.5293283949	search process
0.5290671628	researchers and practitioners
0.5290047902	error analysis
0.5281961541	simulated data
0.5281542152	network parameters
0.5280570651	improve performance
0.5277992335	self organize
0.5276583258	model size
0.5276038734	applications including
0.5264773008	temporal structure
0.5263914522	neural network model
0.5262240278	digit classification
0.5262184048	local search algorithms
0.5261998536	neural network structure
0.5261819991	network connectivity
0.5257590945	access memory
0.5256951654	recent results
0.5256798126	de facto
0.5256782751	prediction models
0.5253750034	dnn models
0.5252111955	linear model
0.5248113486	network architecture search
0.5248048092	pruning framework
0.5240821162	non convex optimization
0.5239879101	self driving
0.5237272567	autonomous learning
0.5235910196	imagenet classification
0.5233823711	ant colony system
0.5232721706	off chip
0.5230907852	evolutionary systems
0.5230538660	machine learning methods
0.5228892015	cnn based
0.5223869321	recursive neural
0.5222269060	statistical properties
0.5221934699	benchmark test
0.5221142279	multi modal multi objective
0.5212126421	gradient based optimization
0.5210446091	vision based
0.5208703690	biological neural
0.5206097159	memory size
0.5204056617	human visual
0.5202988933	snn based
0.5197178256	de novo
0.5190757015	positive and negative
0.5189393180	current paper
0.5180022227	large amounts
0.5178673339	computer vision
0.5172400520	regulatory network
0.5165497373	time windows
0.5164843477	low memory
0.5162822688	sequence classification
0.5162548044	higher energy
0.5154729706	non trivial
0.5154301504	policy network
0.5149500878	recurrent neural network architecture
0.5149294666	self play
0.5143515065	pre trained models
0.5142270099	above mentioned
0.5142040995	self adaptive
0.5141040353	search procedure
0.5140936530	real world optimization problems
0.5127009851	k nearest
0.5122449860	total number
0.5117321343	multiple tasks
0.5116193567	unsupervised manner
0.5111765952	pruning method
0.5108047824	problem domain
0.5105945417	range dependencies
0.5103017833	pre trained model
0.5086717642	difficult problems
0.5085237676	exploitation and exploration
0.5081065316	pattern generators
0.5075539711	training data set
0.5074636418	similar results
0.5073975450	market prediction
0.5073022436	curse of dimensionality
0.5070957071	ann based
0.5065218167	deep generative models
0.5061822254	training cost
0.5059090586	convolution neural networks
0.5041321076	learning systems
0.5040374041	mnist and cifar10
0.5037836053	image classification tasks
0.5037209742	simple genetic algorithm
0.5035024224	proposed method outperforms
0.5034280271	point of view
0.5033678797	exploration and exploitation
0.5031040950	computational results
0.5030379192	context based
0.5026170083	real world data
0.5022165680	optimization process
0.5019117191	general framework
0.5018545770	decision process
0.5015344801	time series
0.5011580607	constrained optimization problem
0.5008587638	data distributions
0.5007975112	global optimal
0.5005057704	order of magnitude
0.5004713585	backpropagation through time
0.5000024926	alternating direction method
0.4999874262	non linearities
0.4999254567	\ le
0.4992118911	trained end to end
0.4991061332	off line
0.4987759460	get stuck
0.4987455498	proposed technique
0.4984852885	science and engineering
0.4980206733	training scheme
0.4980068047	small population
0.4978922959	an open source
0.4976857835	sequential decision
0.4976406426	leaky integrate and fire neurons
0.4975787996	standard datasets
0.4975445860	complex problems
0.4964744733	optimization framework
0.4961016819	image representations
0.4959461423	gop s
0.4958709481	c means
0.4956286649	complex network
0.4952230492	multi objective optimization problem
0.4946365842	proposed approach
0.4945985329	reinforcement learning algorithms
0.4941136858	sub populations
0.4934816527	game of life
0.4929851273	near optimal
0.4923652620	non dominated
0.4915131575	machine learning tasks
0.4914175941	level based
0.4905183786	non idealities
0.4904674791	evolutionary approach
0.4904414544	moea d de
0.4904017642	machine learning applications
0.4901897955	sequence models
0.4901721589	multi objective problems
0.4900301318	upper and lower
0.4894784625	lot of attention
0.4894667569	probabilistic neural
0.4892272779	timing dependent
0.4889926304	evolutionary techniques
0.4888655438	optimization methods
0.4884970998	brief introduction
0.4878871301	\ varepsilon
0.4873379202	algorithm outperforms
0.4868823278	weights and biases
0.4868541406	taking inspiration from
0.4866473207	time delay
0.4864392104	front end
0.4860074800	proposed algorithm
0.4858311151	input layer
0.4856957178	deep learning systems
0.4856359258	based reinforcement learning
0.4855207940	encoder and decoder
0.4853020834	user specified
0.4848947521	data collected
0.4847803258	dynamic problems
0.4845074434	symbolic data
0.4837384525	crossover and mutation
0.4833736651	brain networks
0.4831288765	statistical machine
0.4830621824	factors of variation
0.4829839700	model parameters
0.4826984019	training stage
0.4825988245	optima networks
0.4825693820	learning channel
0.4825548616	immune system
0.4824468791	layer by layer
0.4821127050	training deep networks
0.4819902091	fully connected neural
0.4818151910	trade offs between
0.4816469902	objective optimization problem
0.4814499941	predict future
0.4812092010	\ sqrt
0.4811223413	neural processing
0.4810717263	markov models
0.4809478364	top down
0.4807573003	trade off
0.4798082735	neural systems
0.4795959179	entropy loss
0.4793028263	hybrid algorithms
0.4792969086	modern neural networks
0.4790240984	deep learning architecture
0.4780047935	last decades
0.4772701127	training of neural networks
0.4770755225	number of iterations
0.4768262546	high energy
0.4765603799	efficient computation
0.4764077220	pre trained network
0.4763595033	based methods
0.4759716223	local learning
0.4759479124	mobile and embedded
0.4748951046	\ rho
0.4746705370	inputs and outputs
0.4746494874	learning tasks
0.4746402015	dynamic multi objective optimization
0.4746188164	\ cite
0.4743407248	modern deep
0.4739853768	an immune inspired
0.4739837858	the past decade
0.4735431018	hardware and software
0.4728722542	training algorithm
0.4725326732	segmentation tasks
0.4724243189	neural models
0.4723611913	world model
0.4722586375	ever increasing
0.4721637649	neural code
0.4718883382	convolutional auto
0.4718147234	temporal patterns
0.4714742632	binarized neural
0.4713598858	fully connected neural network
0.4708685648	key challenge
0.4707295770	computational model
0.4705783734	supervised learning algorithm
0.4704506220	et al
0.4703496586	advantages in terms
0.4703459783	network activity
0.4701440692	optimal mutation
0.4694958870	millions of parameters
0.4694785238	ability to learn
0.4692216691	attention model
0.4689156792	control task
0.4688516241	b matrix
0.4688069120	high dimensional problems
0.4688016480	data representations
0.4685794378	\ frac
0.4683872381	high probability
0.4682505729	deep spiking neural
0.4681708235	deep echo state
0.4674101124	model based reinforcement
0.4672637690	learning method
0.4672358499	tree problem
0.4671214187	number of epochs
0.4670020679	put forward
0.4662488731	end to end speech recognition
0.4661182083	bad local
0.4660599686	network complexity
0.4658142792	single hidden
0.4657323193	graph neural network
0.4656910530	paper reviews
0.4656276745	image to image
0.4654558734	\ omega
0.4650846073	graph classification
0.4648826306	human immune system
0.4637506337	outperforms previous
0.4633487820	learning scheme
0.4625403538	\ epsilon
0.4620294836	population based search
0.4619230493	wide range
0.4612079703	current state
0.4605756784	training and testing
0.4599908547	dynamic multi objective
0.4591456802	tasks require
0.4590843366	stand alone
0.4590721462	large population
0.4588873921	number of generations
0.4584966824	cifar 10 dataset
0.4582720811	learning based
0.4582537251	elites algorithm
0.4581484292	without compromising
0.4578801731	search strategies
0.4578703251	small subset
0.4571483221	precision weights
0.4568280654	sparse convolutional
0.4567121426	effectiveness and efficiency
0.4565772640	number of hidden units
0.4559513491	cifar 10 and cifar 100
0.4558681777	publicly available
0.4558411248	theoretical understanding
0.4557214442	structure prediction
0.4556830935	\ pm
0.4550729542	network training
0.4549842553	data efficiency
0.4547857178	optimization procedure
0.4547212299	temporal representations
0.4547021941	box optimization
0.4546144029	\ cdot
0.4545913259	\ infty
0.4543119594	training of dnns
0.4542587440	bottom up
0.4537383847	weights and activations
0.4535539222	number of hidden neurons
0.4534893986	deep architecture
0.4534046090	reinforcement learning tasks
0.4530201914	connected layers
0.4526424499	multiple input
0.4523118054	optimisation algorithm
0.4518202243	heuristic approach
0.4515494947	mutation and crossover
0.4515200232	supervised and unsupervised
0.4513535263	control problem
0.4513032747	32 bit
0.4509583053	network level
0.4507383847	spatial and temporal
0.4504391404	deep neural network models
0.4500390498	main objective
0.4500289278	first hitting
0.4494054090	improve accuracy
0.4493658560	automatic generation
0.4487719345	time consuming
0.4487603061	\ textsc
0.4486925405	\ leq
0.4486638146	state machine
0.4478851887	simple and efficient
0.4475413075	detection and classification
0.4471767951	\ geq
0.4470191645	input and output
0.4469130238	layer perceptron
0.4468069411	second order
0.4465650481	data efficient
0.4465507060	speed and accuracy
0.4463661298	$ \ ell_2
0.4463567957	full precision
0.4463258676	\ alpha
0.4462014946	c + +
0.4461232012	blind source
0.4459303489	state of art
0.4458147123	number of training samples
0.4457603061	\ mathrm
0.4456679146	es algorithms
0.4455507060	computation and memory
0.4454081603	prediction tasks
0.4452780204	efficient training
0.4451942067	\ textit
0.4451133715	pruning algorithm
0.4450383285	sampling method
0.4448341654	paper shows
0.4447410553	self adaptation
0.4444160891	single and multi
0.4443748614	likelihood estimation
0.4441137672	resulting algorithm
0.4435034296	an experimental study
0.4431295790	memory network
0.4429962755	multi objective algorithms
0.4429836070	\ mathbf
0.4426629665	detection and recognition
0.4425759346	diverse range
0.4424071245	learning classifier system
0.4419234691	shallow and deep
0.4419170468	\ theta
0.4416519083	number of function evaluations
0.4414296473	top 5
0.4414170639	\ emph
0.4412519524	cnn models
0.4411030045	vision and speech
0.4410645411	encoding method
0.4409392861	mnist and cifar
0.4407794217	synthetic and real
0.4405426742	memory based
0.4404747755	inspired approach
0.4403717615	number of hidden layers
0.4403469113	networks of spiking neurons
0.4402144847	inference and learning
0.4400739173	pre specified
0.4397603061	\ textbf
0.4397571088	the dendritic cell algorithm
0.4395173839	non dominated solutions
0.4387259038	sparse networks
0.4383920663	0 1 knapsack
0.4379207726	performance and energy efficiency
0.4375767547	training and inference
0.4374296806	cifar 100 datasets
0.4371172739	means clustering
0.4364621738	\ em
0.4363041437	learning dynamics
0.4361145390	proposed methodology
0.4346015887	dnn training
0.4344097593	network weights
0.4342306163	belief networks
0.4340216743	computational framework
0.4337889755	learning methods
0.4336712755	specific features
0.4334436906	terms of classification accuracy
0.4333967456	knowledge based
0.4330627874	priori knowledge
0.4328051196	neurons and synapses
0.4319293839	space exploration
0.4318129377	\ ell
0.4315038606	\ mu
0.4311344232	an empirical study
0.4311192553	standard bit
0.4311054892	solving complex
0.4309698695	performance analysis
0.4305079058	unit models
0.4301308510	swarm optimisation
0.4294539710	packing problem
0.4290711513	paper addresses
0.4287001747	cifar 10 and imagenet
0.4286919746	real world tasks
0.4285862238	high degree
0.4282946431	results provide
0.4271304131	classification and regression
0.4271286596	based models
0.4267561000	local and global
0.4267034082	off policy
0.4262626574	fast and accurate
0.4260330703	a comparative study
0.4259749612	efficient solutions
0.4258143190	data dependent
0.4254622627	present results
0.4245341446	task relevant
0.4234942926	large variety
0.4234063732	n mnist
0.4231138694	mnist and cifar 10
0.4229718007	carried out
0.4229146224	recognition performance
0.4228052263	deep recurrent neural
0.4226646793	\ lambda
0.4226272488	complex functions
0.4221529021	convergence and diversity
0.4217665097	shown to outperform
0.4214425445	recurrent model
0.4213990210	theoretical and empirical
0.4207610373	experimental results indicate
0.4206634698	deep learning networks
0.4205518114	training and test
0.4204445830	not necessarily
0.4197973976	taken into account
0.4197400854	source and target
0.4188295892	training of deep
0.4188028950	^ 6
0.4181187361	vast majority of
0.4177210472	learning task
0.4171963106	signal to noise
0.4170468027	graph neural
0.4166049542	population based evolutionary
0.4164124666	machine learning community
0.4159513804	learning representations
0.4154888031	learning and inference
0.4154800986	descent method
0.4147999063	functional data
0.4145708151	quantum machine
0.4144545035	co evolution
0.4143756203	robust and efficient
0.4137016655	series forecasting
0.4132159103	linear and nonlinear
0.4128835119	trained from scratch
0.4128618710	last years
0.4127819837	^ 2
0.4124831152	neural network design
0.4123424156	simulated and real
0.4122495111	takes advantage of
0.4118520259	fast learning
0.4115873647	speed and energy
0.4111964225	multi objective optimization algorithms
0.4111149270	promising solution
0.4109324086	non differentiable
0.4101611115	software co
0.4100192986	solve complex
0.4094708781	classification model
0.4092332282	framework called
0.4092219651	training of neural
0.4090228333	transportation systems
0.4084705492	effective and efficient
0.4082108180	cnn model
0.4081279710	input information
0.4078799466	learning model
0.4073461266	based attacks
0.4071068643	learning problems
0.4067065239	results prove
0.4065795264	fast and efficient
0.4063612188	well understood
0.4062876396	machine learning approach
0.4062407072	method yields
0.4062131504	distribution algorithms
0.4058838799	machine learning model
0.4057384913	non convex
0.4054035010	image features
0.4053301961	r cnn
0.4050482475	three dimensional
0.4049347550	paper suggests
0.4045730201	efficient design
0.4032390794	field of machine learning
0.4028041228	large numbers
0.4026455174	well established
0.4025256569	network model
0.4022705492	computational and memory
0.4021896218	end to end speech
0.4021362158	classification benchmark
0.4019140644	control systems
0.4015700164	built upon
0.4013591387	generative neural networks
0.4012253062	rl algorithm
0.4009821032	last two decades
0.4007103852	achieves competitive
0.4007068300	an upper bound
0.4003760688	reinforcement learning based
0.4001882091	idea behind
0.3995902871	$ l_p
0.3992681831	large scale neural
0.3992646923	no extra
0.3989254505	loss in accuracy
0.3987597306	\ ln
0.3985722302	highly non linear
0.3985316796	no longer
0.3980752723	computational tasks
0.3978458636	optimization heuristics
0.3977685095	selection problem
0.3977448763	network models
0.3974966908	sequence to sequence learning
0.3974800154	deep learning algorithms
0.3965834429	regression methods
0.3964242555	de identification
0.3958320471	assisted evolutionary
0.3952478197	leaky integrate and fire
0.3951554591	quantized neural
0.3950343750	existing models
0.3950169254	non separable
0.3941565511	based evolutionary algorithm
0.3937493178	non negative
0.3930002650	inspired optimization algorithms
0.3928689065	detection systems
0.3928156785	free optimization
0.3927774630	^ 3
0.3927429867	extract features
0.3925728188	prediction task
0.3918998139	neural network approach
0.3915530636	large number
0.3915502804	multi layer neural networks
0.3911905881	this paper presents
0.3911100449	training methods
0.3910133618	series prediction
0.3908751418	$ l_2
0.3906568145	major challenge
0.3905812389	speed up
0.3897860929	additional training
0.3892754544	search methods
0.3888710541	low computational
0.3886357092	problem structure
0.3885171262	neural units
0.3884876631	time series data
0.3881700604	recent success
0.3880458150	dynamic neural
0.3879023211	tasks including
0.3872661702	input spike
0.3872142187	optimal parameter
0.3863750308	set problem
0.3863549845	this paper proposes
0.3862416880	paper analyzes
0.3861364891	becoming increasingly
0.3852644554	brain computer
0.3852481459	performance and energy
0.3849167179	number of parameters
0.3844937364	real time
0.3844122402	upper bounds on
0.3841832097	multi objective approach
0.3838124820	regression model
0.3834547086	training dataset
0.3831997803	model building
0.3831653227	a case study
0.3829175789	intelligence based
0.3827428190	learning capabilities
0.3823002803	while keeping
0.3822739861	without sacrificing
0.3819061499	dimensional representation
0.3814294073	\ ge
0.3813557082	number of spikes
0.3802881609	learning theory
0.3802658598	end to end learning
0.3799792848	deep learning approaches
0.3797448998	significant improvement over
0.3796547776	optimization approach
0.3795194894	non linear
0.3790693829	difficult to train
0.3786595011	accuracy comparable
0.3786389268	achieve good performance
0.3784351848	under consideration
0.3779305476	large scale data
0.3774593902	multi modal multi
0.3772250625	training recurrent neural networks
0.3771741156	learning mechanism
0.3760039268	system identification
0.3753015899	in depth analysis
0.3752094922	close to optimal
0.3751585566	data classification
0.3750294085	valued neural
0.3750085087	take into account
0.3749181812	\ delta
0.3749109325	proposed model
0.3748332358	research shows
0.3745586319	$ \ mathcal
0.3740966752	lstm model
0.3739821507	cifar 10 datasets
0.3738094253	so far
0.3732551448	co design
0.3731560568	training algorithms
0.3728337198	two player
0.3726196313	method consists
0.3720213717	unsupervised training
0.3718730684	deep multi
0.3718615504	solving multi objective
0.3718121562	nervous system
0.3715679928	dynamic network
0.3711290132	an evolutionary approach
0.3711120464	distributed training
0.3709826409	\ log \ log
0.3708233293	\ ln n
0.3707767960	design framework
0.3706777828	function optimization
0.3703666272	general problem
0.3702564990	prediction performance
0.3701416989	an unsupervised manner
0.3695584394	number of clusters
0.3695058287	times faster than
0.3691983617	random field
0.3691420450	trained networks
0.3690100492	trained models
0.3689390546	deep models
0.3686611718	dnn model
0.3684410119	search technique
0.3680501910	real world problem
0.3679916578	function values
0.3678203266	experiment results
0.3672491829	neural encoding
0.3671742488	selection algorithm
0.3671604690	efficient deep neural
0.3667045525	large number of parameters
0.3665197047	divided into
0.3663039736	computing architecture
0.3659814993	time dependent
0.3659251905	take place
0.3655548857	end to end training
0.3646221236	class classification
0.3643743994	networks of spiking
0.3641384196	learning approaches
0.3640228697	\ mathbb r
0.3639251700	series datasets
0.3636453704	accurate results
0.3635898538	competitive classification
0.3634649879	number of steps
0.3634349691	integrate and fire neurons
0.3634006451	while retaining
0.3630639866	complex real world
0.3630483398	computer vision tasks
0.3627295833	4 bit
0.3624774392	integrate and fire
0.3623754933	\ phi
0.3619884404	supervised and unsupervised learning
0.3616888259	commonly used
0.3616327851	number of bits
0.3615905134	deep learning approach
0.3614817126	\ mathbb
0.3614038381	adversarial network
0.3609057561	sub optimal
0.3606490231	from scratch
0.3606376743	optimal parameters
0.3600687347	first order
0.3596055122	expected optimization time
0.3593048586	training error
0.3590057988	layer feed
0.3588760934	recognition systems
0.3588587908	term memory networks
0.3588523921	n ^ 2
0.3585008047	mean squared
0.3582063633	dimensional data
0.3578866845	based classifier
0.3575872742	networks with relu
0.3575699180	attention networks
0.3573877722	mainly focus
0.3573110436	neural attention
0.3571706294	very deep networks
0.3570853109	significantly better performance
0.3570305403	capable of generating
0.3565885387	accuracy and computational
0.3558240423	so called
0.3557997323	speech recognition systems
0.3555901697	\ approx
0.3555033957	recently emerged
0.3554462620	training images
0.3549505952	number of evaluations
0.3549113169	algorithm named
0.3545481612	n ^
0.3542400558	quantum computer
0.3541570204	efficient method
0.3540380834	label classification
0.3539483955	1 + 1
0.3538903075	this paper introduces
0.3537553691	available at https
0.3536781391	training of deep neural networks
0.3535173157	well defined
0.3529381257	over parameterization
0.3529147789	area of research
0.3527690466	architecture called
0.3526629763	achieve high
0.3523317116	computing hardware
0.3521373002	robust to noise
0.3520998198	under certain conditions
0.3519358787	translation model
0.3512438739	last few years
0.3509172527	number of layers
0.3508426368	learning machine
0.3507857265	complex data
0.3504355723	$ \ mathcal o
0.3498854946	$ l ^
0.3496952482	taking advantage of
0.3495718053	specific task
0.3494557326	search optimization
0.3492549889	compact genetic
0.3491441435	simulation based
0.3488096753	classification results
0.3486728521	order to generate
0.3484153311	many objective
0.3482987042	based online
0.3482881399	trained model
0.3481723642	evolution algorithm
0.3479159959	process information
0.3476572760	an agent based
0.3471690735	the baldwin effect
0.3470285490	learning mechanisms
0.3466560017	next generation
0.3460792597	optimal set
0.3456453658	more importantly
0.3454687516	efficient neural
0.3454122672	inspired algorithm
0.3451118030	decomposed into
0.3450820338	classification based
0.3449194680	without losing
0.3448993420	memory model
0.3448954815	q learning
0.3448598939	learning models
0.3448302004	great interest
0.3448082748	a multi layer perceptron
0.3447463516	continuous learning
0.3446594607	output weights
0.3445117216	art approaches
0.3444979349	recent advances in deep learning
0.3443932038	method of multipliers
0.3442499634	aimed at
0.3440428769	paper reports
0.3436882684	dnn weight
0.3436785325	wide range of
0.3435850131	e ^
0.3433299551	an open question
0.3433176256	self supervised learning
0.3424527830	gap between
0.3423474195	driven approach
0.3421010324	$ \ theta
0.3420469838	\ mathcal
0.3419225258	become increasingly
0.3413878869	recent advances in
0.3407361678	prediction results
0.3403758270	models trained
0.3402272973	neural computing
0.3401901377	learning problem
0.3399064438	experimental results show
0.3391390370	model accuracy
0.3388997364	a unified framework
0.3387369225	multi layer neural
0.3380970886	target task
0.3380063394	an in depth
0.3378528984	data distribution
0.3374147593	^ *
0.3368737790	top 1 accuracy
0.3362651693	\ sigma
0.3359909700	classification algorithms
0.3359425610	learning framework
0.3357984433	loss of accuracy
0.3355513562	difficult to solve
0.3350983635	paper explores
0.3350242017	proposed architecture
0.3350119549	recognition benchmark
0.3348347566	model structure
0.3344624525	computer simulations
0.3342437592	genetic algorithm ii
0.3341174646	art algorithms
0.3340670752	standard machine
0.3338494969	success of deep learning
0.3332610362	quantum neural
0.3323153404	approach called
0.3322364995	machine learning algorithm
0.3319460200	many objective evolutionary
0.3314729679	large number of
0.3313531946	memory models
0.3313316000	n \ log n
0.3312294315	random neural
0.3308511238	$ \ approx
0.3308286541	run time
0.3305561207	algorithm for solving
0.3305245036	specific problems
0.3303033708	without affecting
0.3299864052	this short paper
0.3298279362	model training
0.3296635945	neural memory
0.3291559526	deep q
0.3290954075	gradient based learning
0.3289292552	leads to significant
0.3285716886	ant system
0.3281101440	learning architectures
0.3280726096	specific knowledge
0.3278352472	machine learning based
0.3277344571	$ \ phi
0.3275721987	problem at hand
0.3274693092	second generation
0.3273905859	method called
0.3271785508	neural structure
0.3269141753	most importantly
0.3267327803	neural machine
0.3260537962	features learned by
0.3258072781	detection task
0.3256368150	based attention
0.3255579487	widely used
0.3255057659	large amounts of
0.3253476167	evolutionary deep
0.3252749295	in situ
0.3248671134	p systems
0.3246695530	self attention
0.3244364916	applied to solve
0.3241801964	adaptive learning
0.3235793240	compared to conventional
0.3234733457	limited computational
0.3230030058	specific problem
0.3228997429	important step
0.3228762385	efficient deep neural networks
0.3228054979	\ mathcal o
0.3227795065	compared to traditional
0.3227029381	best suited
0.3224077306	efficient neural network
0.3223105846	optimization algorithm based
0.3222994282	a deep learning model
0.3221212761	dynamic model
0.3221053620	even though
0.3220740575	an evolutionary algorithm
0.3220237661	large neural networks
0.3219982667	\ log n
0.3218307212	paper extends
0.3218194974	sparse neural
0.3216873889	number of nodes
0.3216565019	this paper describes
0.3216007752	doing so
0.3211239387	non dominated sorting genetic
0.3209805074	training deep
0.3209770258	imagenet datasets
0.3208535864	evolutionary learning
0.3208458409	learning in spiking neural networks
0.3206917560	search problem
0.3203027723	while maintaining
0.3202438245	in recent years
0.3202338477	model performance
0.3200626093	a major challenge
0.3198743143	training neural networks
0.3182208227	paper studies
0.3181070329	real world optimization
0.3180134548	n \ log
0.3177750770	temporal classification
0.3168348006	sparse deep
0.3166665311	reduce memory
0.3165167898	bits per
0.3161775826	baseline model
0.3161464772	a priori knowledge
0.3158618130	inspired by biological
0.3157546724	well suited
0.3156896649	$ \ sigma
0.3156228792	all optical
0.3148114313	many real world applications
0.3146794832	state network
0.3145731906	distances between
0.3140736787	neural language
0.3140542470	type 2
0.3136363239	network based
0.3135876659	\ times
0.3132076331	accurate models
0.3131892740	number of connections
0.3129712375	reinforcement learning algorithm
0.3124215081	tradeoff between
0.3122503256	based optimization
0.3119704009	$ \ omega
0.3118209638	dimensional continuous
0.3116551241	proposed methods
0.3116242376	based selection
0.3113105362	prior knowledge about
0.3112510874	computer security
0.3111657218	model predictive
0.3111009324	a wide variety
0.3108075749	deep learning method
0.3107095089	governed by
0.3106571102	model architecture
0.3106032327	much smaller
0.3104320814	evolutionary multi
0.3103544793	based search
0.3102941930	\ mu +
0.3101570924	into consideration
0.3100687012	machine learning problems
0.3097671705	a computational model
0.3097503423	growing interest
0.3092343107	properties of biological
0.3089346019	limited number
0.3088776361	optical neural
0.3088568384	online training
0.3087981880	number of variables
0.3086822196	noise ratio
0.3085511568	neural population
0.3083838535	one shot learning
0.3082443850	test problem
0.3080362702	large neural
0.3078761029	original data
0.3076724573	more precisely
0.3074798968	heuristic optimization
0.3074064542	problem of finding
0.3072540719	distributed deep
0.3071092692	\ mathbb r ^
0.3069251326	learning approach
0.3064124788	based systems
0.3058447216	^ 5
0.3056211132	classical genetic
0.3051871570	ranging from
0.3051329998	discriminate between
0.3050440690	classification methods
0.3047819255	trained neural networks
0.3043907830	input parameters
0.3040963966	network performance
0.3040620855	training recurrent
0.3040326363	number of samples
0.3038195515	recently emerged as
0.3037651566	human performance
0.3036782010	evolutionary image
0.3034624661	hardware based
0.3029693852	without requiring
0.3028662059	learning paradigms
0.3024133587	worse than
0.3023540830	performance analysis of
0.3023062317	power system
0.3022910788	insight into
0.3022851076	algorithm based
0.3022461653	3d shape
0.3020919092	search techniques
0.3015818140	trained to predict
0.3013278980	gradient descent algorithm
0.3011693756	analysis methods
0.3011375134	simple tasks
0.3010530505	based method
0.3010014871	mu +
0.3008088305	important task
0.3005613611	out of sample
0.3004362917	diffractive deep
0.2999502159	generated data
0.2998899489	propagation algorithm
0.2997746500	huge number of
0.2997200521	shown promising
0.2995556686	using genetic algorithm
0.2993725487	resulting network
0.2991312718	suffer from
0.2990863666	incorporated into
0.2988871278	training deep neural
0.2988842205	the neural gpu
0.2985752017	spike time
0.2985088169	propagation through time
0.2984689155	this paper investigates
0.2983725544	modern neural
0.2982164974	deep learning model
0.2982036056	training method
0.2979472726	biological networks
0.2979169921	theoretical model
0.2978172767	time variant
0.2977785132	1 + \ lambda
0.2975744922	10 ^
0.2972435746	effective approach
0.2969593717	model called
0.2969542699	one dimensional
0.2969111296	world optimization problems
0.2968709026	trade off between
0.2967077152	detection methods
0.2966806451	top 1
0.2964176900	well known
0.2963573773	estimation of distribution
0.2960861421	very expensive
0.2959737743	operating system
0.2956787070	traditional algorithms
0.2956479087	+ n \ log
0.2955837539	based framework
0.2954062813	area under
0.2950130738	first attempt
0.2949634158	connected neural networks
0.2949526739	search approach
0.2947790306	graph data
0.2945291976	approach improves
0.2940857961	\ log
0.2940059022	distribution algorithm
0.2937937196	the human immune
0.2935283088	^ n
0.2933996206	recent advances in deep
0.2933171355	most likely
0.2932442438	network depth
0.2931309478	based model
0.2923483652	fast training
0.2921367629	\ sqrt n
0.2919800636	based control
0.2919754286	= 1
0.2917451803	faster than
0.2917371832	network connections
0.2915206689	based learning
0.2914299931	one class
0.2912536105	polynomial time
0.2903899844	a genetic algorithm
0.2903171324	solving large
0.2900118289	an efficient
0.2899703977	closely related to
0.2895652224	achieves state of
0.2894777730	well studied
0.2889865807	adaptive structural
0.2888471102	over fitting
0.2882535389	successfully applied to
0.2882168258	based algorithms
0.2881995024	effective training
0.2880564645	rather than
0.2877928952	gives rise to
0.2870339475	the proposed algorithm
0.2869086773	modeling tasks
0.2863120791	based pruning
0.2861295981	descent algorithm
0.2856382850	capable of learning
0.2856104973	time varying
0.2854932680	the proposed method
0.2854421592	input image
0.2853937226	last decade
0.2853669120	biologically plausible neural
0.2853021101	there exist
0.2850282759	compared with existing
0.2850255265	q network
0.2848273090	promising approach
0.2841270970	= \ omega
0.2841002600	search method
0.2840445418	single model
0.2839112541	on line learning
0.2836657651	an overview
0.2831583264	taken together
0.2830615000	an energy efficient
0.2830034660	number of neurons
0.2829727472	local optimization
0.2829627851	large data
0.2828057319	analog neural
0.2827081562	time constant
0.2825954985	lstm network
0.2825394620	detection algorithm
0.2824416350	an alternative approach
0.2822169954	using genetic programming
0.2820280333	learning frameworks
0.2819604964	the proposed approach
0.2818488198	network accuracy
0.2818246812	training approach
0.2817698597	\ log ^
0.2817612451	three level
0.2817380119	a comparative analysis
0.2812623420	under uncertainty
0.2811612040	originating from
0.2805948905	interplay between
0.2805445801	method for solving
0.2801064674	network layer
0.2800391174	efficient hardware
0.2799626183	robustness against
0.2799056070	make decisions
0.2798604518	interacts with
0.2796595510	network compression
0.2796188547	neural network learning
0.2794253289	using convolutional neural
0.2794065131	class of problems
0.2793488411	towards understanding
0.2791737329	optimizing deep
0.2790996908	binary networks
0.2789716928	principled way
0.2789413383	compared to previous
0.2787951887	art performance
0.2787856725	simple way
0.2786301821	evolutionary based
0.2783522965	continuous time
0.2782459258	assumptions about
0.2782024505	neural network algorithm
0.2780517457	bayesian neural
0.2778234581	\ gamma
0.2775879240	on chip
0.2775737506	$ \ mu =
0.2775574498	+ 1
0.2775211102	using particle swarm optimization
0.2770634194	\ `
0.2769359430	artificial networks
0.2760903317	learn complex
0.2760373902	learning agent
0.2759691502	^ k
0.2758005216	analysis of evolutionary algorithms
0.2757278769	evolutionary neural
0.2756681434	over parameterized
0.2755151619	non uniform
0.2751898462	first steps
0.2751104518	accordance with
0.2750359402	current deep
0.2748666406	approach achieves
0.2746755459	one hidden layer
0.2746209716	many real world problems
0.2743679868	level of accuracy
0.2742248069	suffers from
0.2742143827	generalize well
0.2741193357	processing systems
0.2740342812	based architecture
0.2739432703	linear networks
0.2738201446	hardware neural
0.2733115868	standard evolutionary
0.2732471012	conventional deep
0.2730890688	an artificial neural network
0.2730158415	large training
0.2728868915	architecture optimization
0.2723761080	shed light
0.2722530954	features learned
0.2721872011	for open ended evolution
0.2720831547	fed into
0.2718323490	classification models
0.2717685904	step toward
0.2712892051	sequence data
0.2712456122	an active area
0.2709951772	training objective
0.2708992921	back propagation through time
0.2707996603	objective optimization
0.2707334554	features extracted from
0.2705987360	many real world
0.2705380372	design problems
0.2702791134	augmented neural networks
0.2700703157	including image
0.2699641026	this article presents
0.2696921707	k =
0.2696559221	optimization performance
0.2695564662	connected neural network
0.2694387282	results on mnist
0.2693189259	a central role
0.2691045069	p =
0.2689535428	genetic algorithm approach
0.2687388128	third generation
0.2685972852	shed light on
0.2682165841	learning capability
0.2681931069	+ +
0.2681640746	relationship between
0.2681440459	detection algorithms
0.2680904131	order to achieve
0.2679780321	more biologically plausible
0.2679633898	brain like
0.2675194408	domain data
0.2671958233	diverse range of
0.2670721260	model architectures
0.2669475309	$ \ mu
0.2668887934	efficient architecture
0.2668061479	linear neural networks
0.2667187655	non linear functions
0.2666943503	this paper explores
0.2666195523	f1 score of
0.2666012992	back propagation algorithm
0.2665455743	standard methods
0.2664087061	learning technique
0.2659780022	into account
0.2658507854	based modeling
0.2657429536	synaptic learning
0.2654733920	inference method
0.2653179535	a long standing
0.2645816379	deep intelligence
0.2644897248	efficient learning
0.2644272018	success of deep
0.2643670074	extraction method
0.2641002149	learning phase
0.2638910938	the previous state
0.2634196663	goodness of
0.2632424694	networks achieve
0.2630998354	co evolutionary
0.2630653078	inference algorithm
0.2629595679	relations among
0.2627716326	particularly suitable
0.2627439597	\ beta
0.2627109436	investigate whether
0.2626179403	network trained
0.2625972475	value function
0.2623844722	inspired optimization
0.2621803816	\ sim
0.2621557459	sparse data
0.2620333537	ln n
0.2619983637	up to date
0.2617523000	computational performance
0.2617436295	$ \ epsilon
0.2616059901	using genetic algorithms
0.2614916901	the art
0.2614770792	non gaussian
0.2614623085	attracted much
0.2614210823	lower computational
0.2608234615	lower bounds on
0.2607474997	variety of applications
0.2606553964	more energy efficient
0.2605531312	does not
0.2604397141	using deep neural networks
0.2604356347	word error
0.2603005123	network layers
0.2602385877	shown to achieve
0.2600043882	layer neural network
0.2599941079	did not
0.2599801293	reinforcement learning methods
0.2598917723	dimensional search
0.2598541584	algorithm for training
0.2595329966	performance data
0.2594168605	a neural network
0.2585277493	optimization based
0.2583624535	at least
0.2580003343	a deep convolutional neural network
0.2579402807	level features
0.2578262549	insights into
0.2577316333	per layer
0.2575925764	design optimization
0.2571085322	amounts of data
0.2569118878	function network
0.2566006121	deep model
0.2565013820	range of applications
0.2564728830	forward neural network
0.2563046869	apart from
0.2559934501	test performance
0.2559754517	large networks
0.2559600403	using deep convolutional
0.2558511218	recent progress in
0.2557787412	first spike
0.2555859487	affected by
0.2555505286	\ tau
0.2554801468	neural network layers
0.2553489903	classifier based
0.2553096949	building block for
0.2548305677	simulation results show
0.2547202255	range of tasks
0.2546375857	simple evolutionary
0.2542800710	proposed techniques
0.2541237464	learning applications
0.2540994008	converted into
0.2539068228	method performs
0.2538853958	training of deep neural
0.2537108539	distribution system
0.2536731764	rnn model
0.2535111507	modified version of
0.2531294578	numerical results show
0.2530809501	inspired by recent
0.2530140085	compared to standard
0.2529070346	based machine
0.2529047025	task based
0.2527712279	optimal performance
0.2526366799	simple genetic
0.2521201161	this paper develops
0.2518109671	test results
0.2516626165	this paper examines
0.2515570297	simple neural
0.2514397037	neural model
0.2513965477	an adaptive
0.2510530200	language inference
0.2509327398	$ o
0.2507878640	\ left
0.2507762247	recurrent neural networks with
0.2506872675	broad range of
0.2504691120	relationships between
0.2503273218	convolution neural
0.2502310155	belong to
0.2501239941	this paper
0.2498833779	complex models
0.2497825878	network language models
0.2496260943	inference systems
0.2494184707	weighted sum of
0.2491801400	based algorithm
0.2490694587	on mobile devices
0.2484575399	algorithm performance
0.2484573707	based classification
0.2484443492	inspired algorithms
0.2483228207	dealing with
0.2482196185	set of parameters
0.2480823037	non smooth
0.2480579858	datasets demonstrate
0.2478188307	do not
0.2478106087	optimization task
0.2475615544	$ \ mathbb r
0.2472662106	representations learned by
0.2472252457	the artificial bee colony
0.2471700722	log n
0.2469520372	classification systems
0.2465527580	proposed to solve
0.2465035083	does not require
0.2464898219	short term memory networks
0.2464058405	achieve state of
0.2463388782	discrete time
0.2460709574	structural learning
0.2459679033	an event driven
0.2457446768	alternative approach
0.2456318146	serve as
0.2454422351	large search
0.2452120970	greater than
0.2447027050	range of problems
0.2444840881	give rise
0.2441060645	network approach
0.2438761480	2 d
0.2438734812	caused by
0.2437361647	one hour
0.2435426777	\ pi
0.2433985462	series data
0.2433896649	an important step
0.2432112376	performance in terms
0.2430673107	search problems
0.2429602951	first step towards
0.2428755049	active area of
0.2425140725	still remains
0.2421520827	based implementation
0.2421189693	approach for solving
0.2420227545	depending on
0.2418717737	differences between
0.2417935631	optimization approaches
0.2417584664	experiment results show
0.2416585704	algorithms require
0.2416424277	lower bounds for
0.2415416166	non structured
0.2414509484	most prominent
0.2414266548	aims at
0.2412476992	method to train
0.2412402240	learn features
0.2411902803	class of algorithms
0.2411497709	trained network
0.2411231037	method for training
0.2410981448	networks with random
0.2409984365	parameter less
0.2409733146	upper bound on
0.2409726631	advances in deep learning
0.2409480404	viewed as
0.2408867019	classification datasets
0.2408686422	diverse set of
0.2407690807	prior work
0.2407348329	proposed solution
0.2407232151	approach proposed
0.2405695689	large memory
0.2404597813	intelligence techniques
0.2404212985	level model
0.2403944702	regarded as
0.2402741440	simple method
0.2402651246	based design
0.2400591871	using particle swarm
0.2400276985	series of experiments
0.2397970853	state of
0.2397946830	with high probability
0.2394031930	n \ lambda \
0.2393962505	gradient problem
0.2391298042	networks provide
0.2389811722	a deep neural network
0.2389327111	achieving state of
0.2387298042	efficient algorithm
0.2379041735	questions about
0.2378822904	biased towards
0.2378722065	recognition problem
0.2378311061	the simple genetic algorithm
0.2376515608	paper demonstrates
0.2373476940	belongs to
0.2370494156	temporal data
0.2369758967	fuzzy system
0.2369755510	previous work
0.2367884467	number of objectives
0.2367679413	standard neural
0.2367672693	akin to
0.2366915791	neural network algorithms
0.2365746158	networks trained
0.2361718367	effective method
0.2361530607	better generalization
0.2361362208	level classification
0.2360958990	inspired by
0.2358349090	paper deals with
0.2356238719	dynamical system
0.2354494766	followed by
0.2353257969	1 +
0.2351317799	recent deep
0.2349382702	$ \ ell
0.2348710030	$ norm
0.2346642131	learning techniques
0.2344540811	learning parameters
0.2344361614	using artificial neural network
0.2343510126	achieved state of
0.2343050958	connected networks
0.2342826849	order to solve
0.2342038976	learning efficiency
0.2340673615	maximum number of
0.2340556454	sequential learning
0.2339103265	this chapter
0.2338609123	in other words
0.2338390781	on chip learning
0.2337626214	proposed algorithms
0.2335860052	sub population
0.2335476848	expected running time
0.2334195958	networks learn
0.2333748931	neuron models
0.2333365619	observed data
0.2332851940	corresponds to
0.2332670572	this paper discusses
0.2329992357	execution time
0.2328578184	performs better than
0.2327436255	classifier system
0.2325538903	best performing
0.2324873627	influenced by
0.2322283202	training recurrent neural
0.2321861439	computation time
0.2321329473	current state of
0.2320769665	compromise between
0.2319331295	this paper addresses
0.2318778333	world applications
0.2318688676	wide variety of
0.2315705843	dynamic multi
0.2315600746	approach based
0.2310140970	rely on
0.2309091097	relied on
0.2308859274	based techniques
0.2308709938	bayesian deep
0.2307713826	per second
0.2307589190	results indicate
0.2307430875	coordinate system
0.2306346902	colony system
0.2303509608	method shows
0.2303413769	automatic generation of
0.2302800490	complex neural
0.2302475531	training techniques
0.2302124569	solving multi
0.2301571598	learning strategy
0.2301400929	a broad range
0.2298914365	the art methods
0.2298804443	trained neural network
0.2297347879	compared to
0.2296531362	algorithm performs
0.2295384436	computing based
0.2294410604	seen during training
0.2293597005	information about
0.2292843447	near future
0.2292773669	based training
0.2292705939	this article
0.2292407837	order to obtain
0.2291579079	bit per
0.2290635035	referred to as
0.2289723635	gradient method
0.2287536036	experiments conducted on
0.2287344163	networks exhibit
0.2286156664	important problem
0.2283920391	among others
0.2281053297	well performing
0.2279823234	s w
0.2276615230	learning classifier
0.2276438207	balance between
0.2275754112	serves as
0.2274858193	classification algorithm
0.2271928985	differs from
0.2270836502	detection performance
0.2270789218	a powerful tool
0.2270531317	runtime analysis of
0.2267581770	the proposed model
0.2267057974	methods in terms
0.2265954825	refers to
0.2265506338	focuses on
0.2264660578	an echo state network
0.2264299110	conventional neural
0.2258470125	temporal learning
0.2257882651	robust against
0.2256167452	deep learning approach for
0.2254843944	in accordance
0.2253825162	layer spiking
0.2252812946	computer programs
0.2252356923	not well understood
0.2251197856	characterized by
0.2248976507	learning architecture
0.2248549029	based applications
0.2248549029	based solutions
0.2247354857	layer network
0.2247099017	to learn long term
0.2242980362	smaller than
0.2242367804	1 bit
0.2241933427	$ \ pm
0.2240295719	$ \ lambda =
0.2239434985	performing solutions
0.2238044359	based on
0.2238043926	network optimization
0.2237642545	linear activation
0.2235948518	learning in spiking neural
0.2235674764	learning to learn
0.2235169585	algorithm design
0.2234805179	efficient algorithms
0.2233990525	of great importance
0.2233089090	previous state of
0.2231912367	data instances
0.2230010169	motivated by
0.2228566137	a large margin
0.2225894338	performance compared
0.2222814392	detection problem
0.2221206723	the building block
0.2221123513	trained neural
0.2218009469	extensive experiments on
0.2217510103	training performance
0.2216055507	a power law
0.2215210140	very deep
0.2213401746	in terms of prediction accuracy
0.2211381206	training step
0.2209602374	a diverse set
0.2207928772	relatively small
0.2202991064	an investigation
0.2201842250	learning scenarios
0.2201813883	variety of tasks
0.2199300623	improve upon
0.2198250603	small networks
0.2196687591	a recurrent neural network
0.2195632491	learning performance
0.2192512442	an evolutionary
0.2192399591	the last decade
0.2192249505	network classifiers
0.2190849433	obtained results
0.2188771806	complete problems
0.2185911687	time steps
0.2185883866	of open endedness
0.2185507409	the art approaches
0.2185408735	a key role
0.2185239518	learned models
0.2184952622	an important role
0.2183686136	computing techniques
0.2180793034	the art performances
0.2178829413	optimal architecture
0.2176443993	3 dimensional
0.2176085098	large variety of
0.2175710748	a crucial role
0.2172724059	much larger
0.2170858541	state networks
0.2170123107	replaced by
0.2169759935	detection method
0.2169564456	deep cnn
0.2169443277	learning strategies
0.2169102976	improvement over
0.2168261303	the art performance
0.2167546913	$ \ mathbb
0.2164730512	high degree of
0.2164272678	detection system
0.2162039310	higher classification
0.2161671960	optimal results
0.2160991148	linear neural
0.2159369298	great success in
0.2159192864	learning networks
0.2158908380	long training
0.2156538857	a handful
0.2153591013	$ \ lambda
0.2153006797	the art results
0.2150976151	general method
0.2150601952	prediction problems
0.2150449413	experimental results on
0.2146876626	order to evaluate
0.2146478554	the univariate marginal distribution
0.2143482507	owing to
0.2142677143	out of distribution
0.2142223191	leads to
0.2139018432	algorithms perform
0.2137887962	represented by
0.2136265632	networks require
0.2134176059	more accurate
0.2131385080	two phase
0.2130085012	connection between
0.2129683542	while preserving
0.2128165442	recent work
0.2128070775	fall into
0.2128061933	an improved
0.2125001355	arrive at
0.2124203734	algorithms for solving
0.2123708909	method significantly
0.2120372222	representational power of
0.2118014208	recent developments in
0.2116755739	this paper extends
0.2116348881	relatively little
0.2114726072	results presented
0.2114497855	root mean
0.2114062270	approach significantly
0.2113393567	most notably
0.2112268562	models achieve
0.2106449539	focused on
0.2105275856	with spike timing dependent
0.2104782134	diversity algorithms
0.2104412399	dependencies among
0.2103938408	trained on large
0.2101476235	some cases
0.2100926594	this thesis
0.2100200466	$ f
0.2097764210	a unified
0.2097414462	a key challenge
0.2096923467	a deep reinforcement learning
0.2095312313	terms of accuracy
0.2093526243	neural network trained
0.2091439155	distributed system
0.2091437410	derived from
0.2091043295	hierarchical learning
0.2089356860	significant improvement in
0.2089025401	number of patterns
0.2088779409	relations between
0.2085616250	do not require
0.2083114938	a key component
0.2079828354	benefit from
0.2079180603	total number of
0.2077802829	an incremental
0.2076468035	standard genetic
0.2076168320	an order of magnitude
0.2075137394	3 d
0.2075020224	a neural network model
0.2074458529	focus on
0.2074244301	shallow neural
0.2074024582	a deep network
0.2073791491	finite number of
0.2069056396	learning ability
0.2068733848	grid like
0.2067978067	results comparable
0.2065081667	present paper
0.2064428718	number of
0.2064072073	tend to
0.2063228083	virtue of
0.2062539362	for speech enhancement
0.2061398247	model trained
0.2060414320	of such models
0.2059498334	binary neural
0.2057082295	existing state of
0.2054885038	level performance
0.2053286855	heuristic algorithm
0.2051078746	running time
0.2049547841	accompanied by
0.2049375388	general video
0.2045926905	traditional neural
0.2042455025	networks outperform
0.2041923386	in most cases
0.2040614527	faster and more
0.2040139476	subjected to
0.2039668228	classical neural
0.2039574419	a level
0.2036517879	an online
0.2035591357	architecture based
0.2033806947	based on genetic algorithms
0.2032488773	advent of
0.2031425436	this paper demonstrates
0.2031334373	two fold
0.2030914993	linear combinations of
0.2030414320	of such algorithms
0.2030188979	small number of
0.2029970872	plausible neural
0.2028120986	compared against
0.2027906797	\ max
0.2026311463	depends on
0.2026297551	an interactive
0.2024604945	mismatch between
0.2021066686	new insights
0.2020985040	connected layer
0.2019150841	deep learning system
0.2016556585	outperforms state of
0.2016009219	to solve
0.2015615490	art performance on
0.2015048130	black box complexity of
0.2011591199	over successive
0.2010486103	the long short term memory
0.2008114527	propose two new
0.2005834894	evolving neural
0.2003372751	this paper reviews
0.2003131558	key features of
0.2002799386	dataset containing
0.1999368861	small subset of
0.1997878704	of such networks
0.1995318154	best fit
0.1995293697	each iteration
0.1995285944	interpreted as
0.1989243089	similarities between
0.1989173643	r ^
0.1988672431	using artificial neural networks
0.1988490631	extracted from
0.1987556657	devoted to
0.1987056294	compared to existing
0.1984877721	inspiration from
0.1984617408	a convolutional neural network
0.1983760022	very fast
0.1982363468	relies on
0.1982042686	an open problem
0.1981871103	$ k
0.1980614850	end to end deep
0.1977481004	learning algorithm for
0.1976598294	a general framework
0.1975721014	the proposed framework
0.1973086059	more sophisticated
0.1972802510	limited number of
0.1972177349	0 1
0.1969140463	relying on
0.1965788831	in silico
0.1963663863	key features
0.1963278698	algorithm to solve
0.1963242491	lower bound for
0.1962860161	theoretical understanding of
0.1957046599	very few
0.1955975184	traditional evolutionary
0.1955689787	a wide range
0.1953833667	an fpga
0.1953633507	time scales
0.1952545487	useful tool
0.1952412399	insights about
0.1946798198	re training
0.1945587457	this paper deals
0.1944670865	applied to
0.1942853541	widely applied to
0.1941672223	visual system
0.1941315224	a generative model
0.1941260131	deals with
0.1940805222	m ^
0.1939660755	log ^
0.1938640750	belonging to
0.1938470892	a nature inspired
0.1937245204	lead to
0.1931730304	art models
0.1931713947	due to
0.1928975184	complexity analysis
0.1926457908	those obtained
0.1925305692	frequently used
0.1924400762	tested against
0.1924215758	large amount
0.1923519849	using multi objective
0.1922447101	based on decomposition
0.1921777930	number of weights
0.1919326987	recognition system
0.1919129153	two layer
0.1917530450	with random weights
0.1917164591	by introducing
0.1916587505	relatively simple
0.1912223897	the art result
0.1911081913	the traveling salesman problem
0.1909484822	solve problems
0.1905867748	significant improvements in
0.1902149234	n =
0.1901836992	look at
0.1900614527	space and time
0.1899166169	the human brain
0.1897525403	unable to
0.1895880613	able to
0.1895857709	learning based approach
0.1893053920	refer to
0.1892140516	deep artificial
0.1891105077	into subsets
0.1890804183	consisting of
0.1888074777	large numbers of
0.1887581205	network classifier
0.1887031671	adaptive evolutionary
0.1884458597	neural networks with
0.1884437649	these results suggest
0.1884385564	according to
0.1884257123	control system
0.1884177154	time and energy
0.1883819691	per neuron
0.1883093620	$ ea
0.1883071493	significantly better than
0.1881951733	a multi agent
0.1879455067	reliance on
0.1877021830	non linear activation
0.1873401049	based on fuzzy
0.1872543558	less than
0.1872424628	reinforcement learning with
0.1870252017	plethora of
0.1868641192	a hybrid
0.1867501326	aiming at
0.1867228551	a spiking neural network
0.1864967217	non deterministic
0.1862008618	detailed analysis of
0.1858503565	neural gpu
0.1858133845	more efficient
0.1856090529	original model
0.1855817505	regression neural
0.1854598325	one fifth
0.1853724710	depend on
0.1853251268	an empirical
0.1853160966	on edge devices
0.1852991940	an optimal solution
0.1851417644	this issue
0.1848476330	computing system
0.1848262512	the machine learning community
0.1846235966	based analysis
0.1844492259	results show
0.1841503701	a simple method
0.1840795518	use case
0.1835608373	more complex
0.1835244083	the key idea
0.1834801592	starting from
0.1833539731	susceptible to
0.1831997326	consists of
0.1831684176	multivariate time
0.1830195149	in digital ecosystems
0.1829737142	the time of
0.1829640005	correspond to
0.1824928632	step towards
0.1821572092	a long short term memory
0.1821199696	fundamentally different
0.1821067239	a hybrid model
0.1819245681	capable of
0.1817864181	depth two
0.1817820686	algorithm based on
0.1815688470	performance comparable to
0.1813481039	hybrid neural
0.1812789496	human like
0.1812023876	power of deep
0.1807521990	\ right
0.1807015668	the proposed methodology
0.1806995871	the penn treebank
0.1806220378	much faster
0.1804263631	as well as
0.1804047093	important role in
0.1803346498	fitness value
0.1802799413	system design
0.1799128128	many optimization problems
0.1798781035	statistical properties of
0.1797964550	special case of
0.1797133368	during training
0.1795792210	large class of
0.1795013411	set of experiments
0.1793140025	to adversarial attacks
0.1792680438	an efficient algorithm
0.1790972136	each layer
0.1790448972	\ ^ n
0.1789470503	higher than
0.1787309406	global convergence of
0.1787207402	using deep learning
0.1785001136	point operations
0.1783930055	a promising solution
0.1783623165	$ n
0.1780569540	two dimensional
0.1779237313	an automated
0.1778203642	equipped with
0.1776995295	$ dimensional
0.1773897745	two stage
0.1772902282	results obtained with
0.1771454148	against adversarial
0.1771016996	performance in many
0.1770669680	coming from
0.1770287972	neural networks for
0.1767990852	indistinguishable from
0.1762260919	more and more
0.1762151908	the compact genetic algorithm
0.1760840328	evolutionary reinforcement
0.1758975672	links between
0.1757899014	to date
0.1757634470	an enhanced
0.1757300269	diversity algorithm
0.1756330525	empirical analysis of
0.1755232068	each time step
0.1753995610	the proposed methods
0.1752907934	the input space
0.1752047421	increasing number of
0.1750868368	genetic algorithm for
0.1750820447	connections between
0.1749183946	sub network
0.1749034586	easy to use
0.1748804070	a single neuron
0.1748302938	the other hand
0.1747994765	1 2
0.1747983229	k ^
0.1743737142	\ in \
0.1743317374	an attention mechanism
0.1742886841	the art algorithms
0.1741678905	the input layer
0.1741081701	represented as
0.1740892850	compared with
0.1740381118	full potential
0.1736516329	results obtained by
0.1735718856	mainly focused on
0.1734019117	a single layer
0.1733301370	the nds
0.1730530793	the world of
0.1729570476	the help of
0.1729026758	classification using
0.1727549498	an embedded
0.1726188418	the main idea
0.1726141140	programming approach
0.1725673236	most appropriate
0.1725367954	particularly useful
0.1724870969	denoted as
0.1724380347	the art models
0.1722940593	high classification
0.1722082119	more effective
0.1721937582	key role in
0.1721805821	structural properties of
0.1717801887	based on genetic
0.1717644277	take advantage of
0.1716249503	an important
0.1715560214	deep convolutional neural networks for
0.1715042657	application of deep
0.1715030665	the ant colony
0.1711552110	composed of
0.1710795742	the output layer
0.1710260919	on top of
0.1709734290	in order to
0.1708762819	the results of
0.1707434738	an interpretable
0.1707147055	paper aims to
0.1705074170	based upon
0.1702793943	correlations between
0.1702530793	the responses of
0.1702023697	larger than
0.1701911350	previously known
0.1700853485	spite of
0.1700836701	distance between
0.1700507046	a special case
0.1700454779	a markov chain
0.1699171002	a promising approach
0.1697823534	+ 2
0.1697607075	short time
0.1696889494	a decomposition based
0.1695272884	usually requires
0.1694372664	a feed forward
0.1694143164	convolutional recurrent
0.1692612801	an accelerator
0.1692566290	the proposed technique
0.1691984798	q networks
0.1690530793	the interaction of
0.1689209794	non spiking
0.1688586349	the question of
0.1688278853	number of training
0.1687943164	a brief
0.1684302471	an automatic
0.1682530793	and limitations of
0.1682530793	the exploitation of
0.1679611052	problem of learning
0.1679424570	to improve
0.1676237769	dependencies between
0.1674467948	accuracy comparable to
0.1673594974	with respect to
0.1672753016	the requirements of
0.1672437580	interactions among
0.1672047507	learning to solve
0.1672047495	the proposed architecture
0.1671885632	the promise of
0.1669868124	inspired learning
0.1669791611	collected from
0.1669327552	this study
0.1668616498	best reported
0.1664753016	the likelihood of
0.1664441619	the art accuracy
0.1664210159	the obtained results
0.1663167786	determined by
0.1662656174	leading to
0.1662530793	the topology and
0.1662350748	the implications of
0.1662260919	with and without
0.1660346657	a large scale
0.1659885632	the flow of
0.1658990199	comparisons between
0.1658899574	convolutional neural networks for
0.1658053694	classified into
0.1657928450	interactions between
0.1657753016	the addition of
0.1655463233	characterised by
0.1655286933	produced by
0.1655041162	world problems
0.1654816119	recurrent neural networks for
0.1653476425	hardware implementation of
0.1651795479	to learn
0.1651557302	on line
0.1650530793	the change of
0.1650530793	the results with
0.1650108797	millions of
0.1649858251	algorithm for learning
0.1647753016	the precision of
0.1647229572	2 ^
0.1645272976	emerged as
0.1645147030	using artificial neural
0.1643457853	in terms of
0.1642753016	the frequency of
0.1642212447	a challenging task
0.1641532586	thousands of
0.1640427586	and as such
0.1640369816	generated by
0.1640282015	led to
0.1637753016	the sparsity of
0.1637753016	the source of
0.1637753016	the dimensionality of
0.1637586349	the weights of
0.1634753016	a speedup of
0.1633586349	the learning of
0.1633156394	an end to end
0.1632957131	great potential in
0.1632753016	the topology of
0.1632406304	the domain of
0.1632036289	based spiking
0.1630847203	by virtue of
0.1630530793	and control of
0.1629753016	a total of
0.1629429042	in real world applications
0.1625937924	this paper studies
0.1625795315	this work
0.1624302168	the search space
0.1624156519	$ \ textit
0.1623682610	1 5
0.1623586349	the detection of
0.1623575188	works well
0.1622883406	to understand
0.1622569916	cope with
0.1621040735	propose two novel
0.1620441188	the performances of
0.1618586349	the objective of
0.1618586349	the classification of
0.1618585589	art systems
0.1617958497	efficient implementation of
0.1617941188	the challenge of
0.1617941188	the perspective of
0.1617753016	the paper also
0.1617753016	of neurons in
0.1617753016	the end of
0.1617317747	for training neural networks
0.1615427586	the way for
0.1614393061	evolution of neural
0.1613762819	the parameters of
0.1613291094	in addition
0.1612753016	the direction of
0.1612720428	memetic algorithm for
0.1612586349	the optimization of
0.1611776792	the limitations of
0.1611389054	induced by
0.1611086349	the solution of
0.1610530793	the game of
0.1609156991	traditional machine
0.1609059028	\ lambda +
0.1607941188	the framework of
0.1607586349	a network of
0.1605586442	deep neural networks for
0.1604508779	$ ^ 2
0.1602753016	the algorithm on
0.1602753016	the algorithm to
0.1602753016	the gradients of
0.1602673673	coupling between
0.1602586349	the architecture of
0.1602533096	back propagation neural
0.1602107854	the predictions of
0.1601718685	proposed models
0.1601086349	the depth of
0.1601086349	the method of
0.1600191125	sub problems
0.1600109944	deal with
0.1599167665	diffractive neural
0.1599107854	as input to
0.1598456776	long time
0.1597753016	the gradient of
0.1597686650	a fixed length
0.1597107854	the fields of
0.1596641433	the pareto set
0.1594969025	away from
0.1592753016	the reduction of
0.1592753016	in term of
0.1592753016	the control of
0.1590983267	interaction between
0.1590921101	used to train
0.1589572971	in light of
0.1589329961	value functions
0.1588566993	the input image
0.1587753016	the outputs of
0.1587753016	the modeling of
0.1587753016	the computation of
0.1587671226	link between
0.1587107854	the focus of
0.1585933844	more specifically
0.1585359863	learning deep
0.1584494865	the output weights
0.1582832632	limited training
0.1582753016	the knowledge of
0.1582753016	the similarity of
0.1582586349	a population of
0.1582586349	the diversity of
0.1582572971	the timing of
0.1581753016	a dataset of
0.1580612540	predictions about
0.1580530793	the accuracy and
0.1580436367	in comparison to
0.1580421979	gains over
0.1580350748	and robustness to
0.1579662534	successfully applied in
0.1579632305	obtained by
0.1577757217	per weight
0.1577753016	the processing of
0.1577572971	the operation of
0.1576648538	increasing interest
0.1576333271	while avoiding
0.1575922177	of evolutionary algorithms in
0.1575653269	expected number of
0.1575012842	a novel
0.1574572971	the hyperparameters of
0.1574514450	very popular
0.1574245947	formulated as
0.1573133026	$ \
0.1572572971	the concepts of
0.1571406553	kinds of
0.1569753016	the position of
0.1569608839	linear combination of
0.1569256151	an asynchronous
0.1568443459	in relation to
0.1568109305	to predict
0.1567753016	the weights in
0.1567753016	the activity of
0.1564459155	whether or not
0.1564355863	high number of
0.1562916884	to address
0.1562587160	and so on
0.1560876236	a single
0.1560480529	second part
0.1559753016	the novelty of
0.1559372115	several orders of magnitude
0.1557995910	the encoder and
0.1557753016	the solutions of
0.1557753016	the recognition of
0.1557572971	a generalization of
0.1557410528	a self organizing
0.1554545022	give rise to
0.1554110923	method based on
0.1552964919	hundreds of
0.1552856774	close to
0.1550587160	for use in
0.1550530793	of research in
0.1549720896	the training process
0.1549250407	set of features
0.1547504547	photonic neural
0.1546675930	extensively used
0.1545401826	of deep neural networks
0.1545161935	recurrent neural network with
0.1545120948	neuromorphic system
0.1544843480	a large number
0.1544380235	an easy
0.1544260078	the current state
0.1543641664	to train
0.1543050627	different kinds
0.1542972028	the target task
0.1539819192	respond to
0.1539636609	the main objective
0.1538552799	first stage
0.1537710505	thus enabling
0.1537076288	the capability to
0.1536094974	by means of
0.1535563812	large scale deep
0.1534713046	multilayer neural
0.1533784025	portion of
0.1533306406	the dca
0.1533034232	supported by
0.1531455212	the network size
0.1529835900	an important problem
0.1528603033	in line with
0.1528328625	an alternative
0.1528147516	these issues
0.1527995910	of information in
0.1527555092	$ p
0.1526656977	this end
0.1525001353	^ 1
0.1524981861	between exploration and
0.1524488852	comprised of
0.1524359038	better than
0.1521079758	to locate
0.1520584674	responsible for
0.1519424649	promising approach to
0.1518523469	a multi objective optimization
0.1518443459	a source of
0.1518108731	comparative study of
0.1515943459	the setting of
0.1515739372	set of
0.1515495910	a network to
0.1514968838	n \ lambda
0.1513870750	with application to
0.1513836400	this paper shows
0.1512223687	memory capacity of
0.1512019686	to model and
0.1511261238	an objective function
0.1511044744	stochastic neural
0.1510489078	the ground truth
0.1509668549	without forgetting
0.1507860158	neural networks through
0.1507491597	a scalable
0.1505721836	a neural network architecture
0.1504545814	generalization ability of
0.1504059216	a weighted sum
0.1503899538	amount of
0.1503781940	large enough
0.1503780502	mutation only
0.1500238985	not clear
0.1499615817	the training set
0.1497922487	the mnist
0.1497709739	for multi objective optimization
0.1496220779	set of input
0.1493572360	and imagenet datasets
0.1492762683	large set of
0.1491444144	chaotic system
0.1491027288	very small
0.1490468835	such as
0.1490460370	training method for
0.1490099250	part of speech
0.1488785630	art results
0.1486444861	good solutions
0.1486301915	allows users
0.1485000038	makes use of
0.1482947800	space time
0.1482719438	+ \ lambda
0.1480657408	many researchers
0.1480575888	a new paradigm
0.1478543380	performance of evolutionary
0.1477831418	performance computing
0.1477285393	this problem
0.1477149681	a specific task
0.1476856993	variety of
0.1476343701	approach to solve
0.1475885352	network framework
0.1475791543	integrated into
0.1475116936	second step
0.1474491119	the most common
0.1472457727	two phases
0.1472206543	$ \ times
0.1470335243	using convolutional neural networks
0.1469076288	a means to
0.1468746982	endowed with
0.1468553851	so as to
0.1467771479	change over time
0.1467754091	to overcome
0.1466253092	an optimization problem
0.1462493871	relatively low
0.1462282073	for future research
0.1460561205	arising from
0.1460013905	offered by
0.1458455607	time required
0.1456422327	learning agents
0.1456380811	and classification of
0.1455663519	an effective
0.1455416198	deep networks with
0.1453923785	one step
0.1452979991	to optimize
0.1452559738	a simple
0.1450766315	in machine learning
0.1449772282	consist of
0.1447920901	combined with
0.1446173991	act as
0.1446107367	difference between
0.1444708981	vector representations of
0.1444485819	to generate
0.1444399567	using deep neural
0.1444114125	to create
0.1443907941	three distinct
0.1442883877	proposed network
0.1441399652	of spiking neural networks
0.1439842962	to reduce
0.1439066475	in spite of
0.1436959175	at http
0.1436750922	scale datasets
0.1436698090	the present study
0.1436628637	error rate of
0.1436455355	$ m
0.1435548060	a challenging problem
0.1434802372	method based
0.1434541228	to avoid
0.1433202374	good approximation
0.1432942273	learning capability of
0.1432229767	inspired neural
0.1430364957	the self organizing map
0.1429666514	the objective space
0.1429259450	the global optimum
0.1425424361	better results than
0.1425252625	an elegant
0.1424787430	$ g
0.1424420313	world dataset
0.1423685081	the target domain
0.1422557637	each neuron
0.1422161971	the solution space
0.1421529954	more than
0.1420623235	as opposed to
0.1419105814	a deep convolutional
0.1416857367	advantage over
0.1416037491	convergence time
0.1415915754	very little
0.1415852681	a systematic
0.1415041872	distributed representations of
0.1414856213	co adaptation
0.1414714994	$ iterations
0.1412655459	a survey
0.1412410634	the hidden layer
0.1409100211	a wide range of
0.1408515315	a new perspective
0.1407188454	on board
0.1406928445	number of input
0.1405331861	to extract
0.1403104411	1 d
0.1402997457	more realistic
0.1402155295	to detect
0.1401620894	a simple model
0.1401515310	correlation between
0.1401184876	an effective approach
0.1401022670	experiments show
0.1400426434	in vivo
0.1397456691	3 layer
0.1396754401	a priori
0.1395530255	box models
0.1394954753	large set
0.1394763514	more likely
0.1394470274	to produce
0.1394224847	comparison between
0.1393639584	making use of
0.1393071648	simple yet
0.1392829362	l ^
0.1389009948	propagation neural
0.1386707878	simpler than
0.1386139338	tested on
0.1385985291	better solution
0.1385902000	to achieve high
0.1385262178	$ h
0.1384945686	the arts
0.1383294875	to noise ratio
0.1381424756	optimal solution for
0.1380745068	\ text
0.1379845273	intelligence systems
0.1379628790	a variety of
0.1378835256	a model based
0.1377370644	paper focuses on
0.1376906264	present study
0.1376580200	approach based on
0.1376186353	by learning to
0.1375458707	set of solutions
0.1374317012	very limited
0.1372805752	the search process
0.1371717068	expected optimization
0.1371645042	a low dimensional
0.1371380453	catastrophic forgetting in
0.1369680538	to perform
0.1369394501	the shelf
0.1368735496	version of
0.1368352959	much attention
0.1367766731	of parameters in
0.1366610595	well suited for
0.1366000999	trained on
0.1365797849	process based
0.1365469732	increasingly used
0.1365377560	to spike
0.1364673732	more robust
0.1364644325	= \
0.1363561482	a semi supervised
0.1363054764	even if
0.1361822574	networks trained with
0.1359143066	very large
0.1358615192	hard optimization
0.1357900060	looking at
0.1357608946	related to
0.1356787726	forward neural
0.1356763061	sub networks
0.1356475064	very successful
0.1356186353	this model to
0.1354622777	comparable performance to
0.1352785955	the forward pass
0.1351341414	time domain
0.1351095120	concerned with
0.1350822832	emphasis on
0.1350797313	the mnist dataset
0.1350236565	significantly less
0.1347791237	number of hidden
0.1347552543	few years
0.1347372596	the imagenet dataset
0.1345841556	pre trained on
0.1344823066	with long short term memory
0.1344070982	a real world
0.1343252607	concentrate on
0.1340891291	evolutionary algorithm for
0.1339599557	r \
0.1339486756	approach leads to
0.1338491305	the original
0.1336438299	number of features
0.1334566671	various fields
0.1334454569	a bi objective
0.1334419884	learning research
0.1333985144	the population size
0.1333732201	as good as
0.1333110646	neural model for
0.1332736991	more expressive
0.1330987366	$ c
0.1330570274	to identify
0.1329021243	the research community
0.1327456839	the representations learned
0.1327359937	different network architectures
0.1325693064	the standard lstm
0.1325414657	a high performance
0.1324915320	more powerful
0.1323438533	performs better
0.1323377288	very effective
0.1323220887	deep neural networks with
0.1322385677	suitable for
0.1321954071	for solving multi
0.1320686887	the compact genetic
0.1320185051	recent success of
0.1319734876	the evolutionary process
0.1319050703	the optimal number
0.1318531061	does not depend on
0.1317308853	a data driven
0.1314892488	the original network
0.1310487711	aim at
0.1308682315	performs well
0.1306965992	the global minimum
0.1306126933	used to generate
0.1305141708	inference system
0.1304929609	good generalization
0.1304842039	very similar
0.1304068259	learning rule for
0.1303726991	method for learning
0.1303607924	the expected runtime
0.1303586107	building blocks for
0.1303312491	each pixel
0.1302806072	this approach
0.1300094363	to determine
0.1299157318	second stage
0.1298430412	the decision maker
0.1298080949	network to learn
0.1297538497	each group
0.1296225678	contribute to
0.1296191638	a first step
0.1296136723	the objective function
0.1294987435	algorithm to train
0.1294784252	$ 1
0.1293866299	analysis of evolutionary
0.1293737200	a study
0.1292607376	a pre trained
0.1291933377	analogous to
0.1291549441	by applying
0.1291034168	world tasks
0.1290614836	does not need
0.1289092306	to ensure
0.1283780911	set of problems
0.1282817768	as far as
0.1281959603	supervised deep
0.1281790644	advantages over
0.1281334259	on par
0.1281319938	$ l
0.1280665926	constant time
0.1279407918	use cases
0.1278527782	more compact
0.1278502273	the target function
0.1278213561	exhibited by
0.1277822575	\ c
0.1277598404	impact on
0.1273606268	to build
0.1273603005	a new
0.1271228981	more sensitive
0.1270518641	benefits from
0.1269879669	attempting to
0.1268606804	a significant improvement
0.1267580382	the step size
0.1267308024	a review
0.1266824968	treated as
0.1264944264	multiple optimization
0.1264306672	good solution
0.1263651994	a black box
0.1263474773	not only
0.1262677121	space of possible
0.1261761018	the same
0.1261547892	a high level
0.1261365261	two decades
0.1261213842	easy to
0.1260408392	yet powerful
0.1260170068	for solving optimization
0.1259852052	the cost
0.1258824069	the dynamics of
0.1257843281	accuracy compared
0.1257494759	to select
0.1256532747	the art techniques
0.1256509948	polynomial neural
0.1255070023	focusing on
0.1254605186	world problem
0.1254227488	convergence rate of
0.1253694274	the input data
0.1253372432	a single model
0.1252159276	convergence speed of
0.1251514442	comes at
0.1251140278	to achieve
0.1251112619	experiments indicate
0.1248301771	an advanced
0.1247951627	new methods
0.1246540344	general framework for
0.1245739266	weight changes
0.1245404864	presence of
0.1245130863	without explicit
0.1245088867	more diverse
0.1244320135	using deep
0.1241270483	knowledge about
0.1241127244	the receptive field
0.1241019445	\ ~
0.1237359915	period of time
0.1236633182	using genetic
0.1236441661	non local
0.1234476048	a comprehensive
0.1234368922	supervised training of
0.1233677165	multitude of
0.1233639084	10 100
0.1233550478	an innovative
0.1233321273	a wide variety of
0.1232674840	operate at
0.1232327385	two main
0.1231425246	similarity between
0.1230251267	a broad range of
0.1226716742	the training data
0.1226549333	amount of labeled
0.1226498109	the training phase
0.1226403483	improvements over
0.1225035506	to discover
0.1222312467	more general
0.1221716666	$ 2 \
0.1219944842	replaced with
0.1219719880	^ \
0.1219490836	intend to
0.1218615451	conditions under
0.1218129297	to obtain
0.1216489113	competition between
0.1214518011	l \
0.1213412385	the number of training
0.1212928916	first phase
0.1212810691	different types of
0.1212011003	properties of neural
0.1211665002	compatible with
0.1210739266	existing ones
0.1210641281	by adding
0.1210635695	\ ^
0.1210274401	to protect
0.1209935970	approximated by
0.1208538834	dozens of
0.1208041818	the proposed solution
0.1207824401	a small number
0.1205989592	based on artificial neural
0.1204670486	complexity analysis of
0.1204417639	the proposed
0.1203476344	a long short term
0.1200538333	the proposed hybrid
0.1200289958	less than 1
0.1198598347	a general purpose
0.1197597770	more easily
0.1196022426	memory requirements of
0.1193415604	from raw
0.1192832219	imposed by
0.1192169509	to enhance
0.1191613849	the success of deep
0.1191042159	for learning deep
0.1189588994	learning benchmarks
0.1189191076	recurrent neural network for
0.1189025624	novel evolutionary
0.1188783741	computational model of
0.1188701213	dynamic changes
0.1188502510	a deep learning based
0.1188223666	properties of
0.1187728385	$ x
0.1187691025	billions of
0.1185778116	proportion of
0.1185671589	optimization time
0.1185103313	to handle
0.1184613067	very important
0.1183714556	temporal memory
0.1181351941	communication between
0.1181113481	an important role in
0.1179814434	scale deep
0.1179809573	to share
0.1178978923	at test time
0.1178249406	this purpose
0.1177955735	neural network architecture for
0.1177534122	covered by
0.1177109421	more difficult
0.1176973364	mnist data
0.1176290010	very promising
0.1175938320	fixed number of
0.1175624254	a multi objective
0.1171862986	different parts
0.1171539835	for training deep neural
0.1171283519	on cifar 10
0.1170806623	a simple yet
0.1170203546	the art neural
0.1169406187	based prediction
0.1169216539	presented here
0.1167711578	in many cases
0.1166901169	do so
0.1166637295	an abstract
0.1166539909	controlled by
0.1165434221	as long as
0.1164692605	model based on
0.1164640883	\ mu \
0.1162896603	across multiple
0.1159590879	reasonable time
0.1158462537	each agent
0.1158228385	$ d
0.1157674448	conditioned on
0.1157503810	the present paper
0.1155626847	at once
0.1153421607	scale well
0.1153125579	on one hand
0.1152389231	the decision making
0.1150294053	training time
0.1150198518	algorithm ii
0.1150012296	relation between
0.1149588978	ability to
0.1149184473	able to achieve
0.1146574928	the high level
0.1146520546	this paper focuses
0.1145445523	very difficult
0.1143844860	the real world
0.1143666525	the covariance matrix adaptation
0.1142302903	acts as
0.1141614159	via deep
0.1141378137	lower than
0.1140949876	the experimental results
0.1140569787	this regard
0.1139409740	a meta heuristic
0.1138706278	while requiring
0.1138462481	the same time
0.1138194866	best known
0.1137952067	aspects of
0.1136956157	to capture
0.1134806770	to accelerate
0.1133681084	these findings
0.1133659637	algorithms based on
0.1132337280	further analysis
0.1131919704	this work presents
0.1131452504	shown to
0.1131435990	optimisation time
0.1131343644	but also
0.1130435270	make use of
0.1129808178	accomplished by
0.1129451881	an artificial neural
0.1129380101	small set of
0.1129377966	decision making in
0.1128645168	intends to
0.1128410300	an evolving
0.1127791272	order to improve
0.1127364442	novel approach
0.1125810214	time step
0.1124810004	classifier based on
0.1124431723	significantly better
0.1123976083	a series of
0.1123302163	a two stage
0.1122349210	to tackle
0.1121888698	the network
0.1121271863	available online
0.1120574804	succeeded in
0.1120490141	achieved by
0.1120490141	obtained from
0.1120259526	the network parameters
0.1120139458	effective way
0.1118605187	guided by
0.1117948534	a set of
0.1116678870	two real world
0.1116103156	generative neural
0.1114189738	exponential time
0.1114113794	experiments on
0.1112507922	the loss function
0.1112386916	^ 2 \
0.1111077026	during inference
0.1110722112	using bayesian
0.1109709427	the parameter less
0.1109508542	recognition using
0.1109002135	the covariance matrix
0.1108973317	2 3
0.1108624839	achieve good
0.1108346671	bag of
0.1108287051	much higher
0.1107830990	the source code
0.1107339366	kind of
0.1107176703	expressive power of
0.1107051241	by replacing
0.1106081062	a single hidden
0.1105675293	better understand
0.1105383351	for text classification
0.1105363345	an intrinsic
0.1105204693	learning system
0.1104992475	associated with
0.1104229588	deep neural network to
0.1104102691	computed by
0.1103950676	with high accuracy
0.1103162130	class of neural
0.1103072580	an extended
0.1103070499	the context
0.1102565555	to sequence learning
0.1102492144	member of
0.1101956674	unlike other
0.1101606188	networks trained by
0.1101450345	spiking neural networks with
0.1100640997	more stable
0.1100403402	prone to
0.1100324555	very competitive
0.1100149479	search space with
0.1099747867	$ t
0.1099250736	thus far
0.1097962167	an explicit
0.1097655456	also discuss
0.1097278798	original algorithm
0.1097262726	n ^ 2 \
0.1094980454	outperforms other
0.1094440149	solved by
0.1093015296	past few
0.1092985875	applicable to
0.1092578827	compared with other
0.1092257657	a new neural network
0.1090745838	a self adaptive
0.1090459016	sum of
0.1089315844	the use of
0.1089010106	a preliminary
0.1087168882	approaches based on
0.1085625676	for training deep
0.1084587274	a hebbian
0.1084148116	large amount of
0.1083795516	not just
0.1082884679	provide better
0.1082826842	the input sequence
0.1082802540	measured by
0.1080852285	small amount of
0.1078706415	the main
0.1076180966	a two dimensional
0.1075216342	best solution
0.1075074872	provided by
0.1074599243	vulnerable to
0.1074528672	tends to
0.1074085830	algorithm inspired by
0.1074067737	$ regularization
0.1073624827	future work
0.1072648927	proportional to
0.1071705011	better performance than
0.1071487888	further reduce
0.1068954395	defined over
0.1067199678	outperform other
0.1066570641	most popular
0.1066237180	good performance
0.1065912365	instantiations of
0.1065670296	spiking neural network with
0.1065473781	an optimal
0.1065143620	the network structure
0.1063472889	x \
0.1063351810	the model parameters
0.1062865483	most existing
0.1062833796	by conducting
0.1062724106	time scale
0.1062191819	accuracy compared to
0.1061933397	modeled as
0.1060963408	respect to
0.1059769273	range of network
0.1059722756	evaluated through
0.1058866218	the artificial neural network
0.1058828790	successfully used
0.1058769913	to evolve
0.1058747980	classification accuracy of
0.1058720552	of distribution algorithm
0.1057944477	unsuitable for
0.1057590779	spiking neural networks for
0.1057435216	more generally
0.1056696247	comparable to
0.1056419515	the nk
0.1056174187	convergence speed and
0.1055290782	the artificial bee
0.1055013075	deep neural network with
0.1054984756	achieves better
0.1054357690	different ways
0.1052415872	each cluster
0.1049559697	this phenomenon
0.1048080326	the single objective
0.1047392747	the genetic algorithm
0.1046517367	nature of
0.1045786344	an arbitrary
0.1045129148	the first time
0.1045065925	access to
0.1044452053	an iterative
0.1044422064	a short
0.1043904136	more complicated
0.1042486673	mapping between
0.1042347023	embedded into
0.1042098808	temporal dynamics of
0.1041944995	this work proposes
0.1041836560	along with
0.1040601617	order to learn
0.1039686394	discover new
0.1038876712	the 0 1 knapsack
0.1038861512	more accurately
0.1038146709	by extending
0.1035556072	three key
0.1035057595	network trained on
0.1035055658	the theoretical results
0.1034913420	implemented using
0.1033402735	dominated by
0.1032328210	a new training
0.1032022404	much faster than
0.1031358546	the state
0.1030865140	the results presented
0.1030755021	without relying on
0.1030530246	faced by
0.1029940230	competitive with state of
0.1029160912	evolutionary algorithm based on
0.1028565206	to graph
0.1028525596	the trained network
0.1028265373	framework based on
0.1027159426	an adequate
0.1026545493	a neural network based
0.1026018704	for sequence to sequence
0.1025409586	an exponential
0.1024693782	does not depend
0.1023563643	a multi
0.1023179014	driven by
0.1020139281	$ r
0.1018032128	demonstrated through
0.1017541649	much less
0.1016850587	insensitive to
0.1016481113	the dendritic cell
0.1016009095	the network architecture
0.1015818762	a novel neural network
0.1015743734	a deep convolutional neural
0.1015542633	two major
0.1014856683	an autoencoder
0.1014794542	no need
0.1014453282	more effectively
0.1012122499	a limited number
0.1010414980	by proposing
0.1010101548	empirically show
0.1009915257	very powerful
0.1009484334	the optimal solution
0.1008928955	the state space
0.1008612104	to accommodate
0.1007785058	to calculate
0.1007638592	as opposed
0.1007342481	deep neural networks on
0.1006647696	a case
0.1005991986	a genetic programming
0.1005979709	further development
0.1005580970	as much as
0.1005404917	evolutionary approach to
0.1005054512	workshop on
0.1004922594	validated using
0.1004540099	the experimental results demonstrate
0.1004284283	perform well on
0.1003409448	various domains
0.1003145411	for anomaly detection
0.1003135466	an extensive
0.1003068889	to represent
0.1001189267	used to
0.0999544605	terms of number of
0.0999407759	explained by
0.0999155898	the convergence rate
0.0998498535	to classify
0.0998069873	the last layer
0.0997987193	a multi level
0.0997625077	more challenging than
0.0997316880	deviation from
0.0996296779	three main
0.0995967392	to maximize
0.0995843591	a fitness landscape
0.0995411359	a fully
0.0994879911	able to capture
0.0993664657	detection using
0.0993586121	more reliable
0.0993078854	this idea
0.0992798970	each other
0.0992289621	concepts from
0.0991162297	more efficiently
0.0990768995	each generation
0.0990689205	the performance of
0.0990061012	a particle swarm
0.0989529342	the test set
0.0989518421	lower bound of
0.0987667026	orders of magnitude more
0.0987665943	framework for
0.0987506899	test time
0.0986968726	some recent
0.0986731613	a new hybrid
0.0985351487	energy consumption of
0.0985048943	used to solve
0.0984968799	while achieving
0.0984763327	the behavior
0.0984587194	the context of
0.0983910687	artificial neural networks for
0.0983265179	a feed forward neural
0.0982845943	bounds on
0.0982463859	a machine learning
0.0982411351	the optimization problem
0.0982407958	to implement
0.0982048204	the feed forward
0.0982047142	efficient way
0.0981794327	modeled by
0.0981466030	performed by
0.0981199198	two distinct
0.0980758920	drawn from
0.0980712124	to fool
0.0980522424	very high
0.0979883066	used to estimate
0.0979675357	each node
0.0979043565	this gap
0.0978672894	types of
0.0978339099	to minimize
0.0978207820	compare several
0.0977447608	often requires
0.0977274569	inference time
0.0976883268	while providing
0.0976633200	the model
0.0975736060	this limitation
0.0975629559	by presenting
0.0974195488	the application
0.0971940324	equal to
0.0971886740	further improve
0.0971208021	a first order
0.0970563677	for multi label
0.0969852279	fire neurons
0.0969095816	the number of
0.0968927257	activation function in
0.0968837048	by employing
0.0968530162	attend to
0.0967537358	of biological neural networks
0.0967161319	escape from
0.0967013910	implementation of
0.0966672504	an attempt
0.0965654223	time complexity
0.0965574975	by combining
0.0964732811	conference on
0.0964716862	a generalized
0.0964513478	to align
0.0964223880	the long short term
0.0964019333	one layer
0.0963606187	attributed to
0.0963412545	most relevant
0.0962634449	realized by
0.0962261115	overall accuracy
0.0961924601	the latent variables
0.0961044589	gradient descent on
0.0960541685	directly from
0.0960485480	an unsupervised
0.0959686019	two parts
0.0958430097	the fly
0.0957666432	an internal
0.0957401905	achieved through
0.0957220831	the first layer
0.0957203712	the feature space
0.0956804598	proliferation of
0.0956678946	accounting for
0.0956410474	the memory capacity
0.0956097642	a supervised learning
0.0955993873	a multi layer
0.0955894382	current work
0.0955077291	a probability distribution
0.0954837298	new solutions
0.0954698700	multiple time
0.0954673372	the size
0.0954531215	the art systems
0.0954290154	the input
0.0954195488	the quality
0.0953816959	based speech
0.0953650124	the use of deep
0.0953209949	perform better
0.0953198936	characterization of
0.0952654912	the deep neural network
0.0952184093	a large number of
0.0951748236	the fitness
0.0951386452	work investigates
0.0951324546	starting with
0.0951002588	based methods for
0.0950999348	convolutional neural networks with
0.0950780066	on par with
0.0950771587	always on
0.0950639988	these limitations
0.0950205971	in order to obtain
0.0950100336	to decide
0.0948761526	the results obtained
0.0948713110	the most successful
0.0948424924	an infinite
0.0948113993	other hand
0.0947687530	learning method of
0.0947516169	the art deep
0.0947270282	perform well
0.0946460207	these challenges
0.0946454640	by leveraging
0.0945514856	in spiking neural networks
0.0944743135	energy efficiency of
0.0944448453	co adaptation of
0.0944246905	an artificial
0.0943977374	a well trained
0.0943228368	an ensemble
0.0942872305	significant amount
0.0942526820	lens of
0.0942514268	increasing interest in
0.0941199131	n +
0.0941199131	+ n
0.0940760173	each individual
0.0939745921	samples from
0.0939610429	to survive
0.0937441748	expressed as
0.0936039748	each step
0.0934327084	an ideal
0.0933635412	similar to
0.0933419568	perform better than
0.0933150292	scale problems
0.0933012183	an effective way to
0.0932691094	a parallel
0.0932558000	recorded from
0.0931891702	very low
0.0931022734	comparison with other
0.0930769340	in particular
0.0930203865	the energy function
0.0930096010	a fixed number of
0.0928959125	also discussed
0.0928793476	to resolve
0.0928322281	an extra
0.0927747058	this survey
0.0926228906	two dimensions
0.0925240338	two orders of magnitude
0.0925237437	any prior
0.0924910786	than traditional
0.0924531509	neural network for
0.0924409002	the convolutional neural network
0.0923391482	the beginning
0.0923238452	requires only
0.0922919744	derivation of
0.0922770732	opposed to
0.0922479149	the effectiveness of
0.0921286888	the original data
0.0921153751	used to construct
0.0919923759	gradient descent with
0.0919901230	existing work
0.0919709950	the long term
0.0919632935	one or more
0.0919621290	most promising
0.0919411005	the mnist handwritten
0.0918342140	a brain inspired
0.0916533866	not yet
0.0914055854	based method for
0.0913816959	objective problems
0.0911751206	well known benchmark
0.0911748236	the convergence
0.0911585649	more flexible
0.0910978899	the attention mechanism
0.0910831102	sensitive to
0.0910665168	of training data
0.0910361874	to improve performance
0.0910301261	classification performance of
0.0910045437	these ideas
0.0909613962	approach to
0.0909501615	the art machine
0.0909306724	piece of
0.0909286778	temporal neural
0.0909161550	attached to
0.0909108162	defined by
0.0908944449	to deal with
0.0908807340	a learning algorithm
0.0908801462	method consists of
0.0908596843	a plethora of
0.0908460404	an essential
0.0908413752	the input output
0.0908208934	hyper parameters of
0.0907788444	the pre trained
0.0907416330	expected time
0.0906735598	neural network model for
0.0906279837	a recurrent neural
0.0906224538	the time series
0.0905576369	1 n
0.0905292333	very useful
0.0905242546	computer systems
0.0904769185	new approaches
0.0903977615	new classes
0.0901968446	captured by
0.0901383973	resulted in
0.0901209075	any additional
0.0898930144	evaluated on
0.0898704606	all cases
0.0898703172	par with
0.0898476572	to convert
0.0898128992	to infer
0.0897988948	by removing
0.0895866505	$ 1 \
0.0895755766	amounts of
0.0895241478	a family
0.0895168858	artificial neural networks in
0.0894569921	a lot of
0.0894386296	to infinity
0.0893475783	the convergence speed
0.0893312624	continues to
0.0893312624	amenable to
0.0893163293	to find
0.0893142746	for spiking neural networks
0.0892930220	factors such as
0.0892856824	accounts for
0.0892817811	able to generate
0.0891606344	architecture based on
0.0891324962	the problem of
0.0891018846	better performance
0.0889051791	analysis of
0.0888978173	two or more
0.0888972283	each component
0.0888942037	building blocks of
0.0888623235	no loss
0.0888342324	of fitness landscapes
0.0888015659	convolutional neural network for
0.0887808518	a multi task
0.0887469743	the activation function
0.0886224153	the black box
0.0885983210	each sample
0.0885262151	at initialization
0.0885173270	foundation for
0.0884973608	sampled from
0.0884668864	very challenging
0.0883901630	the hyper parameters
0.0882983730	at https
0.0882855445	interested in
0.0882804940	fraction of
0.0882509291	leads to better
0.0882276184	a time series
0.0882093963	an appealing
0.0882025543	performance of
0.0881799103	a neural
0.0881725449	by utilizing
0.0881575510	very efficient
0.0881525111	unsupervised learning of
0.0880724318	different classes
0.0878237332	further improved
0.0877237662	trained with
0.0876331411	a two step
0.0876124711	a population based
0.0876004349	the need for
0.0875867373	two stages
0.0875724734	adapt to new
0.0874077723	tens of
0.0873825526	the second phase
0.0873400138	the last few years
0.0873194916	the brain
0.0872873587	the proposed neural network
0.0872718800	an initial
0.0872704670	into clusters
0.0872594345	much more
0.0872568747	deep learning for
0.0871886458	in fact
0.0871863694	good results
0.0870881859	the hidden
0.0870829608	moea d with
0.0870683570	information available
0.0870678429	two variants
0.0870184618	the effect of
0.0869431907	based approach for
0.0868632381	a computational
0.0868545339	relates to
0.0868016972	used as
0.0867137474	the past few years
0.0866290567	more suitable
0.0865874640	an ongoing
0.0865488225	difficult to
0.0865450397	technique based
0.0864780844	a number of
0.0864272346	$ 2
0.0863878770	both cases
0.0863454115	networks with
0.0862947676	generated from
0.0862556924	the initial population
0.0862499087	success in many
0.0861670384	used to create
0.0861455416	able to learn
0.0861321269	understanding of
0.0861255345	often require
0.0861021078	an intuitive
0.0860963503	emerge from
0.0860121483	these methods
0.0859999902	first part
0.0859530245	the synaptic weights
0.0859353437	a tight
0.0859329426	while reducing
0.0859134392	each class
0.0858544598	evolutionary algorithm with
0.0858369857	paid to
0.0858114076	after training
0.0857684618	the field of
0.0857480362	the surrogate model
0.0857279496	distribution over
0.0856975443	the problem of finding
0.0855599343	performance compared with
0.0855400744	consistent with
0.0855026751	contributed to
0.0855001683	local minima of
0.0854634484	reduced by
0.0854595779	not always
0.0853880995	the efficiency
0.0853839474	the number of objectives
0.0853133645	strategy based on
0.0852884955	classification accuracy on
0.0852819139	scale data
0.0851635732	the search
0.0851109496	relative to
0.0851004974	new hybrid
0.0850696668	computational cost of
0.0850688919	two steps
0.0850559669	an exhaustive
0.0849354841	neural network with
0.0848601276	a description
0.0848552944	described here
0.0846945345	an extension of
0.0846666277	various types
0.0845754107	composed by
0.0845320672	the aim of
0.0845233183	this tutorial
0.0844858960	promising results for
0.0844112013	determination of
0.0843733854	light on
0.0843634023	thus making
0.0843161525	networks trained on
0.0842986574	to gather
0.0842708347	new variants
0.0840545079	an additional
0.0839957847	theoretical analysis of
0.0839807151	prediction accuracy of
0.0839502546	these properties
0.0839122582	by comparing
0.0839041012	adapt to
0.0838052279	work proposes
0.0837984192	between exploration and exploitation
0.0837738441	converge to
0.0837195254	many fields
0.0837176110	the state of
0.0836968589	a generative
0.0836582861	into smaller
0.0836277734	to facilitate
0.0835839278	this method
0.0835717205	at hand
0.0835107204	while simultaneously
0.0835090600	the art classification
0.0834868071	considered as
0.0834739718	many practical
0.0834379456	gives better
0.0834097502	the local optima
0.0833773637	computational power of
0.0833765163	the simple genetic
0.0833541687	an equivalent
0.0833471997	compared to state of
0.0833128064	performs well on
0.0831975958	posed by
0.0831272782	test set of
0.0830687804	two classes
0.0830676834	classification accuracy by
0.0830094973	computational complexity of
0.0829486082	an intermediate
0.0829373251	the dendritic
0.0827917097	several examples
0.0827915601	these algorithms
0.0827803752	do not provide
0.0827623767	particle swarm optimization for
0.0827456248	to prune
0.0826693338	instead of
0.0826681636	contributes to
0.0826554398	the fitness function
0.0826504168	an encoder
0.0826186783	guidelines for
0.0826039432	used to evaluate
0.0825536791	arise from
0.0825488225	designed to
0.0825267248	the cost function
0.0825263484	an approximate
0.0824990413	number of function
0.0824950689	in essence
0.0824663006	to remember
0.0824649992	in practice
0.0824424819	agreement with
0.0823896382	to remedy
0.0823766201	range of
0.0823653240	control over
0.0823586761	performance compared to
0.0823061051	in practical applications
0.0823005001	to walk
0.0822664866	the mutual information
0.0822341569	more interpretable
0.0822257077	the computational cost
0.0822140759	to recognize
0.0822042113	in contrast to
0.0821809750	the most important
0.0821277708	versions of
0.0820573240	involved in
0.0820487476	the art results on
0.0820346361	solely on
0.0819786462	an analytical
0.0819619628	the limit
0.0819230907	by incorporating
0.0819187444	model of
0.0818430958	subject to
0.0818297209	this assumption
0.0818148790	by transferring
0.0818129672	compared to state
0.0817934115	verified by
0.0817835937	obtained through
0.0817708277	converges to
0.0817651437	regardless of
0.0817581161	the parameter space
0.0816969987	compared to other
0.0816176898	exposed to
0.0815725449	by integrating
0.0815606785	to express
0.0815451335	the problem
0.0815373085	to enable
0.0815132955	on demand
0.0814910553	the art performance on
0.0814842174	come from
0.0814603956	interact with
0.0814311085	a model
0.0813836872	to memorize
0.0813817784	performance on
0.0813615115	under certain
0.0813400487	to solve complex
0.0812914075	to deploy
0.0812853737	evaluated on three
0.0812538965	other machine learning
0.0811587245	the backpropagation algorithm
0.0811504052	a large
0.0810968956	learning method for
0.0809492281	more challenging
0.0809296585	validated by
0.0809200301	an accurate
0.0809161824	important problem in
0.0809152239	ideas from
0.0808933793	much better than
0.0808653351	in isolation
0.0808501034	optimization method for
0.0808364036	in conjunction with
0.0807997935	a high dimensional
0.0807629376	to construct
0.0806840143	the form
0.0806242755	a set
0.0806135271	useful information
0.0805599525	neural network based on
0.0805292530	hybridized with
0.0804513065	a single objective
0.0804497164	need to
0.0804290154	the optimal
0.0803860743	the original model
0.0803058260	process based on
0.0802717696	the energy efficiency
0.0802466784	a modified
0.0802025015	swarm optimization for
0.0801983957	backpropagation through
0.0801154166	the proposed multi
0.0801065201	faced with
0.0800574363	built from
0.0799964463	test problems with
0.0799964164	paper gives
0.0799641743	these questions
0.0799570303	by defining
0.0798911529	by exploiting
0.0798654658	very long
0.0797404748	several times
0.0797351501	n \
0.0796634212	better understanding
0.0796287639	to fuse
0.0796284540	an image
0.0796076210	formed by
0.0795807644	learning techniques for
0.0795640974	able to reach
0.0795011834	the fact
0.0794923371	network approach for
0.0794504398	sort of
0.0794418794	stuck in
0.0794315568	to maintain
0.0793963080	illustrated by
0.0793504207	a hierarchy
0.0793315849	based approach to
0.0793129221	the proposed learning
0.0793102370	six different
0.0793095772	significantly more
0.0792872700	the data distribution
0.0792458651	this reason
0.0792252075	learning architecture for
0.0791107468	technique based on
0.0790970393	over time
0.0790194830	a practical
0.0790022611	parts of
0.0789885061	for diagnosing
0.0789105531	different machine learning
0.0788890655	method for
0.0788305625	signals from
0.0788198948	by adjusting
0.0787891172	a reinforcement learning
0.0787776508	estimated by
0.0787437883	the proposed models
0.0786893827	different activation
0.0786244987	for image classification
0.0786009177	the sum
0.0785563081	conducted on
0.0785169275	the vehicle routing
0.0785060452	available at
0.0785019507	used for
0.0784964677	existence of
0.0784350560	many applications
0.0783951186	this observation
0.0783281996	but rather
0.0783102075	by minimizing
0.0782541162	description of
0.0782476241	contrary to
0.0781637333	on several benchmark
0.0781364221	to acquire
0.0781324714	an activation function
0.0780502478	role in
0.0780423555	advantage of
0.0779878315	a handful of
0.0779736438	a hidden layer
0.0779466919	seek to
0.0778870156	to accomplish
0.0778834666	activation function for
0.0778418690	an unsupervised learning
0.0778362352	cifar 100 and
0.0778145056	3d point
0.0777907271	this work aims
0.0777899620	learning approach for
0.0777676725	the latter
0.0777621968	the building blocks
0.0776724503	power consumption of
0.0776152830	examination of
0.0776078143	the training
0.0775890725	to optimise
0.0775594535	an interesting
0.0775319322	the indirect
0.0775156591	two ways
0.0774985243	deep learning with
0.0774983598	of model parameters
0.0774905453	an area
0.0774836340	the second stage
0.0774372928	a series of experiments
0.0774084487	sub models
0.0773616452	and lower bounds
0.0772989265	the proposed network
0.0772221624	implemented by
0.0772220223	illustrated on
0.0772214324	to choose
0.0771996448	absence of
0.0771601732	by modifying
0.0771426490	a general
0.0771052620	the existing methods
0.0770936097	coupled with
0.0770417051	advancements in
0.0770242674	able to identify
0.0769532212	an average
0.0768468608	an embodied
0.0768003189	test accuracy of
0.0767368682	hidden layer of
0.0767284520	this project
0.0767056355	in order to improve
0.0766665157	drop in
0.0766610497	the fitness landscape
0.0766217709	patients with
0.0766021602	work presents
0.0765071065	than classical
0.0764990413	number of tasks
0.0764601262	most important
0.0764526958	$ ga
0.0764045480	both synthetic
0.0763761886	to decompose
0.0762951853	comes from
0.0762548048	enabled by
0.0761988058	to assist
0.0761604330	new perspective
0.0761551354	$ 3
0.0760579330	10 dataset
0.0760462513	the end to end
0.0759661300	to execute
0.0758305980	generative models for
0.0758271784	an estimation
0.0757300241	other well known
0.0756591410	n 2
0.0756584511	the classification accuracy
0.0756165167	reduction in
0.0756009176	overall performance
0.0755299553	in order to solve
0.0755251042	to regularize
0.0754989534	and memory requirements
0.0754981194	starts with
0.0754871279	to realize
0.0754554194	defined as
0.0753902577	a proof of concept
0.0753865430	used to build
0.0753741123	neural networks via
0.0752916565	an agent
0.0752854990	ease of
0.0752614115	a novel evolutionary
0.0752206675	the mutation rate
0.0751871002	by constructing
0.0751670794	a neuromorphic
0.0751588602	by studying
0.0751307107	attempt to
0.0751080177	the performance
0.0750911766	the results reveal
0.0750853129	learning framework for
0.0750713165	to point
0.0749756475	new tasks
0.0748958750	best possible
0.0748744857	a local search
0.0748389746	the case of
0.0748311082	the optimization process
0.0748017532	by analyzing
0.0747950984	workings of
0.0747429797	the ability
0.0746867654	evolution of
0.0745628164	to reach
0.0745485625	several orders
0.0745360860	the resultant
0.0744969264	to develop
0.0744765054	an application
0.0743257006	a large set
0.0742793451	the target
0.0742749400	the task of
0.0742409271	through simulations
0.0742250626	proposed algorithm with
0.0742141012	degree of
0.0741195055	able to produce
0.0740206050	efficient than
0.0739970362	of input data
0.0738709691	art performance in
0.0738600698	an immune
0.0738496909	refer to as
0.0738010765	a vector space
0.0737635275	an asymmetric
0.0737399279	the latent space
0.0737246788	optimization problems with
0.0736745650	power of
0.0736437414	the number
0.0735847138	architecture search for
0.0734268485	dependence on
0.0733587507	the synaptic weight
0.0733468608	an attractive
0.0733079165	the population
0.0732844551	these models
0.0732806385	several well known
0.0732445604	more effective than
0.0731260904	principles of
0.0731139500	able to outperform
0.0730888080	methods allow
0.0730627813	accuracy on
0.0730435004	the curse of dimensionality
0.0730355865	the design space
0.0729297199	search algorithm for
0.0728791289	on several datasets
0.0728727133	different types
0.0728700318	to deliver
0.0728486459	an excellent
0.0728243443	comparable or
0.0728012051	all layers
0.0727799693	repertoire of
0.0727689117	tailored to
0.0727390563	higher level of
0.0726309666	the reservoir
0.0725403433	built on
0.0725145636	believed to
0.0725025899	used to evolve
0.0724966341	than others
0.0724491769	the hidden layers
0.0723991912	to compute
0.0723949791	solved using
0.0723866206	conjunction with
0.0723524450	the time domain
0.0723413106	form of
0.0723170818	a new state
0.0722647151	a novel approach
0.0721674948	a group of
0.0720929705	the time complexity
0.0720430549	better solutions
0.0720420252	using multi
0.0720312972	representation of
0.0719686935	relate to
0.0719379401	trained using
0.0719091742	couple of
0.0718889417	learning algorithms for
0.0717886298	a combination
0.0717517363	using reinforcement
0.0717375632	the input features
0.0717328042	favorably with
0.0717220729	each input
0.0716423266	linked to
0.0715708877	combinations of
0.0715511631	very good
0.0715216476	emergence of
0.0715105346	gradient descent for
0.0714610432	to initialize
0.0713729619	the number of neurons
0.0713198538	to compensate for
0.0713089478	the road
0.0712954610	selection of
0.0712701991	created by
0.0712640870	named as
0.0712554088	this note
0.0712553930	inspiration for
0.0712547445	able to improve
0.0711263263	transferred to
0.0710923555	family of
0.0710793539	the learned representations
0.0710114233	the first stage
0.0709505604	enhanced by
0.0709337555	a given task
0.0709200987	accuracy of
0.0708638164	learning rules for
0.0708231913	the 0 1
0.0708074280	a new model
0.0707920234	most current
0.0707671424	this situation
0.0707481895	known as
0.0707437491	in order to achieve
0.0707295844	these results
0.0706853525	used in practice
0.0706397363	by providing
0.0706131666	the energy consumption
0.0705952065	for object recognition
0.0705752548	a graph
0.0705590083	deep neural network for
0.0705192203	three layer
0.0704809683	two objectives
0.0704586599	lies in
0.0704476541	able to solve
0.0704161063	an enormous
0.0703865113	network architectures for
0.0703788128	even more
0.0703658303	parametrization of
0.0703391345	a deep
0.0703362598	techniques like
0.0703279969	an inherent
0.0703198538	two sets of
0.0702914319	two key
0.0701615845	to automate
0.0701172574	a complete
0.0701071866	a test set
0.0700515442	the prediction accuracy
0.0700494022	a consequence
0.0700479141	of spiking neurons
0.0700467064	arising in
0.0699558467	to alleviate
0.0698882564	biological system
0.0698822471	to assign
0.0698362932	evaluated using
0.0698362053	the long short
0.0697587484	to read
0.0697243941	notion of
0.0696748429	seen as
0.0696132559	used to determine
0.0695565761	implemented on
0.0695171226	expressiveness of
0.0694971465	for example
0.0694809845	optimization problem with
0.0694349711	orders of
0.0693643368	first results
0.0692831591	to track
0.0692461002	the design
0.0692000559	$ approximation
0.0691855562	k \
0.0691504172	algorithm for
0.0689859892	to compress
0.0689598580	tool for
0.0689311429	to propagate
0.0688614853	far from
0.0688427371	the one hand
0.0688418794	contained in
0.0688398559	propose to use
0.0688362218	to modify
0.0688362145	paper provides
0.0688124393	the minimum
0.0688114819	accuracy than
0.0688035604	the basis of
0.0687287256	case study of
0.0687251633	implications for
0.0687239489	advances in
0.0687041598	run on
0.0686409157	copies of
0.0686087895	the impact
0.0685927681	failed to
0.0685849134	space of
0.0685807871	the runtime
0.0685734471	an array of
0.0685594835	system level
0.0685386889	communicate with
0.0685359845	represented using
0.0685303907	case study on
0.0685184287	a recurrent network
0.0684857974	consists of three
0.0684756597	by measuring
0.0684714944	by maximizing
0.0684702498	metrics such as
0.0684156367	to adversarial examples
0.0683685855	an active
0.0683628109	in contrast
0.0683462820	many machine learning
0.0683354610	complexity of
0.0683310295	to encourage
0.0683276617	merits of
0.0682480375	to evaluate
0.0682434355	to attend
0.0682397370	constructed by
0.0681934546	time analysis
0.0681108719	for solving
0.0680450072	to cope
0.0680385345	to estimate
0.0680134400	application of
0.0679163049	used to predict
0.0678906744	consideration of
0.0678636685	representation learning with
0.0678615333	used to test
0.0678534632	to attain
0.0677946667	different variants
0.0677799264	to collect
0.0677721340	promising results in
0.0677540121	to solve problems
0.0677315548	assumed to
0.0676885076	the resulting
0.0676807231	the supply
0.0676539350	addressed by
0.0676491838	availability of
0.0676452059	becomes more
0.0676276226	some well known
0.0675936308	the output
0.0675565101	all previous
0.0675196514	lead to better
0.0675004438	explanation for
0.0674727082	an integrated
0.0674491890	a given input
0.0674331392	new insights into
0.0674153383	with minimal
0.0673599692	to end
0.0672931106	replacement for
0.0672866201	effectiveness of
0.0672315548	notions of
0.0672109150	structure of
0.0670538727	a new framework
0.0669477100	the model of
0.0669167892	computational time
0.0669158457	sampling from
0.0669043539	the results showed
0.0667819178	operators such as
0.0667699264	performed on
0.0666971398	an object
0.0666923136	a slight
0.0666511962	an instance
0.0666386703	a quantum
0.0666381503	to provide
0.0666247580	learned from
0.0666187312	and cifar 10 datasets
0.0665942676	a mathematical model
0.0665748797	approach uses
0.0665331071	the cifar 10 dataset
0.0665244087	an open
0.0664217096	new method
0.0664187354	to increase
0.0663882691	novel neural network
0.0663413106	characteristics of
0.0663280186	holds for
0.0663262320	these techniques
0.0662801491	aspect of
0.0662596504	challenging due to
0.0662187254	suited for
0.0661347508	in turn
0.0661247269	as measured
0.0660984965	the hierarchy
0.0660975985	an algorithmic
0.0660301995	class of
0.0659996126	to monitor
0.0659772702	removal of
0.0659770975	of fitness evaluations
0.0659726240	helpful for
0.0659273957	a neural architecture
0.0659200987	design of
0.0658686582	well trained
0.0658480801	a powerful
0.0658443578	mapping from
0.0658326906	a framework
0.0658302219	network based on
0.0658062358	by changing
0.0657557833	three types
0.0657223908	show empirically
0.0656898252	to decode
0.0656845392	problem based on
0.0656330876	inability to
0.0655945006	efficient use of
0.0655815977	very simple
0.0655636907	to discriminate
0.0655596220	the jump
0.0655557322	results on
0.0655382136	knowledge from
0.0654141062	to eliminate
0.0653854104	to save
0.0653305625	obtained using
0.0653175500	the most effective
0.0652820217	incorporation of
0.0652749956	to meet
0.0652702828	a solution
0.0652538306	data set of
0.0652026564	superior to
0.0651775360	used to improve
0.0651585352	a sequence
0.0651524046	approach for
0.0650816118	to continue
0.0650784502	to reconstruct
0.0649230685	first layer
0.0648862755	methods for
0.0647844580	discovered by
0.0647394937	the effect
0.0647310295	to manage
0.0646414453	degrees of
0.0646144482	inspired from
0.0645632791	validated on
0.0645611896	occurrence of
0.0644485217	the number of layers
0.0644459529	the most popular
0.0644388744	the impact of
0.0644309427	identified by
0.0644110068	to fill
0.0643832525	between input
0.0643490253	present here
0.0643353630	with respect
0.0643273982	the number of parameters
0.0642956986	thought to
0.0642608265	a significant amount of
0.0642445925	experiments on three
0.0642066216	the purpose of
0.0641861199	used to perform
0.0641854173	a physical
0.0641656841	network with
0.0641522781	second phase
0.0641481937	a variety
0.0641349426	attention due to
0.0641267651	achieve better
0.0641134015	a two layer
0.0641000670	an exact
0.0640518046	the most challenging
0.0640313799	lack of
0.0639995920	the simulation results
0.0639927783	as well
0.0639920986	used to represent
0.0639621325	an intelligent
0.0639324227	dependent on
0.0639050387	interplay of
0.0638320050	for instance
0.0638250933	investigation of
0.0638244859	with regard
0.0637780834	this report
0.0637447241	to communicate
0.0637349471	more biologically
0.0637118013	account for
0.0636993731	understood as
0.0636929128	a method based
0.0636705845	optimization algorithm for
0.0635525574	reductions in
0.0635481860	different characteristics
0.0635368000	combination of
0.0635296468	order to find
0.0635276073	the classification performance
0.0635019319	the number of bits
0.0633723622	to mimic
0.0633667485	genetic algorithm with
0.0633609942	implemented as
0.0633137803	the weight matrix
0.0632372939	collection of
0.0631703529	to render
0.0631557242	show experimentally
0.0631416915	this investigation
0.0631051785	an introduction
0.0631035368	a given
0.0630831594	improvement in
0.0630364502	used to obtain
0.0629999081	to better understand
0.0629750992	operate on
0.0629543260	three datasets
0.0629521783	to mitigate
0.0629399389	$ ^
0.0628920890	this research
0.0628659073	a second
0.0628568009	the model in
0.0628132398	sequence to
0.0628014059	the particle swarm
0.0627708118	increase in
0.0627553930	families of
0.0627551158	to draw
0.0626867654	level of
0.0626861125	burden of
0.0626697625	proven to
0.0626692515	used to extract
0.0626537869	the echo state
0.0626425040	more efficient than
0.0626355197	with less than 1
0.0626321075	well as for
0.0626223384	the entire
0.0625967801	number of neurons in
0.0625923555	subset of
0.0625565101	not directly
0.0625249400	the concept of
0.0624621323	widely used for
0.0624443987	a new class of
0.0624342708	operates on
0.0623912301	to satisfy
0.0623778330	the original algorithm
0.0623587532	to recover
0.0623413106	result in
0.0623320878	two levels
0.0623248801	seems to
0.0623117262	to imitate
0.0622753692	also introduce
0.0622698895	the most widely
0.0622235391	method uses
0.0622204990	fed to
0.0622166729	development of
0.0621422372	five different
0.0621277108	a convolutional neural
0.0621146367	algorithms like
0.0621017916	the choice
0.0620580838	requirement for
0.0620229952	many layers
0.0619720608	a small
0.0619651897	speed up of
0.0619118903	proved to
0.0618657123	a new type
0.0618185579	a class
0.0617663243	various tasks
0.0617280616	taken into
0.0617279938	influence on
0.0617189428	equivalent to
0.0617090136	by exploring
0.0616641625	to tune
0.0616506859	the case
0.0616496962	aims to
0.0616219264	to explore
0.0615904740	a critical
0.0615100766	the self organising
0.0615001700	model for
0.0614783805	by simulating
0.0614093445	inclusion of
0.0613982501	a memetic
0.0613436004	search space of
0.0612855652	the computational complexity
0.0612069684	an algorithm
0.0611672689	of deep learning in
0.0611243954	devices such as
0.0610772065	possibilities for
0.0610679238	the development of
0.0610413391	with negligible
0.0610071179	competitive with
0.0609383988	builds on
0.0609039605	overview of
0.0608693314	a proof
0.0608410508	back propagation through
0.0608111728	to emulate
0.0608096261	heavily on
0.0608018842	assigned to
0.0607682793	amount of data
0.0607600657	well suited to
0.0607455389	to fit
0.0607172753	also presented
0.0606782484	a sequence of
0.0606317592	this area
0.0605789882	drawback of
0.0605761063	appears to
0.0605757381	trained without
0.0605539518	consists of two
0.0605332955	the same number
0.0605312972	proposed for
0.0605150967	other approaches
0.0605121117	with one hidden
0.0604837941	also provide
0.0604772644	a novel deep learning
0.0604685509	mapped to
0.0604443099	also called
0.0604255564	further research
0.0604192646	to interpret
0.0603943131	an autonomous
0.0603631528	the total
0.0601950839	a given problem
0.0601635121	the size of
0.0601167570	a deep neural
0.0601122511	a solid
0.0601062340	in mind
0.0600686179	branch of
0.0600079390	in advance
0.0600078053	to cope with
0.0599975858	better accuracy
0.0599893595	a subset of
0.0599610995	the brain's
0.0599039375	an effort
0.0598092612	most effective
0.0598019839	to assess
0.0597666574	highly non
0.0597320304	different tasks
0.0596998921	a stochastic
0.0596987627	to guide
0.0596484492	on device
0.0596340532	new model
0.0596240433	the current state of
0.0596197206	learned using
0.0595850599	proposed method on
0.0595736941	to adjust
0.0595011718	employed by
0.0593962907	levels of
0.0593288413	cryptanalysis of
0.0593262320	other methods
0.0593257751	computing with
0.0592788241	the same class
0.0592698981	an introduction to
0.0592512089	a few
0.0592262961	system dynamics
0.0592196617	in summary
0.0591750461	the art of
0.0591641625	to prevent
0.0591002605	a reservoir
0.0590875317	the design of
0.0590788534	adapted to
0.0590622447	a viable
0.0589790325	a key
0.0589019856	a lot
0.0588981157	to add
0.0588959600	composed of two
0.0587881985	curse of
0.0587636693	an unknown
0.0587346204	this kind
0.0586976491	used to implement
0.0586915111	to replace
0.0586852203	effect on
0.0586651008	these problems
0.0586395206	to make
0.0585660899	different domains
0.0584844834	variant of
0.0584426147	different layers
0.0583246696	the final
0.0583238321	developed by
0.0583140973	to encode
0.0583108434	tasks such as
0.0582954610	distribution of
0.0582913106	resulting in
0.0582555781	$ ~
0.0582480863	most useful
0.0582114765	built using
0.0582028515	by up to
0.0581785006	the new algorithm
0.0581287950	by showing
0.0581231594	estimation of
0.0581073340	work focuses on
0.0580842607	much better
0.0580736941	to bring
0.0580424926	data from
0.0580304625	reasons for
0.0580214488	the recent advances
0.0580135508	summary of
0.0579965667	the role of
0.0579724098	an analog
0.0579271631	this task
0.0579150825	eigenvalues of
0.0578566968	a supervised
0.0578545464	also demonstrate
0.0578330310	features from
0.0578140973	to combine
0.0578028141	less parameters
0.0577950273	helps in
0.0577543433	the expected
0.0576922788	$ n \
0.0576868988	alternative to
0.0576354012	required by
0.0576316381	the task
0.0575859556	an original
0.0575844432	the key
0.0575579773	type of
0.0575394767	the first step
0.0575082253	continue to
0.0574899525	three different
0.0574670872	knowledge into
0.0574460117	the current
0.0574105242	regard to
0.0573857182	used to learn
0.0573538076	an external
0.0573194053	this technique
0.0572927142	far more
0.0572539265	terms of accuracy and
0.0572226237	evaluation of
0.0571609111	encoded by
0.0571371332	such cases
0.0571210332	collections of
0.0570778724	extracted by
0.0570740797	a new task
0.0570385121	the quality of
0.0570305170	by creating
0.0569965667	the robustness of
0.0569894946	this new approach
0.0569666729	implemented in
0.0569447188	a fixed
0.0568945157	done by
0.0568925796	characteristic of
0.0568152015	this family
0.0567845265	this property
0.0567117358	decrease in
0.0566983323	while still
0.0566791164	limited by
0.0566178849	this context
0.0566124416	through extensive
0.0565967131	also shows
0.0565723799	novel architecture
0.0564423757	extension of
0.0564396347	the difference
0.0563705561	the importance of
0.0563540929	the use of genetic
0.0563497602	similarly to
0.0563140960	the influence
0.0563124069	evaluated by
0.0562910632	more traditional
0.0562812200	a step
0.0562504298	insights from
0.0561851289	to escape
0.0561755585	controllers for
0.0561178167	interpretation of
0.0561073033	the literature
0.0560984395	the skill
0.0560236548	part of
0.0560088886	used for solving
0.0560045253	a novel method
0.0559964529	crucial for
0.0559707994	algorithms for
0.0559666243	network for
0.0559339008	able to train
0.0558859459	tailored for
0.0558768855	two components
0.0557901635	also present
0.0557847739	resilience of
0.0557541983	the efficiency of
0.0556903846	research into
0.0556339341	framework allows
0.0556280857	performance across
0.0555953074	taken from
0.0555789158	a real time
0.0554946207	also investigate
0.0554453765	network model for
0.0554280521	a result
0.0554269261	this question
0.0553655603	many tasks
0.0553387716	a real
0.0553121963	a comparative
0.0553117429	in order to evaluate
0.0552842460	to drive
0.0552395136	theories of
0.0552330571	use of neural networks
0.0551309189	accessible to
0.0551282484	the efficacy of
0.0550662313	by selecting
0.0550510453	correlated with
0.0550350770	co evolution of
0.0550184877	era of
0.0549417431	a major
0.0549376936	but not
0.0549113359	realization of
0.0549052218	less computational
0.0548926587	a key role in
0.0548901775	a collection
0.0548737568	an input
0.0548444681	learned by
0.0548413106	required to
0.0547082113	a deep reinforcement
0.0546957135	magnitude more
0.0546942838	further study
0.0546601437	more recently
0.0546468100	a program
0.0546336390	to simulate
0.0545854599	to compensate
0.0545594350	the state of art
0.0544206815	a list
0.0544168729	information from
0.0543337262	superiority of
0.0543265115	to analyze
0.0542393595	the existence of
0.0541500686	to aid
0.0541372379	best performance
0.0541112995	first step
0.0540926587	a small set of
0.0540893926	four different
0.0540817499	to distinguish
0.0540709870	motivation for
0.0539644176	research on
0.0539481471	this field
0.0539408835	mixtures of
0.0539355818	added to
0.0539263520	a promising
0.0539175175	helpful in
0.0539158675	to respond
0.0539057580	some other
0.0539006962	uncertainty in
0.0538985479	a feedforward neural
0.0538873266	simulations show
0.0537707470	the components of
0.0537649192	a fair
0.0537565328	on average
0.0537469297	foundation of
0.0537438926	domains such as
0.0537214420	the problem at hand
0.0537122595	a distance
0.0536410166	efficiency of
0.0536164776	significantly different
0.0536099425	differences in
0.0535939045	very close to
0.0535680443	the values of
0.0535600808	a new approach
0.0535453005	the aforementioned
0.0535024745	one way
0.0534846791	also propose
0.0534722285	capability of
0.0534718829	other state of
0.0534168841	such as image
0.0534136696	with limited
0.0534096273	a new method
0.0533940404	an optimized
0.0533862755	problems with
0.0533446635	approach provides
0.0532930008	selected from
0.0532919855	compensate for
0.0532857168	a predefined
0.0532799936	improved by
0.0532514218	response to
0.0532343404	the aid of
0.0532268381	the system
0.0532206068	a method
0.0532169723	to verify
0.0532075502	assist in
0.0531885121	d \
0.0531666729	variants of
0.0531316327	various applications
0.0530903176	needed for
0.0530801783	an existing
0.0530681484	effective at
0.0530270375	the least
0.0530216743	to store
0.0530113053	ideal for
0.0530046486	a variety of tasks
0.0530035875	a student
0.0529790325	to support
0.0529608070	region of
0.0529544207	a trade off between
0.0529531407	deployed on
0.0529451723	a significant
0.0529070647	the most recent
0.0528888744	the presence of
0.0528827540	to check
0.0528758069	best results
0.0528547340	seeks to
0.0528358129	in many areas
0.0528345474	trained by
0.0527979071	very well
0.0527579662	applies to
0.0526843886	an ensemble of
0.0526517141	the last few
0.0526515532	often used
0.0526480991	possible solutions
0.0526265225	unlikely to
0.0525905006	architecture for
0.0525804514	the field of machine
0.0525401342	to learn to
0.0525333734	in many real world
0.0525242990	introduction to
0.0524768547	demand for
0.0524405793	this fact
0.0524174459	fail to
0.0523985483	space into
0.0523932040	interacting with
0.0523899525	or even
0.0523616955	by allowing
0.0523465417	the basis
0.0523352968	the number of nodes
0.0523348295	performs well in
0.0523297462	further demonstrate
0.0522941390	an analysis
0.0522808411	work shows
0.0522798195	accuracy over
0.0522652818	every time
0.0522634446	algorithms such as
0.0522617804	weaknesses of
0.0522465667	the success of
0.0522404809	behavior of
0.0522371566	last two
0.0522116059	applied to other
0.0521899373	at least one
0.0521701730	the field
0.0521417401	dedicated to
0.0521000307	tools for
0.0520875317	the ability of
0.0520719761	to use
0.0520595427	definition of
0.0520497841	the resulting algorithm
0.0520335334	this contribution
0.0520254980	any given
0.0520224403	by evaluating
0.0520151964	motifs in
0.0519965667	a function of
0.0518268278	the winner
0.0518171275	these approaches
0.0518030787	a way to
0.0517842460	a vast
0.0517796861	a specific
0.0517718029	a range
0.0517502540	the problem of learning
0.0517236264	suited to
0.0517067242	the user
0.0517035541	the non linear
0.0516970771	three types of
0.0516675620	to design
0.0516514970	this result
0.0516055004	while making
0.0515932394	properties such as
0.0515888082	consequence of
0.0515757090	choice of
0.0514954152	a new methodology
0.0514929692	the origin of
0.0514911340	disadvantages of
0.0514684246	to further improve
0.0514340293	search for
0.0514311952	used to optimize
0.0514284249	algorithm uses
0.0514249127	an extension
0.0514076420	wish to
0.0513849136	different approaches
0.0513468551	pieces of
0.0513257433	images from
0.0513180443	a new approach to
0.0512909704	to induce
0.0512859378	the system's
0.0512686405	tools from
0.0512528515	the course of
0.0512465667	a range of
0.0512463400	1 \
0.0512268889	a toy
0.0512023659	to synthesize
0.0511862371	the trade off
0.0511774827	constructed from
0.0511457726	the lack
0.0511441307	widely used in
0.0511231594	rate of
0.0511099425	assessment of
0.0510925504	information into
0.0510598317	an estimation of distribution
0.0510425940	new ones
0.0510230542	the success
0.0509844811	to state of
0.0509733242	this review
0.0509147530	the formation
0.0509011932	by reducing
0.0508744352	several recent
0.0508558475	quest for
0.0508193476	used in
0.0508158135	a database
0.0508029012	and up to
0.0508026421	the center
0.0507842460	to quantify
0.0507456104	the actual
0.0507366068	novel framework
0.0507094693	and out of
0.0507025077	generalize to
0.0506539504	invariant to
0.0506470404	more natural
0.0506410166	quality of
0.0506186023	several state of
0.0506161953	problem into
0.0505134400	robust to
0.0504925697	by adapting
0.0504920933	a dynamic
0.0504463406	all tasks
0.0503955657	both in terms
0.0503729248	together with
0.0503312115	placed on
0.0502264206	the application of
0.0501922389	by increasing
0.0501624791	performed better
0.0501548166	to visualize
0.0500945842	the art results in
0.0500875317	the complexity of
0.0500520391	introduction of
0.0500370889	an ant
0.0500191236	an attention
0.0500069842	searching for
0.0499969593	at most
0.0499661556	a non linear
0.0499443752	a structured
0.0498968104	to remove
0.0498472812	a strict
0.0498176959	introduced by
0.0497931212	$ 10
0.0497906477	terms of
0.0497747738	this framework
0.0497251991	the model on
0.0497183304	to exploit
0.0497009032	expressed in
0.0496919540	redundancy in
0.0496754528	to translate
0.0496160203	ratios of
0.0496037927	network architecture for
0.0495897729	method does not
0.0495841504	an active area of
0.0495831594	independent of
0.0495597539	the evolution of
0.0495597539	the accuracy of
0.0495087476	a small number of
0.0494693979	to further
0.0494634400	technique for
0.0494378560	to form
0.0494373259	good at
0.0494246329	adoption of
0.0494246143	trends in
0.0494135121	the effects of
0.0494096906	far as
0.0493559490	to study
0.0493479432	used to compare
0.0493387716	a linear
0.0492578594	possible to learn
0.0492262507	to derive
0.0492211055	the features of
0.0492104544	this challenge
0.0491967474	an approach
0.0491658151	over state of
0.0491128209	the mnist and cifar 10
0.0490568442	a radial
0.0490211839	well as
0.0490195567	an appropriate
0.0490184698	also compare
0.0489747600	the idea of
0.0489105986	a trade off
0.0488926587	a general framework for
0.0488624323	in terms of classification
0.0488518210	functioning of
0.0488449101	a flexible
0.0488259506	consequences of
0.0487923311	a genetic
0.0487738407	text to
0.0486870900	hard to
0.0486490493	each parameter
0.0486242761	trained through
0.0486066558	a principled
0.0485872786	the posterior
0.0485730392	essential for
0.0485597539	the structure of
0.0485387132	the importance
0.0484612565	various techniques
0.0484288739	these concepts
0.0484212044	the influence of
0.0483824973	to embed
0.0483768535	the past few
0.0483204136	english to
0.0482912752	not possible
0.0482838719	function based on
0.0482452839	these factors
0.0482136952	contents of
0.0481966741	only if
0.0481635121	the goal of
0.0481482449	of data from
0.0481397187	stability of
0.0481289529	the boundary
0.0481238010	the same accuracy
0.0480946369	in terms of convergence
0.0480070867	encoded as
0.0479828717	novel type of
0.0479712044	a combination of
0.0479303247	minimization of
0.0478697460	given task
0.0478616955	this goal
0.0478532103	the lottery
0.0477368144	the set of
0.0477316908	also observed
0.0477296861	to investigate
0.0476711055	the difficulty of
0.0476667242	the maximum
0.0476539741	system size
0.0476507980	new technique
0.0476488696	the reader
0.0476410166	theory of
0.0476407820	the concept
0.0476383817	a common
0.0475597539	the behavior of
0.0475582590	than conventional
0.0475234863	the first
0.0475210948	the environment
0.0475002461	to set
0.0474930783	a family of
0.0474910252	variety of different
0.0474682171	a state of
0.0474657733	the potential
0.0474494359	in order to make
0.0473898503	the trade off between
0.0473490188	shown by
0.0473482155	the loop
0.0472880068	able to provide
0.0472727703	to boost
0.0472588641	the decoder
0.0472211055	the runtime of
0.0471375523	a bio
0.0471100173	formulation of
0.0471093571	the amount of
0.0470914602	prediction of
0.0470875317	the process of
0.0470875317	the cost of
0.0470575972	the optimum
0.0470467260	optimized by
0.0470002506	transition from
0.0469727329	two step
0.0469711548	a suitable
0.0469376769	for further research
0.0468750285	landscape of
0.0468728325	information through
0.0468493164	the number of hidden
0.0468417573	mixture of
0.0467302217	learn from
0.0467119412	a recurrent
0.0466443686	a standard
0.0466099425	ensembles of
0.0466024275	the art results for
0.0465786328	as much
0.0465386966	tested using
0.0465172660	a broad
0.0465106081	certain conditions
0.0464600237	a class of
0.0464553995	representations from
0.0464368385	the curse
0.0463600173	change in
0.0463175542	to apply
0.0463164206	the sum of
0.0463093254	various types of
0.0461968162	2 \
0.0461930151	creation of
0.0461518654	proof of
0.0461032470	a fundamental
0.0460875317	the choice of
0.0460620159	generated using
0.0460298963	opportunities for
0.0460002802	performs as
0.0459865537	two types of
0.0459716911	subsets of
0.0459155259	an inverse
0.0459086135	properties such
0.0458942154	bounds for
0.0458941093	experiments on several
0.0458632334	a factor of
0.0458468187	also shown
0.0458224073	the rest
0.0458086298	a finite number of
0.0457631121	not fully
0.0457300466	the past
0.0457132989	by extracting
0.0457123787	a neural network to
0.0455979860	limited to
0.0455850141	a hierarchy of
0.0455246964	to noise
0.0454908110	+ \
0.0454697516	better results
0.0454464521	an improvement
0.0454392494	these areas
0.0454356730	struggle to
0.0454240631	fails to
0.0454158778	the previous state of
0.0454144472	this case
0.0453973362	investigation on
0.0453909982	to lead
0.0453007523	the percentage
0.0452230871	suitability of
0.0452046159	to highlight
0.0451905839	as well as for
0.0451379993	in response to
0.0450864656	different datasets
0.0450843017	referred to
0.0450709635	same time
0.0450689471	the development
0.0450584372	in order to learn
0.0450186190	the network to
0.0450119190	the initial
0.0449147770	even further
0.0448529126	the merits
0.0448176516	many areas
0.0447783127	the need of
0.0447553915	a fast
0.0447042495	the art in
0.0446769789	the most promising
0.0446640016	an action
0.0445915337	on cifar 10 and
0.0445913860	a measure
0.0445862340	problems such as
0.0445359455	augmented with
0.0445217888	promise for
0.0445124415	this insight
0.0444991552	tries to
0.0444972195	restricted to
0.0444778456	able to perform
0.0444697461	model with
0.0444259617	the total number of
0.0444073531	different architectures
0.0443806123	intended to
0.0443711663	probability of
0.0443655092	field of
0.0443556137	to reproduce
0.0442950691	modes of
0.0442921620	a greedy
0.0442609188	performed using
0.0442024145	designed for
0.0441888173	demonstration of
0.0441711055	this type of
0.0441597386	effects of
0.0441528919	good as
0.0441503257	the expected running time
0.0441480103	described by
0.0441065671	cost of
0.0440999171	function for
0.0440651062	the seed
0.0440585476	problem with
0.0440229853	a simplified
0.0440112193	the former
0.0440030541	an error
0.0439927469	a theoretical
0.0439782117	origin of
0.0439637427	the benefit
0.0439610061	taking into
0.0439584097	to adapt
0.0439384469	different levels of
0.0439324230	the basic
0.0439291781	branches of
0.0439014694	tuning of
0.0438912729	problem with time
0.0438629141	between neurons
0.0438436613	the idiotypic
0.0438302862	effects on
0.0438261118	a new evolutionary
0.0438247203	a system
0.0438056754	a formal
0.0437713702	an experimental
0.0437250734	written in
0.0436902267	a detailed
0.0436888883	then propose
0.0436764374	the algorithm's
0.0436341703	performance over
0.0436169516	required for
0.0436135121	the emergence of
0.0435908574	run time of
0.0435679008	the limits of
0.0435499596	needed to
0.0435079662	quantities of
0.0434978625	specified by
0.0433218818	in doing so
0.0432921794	with state of
0.0432713588	the interplay
0.0432636053	amount of training
0.0432475972	leveraged to
0.0432264206	the space of
0.0431857039	both approaches
0.0431231594	comparison of
0.0431150665	to do so
0.0431143190	applications such as
0.0431129812	this paper deals with
0.0430809678	the most relevant
0.0430421984	under various
0.0430355118	by using
0.0430279006	done using
0.0429894630	growth of
0.0429801208	advantages of
0.0429747056	to incorporate
0.0429574957	a robot
0.0429526742	parameters than
0.0429157857	problem of
0.0428097539	the ability to
0.0427872232	these representations
0.0427548595	by considering
0.0427236625	transferability of
0.0427203547	the author
0.0426711055	the utility of
0.0426418092	operates in
0.0426256550	a limited number of
0.0426062320	a target
0.0426047991	rise to
0.0425524151	a novel model
0.0425326439	most recent
0.0425210948	the standard
0.0424555343	an associative
0.0424465667	the superiority of
0.0424275927	such as image classification
0.0424240648	those methods
0.0424176133	in addition to
0.0423951266	among other
0.0423910727	with varying
0.0423250470	the applicability
0.0423095941	task at
0.0423041537	conducted by
0.0422937483	datasets show
0.0422804263	methods such as
0.0422801065	to new tasks
0.0422631649	algorithm with
0.0422182406	adapts to
0.0422156116	also outperforms
0.0422049489	this hypothesis
0.0421962374	comparable with
0.0421848490	pool of
0.0421810893	the other
0.0421696001	than just
0.0421234631	similar or
0.0421055045	to promote
0.0420996295	given problem
0.0420766806	possibility of
0.0420744782	able to find
0.0420670636	a special
0.0420412179	a pre
0.0420260288	up to
0.0420259129	explanation of
0.0420096025	to forecast
0.0419917137	the degree
0.0419732007	to demonstrate
0.0419529070	methods in terms of
0.0419435416	ability of
0.0419433257	route to
0.0419398316	to gain
0.0419285103	demonstrated on
0.0419185976	the art performance in
0.0418819504	a strong
0.0418659723	proposed to
0.0418656961	size of
0.0418094395	to leverage
0.0417541983	the behaviour of
0.0417508306	the cifar 10 and
0.0417331607	first propose
0.0417312526	want to
0.0417264206	the output of
0.0417264206	the training of
0.0417231651	needs to
0.0416430872	a model of
0.0416168955	aiming to
0.0415961050	performance than
0.0415791433	the expected number of
0.0415743101	also study
0.0415740907	the extent
0.0415595844	impact of
0.0415210993	3 \
0.0415188501	the ultimate
0.0415033864	convergence of
0.0414974406	these measures
0.0414626120	an output
0.0414316640	length of
0.0414237779	seen during
0.0414024606	to test
0.0413848500	self organization of
0.0413141353	series with
0.0412708804	study of
0.0412568074	amount of information
0.0412534676	operator for
0.0412435800	approaches such as
0.0412411064	contrast to
0.0412389646	corresponding to
0.0412264206	the study of
0.0412242580	the user's
0.0412227187	any other
0.0412052055	this direction
0.0411903643	observed in
0.0411866311	even without
0.0411843613	to bridge
0.0411826445	to carry
0.0411573008	stored in
0.0411423026	information between
0.0411226524	taxonomy of
0.0411226524	shortcomings of
0.0411225800	this process
0.0411207086	aid of
0.0410408209	running on
0.0410393595	a collection of
0.0410354876	to achieve state of
0.0410350102	start with
0.0410332350	a subset
0.0410218977	also give
0.0409709330	an individual
0.0409355337	the energy efficiency of
0.0409291529	by identifying
0.0408986168	several standard
0.0408904601	ratio of
0.0408856848	very different
0.0408670990	this aspect
0.0408493277	the shape of
0.0408097539	the form of
0.0408061626	each task
0.0407990551	two orders
0.0407745840	to compare
0.0407645334	error between
0.0407618420	the primate
0.0407433472	efficacy of
0.0407426520	the flexibility
0.0407280884	spectrum of
0.0406636800	a very simple
0.0406113618	proposed as
0.0406045899	survey on
0.0405813113	the input and output
0.0405784681	to integrate
0.0405535689	rules from
0.0405383833	an efficient way
0.0405375317	the notion of
0.0405187909	to deal
0.0405134162	improvements in
0.0404750924	maintenance of
0.0404611828	biases in
0.0404580935	the origin
0.0404240551	work aims
0.0403196762	the properties of
0.0403117287	number of possible
0.0403053247	utilization of
0.0402939470	than existing
0.0402824455	in computer vision
0.0402809880	to specify
0.0402395401	an environment
0.0402264206	the convergence of
0.0401980677	a positive
0.0401726524	quantity of
0.0401394067	lot of
0.0401120560	a rigorous
0.0401081499	organization of
0.0400979576	result of
0.0400553146	the true
0.0400301787	a review of
0.0400229175	mainly due to
0.0399795285	the generality
0.0399764114	a library
0.0399613987	generation of
0.0399577345	the bottom
0.0399573927	other algorithms
0.0399565976	used for training
0.0399440727	allowing for
0.0399428573	proposed by
0.0399024813	to segment
0.0398978367	the hessian
0.0398569241	an architecture
0.0398157781	range from
0.0398105932	same accuracy
0.0397878789	the edge of chaos
0.0397849407	to establish
0.0397803375	models such as
0.0397705760	used to model
0.0397699484	to extend
0.0397641020	across many
0.0397626218	several methods
0.0397465005	function of
0.0397402755	a huge
0.0397402573	usefulness of
0.0396722285	extended to
0.0396690291	solutions than
0.0396565447	an adversarial
0.0395851600	the next generation
0.0394964802	the neocortex
0.0394893731	the analysis of
0.0394753499	system called
0.0394121171	the difficulty
0.0393910727	to explain
0.0392844463	located in
0.0392802184	property of
0.0392598141	the art on
0.0392489743	case of
0.0392248102	other tasks
0.0392008243	a system to
0.0391398888	results on two
0.0391355042	possible to use
0.0391152164	simulation of
0.0391046660	a diverse set of
0.0391034239	2 +
0.0390590146	does not use
0.0390497930	factor of
0.0390301787	the benefit of
0.0390194952	a unique
0.0390058392	the edge of
0.0390035469	the correct
0.0390017699	two different
0.0389942777	the results show
0.0389926525	likely to
0.0389796516	a popular
0.0389773030	a new type of
0.0389695377	feasibility of
0.0389444250	the retina
0.0389200266	cycle of
0.0389030758	studies on
0.0388892692	several existing
0.0388774237	survey of
0.0388608500	to map
0.0388557271	defined on
0.0388117589	experiment with
0.0388039446	among several
0.0387932404	platform for
0.0387565673	a proper
0.0387345905	the result of
0.0386915052	to devise
0.0386848513	performance under
0.0386805781	a big
0.0386736492	several experiments
0.0386469697	architectures such as
0.0386421996	a novel architecture
0.0386418710	a generic
0.0386212120	measure of
0.0386135121	the feasibility of
0.0386123343	geometry of
0.0385776316	a feed
0.0385718980	to simplify
0.0385470725	the benefits of
0.0385316792	encountered in
0.0384798143	the most
0.0384510013	this document
0.0384376389	new method to
0.0383980025	an example
0.0383885882	also show
0.0383747691	pair of
0.0383173568	a group
0.0382854675	the utility
0.0382533000	to analyse
0.0381847110	the extent to
0.0381647985	strategy for
0.0381546045	a large set of
0.0381501039	a message
0.0381157950	comes with
0.0381108539	not known
0.0381084973	expense of
0.0381045304	specification of
0.0380858230	each solution
0.0380746003	in combination with
0.0380597643	the ntk
0.0380569197	the effectiveness
0.0380550575	not suitable
0.0379984173	several techniques
0.0379662992	several approaches
0.0379474996	internet of
0.0379234326	performance of different
0.0379213068	a basic
0.0379152437	to serve
0.0379123417	still not
0.0379120376	the dimension of
0.0379114646	new architecture
0.0379010177	other existing
0.0378953783	in many applications
0.0378719024	a neural network for
0.0378706649	approach does not
0.0378676475	the agent's
0.0378593025	trying to
0.0378570748	a classifier
0.0378301787	the order of
0.0377665894	the superiority
0.0377641745	the input to
0.0377413460	the asymptotic
0.0377246005	allowed to
0.0377196036	to illustrate
0.0376724833	to validate
0.0376575163	the outcome
0.0376287156	the goal
0.0376121471	to scale
0.0376113977	a novel technique
0.0375839211	algorithms in terms of
0.0375658225	work provides
0.0374965832	sample from
0.0374884424	a novel neural
0.0374783160	the well known
0.0374710229	the same input
0.0374380622	in many real
0.0373460783	trained to
0.0373284586	an upper
0.0373094395	to match
0.0373023072	a given set of
0.0372953903	obtained with
0.0372792101	any time
0.0372690682	a well known
0.0372472183	techniques such as
0.0372402847	the art methods for
0.0372346943	problems like
0.0372128810	better than other
0.0372069211	to scale to
0.0372045696	the absence
0.0371988972	new state of
0.0371369205	datasets such as
0.0371228412	analyzed by
0.0370856606	a wide
0.0370768231	not available
0.0370421042	a new state of
0.0370404767	to control
0.0370373622	role of
0.0370193725	try to
0.0369992281	system based on
0.0369951835	an easy to
0.0369766688	identification of
0.0369451936	to operate
0.0369412638	a crucial
0.0369316661	suite of
0.0369238659	a distributed
0.0369054094	the covariance
0.0369022615	the existence
0.0368930872	a method for
0.0368728368	both tasks
0.0368638040	best architecture
0.0368534088	the ratio of
0.0368068028	the famous
0.0368037383	reason for
0.0367787120	the desired
0.0367502710	previous work on
0.0367260821	the heart of
0.0367259617	the expressive power of
0.0367239586	running time of
0.0367211016	by optimizing
0.0366622504	tried to
0.0366436137	to characterize
0.0366135121	the possibility of
0.0365772631	a compact
0.0365414710	without loss of
0.0365176171	the expense of
0.0365126289	on mnist and
0.0364923685	the network's
0.0364852449	a typical
0.0364541983	the variance of
0.0364398235	this setting
0.0364205636	change over
0.0364085422	a better understanding
0.0364033260	amount of time
0.0363784620	the vast majority of
0.0363360402	exploration of
0.0363315451	point of
0.0362902267	a central
0.0362603547	to separate
0.0362587794	the type of
0.0362517874	candidates for
0.0362351919	all possible
0.0362346943	method provides
0.0362339249	this view
0.0362302014	a thorough
0.0362110430	the teacher
0.0362060262	the validity of
0.0362056654	a high
0.0361625817	linearly with
0.0360820739	this means
0.0360647845	the art for
0.0360368827	a crucial role in
0.0360250933	represented in
0.0360224973	the sense
0.0360219061	the approximation of
0.0360097820	all existing
0.0360076300	the representation of
0.0360020913	to prove
0.0359852449	to play
0.0359712789	an experiment
0.0359677291	study on
0.0359533060	a clear
0.0359371397	an optimization
0.0359134605	the majority of
0.0359102764	the validity
0.0358803247	modification of
0.0358801272	the usefulness of
0.0358780431	none of
0.0358577821	to advance
0.0358134767	a threshold
0.0357964753	a novel algorithm
0.0357773874	a new architecture
0.0357729030	such as evolutionary algorithms
0.0357541983	the length of
0.0357465667	the potential to
0.0357264180	comparison with
0.0357233476	than state of
0.0357231258	a challenging
0.0357093247	the middle
0.0357020572	new dataset
0.0356836489	integration of
0.0356570537	desire to
0.0356086813	various methods
0.0356074188	to approximate
0.0355841632	synthesis of
0.0355795696	the value function
0.0355692637	series of
0.0355589209	outputs from
0.0355162038	the sensitivity of
0.0355042196	work well
0.0354945111	the aim
0.0354520438	in terms of accuracy
0.0354511829	deployment on
0.0354398315	an advantage
0.0353718963	presented by
0.0353644355	the potential of
0.0353377291	such as images
0.0352962659	the computational complexity of
0.0352602211	grounded in
0.0352587794	the selection of
0.0352489971	the relationship between
0.0352390575	under different
0.0352164167	other techniques
0.0351929759	the necessity of
0.0351799497	the role
0.0351581961	these two
0.0351462095	the model to
0.0351152834	less time
0.0350524950	a substantial
0.0349936272	other types of
0.0349715124	encoded in
0.0349498879	aimed to
0.0349465667	to adapt to
0.0349403404	to measure
0.0349358648	process of
0.0349147207	new task
0.0349113804	the extent of
0.0349046045	the spread of
0.0348962095	the model with
0.0348419663	features such as
0.0348167820	tasks like
0.0347973431	more attention
0.0347912810	an increase in
0.0347906204	a notable
0.0347657757	more than one
0.0347584311	of attraction
0.0347326521	an approximation of
0.0347195210	performance in terms of
0.0346961860	systems such as
0.0346488529	the fundamental
0.0346095723	an implicit
0.0345824986	all three
0.0345687572	choice for
0.0345514092	the outcome of
0.0345366725	a visual
0.0344859161	a system of
0.0344731651	to get
0.0344516048	a suite of
0.0344370897	function value
0.0343963404	the timit
0.0343936771	also known as
0.0343906400	widely used to
0.0343803613	a desired
0.0343779481	accuracy at
0.0343777577	a finite
0.0343769493	approximation of
0.0343614339	capabilities of
0.0343247144	promises to
0.0342746003	a better understanding of
0.0342705252	by designing
0.0342517590	between layers
0.0342309035	two simple
0.0342059048	the efficacy
0.0341950026	to escape from
0.0341865326	a great
0.0341746005	ranges of
0.0341421709	a rich
0.0341381061	the system to
0.0341340848	for calculating
0.0341330207	the sequence of
0.0340986376	search over
0.0340910843	a new dataset
0.0340903058	employed to
0.0340900872	the whole
0.0340875565	bound on
0.0340569789	deployed in
0.0340323682	this paper aims to
0.0339516048	this results in
0.0339312830	a variant
0.0339295050	history of
0.0338615787	the limit of
0.0338581868	to decrease
0.0338488930	many existing
0.0338488930	only local
0.0338252115	methods like
0.0338206657	guaranteed to
0.0338037394	the landscape of
0.0337926480	scheme for
0.0337882006	the inherent
0.0337796550	relationship with
0.0337581747	an accuracy of
0.0337530939	ubiquitous in
0.0337504230	times more
0.0337470725	the absence of
0.0337325967	novel deep
0.0337104655	assumptions on
0.0337019937	a challenge
0.0336872246	a long time
0.0336834182	both training and
0.0336584604	the possibility
0.0335810200	element of
0.0335693448	hold for
0.0335230085	seem to
0.0335202911	the hope
0.0334744851	the usual
0.0334103976	learn to
0.0333932839	$ evolutionary
0.0333927821	an attempt to
0.0333791433	the principles of
0.0333649927	group of
0.0333625423	progress in
0.0333457469	insights on
0.0333387474	a self
0.0332778312	a new class
0.0332770432	the wild
0.0332544374	to suggest
0.0332262189	to other state of
0.0332206657	universality of
0.0331903153	improvement of
0.0331902364	a neural network with
0.0331754485	allows for
0.0331612290	the number of neurons in
0.0331490784	a kind
0.0331309952	the interplay between
0.0330973456	majority of
0.0330910345	the assumption
0.0330767118	different kinds of
0.0330764939	than previously
0.0330634894	generalizes to
0.0330409201	at finding
0.0330138531	the idea
0.0330075711	to keep
0.0329838048	built in
0.0329710547	extraction from
0.0329569211	a small amount of
0.0329546325	a metaheuristic
0.0329343694	to find solutions
0.0328712765	a novel deep
0.0328704407	the primary
0.0328477201	for generating
0.0328017874	utilized for
0.0328007345	several datasets
0.0327930783	to search for
0.0327760635	as well as to
0.0327735707	a mixture
0.0327097450	an analysis of
0.0326863454	applied on
0.0326444260	explanations for
0.0326333393	a cooperative
0.0326269516	both single
0.0326208953	one way to
0.0326152264	evolved by
0.0325737681	the last
0.0325689887	the majority
0.0325576772	learns to
0.0325472757	relevant to
0.0325445162	significance of
0.0324990349	this paper focuses on
0.0324638424	to examine
0.0324618076	an extension to
0.0324444469	but still
0.0324307996	across different
0.0324302312	to generalize
0.0324014339	effect of
0.0323238345	this way
0.0323232628	by taking
0.0323231469	a new algorithm for
0.0322801787	the strength of
0.0322689552	a reasonable
0.0322541983	the fitness of
0.0322388051	new type of
0.0322358676	the quest for
0.0322338657	each image
0.0322036351	parameters such as
0.0321592910	drawbacks of
0.0321461835	a hot
0.0321265311	resulting from
0.0321248433	the inclusion
0.0320995262	presented in
0.0320684955	new concept
0.0320647854	a tool
0.0320585735	increases with
0.0320064868	the evolved
0.0319975148	other applications
0.0319942777	the generation of
0.0319942777	the level of
0.0319934447	introduced as
0.0319527151	a promising approach to
0.0319360646	to answer
0.0318811866	the flexibility of
0.0318520548	arise in
0.0318460783	developed for
0.0318460371	the suitability of
0.0318460371	the generality of
0.0317413148	a valuable
0.0317238487	the gap
0.0317174666	more energy
0.0317069232	the need
0.0316978678	a modular
0.0316899439	a special case of
0.0316833986	these constraints
0.0316465726	crucial to
0.0316360547	a non
0.0315963854	capacity of
0.0315904920	the performance on
0.0315855559	other machine
0.0315651387	to lead to
0.0315257509	the internet
0.0314971977	propose to
0.0314893904	the bbob
0.0314306183	dimension of
0.0314253841	to retain
0.0314017616	a new family of
0.0313631855	the estimation of
0.0313266904	the creation of
0.0313118090	several types of
0.0313017707	most accurate
0.0312949741	the need to
0.0312825277	new feature
0.0312810473	the characteristics of
0.0312732954	a good
0.0312477109	a number
0.0312275466	of interest
0.0312097668	to focus
0.0312080851	with other methods
0.0311999647	an energy
0.0311489666	the training time
0.0311083090	trend in
0.0310717423	to account
0.0310462095	the network with
0.0310147854	a diverse
0.0310049401	speed of
0.0309942777	the combination of
0.0309843713	many other
0.0309456704	guarantees for
0.0309405764	a memristor
0.0309298948	the distribution of
0.0309285320	applicability of
0.0309147754	to exhibit
0.0308782331	for representing
0.0308726137	superposition of
0.0308643904	a bi
0.0308482609	expansion of
0.0308229645	accuracies on
0.0308162765	functions such as
0.0307793905	new algorithm for
0.0306980268	a network with
0.0306970467	use of
0.0306951017	refinement of
0.0306845905	the formation of
0.0306724608	cooperation in
0.0306710921	the applicability of
0.0306579222	accuracy while
0.0306362682	the best known
0.0306328186	a direct
0.0306127051	variance of
0.0305970028	an auto
0.0305842976	loss of
0.0305666775	the first to
0.0305583744	hybridization of
0.0305444350	the beginning of
0.0305387924	performance while
0.0305290910	new approach
0.0305229322	the advantages of
0.0305229322	the lack of
0.0305218025	the body
0.0305029930	basins of
0.0304856809	included in
0.0304663583	implemented with
0.0304516048	an overview of
0.0304507537	the same network
0.0304445162	placement of
0.0304409578	to connect
0.0304250624	a new technique for
0.0304123671	this criterion
0.0304076614	to define
0.0303976599	the recent success of
0.0303958172	applied in
0.0303815530	the scope
0.0303808379	for designing
0.0303803994	success in
0.0303799471	to balance
0.0303729394	a conceptual
0.0303624963	by implementing
0.0303555781	to benefit
0.0303460371	a survey on
0.0303283986	a new technique
0.0303088533	beneficial for
0.0303046660	a novel method for
0.0303017707	most cases
0.0302959638	a proof of
0.0302639830	order to
0.0302602100	done on
0.0302368144	the nature of
0.0302102030	a 2d
0.0301924283	most successful
0.0301515495	the dominant
0.0301375528	not suitable for
0.0301364097	a virtual
0.0301256550	the possibility to
0.0301200203	a mixture of
0.0300554467	a new algorithm
0.0300096279	the necessity
0.0299942777	the advantage of
0.0299629280	a differentiable
0.0299402417	a neighborhood
0.0299280442	a framework for
0.0299133443	key to
0.0299121713	the notion
0.0298999677	a canonical
0.0298986470	mechanism for
0.0298757818	in conjunction
0.0298598522	results in terms of
0.0298521195	demonstrated by
0.0298484148	found by
0.0297933054	promise to
0.0297822459	to reveal
0.0297575682	the comparison of
0.0297116606	all other
0.0296815201	by developing
0.0296685123	100 datasets
0.0296612120	means of
0.0296605412	two orders of
0.0296474085	configured to
0.0296454951	report on
0.0296162083	the second
0.0295927530	this scheme
0.0295804695	for very large
0.0295770549	to partition
0.0295571267	influence of
0.0295523168	solution for
0.0295388562	two types
0.0295363394	several other
0.0294914994	the predicted
0.0294478129	to see
0.0294424713	the magnitude of
0.0294223515	the running time of
0.0294202418	approach allows
0.0293872253	new methodology
0.0293717296	more suitable for
0.0293557156	this kind of
0.0293347458	sense of
0.0292765276	as possible
0.0292741605	for guiding
0.0292709344	an array
0.0292591921	order of
0.0292536875	a certain
0.0292455169	beginning of
0.0292008278	different levels
0.0291963813	limitation of
0.0291769460	vulnerability of
0.0291404034	achieved with
0.0291121901	for extracting
0.0291104900	the same number of
0.0291074040	the respective
0.0291017429	the corresponding
0.0290962562	a large amount of
0.0290789843	the next
0.0290543540	operating on
0.0290449119	adapting to
0.0290219061	the property of
0.0290118462	than other
0.0290072528	an order
0.0290063458	a recursive
0.0289942777	the power of
0.0289733550	the range of
0.0289719423	a given set
0.0289424713	the speed of
0.0289317990	to outperform
0.0289303166	a series
0.0289298948	the capacity of
0.0289120376	the stability of
0.0288872253	most challenging
0.0288720919	comes to
0.0288606310	the problem at
0.0288604062	aim of
0.0288562167	the cga
0.0288460371	the advent of
0.0288142923	view of
0.0287883031	or better than
0.0287476810	essential to
0.0287413292	a genetic algorithm to
0.0287299039	a vital
0.0287211016	by generating
0.0287108657	a novel framework
0.0286905456	differ in
0.0286868709	different aspects
0.0286845674	execution of
0.0286672773	for predicting
0.0286556918	most commonly
0.0286349648	also found
0.0286349648	found at
0.0286233530	as part of
0.0286194717	commonly used in
0.0285938691	fit to
0.0285699647	to plan
0.0285603573	for enhancing
0.0285241517	reported in
0.0285232954	a particular
0.0284824877	time complexity of
0.0284764206	the probability of
0.0284589643	the risk of
0.0284417007	directly on
0.0284077860	problem at
0.0283832579	contribution of
0.0283534088	the location of
0.0283478014	a suite
0.0283418204	a classic
0.0283332579	benefits of
0.0283332579	visualization of
0.0283153828	new paradigm
0.0283069985	an adaptation
0.0282916390	a limitation
0.0282870510	concept of
0.0282810473	the evaluation of
0.0282189573	the degree of
0.0282167675	goal of
0.0282134458	developed to
0.0282127987	exponentially with
0.0281652002	one neuron
0.0281645095	present work
0.0281514843	a variant of
0.0281512474	uncertainties in
0.0281371491	those used
0.0281298218	the classic
0.0281245583	a comparison
0.0281213255	the amount
0.0281199883	other than
0.0281133097	expressions for
0.0281081936	even better
0.0280925541	an independent
0.0280514952	the vast
0.0280051033	an approach for
0.0279910562	a hierarchical
0.0279765021	the presence
0.0279764206	the identification of
0.0279363130	the efficiency and
0.0279348858	the creation
0.0279199119	discretization of
0.0279143485	effective than
0.0279007392	conclude with
0.0278866567	expected to
0.0278801272	a new method for
0.0278797248	to utilize
0.0278703504	the desire to
0.0278303971	new framework
0.0278076563	the relationship
0.0277989730	for storing
0.0277986093	by evolving
0.0277884263	and quantitatively
0.0277763397	successes in
0.0277632016	the relevance of
0.0277558696	an accuracy
0.0277237417	different techniques
0.0277167262	the most appropriate
0.0276660742	calibration of
0.0276612120	area of
0.0276556918	most widely
0.0276516048	the present work
0.0276251032	used to find
0.0275823682	for future work
0.0275821049	trained from
0.0275693055	to preserve
0.0275664781	for classifying
0.0275639886	applied for
0.0275326094	the scope of
0.0275176650	several benchmark
0.0274982973	the expectation
0.0274963849	learns from
0.0274572353	strengths of
0.0274532793	the usefulness
0.0274509627	for accelerating
0.0274268228	aim to
0.0274225339	a probabilistic
0.0273700952	the chance
0.0273267280	a prototype
0.0273233473	the execution of
0.0273006033	the same task
0.0272730658	the second one
0.0272253471	by performing
0.0271999244	methodology for
0.0271695804	model provides
0.0271541701	the interpretability of
0.0271530207	the immune system
0.0271459780	the task at
0.0271385825	to know
0.0271165793	automation of
0.0270985335	a model for
0.0270838231	criticality in
0.0270766079	takes into
0.0270747545	taken as
0.0270695399	a software
0.0270543411	possible to
0.0270410056	also find
0.0270058151	the incorporation
0.0269763287	inference on
0.0269745128	the search for
0.0269705641	different aspects of
0.0269314200	the mammalian
0.0269298948	the prediction of
0.0268555781	a pair
0.0268220138	a novel approach for
0.0268152914	better understanding of
0.0268144758	while most
0.0267599811	new algorithm
0.0267280048	then used
0.0267272425	given set
0.0267051337	heart of
0.0266408410	most common
0.0265909457	in detail
0.0265832579	construction of
0.0265644191	several orders of
0.0265546803	purpose of
0.0265368690	the core
0.0265342118	hierarchies of
0.0265068734	to analyze and
0.0264845905	in case of
0.0264764206	the area of
0.0264350763	the overall
0.0264280620	also provides
0.0264278398	a naive
0.0263683687	a closed
0.0263631855	the exploration of
0.0263539706	introduced in
0.0263216295	to run
0.0262905456	competition on
0.0262887585	appear to
0.0262834002	or higher
0.0262825889	the rate of
0.0262473374	such as object
0.0262372212	the relation between
0.0262369371	by simply
0.0262344477	method allows
0.0262244305	several different
0.0262189573	the theory of
0.0261892672	for assessing
0.0261602193	a trainable
0.0261462562	a novel approach to
0.0261337144	an approach to
0.0261240657	useful for
0.0261234425	to noise and
0.0260514108	the feasibility
0.0260483291	novel approach to
0.0260415112	many recent
0.0260146133	the emergence
0.0260017267	sparsity in
0.0259955176	found to
0.0259944165	time series with
0.0259867349	to employ
0.0259424713	the distance between
0.0259250604	with up to
0.0258710161	performed in
0.0258502759	given input
0.0258395994	to fine
0.0258266904	a measure of
0.0258210082	this gap by
0.0258163740	for controlling
0.0257443539	ensemble of
0.0257311021	a lack of
0.0257289957	question by
0.0257232954	known to
0.0257225865	two important
0.0257115288	acquisition of
0.0257079157	a graphical
0.0256919744	plausibility of
0.0256636636	a novel framework for
0.0256615146	phenomenon of
0.0256573464	by varying
0.0256530207	the loss of
0.0256527847	presented as
0.0256462562	to account for
0.0256383329	the discriminator
0.0256298218	a universal
0.0255680387	a coarse
0.0255445162	window of
0.0255241201	works on
0.0255139291	limit of
0.0255005547	much as
0.0254731154	placed in
0.0254697225	new perspective on
0.0254644701	then show
0.0254483603	to reason
0.0254433144	a form of
0.0254397111	the improvement of
0.0254375466	interest in
0.0254013838	not well
0.0253993277	a pair of
0.0253870046	using backpropagation
0.0253747198	a methodology
0.0253683030	presented to
0.0253611016	then use
0.0253497178	made by
0.0253068442	principle of
0.0252905456	incorporated in
0.0252544580	intensity of
0.0252459407	occur in
0.0252208217	or not
0.0252069272	the calculation of
0.0251972844	potential of
0.0251905442	to conduct
0.0251888273	out of
0.0251699689	a method of
0.0251583932	a mathematical
0.0251580109	to guarantee
0.0251151387	an algorithm to
0.0250585019	used by
0.0250581482	a baseline
0.0250445495	many cases
0.0250065183	a lightweight
0.0250050772	both theoretical
0.0249943539	obtained for
0.0249660742	operate in
0.0249514843	a method to
0.0249487683	further propose
0.0249293186	further show
0.0249274848	an effective way
0.0249106478	the user to
0.0249097834	an implementation of
0.0248939926	occurs in
0.0248897365	the intrinsic
0.0248895727	potential to
0.0248828414	the details of
0.0248634892	connected by
0.0248548987	novel method
0.0248225574	executed in
0.0247463133	at solving
0.0247459407	developments in
0.0247458165	a valid
0.0247266803	adopted for
0.0247212474	to change
0.0247095084	a custom
0.0246700090	outside of
0.0246676244	list of
0.0246347616	a separate
0.0246344716	code for
0.0245887962	a genetic algorithm for
0.0245732222	a modification
0.0245494081	novel genetic
0.0245483965	also compared
0.0245473846	a complementary
0.0245445162	recovery of
0.0245366326	studied in
0.0244218767	consumption of
0.0244077526	necessity of
0.0244009485	a quantitative
0.0243792740	optimality of
0.0243657817	to yield
0.0243212520	working with
0.0243179602	perspective on
0.0243123997	embedded in
0.0243085416	a careful
0.0243043404	challenge in
0.0242954746	meaning of
0.0242681883	propagation through
0.0242537890	impossible to
0.0242376784	rule for
0.0242311866	a study of
0.0242277957	agent system
0.0241544349	this methodology
0.0241357712	integrated with
0.0241357712	degradation in
0.0241288366	different strategies
0.0241128635	each hidden
0.0240846316	for creating
0.0240644931	a simple but
0.0240183030	the incorporation of
0.0239932703	the hippocampus
0.0239930172	a new way
0.0239775928	context of
0.0239757728	a layered
0.0239672844	sequence of
0.0239188361	networks via
0.0239163130	the size and
0.0238930872	a result of
0.0238914622	long as
0.0238871807	successful in
0.0238840471	for evaluating
0.0238723204	employed in
0.0238495228	different from
0.0238415949	to follow
0.0238400060	a quadratic
0.0238382065	little or
0.0238126782	intersection of
0.0237866519	an order of
0.0237376784	discovery of
0.0237222837	result from
0.0236643349	the function of
0.0236640965	the best possible
0.0236456546	robustness of
0.0236267280	for implementing
0.0235673809	the experimental results show
0.0235605358	feasible to
0.0235416540	the availability
0.0235344023	provided with
0.0235330575	not able
0.0235083736	shape of
0.0234593135	such as speech
0.0234180566	deployment of
0.0233953968	system uses
0.0233914511	new loss
0.0233637712	a constructive
0.0233263753	loss in
0.0233132016	in comparison with
0.0233007137	for detecting
0.0232853013	different test
0.0232774579	a discussion of
0.0232725868	either by
0.0232645271	over existing
0.0232624427	less number of
0.0232570665	a high level of
0.0232424117	build on
0.0232247844	other state
0.0232146308	to include
0.0231817272	optimized using
0.0231807470	a considerable
0.0231541701	the scalability of
0.0231469061	the subset of
0.0231341701	the reliability of
0.0231124983	lifetime of
0.0231026959	runtime of
0.0231007321	value of
0.0230821774	introduced to
0.0230811663	variation of
0.0230535885	each time
0.0230531546	critical for
0.0230468473	hope to
0.0230389494	the model's
0.0230366597	to grow
0.0230263951	studies show
0.0230106974	success of
0.0230049743	schemes for
0.0230028128	presented for
0.0229962946	the quest
0.0229522771	aid in
0.0229311866	the usage of
0.0229019924	for simulating
0.0228852928	investigated in
0.0228788746	an implementation
0.0228787357	new computational
0.0228754023	two benchmark
0.0228544969	a discussion
0.0228510584	not very
0.0228039743	paradigm for
0.0227015982	this approach to
0.0226975702	as compared to
0.0226892681	the difference between
0.0226810016	the family of
0.0226562659	for measuring
0.0225737724	a multilayer
0.0225732222	a multidimensional
0.0225637079	outcome of
0.0225468473	extent to
0.0225246159	provided to
0.0224893731	the class of
0.0224830872	designed with
0.0224781954	promise in
0.0224646381	through time
0.0224539806	challenging than
0.0224352928	procedure for
0.0224090825	but only
0.0223680566	stream of
0.0223439359	to update
0.0223320076	details of
0.0223198495	for estimating
0.0223100136	archive of
0.0222986522	few training
0.0222949232	place in
0.0222311866	and robustness of
0.0222288671	the latent space of
0.0222227669	the retinal
0.0222169992	many machine
0.0222163208	by humans
0.0222011068	addition to
0.0221697659	this principle
0.0221559283	jointly with
0.0221539313	exist in
0.0221027791	explored in
0.0221018263	an objective
0.0220791704	just as
0.0220738151	an algorithm for
0.0220710171	in terms of accuracy and
0.0220579484	a dynamical system
0.0220522996	given set of
0.0220323018	the functionality of
0.0220276174	popularity of
0.0220115572	a way of
0.0220058151	the suitability
0.0219890016	to offer
0.0219240582	spread of
0.0218720202	evolved from
0.0218428123	conducted in
0.0218422609	to help
0.0218336397	difficult for
0.0217670015	to act
0.0217620690	capacity for
0.0217594190	the curse of
0.0217585589	challenge for
0.0217557198	many real
0.0217546588	a new family
0.0217255570	evaluated for
0.0216964159	this class of
0.0216574702	a precise
0.0216522398	the best
0.0216424713	the geometry of
0.0216378866	improvement on
0.0216368627	each layer of
0.0216356330	good performance on
0.0216127051	sensitivity of
0.0216026059	to formulate
0.0215840366	the value of
0.0215340564	several new
0.0215295221	the problem as
0.0214851063	magnitude of
0.0214790956	of research on
0.0214715653	coupled to
0.0214529750	new approach to
0.0214393814	combination with
0.0214008064	same task
0.0213944553	the understanding of
0.0213938048	reliability of
0.0213820529	to learn from
0.0213815162	difficulty of
0.0213789314	a type of
0.0213521229	achieved using
0.0213432305	an early
0.0213289752	observed by
0.0213141055	evidence for
0.0212863400	given by
0.0212826828	modulation of
0.0212537479	percentage of
0.0212363492	way of
0.0212325614	optimized for
0.0212264913	tested by
0.0212240254	objects from
0.0212015485	evaluated in
0.0211981829	building on
0.0211951268	by running
0.0211665079	the proposed system
0.0211512963	constrained by
0.0211362026	the default
0.0211193744	or more
0.0211085149	an improvement of
0.0210879503	scope of
0.0210792308	potential for
0.0210699863	presented with
0.0210648983	tuned for
0.0210459407	exchange of
0.0210392459	to interact
0.0210124912	course of
0.0210065121	to take
0.0209954712	a reference
0.0209743311	scalability of
0.0209739360	exists in
0.0209690123	a connection
0.0209628898	for approximating
0.0209539313	adopted in
0.0209531025	enough to
0.0209298948	the capability of
0.0209120376	the construction of
0.0209086896	an associated
0.0208756511	to converge to
0.0208634250	an edge
0.0208291753	a factor
0.0208166558	reported to
0.0207947717	an end to
0.0207884437	a list of
0.0207828830	a minimal
0.0207783140	implemented for
0.0207674406	tested with
0.0207617254	interaction with
0.0207280066	for studying
0.0207276982	the inclusion of
0.0207015485	review of
0.0207008633	metric for
0.0206776259	found in
0.0206760152	idea of
0.0206603994	reasoning with
0.0206511653	to converge
0.0206474246	made for
0.0206442841	profile of
0.0206199868	a straightforward
0.0206158604	the population of
0.0205860099	the significance of
0.0205543526	an application of
0.0205525586	to generalize to
0.0205365519	the derivation
0.0205293023	acceleration for
0.0205275139	to transform
0.0205168611	new class of
0.0205027417	first time
0.0204990573	necessary for
0.0204904087	strength of
0.0204893731	the implementation of
0.0204845248	approach towards
0.0204732227	importance of
0.0204675737	this type
0.0204647310	flexibility in
0.0204474551	basis of
0.0204203194	requirements for
0.0204186431	execution on
0.0204160742	period of
0.0204010639	a mechanism for
0.0203992765	growth in
0.0203990025	measured in
0.0203980803	the availability of
0.0203799267	expensive to
0.0203793231	to interact with
0.0203547718	utility of
0.0203501127	to observe
0.0203133716	the definition of
0.0202392593	interpretability of
0.0202167565	the inner
0.0202161179	an estimation of
0.0202160400	heuristic for
0.0202140260	the gap between
0.0201892593	employed for
0.0201586161	both in terms of
0.0201541701	the utilization of
0.0201285518	issue in
0.0201164244	several real
0.0200875584	interface for
0.0200821376	to confirm
0.0200786371	published in
0.0200724585	the full
0.0200664132	a semi
0.0200526585	emerge in
0.0200414423	a bottleneck
0.0199489918	work on
0.0199444032	to take advantage of
0.0199401671	a novel method to
0.0199390803	for choosing
0.0198965933	selected to
0.0198284725	a survey of
0.0198134569	both supervised
0.0198126632	one hand
0.0198056150	scales with
0.0197744768	a geometric
0.0197502419	evaluated with
0.0197343896	bound for
0.0196997539	risk of
0.0196918853	considered in
0.0196783056	capable to
0.0196469951	consumption by
0.0196207581	also compared with
0.0196095212	the proposed work
0.0196060495	this problem by
0.0196059476	dependence of
0.0195860099	the condition of
0.0195731441	an off
0.0195535873	discussion of
0.0195246694	analyzed in
0.0195117380	by at least
0.0194871069	an end
0.0194764206	the core of
0.0194730368	more robust to
0.0194593810	interest from
0.0194262999	the research on
0.0194261709	usage of
0.0194023180	discussed in
0.0193505353	simplicity of
0.0193238204	this method to
0.0192585589	location of
0.0192276982	a fraction of
0.0192218058	variation in
0.0192206974	issue of
0.0192148983	adapted for
0.0191892593	calculation of
0.0191467342	designed by
0.0191269906	difference in
0.0191246053	criterion for
0.0191207249	deviation of
0.0191146595	conducted to
0.0190809143	central to
0.0190627411	possibility to
0.0190583338	composition of
0.0190387904	the right
0.0189959407	volume of
0.0189736848	become more
0.0189588445	to rank
0.0189469924	relevance of
0.0189057302	sufficient for
0.0189015485	behaviour of
0.0188198495	a surprising
0.0187957249	works by
0.0187707151	probabilities for
0.0187643162	an event
0.0187463621	this paper provides
0.0187137079	validity of
0.0186789179	tolerance of
0.0186321110	a part of
0.0186210639	the history of
0.0186033408	novel framework for
0.0186027177	the issue of
0.0185646020	beneficial in
0.0185537591	classifier with
0.0185274864	utilized in
0.0185273622	more difficult to
0.0185220638	for dealing with
0.0185184920	to focus on
0.0185162038	the discovery of
0.0185025516	formation of
0.0184908563	a framework to
0.0184818968	flexibility of
0.0184615127	in expected time
0.0184566171	assumption of
0.0184312421	straightforward to
0.0184292647	new approach for
0.0183907617	for determining
0.0183707685	a full
0.0183539179	literature on
0.0183097018	the capabilities of
0.0182898523	contrast with
0.0182874159	a smooth
0.0182776982	the potential for
0.0182735161	to perform well
0.0182530252	studied for
0.0182347393	issue by
0.0181737969	a technique for
0.0181731833	not all
0.0181603994	addressed in
0.0181541701	the contribution of
0.0181159015	the composition of
0.0181154078	generality of
0.0180970412	deployed to
0.0180815650	accelerators for
0.0180583338	basis for
0.0180269734	the strengths of
0.0179976522	chosen to
0.0179809920	a focus on
0.0179791337	investigated for
0.0179676259	a way
0.0179390252	little to
0.0178953038	solved in
0.0178934243	accelerator for
0.0178613340	a number of different
0.0178583338	requirement of
0.0178527966	to allow
0.0178278260	extent of
0.0178154792	parallelization of
0.0177895891	benefit of
0.0177587044	a remarkable
0.0177564472	a representative
0.0177542941	a balance between
0.0176760284	the last two
0.0175827953	the deployment of
0.0175746053	exploited in
0.0175547718	measurement of
0.0174870924	performed with
0.0174530983	an alternative to
0.0174210639	the basis for
0.0174010639	the similarity between
0.0173707406	a system for
0.0173208004	criteria for
0.0172549745	take into
0.0172271071	enhanced with
0.0172254274	an expected
0.0171661686	beneficial to
0.0171497752	correlated to
0.0171365899	directions for
0.0171344892	and efficiency of
0.0171300203	an area of
0.0171282565	computed in
0.0171232837	progress on
0.0170795143	optimized with
0.0170078076	topic in
0.0170040582	tuned to
0.0169871428	a comparison of
0.0169752950	instead of using
0.0169734576	the key to
0.0169693920	an approximation
0.0169566011	on two different
0.0169491255	on three different
0.0169143514	necessary to
0.0169029037	the case for
0.0168832068	the popularity of
0.0168754941	a kind of
0.0168598664	to correct
0.0168446099	exploited to
0.0168313721	the sense of
0.0168313721	the introduction of
0.0168286946	width of
0.0167886294	utilized to
0.0167861433	of interest in
0.0167841121	and then use
0.0167817791	a whole
0.0167339037	the balance between
0.0167279432	score for
0.0167075270	for problems with
0.0166992349	the adoption of
0.0165803470	learned with
0.0165746053	theorem for
0.0165547718	helps to
0.0165443327	available data
0.0165274864	candidate for
0.0164744620	a fast and
0.0164716706	to allow for
0.0164198747	but also for
0.0163745744	novel method for
0.0163618239	with three different
0.0163319838	need for
0.0163208004	library for
0.0162859184	this issue by
0.0162378856	appear in
0.0161557443	of up to
0.0161133716	the principle of
0.0161116474	condition of
0.0161055244	allows to
0.0160400294	the interaction between
0.0160335713	not only for
0.0160148436	the above
0.0160056451	this paper uses
0.0159982782	used in many
0.0159734638	new method for
0.0159423055	way to
0.0159163306	adopted to
0.0158946015	a consequence of
0.0158571783	with millions of
0.0158332955	so as
0.0157931024	a challenge for
0.0157844320	a new approach for
0.0157337587	the simulation of
0.0156772533	a tool for
0.0156633716	a problem of
0.0156556070	to serve as
0.0156088252	many different
0.0155835701	the course
0.0155274864	functionality of
0.0154823359	controller for
0.0152651424	also known
0.0152323359	selected for
0.0151799501	a comparison between
0.0151608176	then used to
0.0151481770	an example of
0.0151178833	a solution to
0.0151148983	attempts to
0.0151105731	other methods in
0.0151081800	or better
0.0150866317	regards to
0.0150207853	often used to
0.0150159369	described in
0.0149722818	to give
0.0149044256	especially for
0.0148978120	sufficient to
0.0148341291	desirable to
0.0147683512	a need for
0.0147190328	not able to
0.0145685198	the integration of
0.0144699385	a methodology for
0.0143301774	allow for
0.0143232594	available from
0.0142976924	a large amount
0.0142381651	a step towards
0.0142199232	validated in
0.0141124912	to move
0.0140842470	a variation of
0.0140680567	the nervous system
0.0138639061	to become
0.0138458656	put in
0.0136862075	to describe
0.0135517609	the correlation between
0.0134232837	difficulty in
0.0133793823	to zero
0.0131503025	new way
0.0127254274	an increase
0.0126826035	done in
0.0126190328	a modification of
0.0125889328	better at
0.0122106566	help to
0.0120419281	also able
0.0119489588	available for
0.0119447948	done with
0.0118887337	enough for
0.0112223168	useful in
0.0111888648	to look
0.0110856156	made in
0.0110366703	to do
0.0100505171	to consider
0.0098889341	made to
0.0096205925	done to
0.0086007980	seen in
