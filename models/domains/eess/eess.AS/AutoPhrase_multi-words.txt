0.9678856944	artificial intelligence
0.9673946549	hearing aid
0.9654248009	hearing aids
0.9649522348	impulse response
0.9617466968	parkinson's disease
0.9616047902	question answering
0.9585631425	lip reading
0.9583339734	cover song
0.9569955853	alzheimer's disease
0.9556177337	cocktail party
0.9533435220	deep learning
0.9526306294	gradient descent
0.9525997295	neural networks
0.9515476848	receptive field
0.9514906985	beam search
0.9507760130	vocal tract
0.9501078781	false alarm
0.9476893213	wall street journal
0.9467626430	support vector machines
0.9462374009	sentiment analysis
0.9447874688	fourier transform
0.9443548873	hidden markov model
0.9429773777	mental health
0.9420270832	random forest
0.9414325855	hidden markov models
0.9413579313	predictive coding
0.9404936335	maximum likelihood
0.9386901124	nist sre
0.9378830210	latent variable
0.9378520025	reinforcement learning
0.9377649900	fundamental frequency
0.9372142554	monte carlo
0.9366642098	correlation coefficient
0.9362159709	kalman filter
0.9357458305	distortionless response
0.9350960757	weakly labelled
0.9349776510	machine translation
0.9347316962	support vector machine
0.9335308284	weakly labeled
0.9324195749	packet loss
0.9313469188	microphone arrays
0.9311788031	impulse responses
0.9304047264	mandarin chinese
0.9298688468	anomaly detection
0.9298451494	neural network
0.9286304636	logistic regression
0.9285657515	mutual information
0.9278553992	loss function
0.9277705880	ultrasound tongue
0.9269429728	feature extraction
0.9269295789	complex valued
0.9267762723	dimensionality reduction
0.9267359022	covariance matrix
0.9264733825	lombard effect
0.9263898664	speech recognition
0.9258501216	wavelet transform
0.9253225030	kalman filtering
0.9247910844	automatic speech recognition
0.9244994841	visually grounded
0.9241180597	high fidelity
0.9240589708	knowledge distillation
0.9238174084	speech synthesis
0.9232718775	vector quantization
0.9213452079	musical instrument
0.9197858265	code switching
0.9195018191	sheet music
0.9191900943	polyphonic music
0.9189430023	doa estimation
0.9188987724	black box
0.9187785866	machine learning
0.9183863289	logical access
0.9176044564	latent spaces
0.9175909178	ad hoc
0.9173230090	gated recurrent unit
0.9168351546	alzheimer's dementia
0.9167334938	filter bank
0.9163038901	weakly supervised
0.9162392298	nonnegative matrix factorization
0.9155473217	steady state
0.9148082231	voice cloning
0.9147761130	speaker recognition
0.9145950330	variational autoencoders
0.9142427945	room impulse response
0.9135172573	inductive bias
0.9133317566	open sourced
0.9130701771	recurrent neural network
0.9118760819	small footprint
0.9114997316	natural language processing
0.9114044136	adversarial examples
0.9110482637	principal component analysis
0.9100000176	deep neural networks
0.9096486589	transfer function
0.9084638916	receptive fields
0.9083107273	spoofing countermeasures
0.9075184144	linear prediction
0.9071926335	speech enhancement
0.9071756033	cross entropy
0.9070576712	cross correlation
0.9070171168	wiener filter
0.9064957747	open source
0.9061646157	signal processing
0.9054621262	multitask learning
0.9053818112	variational autoencoder
0.9052013416	glottal closure instants
0.9050926775	fearless steps
0.9044229091	speaker verification
0.9040016674	equal error rate
0.9036244121	unit discovery
0.9032710329	connectionist temporal
0.9026520166	dilated convolutions
0.9023650227	pattern recognition
0.9019984501	hyper parameter
0.9014080730	feed forward
0.9013244161	chord progression
0.9012326677	slot filling
0.9007464594	emotion recognition
0.9001960614	bit rate
0.8998860384	noise pollution
0.8996156358	keyword spotting
0.8994609414	social media
0.8994263712	recurrent neural networks
0.8994183199	tonal tension
0.8992773249	auto encoders
0.8990393328	gaussian process
0.8989244191	dinner party
0.8988990158	spectral subtraction
0.8986222144	linear discriminant analysis
0.8982093629	nearest neighbor
0.8978422783	latent space
0.8978094780	microphone array
0.8973514743	parallel wavegan
0.8972469121	classical music
0.8972255750	voice conversion
0.8971967392	character error rate
0.8971253491	shallow fusion
0.8967842376	adversarial attacks
0.8966693833	knowledge transfer
0.8965872904	replay attack
0.8958280423	room impulse responses
0.8955886710	upper bound
0.8951728645	raw waveform
0.8946949057	blind source separation
0.8945816986	permutation invariant training
0.8940429077	symbolic music
0.8940228145	probabilistic linear discriminant analysis
0.8930554926	transfer learning
0.8928789416	connectionist temporal classification
0.8928371597	depthwise separable
0.8922703231	hand crafted
0.8920832598	wake word
0.8917332828	physical access
0.8909830412	indian languages
0.8906276850	vector quantized
0.8904614225	deep attractor
0.8897552000	convolutional neural networks
0.8896861334	wavenet vocoder
0.8894032114	supervised learning
0.8891853276	bi directional
0.8889597736	acoustic scenes
0.8888775752	bandwidth extension
0.8884690315	active learning
0.8884476564	music genre
0.8871767988	subword modeling
0.8871213726	urban sound tagging
0.8868690547	subword units
0.8866717583	compares favorably
0.8865089610	hearing impaired
0.8864490099	percentage points
0.8860140572	short time fourier transform
0.8860115170	respiratory diseases
0.8859366678	binary mask
0.8857331565	von mises
0.8854567129	embedded devices
0.8845068807	data augmentation
0.8844948559	convolutional neural network
0.8844655384	sound events
0.8841589547	white box
0.8841251787	artificial neural networks
0.8840130289	gaussian distribution
0.8838662889	pop music
0.8834919402	griffin lim
0.8831169673	dialect identification
0.8830665711	dilated convolution
0.8828512144	noise reduction
0.8824201441	listening tests
0.8818173293	drum transcription
0.8817515632	computationally efficient
0.8812704559	optimal transport
0.8801584168	cycle consistency
0.8801266547	late fusion
0.8796114271	matrix factorization
0.8795367654	spoken term
0.8793141431	vocal fold
0.8789200743	contrastive learning
0.8785372461	auto encoder
0.8782826321	source separation
0.8782002685	speaking style
0.8781747816	named entities
0.8780846600	anti spoofing
0.8779001816	code switched
0.8774654504	triplet loss
0.8773977748	style transfer
0.8773552276	word error rates
0.8771026640	mel frequency cepstral coefficients
0.8770693629	information retrieval
0.8769688766	real valued
0.8768807176	presentation attack detection
0.8768666652	frequency domain
0.8764993835	fine tune
0.8759037227	spoken language understanding
0.8755256439	low latency
0.8750611965	closed set
0.8750118573	generative adversarial
0.8748627193	false trigger
0.8748618054	cycle consistent
0.8747445950	rnn transducer
0.8743340180	deep neural network
0.8741081958	unsupervised learning
0.8739486822	angular margin
0.8736178836	error rate
0.8732613284	generative adversarial networks
0.8730678564	raw waveforms
0.8729438400	mel spectrograms
0.8728563640	residual connections
0.8724750024	sound event detection
0.8719573010	prosody transfer
0.8715737548	ground truth
0.8715629377	facial animation
0.8711449128	gaussian mixture
0.8699385636	attention mechanism
0.8696947421	mel frequency cepstral
0.8696371643	replay attacks
0.8694315192	squeeze excitation
0.8693526634	computational complexity
0.8693479654	adversarial perturbations
0.8688248742	feature selection
0.8685980139	post processing
0.8681959633	high quality
0.8681041498	music theory
0.8679174199	image processing
0.8672826850	ablation studies
0.8671715580	tcd timit
0.8671226833	error rates
0.8670100389	privacy preserving
0.8668372943	variable length
0.8665093248	student teacher
0.8661760563	cosine distance
0.8653321788	f1 score
0.8651397752	close talk
0.8649243022	mel spectrogram
0.8646538699	human perception
0.8646023150	bottleneck features
0.8633052768	large vocabulary
0.8630786587	wind noise
0.8630307987	narrow band
0.8629044970	speech coding
0.8626941241	expectation maximization
0.8623760486	music composition
0.8622331245	open set
0.8622313496	music information retrieval
0.8621055050	singing voice separation
0.8619476371	design choices
0.8619384776	latent variables
0.8615682379	conv tasnet
0.8614508314	closed form
0.8614454775	music tagging
0.8614330085	speaker diarisation
0.8611972146	midi files
0.8611929736	heart sound
0.8611384752	glottal flow
0.8611103683	wall street
0.8608383393	sound source localization
0.8605200142	lexical stress
0.8604909416	fine tuned
0.8601263982	discriminant analysis
0.8600716595	mel frequency
0.8598575309	adversarial attack
0.8598364628	event detection
0.8598205576	short term memory
0.8585579770	valence arousal
0.8584202945	frame rate
0.8582459462	log mel
0.8580988860	quasi periodic
0.8573435516	cepstral coefficients
0.8571408820	linear regression
0.8569577620	short term
0.8568281390	reverberant environments
0.8565831010	word error rate
0.8564363190	anomalous sound
0.8555317268	long short term memory
0.8552440159	group delay
0.8551833297	language identification
0.8549722402	auto regressive
0.8549677678	binary classification
0.8547470916	fine grained
0.8543240773	feature extractors
0.8542504530	metric learning
0.8541415939	recurrent neural
0.8539904117	multi talker
0.8539466705	activity detection
0.8526038942	principal component
0.8521594839	cross lingual
0.8521090222	teacher student
0.8520994220	spectral envelope
0.8520180892	speaker diarization
0.8514468935	meta learning
0.8512266903	mini batch
0.8510149802	spoken language
0.8507874012	average pooling
0.8502604973	natural sounding
0.8502343655	quality assessment
0.8501164048	sound event
0.8499995636	bidirectional long short term memory
0.8493688244	spectral clustering
0.8493093761	low resource
0.8492181381	general purpose
0.8490092467	disentangled representations
0.8489707748	minimum variance
0.8489590505	relevance weighting
0.8486766680	popular music
0.8481393998	memory footprint
0.8478918576	giantmidi piano
0.8477889087	denoising autoencoder
0.8476378211	reconstruction loss
0.8475153757	wsj0 2mix
0.8469661694	recurrent neural network transducer
0.8466730275	noise suppression
0.8461965562	spoofing detection
0.8457991067	attention mechanisms
0.8457417376	singing voice
0.8454461013	lf mmi
0.8450601873	single channel
0.8447827988	speaker clustering
0.8445510469	spatio temporal
0.8443141356	mobile devices
0.8428976207	instance normalization
0.8426386077	transformer transducer
0.8426288841	monaural speech enhancement
0.8425436424	average precision
0.8422478590	speech separation
0.8422074436	temporal resolution
0.8420695170	long term
0.8418319485	frequency bands
0.8414368989	attention weights
0.8413653688	batch normalization
0.8413096708	voice trigger detection
0.8411338981	speaker identification
0.8402991652	lip movements
0.8398510328	youtube videos
0.8393565386	vq vae
0.8392986612	semi supervised
0.8392945277	replay spoofing
0.8391021580	modeling units
0.8390520052	long range
0.8390490441	fine tuning
0.8389349313	locata challenge
0.8388431845	perceptual quality
0.8385293963	pattern discovery
0.8381985630	data set
0.8380083798	voice activity detection
0.8378761994	mask estimation
0.8377375684	inverse filtering
0.8376140172	spectro temporal
0.8375939452	kullback leibler
0.8372536520	scene analysis
0.8368245746	domain adaptation
0.8367919904	multi view
0.8364682975	dilated convolutional
0.8361695148	dihard ii
0.8361304944	meeting transcription
0.8361227228	siamese networks
0.8360891379	max pooling
0.8359710398	main contribution
0.8355268363	mispronunciation detection
0.8355019653	cosine similarity
0.8352518331	neural architecture search
0.8350210175	scene classification
0.8346889897	encoding layer
0.8345337430	glottal source
0.8337465846	visual cues
0.8333140820	spoken word
0.8331588701	punctuation prediction
0.8331461829	neural vocoders
0.8328135219	cepstral coefficient
0.8327819230	natural language
0.8326424133	weighted prediction error
0.8326229489	voice leading
0.8325593523	perceived quality
0.8324007402	deep clustering
0.8322621594	variance distortionless
0.8320754907	source localization
0.8320631984	acoustic scene classification
0.8311859535	srp phat
0.8308248609	glottal closure
0.8307744902	voice command
0.8304109227	texture synthesis
0.8300187624	network architecture
0.8297054498	representation learning
0.8295519375	test set
0.8290786988	frame level
0.8288458108	maximum mutual information
0.8288447685	large margin
0.8283126372	pitch dependent dilated convolution
0.8282336485	source position
0.8276336629	bi modal
0.8274640723	unsupervised domain adaptation
0.8271271213	music generation
0.8270593761	equal error
0.8268783353	transfer functions
0.8263876509	environmental sound
0.8258539822	low resourced
0.8256178839	event classification
0.8249462664	environmental sounds
0.8243188041	length normalization
0.8242803216	ami meeting
0.8239755306	user experience
0.8239336736	transformer xl
0.8236153771	super resolution
0.8235846879	edge devices
0.8231333188	labelled data
0.8231179103	singing voice synthesis
0.8230939107	fully convolutional
0.8227979472	normal hearing
0.8227838033	statistical parametric speech synthesis
0.8226588504	spectral mapping
0.8226094724	small footprint keyword spotting
0.8222476537	million song
0.8220640940	machine listening
0.8217035425	dual path
0.8211499352	feature map
0.8207697385	contextual information
0.8206766416	fully connected
0.8206381842	signal to noise ratio
0.8205389496	feature sets
0.8191998276	cycle consistent generative adversarial
0.8189418546	environmental sound classification
0.8188416584	large scale
0.8183281449	singing synthesis
0.8183182396	instantaneous frequency
0.8180700545	music genres
0.8177362804	si sdr
0.8177077884	spatial covariance
0.8175779215	sensor networks
0.8174137546	speaker embeddings
0.8173857212	domain mismatch
0.8170195016	training criteria
0.8169793636	late reverberation
0.8168533324	intent detection
0.8157092932	crowd sourced
0.8155918631	vaw gan
0.8154544422	acoustic events
0.8153164409	cross modal
0.8152547947	skip connections
0.8151205717	cocktail party problem
0.8150548657	background noise
0.8148596023	gated recurrent
0.8141301809	low rank
0.8140605753	convolutional beamformer
0.8137870554	feature vectors
0.8137076780	convolutional recurrent neural network
0.8135927705	e2e asr
0.8135304114	chord recognition
0.8133873787	softmax loss
0.8131349715	language pairs
0.8131114463	singular value decomposition
0.8130973438	adversarial training
0.8129933890	closely related
0.8128925841	convolutional neural
0.8128153932	generative adversarial network
0.8127706083	neural vocoder
0.8127276676	source code
0.8120018720	melody generation
0.8118443351	speech denoising
0.8118258968	multi resolution
0.8115930255	power consumption
0.8110353560	test sets
0.8108476305	noisy labels
0.8107037468	street journal
0.8105899465	waveform generation
0.8105450818	amr wb
0.8104891370	objective measure
0.8104201489	gaussian mixture model
0.8102002947	acoustic echo cancellation
0.8099157662	recurrent unit
0.8092615355	distant speech recognition
0.8091149030	independent vector analysis
0.8089036507	multi modal
0.8086356557	pitch controllability
0.8083420357	empirical study
0.8079395622	power spectral density
0.8074023079	overlap add
0.8072026641	convolutional networks
0.8067946142	speaker extraction
0.8067645439	multiple instance learning
0.8060152774	onset detection
0.8059499264	si snr
0.8054541811	minimum word error rate
0.8054132019	real life
0.8052471561	encoder decoder
0.8051353631	pitch synchronous
0.8050457800	white noise
0.8047185588	streaming asr
0.8046458834	voice search
0.8039749815	reconstruction error
0.8039482570	sound classes
0.8038245616	speaking styles
0.8038160171	context aware
0.8037805614	generative models
0.8036979272	low dimensional
0.8030563964	forced alignment
0.8025242643	complex cepstrum
0.8024497091	cycle gan
0.8021544322	variational auto encoder
0.8017942630	future context
0.8014881153	cough detection
0.8013269482	sound localization
0.8010771273	automatic speaker verification
0.8009502068	jointly trained
0.8006466818	target language
0.8002168009	real world
0.7999729408	root mean square
0.7997646386	diffuse noise
0.7997456847	future research
0.7996394838	speaker embedding
0.7994755686	acoustic modeling
0.7994154947	information bottleneck
0.7992845178	previous works
0.7989728878	scene aware
0.7988587465	voice assistants
0.7978292674	low snr
0.7974233675	phase reconstruction
0.7965498007	long sequences
0.7965191330	subjective evaluation
0.7961510506	dysarthric speech
0.7961065650	aligned lyrics
0.7959023831	margin softmax
0.7958621374	dnn hmm
0.7955355474	lattice free
0.7954073750	linguistic features
0.7953870478	high resolution
0.7951214391	overlapped speech
0.7950371523	piano transcription
0.7950001213	speaker adaptation
0.7949911931	sound synthesis
0.7948043857	deep generative
0.7947883459	recording conditions
0.7946728179	drum sounds
0.7944548809	adversarial learning
0.7944283875	hybrid ctc attention
0.7939417971	generative modeling
0.7939398269	spoofing attacks
0.7936795136	phase aware
0.7936753586	generative model
0.7934385666	universal sound separation
0.7933951693	machine speech chain
0.7932396607	low rank matrix analysis
0.7927866641	noise type
0.7927494672	diarization error rate
0.7924137551	generalization ability
0.7921691981	singing voice conversion
0.7921371459	speaker's identity
0.7919749030	cost function
0.7915409582	post filter
0.7914821651	musical score
0.7914412364	sound propagation
0.7914219811	high level
0.7911470842	contrastive loss
0.7910619765	synthesized speech
0.7908597782	synthetic data
0.7907944602	speech processing
0.7907050319	voxceleb speaker recognition challenge
0.7904634140	noisy environments
0.7904161502	acoustic scene
0.7903284898	speech commands
0.7902328576	data scarcity
0.7898034403	square error
0.7897154665	imagined speech
0.7897128486	autoregressive transformer
0.7896301821	pitch contour
0.7895399080	text independent speaker verification
0.7894968776	adversarial networks
0.7887989562	short utterance
0.7887429502	hidden markov
0.7886214911	sentence level
0.7883715891	playing techniques
0.7880788226	clean speech
0.7879939975	support vector
0.7879008611	context dependent
0.7876922345	piano music
0.7876525939	dynamic time warping
0.7875828218	mel filterbank
0.7873675759	objective functions
0.7870551098	itu t
0.7870266785	polyphonic sound event detection
0.7868860217	sound sources
0.7863273414	monaural speech separation
0.7858253681	bidirectional lstm
0.7856709402	sound source
0.7852071416	style tokens
0.7851400709	latent representation
0.7849074986	speaker attributed
0.7845054617	computationally expensive
0.7844246052	multi scale
0.7842549807	long form
0.7842350761	parameter sharing
0.7836602035	scattering transform
0.7835979940	environmental noise
0.7833368760	joint optimization
0.7826496358	emotional content
0.7823130091	performance improvement
0.7819733257	domain invariant
0.7814178581	spoken words
0.7811930378	cloud based
0.7799670612	speech translation
0.7792033541	multi genre
0.7790535873	feature extractor
0.7787309119	speaker counting
0.7775502684	pre trained
0.7774843024	close talking
0.7773120032	native language
0.7771392930	domain specific
0.7766375821	target domain
0.7765438527	spectral features
0.7763127589	multi channel
0.7761641998	spectral density
0.7755504247	distributed microphones
0.7755493204	pitch tracking
0.7754041634	bi lstm
0.7749555318	short time objective intelligibility
0.7747613166	log likelihood
0.7745957587	recurrent units
0.7743774996	objective intelligibility
0.7742962938	low complexity
0.7741101804	noise robust
0.7740148820	sequence modeling
0.7738970039	f0 contour
0.7736173944	echo cancellation
0.7735273531	sound field
0.7726268746	fully supervised
0.7726238368	detection cost function
0.7723885015	loss functions
0.7723770318	depression detection
0.7722748646	spontaneous speech
0.7721573316	hyper parameters
0.7721253546	cross corpus
0.7719553434	wave u net
0.7716776428	active speaker detection
0.7714066304	embedding spaces
0.7712072395	noise robustness
0.7707795653	synthetic speech
0.7703185165	multi head
0.7701582252	feature representation
0.7701161160	short duration
0.7700905264	soft labels
0.7699226789	emotional voice conversion
0.7694970284	neural machine translation
0.7693429919	minimum phase
0.7692474259	music genre classification
0.7687530656	higher order
0.7686916332	direct path
0.7681604545	accented speech
0.7679851490	acoustic environments
0.7679545935	deep noise suppression
0.7676741620	multi band
0.7675960956	signal to distortion ratio
0.7674562734	evaluation metrics
0.7669534445	word embeddings
0.7666722908	latent representations
0.7664889991	mask based
0.7662691565	target speaker
0.7659815306	ctc attention
0.7658480676	reverb challenge
0.7656787523	label noise
0.7647970008	multi head attention
0.7647802477	labeled data
0.7646158604	modeling ability
0.7641861906	frequency bin
0.7639230948	multi stream
0.7635391888	visual features
0.7634744295	experimental results
0.7633782498	data sets
0.7628160061	` `
0.7626621468	speech recognizers
0.7625492738	opinion score
0.7615812980	comparative study
0.7615740483	cross validation
0.7613791815	pre training
0.7613202446	single microphone
0.7612567893	visual modality
0.7606933392	speech intelligibility
0.7604326094	gaussian mixture models
0.7601571477	convolution neural network
0.7600455600	phonetic structures
0.7600335503	computational resources
0.7597072265	speech command
0.7592762484	nonnegative matrix
0.7592580007	untranscribed speech
0.7588633050	voice quality
0.7584686560	multiple speakers
0.7584594319	text dependent speaker verification
0.7583410344	subjective tests
0.7573784983	children's speech
0.7566826645	training strategy
0.7560450377	music transcription
0.7557641377	interfering sources
0.7555622934	decision making
0.7555122628	language modeling
0.7550208985	feature maps
0.7547084231	cycle consistent adversarial
0.7544320281	cross domain
0.7543660738	intent classification
0.7542467351	melody extraction
0.7530489790	teacher student learning
0.7528706486	cough sounds
0.7528341038	librispeech test clean
0.7528212050	convolutional recurrent neural networks
0.7526007421	room impulse
0.7525421109	singing voice detection
0.7524587943	cnn blstm
0.7521889113	frequency band
0.7520156494	speaker dependent
0.7519423534	pre processing
0.7517825867	weak labels
0.7514973121	target speakers
0.7514488863	pitch contours
0.7512483955	spatial filtering
0.7510931053	speech production
0.7510536160	acoustic event detection
0.7510050786	utterance level
0.7509430966	speaker identity
0.7508073641	long term dependencies
0.7500177079	language model
0.7489691796	visual information
0.7488844735	training data
0.7485553407	speaker identities
0.7482791668	early detection
0.7478804873	acoustic cues
0.7478567721	multi lingual
0.7478348357	eeg signals
0.7477420330	unseen speakers
0.7476307436	feature space
0.7471529661	continuous speech recognition
0.7467221857	takes advantage
0.7465351247	mask ctc
0.7463812315	subjective evaluations
0.7463049618	residual noise
0.7459978516	external lm
0.7452330567	speaker independent
0.7447257823	fixed length
0.7436414987	phoneme level
0.7423306556	objective evaluation
0.7422433856	magnitude spectrogram
0.7417343343	permutation problem
0.7416193952	error reduction
0.7415812504	f1 scores
0.7413830525	secondary task
0.7413017723	multi speaker
0.7412944809	masked conditional
0.7412407850	auxiliary tasks
0.7405198826	recurrent networks
0.7398967293	primary task
0.7393364090	segment level
0.7393323757	magnitude spectrum
0.7390017526	probabilistic linear discriminant
0.7389526074	non negative matrix factorization
0.7387276741	speaker similarity
0.7387089087	low power
0.7379575616	clip level
0.7378453756	deep learning based
0.7374982493	fixed dimensional
0.7368591143	acoustic conditions
0.7366617773	griffin lim algorithm
0.7365710007	ablation study
0.7361511015	low resource languages
0.7360031834	downstream tasks
0.7356306201	human voice
0.7354228972	voice controlled
0.7353540575	singer identification
0.7352369462	attention heads
0.7351573154	residual network
0.7346411681	neural tts
0.7344809673	vector space
0.7340152475	ctc loss
0.7339347897	pseudo labels
0.7339023945	dnn based
0.7337188772	temporal structure
0.7334642080	acoustic models
0.7332405956	previously proposed
0.7316856034	permutation invariant
0.7314817934	mfcc features
0.7313955614	librispeech corpus
0.7313455801	multichannel speech enhancement
0.7308701507	building blocks
0.7304282456	short segments
0.7302295658	results suggest
0.7296990521	mel spectrum
0.7295951430	class label
0.7295702427	phase estimation
0.7295356153	significant progress
0.7294730337	hmm based
0.7294697947	multilingual bottleneck
0.7294471198	embedding space
0.7282484691	research area
0.7282207771	hybrid dnn hmm
0.7278421933	acoustic features
0.7273256580	auxiliary task
0.7271676182	speech signals
0.7271110549	mixture model
0.7268051650	valence and arousal
0.7265870803	prosodic features
0.7265328056	training procedure
0.7264517349	competitive results
0.7260523543	score level fusion
0.7259444030	vocal melody
0.7257997937	listening test
0.7255204928	speech quality
0.7254986892	language models
0.7251423053	low bit
0.7248249343	competitive performance
0.7247870372	deep reinforcement learning
0.7244976116	normal speech
0.7240070936	high degree
0.7238915129	training set
0.7234445096	inference speed
0.7232460285	multi task learning
0.7230921913	pooling layer
0.7229170969	word embedding
0.7228832531	speech perception
0.7227205015	speech emotion recognition
0.7226851540	comparable performance
0.7225691224	human robot
0.7222477007	unlabeled data
0.7219359300	multiple instance
0.7217602631	native english
0.7213919342	teacher model
0.7210442704	recent advances
0.7207218165	transformer based
0.7204938532	mean squared error
0.7200027308	phonetic information
0.7196200335	phone recognition
0.7185383337	success rate
0.7181784321	phoneme recognition
0.7172140981	acoustic event
0.7171925868	human listeners
0.7169568069	low quality
0.7169490795	previous approaches
0.7168886549	attention based
0.7163729007	eeg features
0.7158013533	iemocap dataset
0.7156196345	frequency resolution
0.7153515408	mandarin english
0.7152693205	significantly outperforms
0.7152625117	multi frame
0.7151022631	validation set
0.7144868042	classification accuracy
0.7141789367	additive noise
0.7138191966	relative wer reduction
0.7134826047	acoustic model
0.7134626530	emotion classification
0.7131425122	n gram
0.7130309575	generalization capabilities
0.7129502460	human machine
0.7128474312	multi instrument
0.7127967864	noise types
0.7125311882	environmental sound synthesis
0.7124987778	extensive experiments
0.7122922819	manually labeled
0.7122894948	speaker embedding extraction
0.7121602817	singing voices
0.7120133482	model agnostic
0.7116256355	temporal convolutional networks
0.7110397181	generated music
0.7110267901	frequency cepstral coefficients
0.7103340952	mean opinion score
0.7100571280	emotional speech
0.7100040092	hidden layers
0.7096196942	pre defined
0.7095278074	fully convolutional network
0.7091370791	fewer parameters
0.7087861796	gaussian noise
0.7085664499	input features
0.7085385887	multi stage
0.7082193638	flow based
0.7077808951	music source separation
0.7071737829	scale invariant
0.7069494773	convolution layers
0.7058721995	carefully designed
0.7057299315	voice activity
0.7055486382	related tasks
0.7055317092	computational cost
0.7054618090	acoustic modelling
0.7053283182	gan based
0.7044627467	speech recognizer
0.7042523054	resource constrained
0.7041965172	adaptation techniques
0.7034283473	absolute improvement
0.7034003570	feature engineering
0.7033754852	constant q transform
0.7032080219	machine lipreading
0.7028054376	speaker variability
0.7026889090	performance degradation
0.7026725379	base model
0.7025110022	deep speaker embedding
0.7022551876	input sequence
0.7021949837	recurrent layers
0.7016392452	feature mapping
0.7016207911	esc 50
0.7012965611	objective measures
0.7007071401	data collection
0.7006239904	acoustic word embeddings
0.7003108810	squared error
0.7002922501	converted speech
0.7001091215	strong labels
0.6999026412	semi supervised learning
0.6995755846	source filter
0.6994509409	emotion labels
0.6990509066	convolutional layers
0.6982081549	multi task
0.6980431518	video frames
0.6979654948	speaker's voice
0.6976889342	low level
0.6974422899	frame wise
0.6970353812	speech dereverberation
0.6967242858	multi accent
0.6966673274	recent years
0.6961593410	frequency bins
0.6950380971	inter channel
0.6948809027	embedding vectors
0.6948523965	hidden layer
0.6948341324	phoneme sequence
0.6944271521	high accuracy
0.6936702698	emotion detection
0.6935590429	multi layer
0.6930136551	joint training
0.6920178784	recently proposed
0.6919691701	text independent
0.6919473699	conversational context
0.6918819973	consistently outperforms
0.6910984390	challenging task
0.6906385161	times faster
0.6903955148	data driven
0.6902090478	deep generative models
0.6901462934	manually annotated
0.6899977232	objective function
0.6889965642	wer reduction
0.6889239540	short utterances
0.6888466628	multi target
0.6886748881	deep neural
0.6880182340	existing works
0.6877038507	automatic speech
0.6874871172	promising results
0.6874270465	multilingual asr
0.6873755493	significantly reduces
0.6873494158	existing methods
0.6869783667	remains challenging
0.6868831771	voice separation
0.6868544769	multi label
0.6867687895	signal to noise ratios
0.6863851061	language understanding
0.6851710597	output layer
0.6851602025	phase spectrum
0.6845766577	direction of arrival
0.6843354109	convolutional recurrent
0.6842877706	genre classification
0.6842529579	asr systems
0.6841688029	speech activity detection
0.6838998200	instrument recognition
0.6836052509	multi track
0.6835739166	speech segments
0.6835498771	perceptual loss
0.6832876196	cross entropy loss
0.6826134944	coded speech
0.6823287145	block online
0.6813041688	attention pooling
0.6803476244	reference signal
0.6803371220	classification tasks
0.6798450308	training examples
0.6796334240	paired data
0.6796240625	machine learning techniques
0.6795682441	acoustic echo
0.6788606959	interspeech 2020
0.6788169672	fully connected layers
0.6782593858	classification task
0.6777504346	significantly improve
0.6772463510	human computer interaction
0.6771319339	multi microphone
0.6763937759	low cost
0.6761559780	residual signal
0.6753414556	sampling rate
0.6749243111	conversational speech
0.6746436686	domain adversarial training
0.6742485016	noisy speech
0.6741446751	experimental evaluations
0.6738537697	english language
0.6738115310	results demonstrate
0.6730684183	musical instruments
0.6727060417	temporal convolutional
0.6725647912	encoder decoder architecture
0.6723397586	high dimensional
0.6719579150	speaker characteristics
0.6717637869	relative wer
0.6717304832	speaker adaptive
0.6716415184	parallel data
0.6712237878	low frequency
0.6711524797	residual networks
0.6708044984	user defined
0.6707954193	polyphonic sound event
0.6704594408	recently introduced
0.6703826246	superior performance
0.6695103378	optimization problem
0.6689776743	higher accuracy
0.6676855431	weakly labeled data
0.6675982259	relative improvement
0.6674503545	power spectral
0.6673974339	benchmark datasets
0.6671309029	voiced sounds
0.6669185261	recognition accuracy
0.6662541469	universal adversarial
0.6657301006	recent works
0.6656219246	instrument classification
0.6655223006	results confirm
0.6653977444	word level
0.6646878982	deep embedding
0.6641886988	target source
0.6637584814	sequence generation
0.6637529027	attention based encoder decoder
0.6636038975	real world scenarios
0.6633535298	mandarin speech
0.6631310589	evaluation set
0.6626944387	research topic
0.6613460900	statistical parametric speech
0.6610468184	experimental evaluation
0.6610103979	cross modal retrieval
0.6609263997	student model
0.6604050566	musical structure
0.6603465476	real world applications
0.6600989883	large scale datasets
0.6599659040	multi source
0.6599169726	masked conditional neural
0.6593053193	neural network architectures
0.6588260469	noisy conditions
0.6588187985	significantly improves
0.6586752359	linguistic content
0.6571838446	temporal attention
0.6571462771	significantly improved
0.6566940849	text to speech synthesis
0.6564593523	temporal dependencies
0.6564022920	widely studied
0.6563264533	covid 19
0.6563104532	speaker verification systems
0.6557497783	overlapping speech
0.6542726212	parametric speech synthesis
0.6542193921	results showed
0.6539007602	proof of concept
0.6538685240	internet of things
0.6522329262	neural network architecture
0.6516524810	significant improvement
0.6516427581	model achieves
0.6514684594	multi level
0.6513072209	excitation source
0.6511255751	learning paradigm
0.6508750859	relative reduction
0.6502162769	source signal
0.6500715943	zero shot
0.6499032047	speech commands dataset
0.6497079023	text independent speaker recognition
0.6495230366	high frequency
0.6492516991	objective metrics
0.6491852544	significantly outperform
0.6475631506	subjective listening tests
0.6475485270	target speaker's voice
0.6475237843	experimental results confirm
0.6474805643	taking into account
0.6473507395	deep convolutional neural network
0.6469023648	grapheme based
0.6468248533	consistently improves
0.6464525871	convolutional network
0.6460292759	weakly labelled data
0.6452570884	expressive speech
0.6448607501	multiple domains
0.6443737812	sound separation
0.6443244032	prior knowledge
0.6439967797	mean square error
0.6410582465	multi condition
0.6407704358	recent studies
0.6401752429	language processing
0.6400783825	soft attention
0.6395755309	multi class
0.6387255810	cnn architecture
0.6384755262	relative word error rate
0.6378383204	relative improvements
0.6369622381	speaker separation
0.6368447579	paper proposes
0.6368308056	neural network based
0.6362884859	signal to interference ratio
0.6362372053	significant improvements
0.6362097122	sequence to sequence
0.6358765932	multi modality
0.6356989494	pre train
0.6351121526	visual modalities
0.6349571034	background music
0.6346156989	linguistic information
0.6341900784	speaker discriminative
0.6340346025	previous studies
0.6336736857	low resource language
0.6336021689	regression model
0.6331909974	aware training
0.6328449919	urban sound
0.6324346205	aurora 4
0.6323970474	long short term
0.6322264761	regression task
0.6316258365	multi objective
0.6315998032	sequence length
0.6312559574	continuous speech
0.6312027350	sound event localization
0.6307758208	objective and subjective evaluations
0.6307696795	ratio mask
0.6307052760	localization and tracking
0.6298140557	speaker recognition evaluation
0.6295197703	subjective test
0.6290814425	external language model
0.6288586830	speech signal
0.6287130372	ctc based
0.6284867570	phonetic content
0.6283072836	preliminary results
0.6282945060	perceptual evaluation of speech quality
0.6279874263	data augmentation techniques
0.6275175676	wsj0 2mix dataset
0.6261012166	channel wise
0.6254497972	multi domain
0.6245354726	training scheme
0.6243571530	higher level
0.6241011560	noise ratio
0.6239570128	music style transfer
0.6238036141	native speakers
0.6234683054	hand crafted features
0.6231641689	graph based
0.6230497227	improved performance
0.6227894827	cost effective
0.6220224200	few shot
0.6211454638	desired signal
0.6200387478	image source
0.6194631456	deep residual
0.6191341731	u net
0.6191279839	experimental results demonstrate
0.6189780712	musical styles
0.6189588689	feature representations
0.6189104832	machine generated
0.6187622970	target speaker's
0.6182157772	challenging problem
0.6181922030	report describes
0.6181454652	attention fusion
0.6181023385	recognition systems
0.6179329978	objective quality
0.6178849883	target speech separation
0.6178612291	short time fourier
0.6178079274	lombard speech
0.6178074859	evaluation sets
0.6175438963	reverberant speech
0.6167186342	phoneme based
0.6158499286	unlabelled data
0.6157814741	language independent
0.6155985379	sequential data
0.6149317471	attend and spell
0.6146007340	sound quality
0.6142506169	perceptual evaluation
0.6139691934	deep learning approaches
0.6132507526	raw speech
0.6132305131	natural speech
0.6121248654	constant q
0.6118833253	hidden representations
0.6101047511	single channel speech enhancement
0.6095544109	model size
0.6091840480	emotion related
0.6083134062	feature set
0.6081492218	text to speech
0.6081406696	cnn architectures
0.6080176992	independent vector
0.6071674720	timit corpus
0.6069184926	squeeze and excitation
0.6068671166	trainable parameters
0.6067753605	rnn t
0.6065676044	proposed method
0.6061299985	convolution layer
0.6058532196	acoustic signals
0.6057684443	eeg based
0.6048311146	single channel speech separation
0.6047917133	acoustic units
0.6043704053	asv systems
0.6038455876	sound event localization and detection
0.6037737413	t dcf
0.6032767899	sound event classification
0.6032277894	musical piece
0.6025013864	autoregressive models
0.6023694564	single stage
0.6022994972	attention module
0.6022830160	separation performance
0.6022606989	great success
0.6022118685	higher quality
0.6021470473	model parameters
0.6020379950	text embeddings
0.6019138015	speech waveforms
0.6008322237	separation quality
0.6006579430	significantly higher
0.6003849536	speech waveform
0.5995805551	speaker representations
0.5991517476	fusion strategy
0.5981663567	sound classification
0.5981202222	language specific
0.5976187316	feature learning
0.5975472836	chime 6
0.5965637502	event based
0.5965540720	limited data
0.5957712989	statistical model
0.5955446241	cnn based
0.5954855945	lattice based
0.5951621547	onset and offset
0.5950896808	conventional approaches
0.5948090752	enhanced speech
0.5946750710	cross lingual voice conversion
0.5940552227	room reverberation
0.5935504819	mean opinion scores
0.5926543331	experimental results showed
0.5921932770	subjective assessment
0.5910464711	deep speaker embeddings
0.5901795153	markov model
0.5901546200	pooling function
0.5900981320	encoder decoder framework
0.5894752187	multi label classification
0.5893812624	source domain
0.5893034702	handcrafted features
0.5891018055	similarity matrix
0.5879086731	multilingual speech recognition
0.5874424921	speaker information
0.5870953264	generation process
0.5870497144	lstm based
0.5866629804	multimodal learning
0.5858351364	parallel training data
0.5840089484	event localization
0.5839674941	background model
0.5838351144	discriminative feature
0.5838068800	transformer architecture
0.5837354439	semi supervised training
0.5834781559	linguistic representations
0.5834289042	front ends
0.5834046476	open domain
0.5829442707	tdnn f
0.5828205432	domain adversarial
0.5827764840	speech driven
0.5825857133	adversarial loss
0.5821821353	autoregressive model
0.5821258202	architecture search
0.5818351173	long utterances
0.5814850652	competitive baselines
0.5811085457	mir tasks
0.5807975479	existing approaches
0.5805337472	long duration
0.5803869453	orders of magnitude
0.5801932523	greatly improved
0.5801621365	time delay neural network
0.5799231830	speaker specific
0.5784509687	trade off
0.5783505537	asvspoof 2017
0.5773960567	transformer network
0.5773700485	self attentive
0.5772745977	data selection
0.5771658875	transformer tts
0.5771270945	quantitative and qualitative
0.5768927779	end to end
0.5758754538	content based
0.5752686229	real recordings
0.5750494581	global style
0.5736261063	music streaming
0.5734613093	sound source separation
0.5728982456	transformer encoder
0.5728378996	singular value
0.5727086390	method outperforms
0.5721321679	music classification
0.5721075385	microphone signals
0.5716956425	multiple languages
0.5712259985	rnn based
0.5709622973	discriminative features
0.5701103582	major challenge
0.5700540087	convolutional layer
0.5700252920	output sequence
0.5688716797	human emotion
0.5685498588	submitted systems
0.5684787300	audio signal processing
0.5683473297	specifically designed
0.5681142274	proposed approach
0.5679910511	conventional hybrid
0.5674706420	target domains
0.5667029319	music pieces
0.5661489657	clustering algorithm
0.5659825025	k means
0.5651208459	zero resource
0.5647621334	reverberant conditions
0.5646807986	conditional neural network
0.5644544847	t f
0.5640381043	analysis synthesis
0.5639171446	full rank spatial
0.5636033479	recording devices
0.5633757081	phase information
0.5632596082	test clean
0.5625612922	output sequences
0.5620755644	unknown number of speakers
0.5617920596	approach outperforms
0.5617084082	jointly optimized
0.5615818357	binaural cues
0.5612775682	music tracks
0.5611636765	development set
0.5608954463	speech recognition systems
0.5599437553	transcribed speech
0.5598354039	multi talker speech separation
0.5598010127	subjective quality
0.5594652223	sdr improvement
0.5593945175	result shows
0.5589309980	recurrent network
0.5583063628	phoneme to viseme
0.5578431438	stand alone
0.5573465159	speaker recognition systems
0.5571999699	benchmark dataset
0.5564820457	deep feature
0.5562747066	model complexity
0.5560214319	music production
0.5559946279	unlabeled dataset
0.5555613934	objective evaluations
0.5555130465	classification problem
0.5552649043	frequency components
0.5548942107	invariant training
0.5546586994	feature level
0.5544236067	feature embedding
0.5532656349	acoustic to articulatory
0.5530734282	text dependent
0.5522950077	voice recordings
0.5520041806	unseen noise
0.5513059095	esc 10
0.5511825436	waveform synthesis
0.5501373626	standard cnns
0.5500875018	x vectors
0.5497231317	vocabulary continuous
0.5495495870	sub band
0.5494825967	acoustic properties
0.5494057408	deep learning approach
0.5493170701	text data
0.5486193968	deep learning based speech enhancement
0.5467654629	data processing
0.5462704158	noise conditions
0.5458940695	chime 4
0.5453691636	sound scene
0.5452022235	significantly reduce
0.5446751669	input signal
0.5441643509	deep learning methods
0.5439849166	chime 5
0.5437387026	et al
0.5433575549	image classification
0.5432133884	audio signal
0.5422192135	librispeech dataset
0.5422190660	high level features
0.5419228008	paper describes
0.5418136663	far field
0.5414028208	probabilistic linear
0.5412256027	single source
0.5407468903	residual learning
0.5401204219	model outperforms
0.5390429253	music score
0.5388319583	computer vision
0.5386431414	automatic music generation
0.5384424886	sound recognition
0.5382689967	spatial information
0.5378150912	sequence level
0.5377523082	direction of arrival estimation
0.5375652982	processing tasks
0.5374137993	asvspoof 2019
0.5364251659	clustering based
0.5356769481	dcase 2017
0.5353214490	computer assisted
0.5352640651	speaker encoder
0.5341380282	cross task
0.5336357641	image domain
0.5331210406	image to image
0.5314651962	noise signals
0.5312845678	source language
0.5302396420	increasing attention
0.5292117088	input feature
0.5290073983	data sparsity
0.5286358594	mixed speech
0.5284899612	aishell 1
0.5283106013	machine learning models
0.5282975787	denoising and dereverberation
0.5272817586	evaluation dataset
0.5271923806	multiple sources
0.5265776038	silent speech
0.5263439608	x vector
0.5256707517	maximum mutual
0.5256471715	statistical parametric
0.5240518071	musical scores
0.5237877271	high performance
0.5237813006	target sequence
0.5237563697	robust speaker recognition
0.5237532462	data efficient
0.5236766266	noisy reverberant
0.5227532421	tacotron based
0.5225417461	entire input
0.5220443952	deep learning models
0.5219888390	previous methods
0.5213447819	supervised manner
0.5212558827	development dataset
0.5206600668	public datasets
0.5203808314	high level feature
0.5202535388	acoustic to word
0.5196803749	linear predictive
0.5189267488	hybrid asr
0.5188006210	network architectures
0.5180651303	speech to text translation
0.5180338450	text independent speaker
0.5173951995	transcribed data
0.5164365668	audio source separation
0.5161416320	voxceleb1 dataset
0.5157991417	end to end speech recognition
0.5154863563	attention model
0.5153513129	acoustic signal
0.5150420569	articulatory to acoustic
0.5145181391	reference speech
0.5144198458	telephone speech
0.5136930635	domain translation
0.5134304426	deep neural network based
0.5125045629	speech representations
0.5116759110	synthesis quality
0.5116026962	recently developed
0.5107424451	task specific
0.5103154461	pass filter
0.5094221526	target voice
0.5093802765	chime 3
0.5086301847	speech activity
0.5085701543	opinion scores
0.5081855568	speech data
0.5075935444	acoustic embeddings
0.5072661995	acoustic representations
0.5072054042	non parallel voice conversion
0.5069888204	code switched speech
0.5066520625	proposed model outperforms
0.5065902603	active speaker
0.5065407208	mixture signal
0.5062503836	target signal
0.5061902447	pre trained models
0.5060272618	successfully applied
0.5058991681	classification of acoustic scenes and events
0.5050095730	this technical report
0.5049867971	naturalness and similarity
0.5045371868	source and target
0.5044197690	speaker aware
0.5037845856	shown promising results
0.5036354732	paper presents
0.5021501903	amplitude and phase
0.5020814470	child speech
0.5013444629	performance gain
0.5007986438	clean and noisy
0.5003643218	subjective listening
0.5000937801	noisy and reverberant
0.4998968894	adversarial network
0.4990998595	whispered speech
0.4989654521	performance gains
0.4988689432	time frequency scattering
0.4987986438	objective and subjective
0.4985322339	network learns
0.4984859369	magnitude and phase
0.4984245261	diarization systems
0.4978815224	results reveal
0.4977505263	data augmentation method
0.4976016275	encoder decoder models
0.4972392597	audio adversarial examples
0.4967830430	extraction network
0.4964842693	unpaired data
0.4957922264	pre trained model
0.4954832933	relative word error
0.4952957811	inter speaker
0.4950249571	source target
0.4947986438	subjective and objective
0.4947980735	acoustic unit
0.4947748343	noise and reverberation
0.4945593811	timbre and pitch
0.4943427818	large scale weakly
0.4942898585	reverberant and noisy
0.4941953701	spatial sound
0.4941195077	co occurrence
0.4939004942	extracted features
0.4935242513	speech chain
0.4932933793	small footprint keyword
0.4931193859	timit dataset
0.4929406273	gradient based
0.4928122878	under noisy conditions
0.4917244374	audio event detection
0.4916663423	complementary information
0.4916535849	asr and tts
0.4916274874	carried out
0.4913958245	semantic information
0.4905147127	multiple modalities
0.4904164828	evaluation shows
0.4902795737	multi speaker text to speech
0.4899822623	deep learning techniques
0.4898952900	bidirectional long short term
0.4893474828	non negative matrix
0.4884663122	dcase 2018 challenge
0.4884303286	encoder and decoder
0.4883517786	area under
0.4882496242	quality and intelligibility
0.4880633238	paper introduces
0.4880611316	target text
0.4878557257	previous research
0.4870296334	acoustic characteristics
0.4857034346	asr errors
0.4854205649	speech representation learning
0.4853850695	high fidelity audio
0.4852790360	reverberation and noise
0.4850270761	unsupervised pre training
0.4846396106	voice over
0.4846204859	biometric systems
0.4836678328	difficult task
0.4830166984	area of research
0.4829016220	neural architecture
0.4825885097	streaming speech recognition
0.4825642404	self supervised
0.4815284857	directly generate
0.4813148564	practical applications
0.4811682136	noisy environment
0.4808393849	front end
0.4807252871	dcase 2018
0.4805675742	presence or absence
0.4804084596	attention network
0.4802885210	image recognition
0.4801823666	training and inference
0.4799200703	character based
0.4798142072	vae based
0.4797305791	back propagation
0.4794659479	dataset collected
0.4793926598	music signals
0.4787969829	multi speaker tts
0.4785961151	teacher models
0.4785291241	convolutional and recurrent
0.4779769854	achieve comparable
0.4775237168	input signals
0.4772685220	non intrusive
0.4772674523	source localization and tracking
0.4761819412	embedding vector
0.4758196075	machine learning algorithms
0.4756343046	analysis shows
0.4752673133	end to end speech translation
0.4752321764	deep convolutional neural networks
0.4751085095	proposed method outperforms
0.4747537532	neural text to speech
0.4746987871	speaker tracking
0.4746749032	segment based
0.4746686997	recent research
0.4741343841	localization and detection
0.4740423182	energy based
0.4739653835	relevant features
0.4739171660	tut sound
0.4734534217	audio fingerprinting
0.4734151541	method achieves
0.4733635821	speech corpus
0.4733592267	voxceleb dataset
0.4727274539	pass model
0.4726425959	speech and text
0.4726394738	scenes and events
0.4720796812	visual context
0.4717742755	\ cite
0.4710139592	domain knowledge
0.4705870792	publicly available datasets
0.4704305871	machine learning methods
0.4704238600	autoencoder based
0.4702424351	neural speech synthesis
0.4701405159	distortion ratio
0.4700543527	digital signal
0.4695457905	test accuracy
0.4695349288	non stationary
0.4694206453	voice conversion challenge
0.4690738006	spectrogram based
0.4688150625	non invasive
0.4685710864	word recognition
0.4685123534	wide range
0.4677068371	information processing
0.4675209972	speech to text
0.4673485563	real time
0.4672090297	speech enhancement algorithms
0.4671298807	speaker verification task
0.4667755923	capture long term
0.4666114741	multi channel speech separation
0.4664897781	temporal context
0.4662396118	variational auto
0.4655214838	self attention
0.4654180039	supervised and unsupervised
0.4651664687	unseen data
0.4646821756	hybrid models
0.4644265963	training and testing
0.4637448671	asvspoof 2019 challenge
0.4637202885	full band
0.4623603006	experiments demonstrate
0.4622892233	speech corpora
0.4622298120	end to end slu
0.4618357418	class labels
0.4616579322	training and test
0.4605260376	speech and language
0.4604501631	read speech
0.4596978465	input and output
0.4596499146	feature vector
0.4594487583	data for training
0.4592573483	performance gap
0.4592073357	learned representations
0.4587231224	end to end speech synthesis
0.4585288152	prediction error
0.4582207197	asr performance
0.4581709090	audio visual speech enhancement
0.4580644874	promising performance
0.4579810130	research community
0.4576838072	frequency masking
0.4566642354	dcase 2020
0.4565662270	a comparative study
0.4560943233	text dependent speaker
0.4560812936	dcase 2019
0.4551926377	learned features
0.4549406298	non autoregressive
0.4545280442	english speech
0.4544369835	conversion challenge 2020
0.4536955697	single speaker
0.4531706048	attention based sequence to sequence
0.4526568371	processing methods
0.4526403023	taking advantage of
0.4524231645	comparable results
0.4523254181	additional information
0.4521377739	visual speech
0.4513469096	wavenet based
0.4512281013	input text
0.4511074123	lstm network
0.4510342183	target speech
0.4509368983	far field speech recognition
0.4507057492	augmentation methods
0.4503646621	end to end asr
0.4501988610	deep convolutional neural
0.4499646045	mean opinion
0.4497470138	trained and tested
0.4491436981	zero shot learning
0.4491373868	sequence to sequence models
0.4490758247	speech and noise
0.4487491051	annotated dataset
0.4485765610	times faster than
0.4480755977	emotion prediction
0.4479596757	dcase 2018 task
0.4478680842	e2e models
0.4478234147	neural network acoustic models
0.4476894626	this paper proposes
0.4461043568	monolingual data
0.4458149524	transfer learning approach
0.4455827807	domain audio separation network
0.4455050083	\ textit
0.4453493950	detection and localization
0.4448491564	under resourced
0.4444885684	interfering speech
0.4438183851	end to end spoken language understanding
0.4437685394	noisy data
0.4435612200	self supervision
0.4435552210	robust asr
0.4435433553	processing techniques
0.4431435169	capable of generating
0.4427999741	speech enhancement methods
0.4421041540	tts systems
0.4420431226	recently shown
0.4418505470	embedding features
0.4412941909	must c
0.4412785404	multilingual models
0.4402375028	audio visual
0.4389675800	vector and x vector
0.4376243797	training pipeline
0.4340329150	gmm based
0.4339899480	recent advancements
0.4337301864	probabilistic model
0.4336806335	audio captioning
0.4328244867	audio tagging
0.4325327852	autoregressive neural
0.4322846429	time domain
0.4321309848	non native
0.4313147061	performance evaluation
0.4312252720	mean square
0.4306715435	dcase 2020 challenge
0.4299805024	an open source
0.4295990041	representations of speech
0.4292374554	experimental results show
0.4289392737	children speech
0.4286642782	large scale dataset
0.4285646332	parallel vc
0.4285282288	this paper presents
0.4281330201	augmentation techniques
0.4280759132	source signals
0.4279482222	synthesis systems
0.4276712418	train and test
0.4276435916	dnn training
0.4274551619	large amounts of
0.4272924193	method called
0.4269260269	self supervised learning
0.4263284508	streaming and non streaming
0.4255512189	i vectors
0.4253460356	deep learning architectures
0.4250411908	t s
0.4248173830	verification tasks
0.4247401918	multi speaker speech recognition
0.4244930429	speech and music
0.4237300141	attention based models
0.4236244881	and accompaniment separation
0.4234174300	\ emph
0.4234168309	temporal classification
0.4233028210	long short
0.4230481890	achieves comparable
0.4226114109	results obtained
0.4223192811	encoder decoder based
0.4221221522	low rank matrix
0.4218921401	number of microphones
0.4217203156	acoustic and visual
0.4213489548	error rate reduction
0.4212148067	detection and classification
0.4212109600	hybrid model
0.4208914966	end to end neural
0.4208583623	fixed number
0.4204579859	e2e model
0.4202392392	external language
0.4198383124	training samples
0.4197465546	piece of music
0.4191802109	based vad
0.4189673139	automatic speech recognition systems
0.4181555357	high quality speech
0.4180312477	labeled dataset
0.4179001666	truth labels
0.4178855980	human level
0.4178022905	require large
0.4173183182	strong baseline
0.4170886629	recognition from speech
0.4170402863	speaker diarization system
0.4169306597	feature sequence
0.4166881076	gated convolutional
0.4153423730	noise sources
0.4148650799	large datasets
0.4144627811	context information
0.4144174061	end to end automatic speech recognition
0.4144164546	robust automatic speech recognition
0.4143582198	ctc model
0.4140230636	based speech enhancement
0.4138960579	based approaches
0.4130016980	training strategies
0.4125450024	diarization performance
0.4116996425	based methods
0.4113798455	speech samples
0.4112715072	performance improvements
0.4105227913	end to end models
0.4103733887	speech emotion
0.4103456608	time delay
0.4103366154	classification loss
0.4097723359	number of speakers
0.4096378591	waveform model
0.4094582729	term memory
0.4093800178	an attention mechanism
0.4091265714	enhancement and separation
0.4087438419	speech distortion
0.4083519836	end to end tts
0.4083190239	music information
0.4082130457	model compression
0.4066049835	publicly available
0.4056748756	unsupervised manner
0.4053330607	acoustic and language
0.4052987144	test data
0.4045322617	tts model
0.4043112571	meeting corpus
0.4039476036	neural network models
0.4037407136	signal to interference
0.4034988131	parallel training
0.4029705532	recognition task
0.4028615848	encoder decoder network
0.4015739010	left to right
0.4005112431	one shot
0.4001284482	a case study
0.3999087551	freely available
0.3998844902	back end
0.3995659966	high fidelity speech
0.3990642340	on mobile devices
0.3981759279	training process
0.3980490426	distant speech
0.3979577882	presence of background
0.3975045206	character error
0.3967214578	time frequency
0.3963591807	bidirectional long
0.3959636577	sound detection
0.3958817438	a large scale
0.3949191112	rate reduction
0.3945730889	function based
0.3942472982	time domain audio separation network
0.3942358392	paper explores
0.3942239745	i vector
0.3936275137	source model
0.3934578558	separation and recognition
0.3932966721	sequence to sequence model
0.3932766179	this paper describes
0.3931059395	acoustic scenes and events
0.3928696291	attention networks
0.3927514427	query by example
0.3927277366	current methods
0.3923058200	number of parameters
0.3920872761	$ \ pm
0.3920665311	student training
0.3919589801	lingual voice conversion
0.3919334115	time frequency domain
0.3912519165	librispeech test
0.3908360939	acoustic scenarios
0.3906802148	end to end manner
0.3899797285	joint model
0.3894287659	a fully convolutional
0.3893771121	resource language
0.3880449480	speech utterances
0.3879361616	independent low rank
0.3877336554	simulated data
0.3876727989	$ \ beta
0.3875547284	tacotron 2
0.3875073537	feature loss
0.3870114196	without sacrificing
0.3857656789	training objective
0.3857554230	classification performance
0.3854548541	mandarin speech recognition
0.3848073538	i vector extractor
0.3846125913	blind source
0.3837871732	audio watermarking
0.3834830602	verification task
0.3831019661	a deep neural network
0.3827917680	main idea
0.3827710340	event localization and detection
0.3824078407	few shot learning
0.3821202599	transformer model
0.3818345871	speech frames
0.3817876402	achieved competitive
0.3817140320	approach achieves
0.3814953420	^ 2
0.3813142261	challenge 2020
0.3803168106	audio classification
0.3802469331	slu model
0.3799981526	first order
0.3795571529	root mean
0.3791664202	unsupervised subword
0.3787103385	learning based
0.3786026684	adversarial attacks on
0.3785757415	determine whether
0.3783350749	state of art
0.3777137122	seq2seq model
0.3761355884	follow up
0.3761187209	word error
0.3759020468	speaker verification tasks
0.3756809699	synthetic dataset
0.3755297923	related features
0.3755071554	conditional generative
0.3747886942	acoustic environment
0.3743629026	speech generation
0.3736230288	training stage
0.3732723068	time frequency bins
0.3711795164	achieved great
0.3709897542	similar performance
0.3706027976	discriminative information
0.3699038943	speaker verification system
0.3698580911	audio synthesis
0.3694705335	deep architectures
0.3693073908	features extracted
0.3688577216	testing data
0.3683337820	significant performance
0.3682680934	unseen during training
0.3680112732	proposed framework
0.3672902988	instance learning
0.3672046351	related information
0.3669154910	input representation
0.3659316072	baseline systems
0.3657011779	matrix analysis
0.3655740968	non negative
0.3655481406	conversion model
0.3652862109	audiovisual speech
0.3651723510	augmentation method
0.3651248933	voice corpus
0.3649603722	audio visual speech recognition
0.3649520745	invariant features
0.3641662942	baseline model
0.3641484895	prediction model
0.3637665608	dcase 2019 task
0.3633313219	raw audio
0.3621213485	automatic music
0.3613768356	enhancement network
0.3606173051	recognition rate
0.3606071325	short time
0.3604693064	prediction task
0.3602078777	acoustic feature
0.3601956416	verification systems
0.3599807419	this paper introduces
0.3589378435	divided into
0.3589298992	streaming end to end
0.3586323336	second order
0.3585941802	proposed methods
0.3584286889	recognition and speaker
0.3583147656	speech related
0.3580461117	conventional methods
0.3578594370	self attention mechanism
0.3573384356	attack detection
0.3572811338	convolutional recurrent neural
0.3568569804	based approach
0.3566226638	transformer models
0.3564711479	event labels
0.3558030814	sub challenge
0.3550961513	sequence learning
0.3547676633	estimated speech
0.3543829240	one hot
0.3539615804	f score
0.3537514855	recognition performance
0.3536289520	human speech
0.3532800985	learning framework
0.3530910808	score following
0.3525110591	signal to distortion
0.3524789482	term dependencies
0.3524175620	a single speaker
0.3523090536	latent features
0.3522549992	separation and enhancement
0.3518781938	waveform based
0.3516703061	out of vocabulary
0.3510607770	lstm model
0.3508944819	asr models
0.3505947343	learning scheme
0.3501109515	proposed method achieves
0.3498583719	noise components
0.3498151966	real room
0.3482074292	speaker invariant
0.3481747078	tts models
0.3477245810	detection and classification of acoustic scenes
0.3475575985	adaptation method
0.3471295677	baseline results
0.3464654607	f measure
0.3460391981	human computer
0.3458730762	embedding extraction
0.3451943194	multilingual model
0.3435232070	task 1
0.3431648688	challenge 2019
0.3426847374	polyphonic sound
0.3424456800	low signal to noise
0.3424175821	voice trigger
0.3419687415	an unsupervised
0.3419555775	recognition challenge
0.3417242534	in recent years
0.3411777972	proposed model
0.3411067229	attention based end to end
0.3410660183	resource languages
0.3409109758	model learns
0.3407439580	experiment results show
0.3405206584	visual representations
0.3404772785	well suited
0.3401582216	audio signals
0.3398992592	recognition tasks
0.3398930533	advances in deep learning
0.3394292130	value decomposition
0.3392899231	out of domain
0.3392611088	separation methods
0.3391601151	datasets demonstrate
0.3390513811	generation method
0.3390507349	full rank
0.3387822530	traditional methods
0.3386774037	spatial features
0.3383868169	end to end automatic speech
0.3380935213	widely used
0.3374775360	recent advances in
0.3366666794	speech to speech
0.3365358128	component analysis
0.3356450641	level fusion
0.3355406008	translation task
0.3354464372	estimation accuracy
0.3353747250	this paper investigates
0.3352454668	cnn model
0.3348237187	learning of speech
0.3339345696	single task
0.3339249074	an attention based
0.3335161687	rnn t models
0.3333795662	temporal information
0.3328042851	last years
0.3325849885	speech technology
0.3324880387	temporal features
0.3324856713	self attention network
0.3321489762	speech content
0.3321204275	conversion performance
0.3317077262	current state of
0.3313988363	conventional method
0.3311169869	effective method
0.3311105408	model architecture
0.3309619457	learning algorithm
0.3305409460	code switching speech
0.3302549913	visual data
0.3302463632	non parallel
0.3301989928	audio visual scene
0.3293868148	not necessarily
0.3292634360	enhancement systems
0.3289813445	supervised training
0.3285291581	resulting model
0.3284758030	non stationary noise
0.3283224469	vector machines
0.3280206525	non streaming
0.3278341723	deep reinforcement
0.3271892170	conduct experiments on
0.3270347257	signal to noise
0.3269934366	top down
0.3266557341	deep learning framework
0.3263345665	trade off between
0.3257653384	model adaptation
0.3256838236	conventional asr
0.3256530429	self attention layers
0.3241735514	discriminative training
0.3236571014	score level
0.3236293351	robust speaker
0.3235172567	data collected
0.3229222863	speaker labels
0.3227149262	for keyword spotting
0.3226137327	mel filter
0.3224406353	experimental results indicate
0.3221278029	voice synthesis
0.3214818936	network based
0.3214772874	non linear
0.3207958420	encoder decoder model
0.3207576973	end to end text to speech
0.3204397668	based vc
0.3198118388	level features
0.3196404219	multi task training
0.3192645647	acoustic data
0.3191858497	audio events
0.3188837389	non parallel data
0.3185493015	speech applications
0.3184713679	noise ratios
0.3184551379	gap between
0.3180353987	voice detection
0.3177670371	coming from
0.3177441541	voice data
0.3176202017	embedding learning
0.3175899732	f1 score of
0.3174784292	obtained results
0.3173201776	large amounts
0.3172411407	mapping based
0.3171629487	generate music
0.3170774441	the art
0.3168484270	robust automatic
0.3167270716	sounding speech
0.3163168239	dnn model
0.3161821102	results revealed
0.3159530260	commonly used
0.3159340452	two stage
0.3153528190	method achieved
0.3152125701	test time
0.3150383240	task 4
0.3149906856	this paper addresses
0.3146377931	speech encoder
0.3142058096	experiments conducted
0.3140412569	generalizes well
0.3138769910	method outperformed
0.3137394673	neural network transducer
0.3136763537	a large margin
0.3134069080	audio data
0.3132016418	for speaker verification
0.3131633601	conditional neural
0.3130023561	speech recognition system
0.3128230908	input representations
0.3127658139	source speaker
0.3127601188	separation tasks
0.3124893548	significant improvement over
0.3122004335	arrival estimation
0.3121659748	the proposed method
0.3121480635	conduct experiments
0.3118053840	significant improvements over
0.3115018903	insight into
0.3113174860	state of
0.3113133265	sub word
0.3100145013	separation systems
0.3100059179	this paper explores
0.3099832557	unsupervised representation
0.3099014093	audio processing
0.3096987590	encoder network
0.3095818849	voxceleb speaker
0.3093214460	human evaluation
0.3090018993	synthesis model
0.3089856541	multilingual speech
0.3084716388	supervised acoustic
0.3081390901	audio inpainting
0.3077527703	memory network
0.3077002948	model takes
0.3076901820	music style
0.3073770719	music performance
0.3072930676	sequence models
0.3067787737	differences between
0.3067545634	speech segment
0.3065060972	localization method
0.3065005105	text only
0.3063675500	enhancement model
0.3062237845	adaptation data
0.3059308402	a single
0.3058923236	image translation
0.3057985169	a limited number
0.3051697662	top performing
0.3051122500	convolutional and recurrent neural
0.3049757238	separation task
0.3048997947	enhancement algorithm
0.3048584633	two pass
0.3043506142	robust speech recognition
0.3042184304	learning strategy
0.3042176461	performance metrics
0.3040890774	non trivial
0.3038154297	an efficient
0.3038051090	an unsupervised manner
0.3036154018	time consuming
0.3034862234	an improved
0.3033115423	time variant
0.3032500193	track 1
0.3029332026	does not require
0.3023238303	identification accuracy
0.3023011053	the voxceleb speaker recognition challenge
0.3009740041	separated speech
0.3009334672	speech mixtures
0.3005831371	processing applications
0.3005052583	while maintaining
0.3004528221	automatic detection
0.2998271324	proposed technique
0.2997717063	enhancement framework
0.2995477103	attention based neural network
0.2993958934	rnn t model
0.2990700933	the proposed approach
0.2990307018	end to end approaches
0.2989112128	model based
0.2987844433	large scale audio
0.2985920385	speech recordings
0.2982147735	decoder network
0.2975840643	an end to end fashion
0.2974828293	the art performance
0.2969495974	synthesize speech
0.2965411000	supervised deep
0.2963232163	asr task
0.2955735565	run time
0.2955338750	improves performance
0.2954769012	doing so
0.2951176032	10 db
0.2949148178	voice based
0.2946956139	the proposed framework
0.2942253678	data distribution
0.2941124474	an overview
0.2940609476	features extracted from
0.2939919033	speaker representation
0.2939221210	diarization error
0.2938509182	audio features
0.2937485510	x vector based
0.2936581126	across languages
0.2936233172	short time objective
0.2934912515	the art methods
0.2934383140	learning based speech enhancement
0.2929440161	propose to apply
0.2928515295	a convolutional neural network
0.2925868728	generation tasks
0.2924727825	speech analysis
0.2918384625	time series
0.2917987913	this paper
0.2917594797	a generative model
0.2913679265	to interference ratio
0.2909396734	5 db
0.2899277512	referred to as
0.2898058307	proposed algorithm
0.2897575734	audio streams
0.2881132713	language recognition
0.2880025035	text to speech systems
0.2879359422	transform domain
0.2874757028	baseline methods
0.2874029960	audio recordings
0.2867841761	learning process
0.2863893147	experimental results on
0.2863364855	number of sources
0.2862696762	separation problem
0.2861509756	the proposed model
0.2861440740	into account
0.2858203189	model structure
0.2857731973	music dataset
0.2857156809	first pass
0.2856239797	model architectures
0.2847310331	speech recognition models
0.2836848382	baseline system
0.2834583912	aimed at
0.2833796555	the proposed approach outperforms
0.2824328214	time scale
0.2823965497	robust against
0.2818580229	trained model
0.2817363419	well established
0.2816456910	a two stage
0.2815269279	learning tasks
0.2810604589	second pass
0.2810378230	two dimensional
0.2808395030	deep models
0.2808265771	for automatic speech
0.2808006090	task 2
0.2805036282	the main idea
0.2805012840	enhancement task
0.2802997800	time frequency representations
0.2802020755	based on
0.2798125963	source separation performance
0.2789438974	experiments conducted on
0.2787100944	i vector based
0.2786356561	method improves
0.2780712487	speaker detection
0.2777833241	speech representation
0.2776195886	the proposed method achieves
0.2774762714	deep convolutional
0.2769780812	time varying
0.2768103011	for speech enhancement
0.2767951520	voice conversion system
0.2765768108	adaptation methods
0.2760554642	neural machine
0.2756442748	suffer from
0.2755208619	recent neural
0.2754422060	speech embeddings
0.2753711319	approach shows
0.2749504812	wide variety of
0.2748764259	multi channel audio
0.2748559003	an utterance level
0.2743923611	deals with
0.2742633525	automatic speaker
0.2739543387	multi talker speech
0.2737282857	model called
0.2736122287	real data
0.2731067097	human auditory system
0.2729134603	deep features
0.2728752968	vector machine
0.2726796001	time and frequency
0.2725353284	simulation results
0.2721843451	high quality audio
0.2720250757	verification performance
0.2716391675	end to end speech
0.2713917900	self training
0.2713761735	a challenging task
0.2710219288	attention models
0.2709822115	multi channel speech
0.2703797999	based encoder decoder
0.2703181795	in noisy environments
0.2699804590	recent success of
0.2699363871	an encoder decoder
0.2697548393	so far
0.2694410728	audio to score
0.2693649150	word based
0.2692256108	even though
0.2692213414	successfully applied to
0.2685116724	speech synthesis using
0.2684544118	automatic detection of
0.2683600867	rather than
0.2682797362	speech dataset
0.2682214116	using deep neural networks
0.2676335178	training dataset
0.2672399751	mean average
0.2671879419	over fitting
0.2671061017	speech features
0.2671021150	learn representations
0.2668231288	paper reports
0.2668169272	fed into
0.2666698459	more accurate
0.2666037870	separation network
0.2662404997	real time applications
0.2658527555	dataset demonstrate
0.2655739598	modern deep
0.2643968468	asr tasks
0.2641842812	audio declipping
0.2641628104	visual and audio
0.2636796001	audio and text
0.2634603960	deep networks
0.2633985362	task 5
0.2630539619	an equal error rate
0.2626577752	d vector
0.2625336088	important task
0.2622726371	conversion task
0.2621770651	recorded speech
0.2620330484	the audio domain
0.2619822014	aiming at
0.2616732947	trigger detection
0.2615972041	task 3
0.2615552465	in domain data
0.2614746325	detection and classification of acoustic
0.2614649417	generated speech
0.2613895147	achieve state of
0.2611918251	the previous state
0.2611843939	learning strategies
0.2610881055	scale invariant signal
0.2606864852	average relative
0.2605437258	based voice conversion
0.2604201118	faster than real time
0.2603952224	robust automatic speech
0.2599146069	resource speech
0.2598266163	based models
0.2598017759	+ +
0.2595430812	text based
0.2595010363	single channel speech
0.2593425680	based classifier
0.2591676952	enhancement method
0.2589681410	end to end model
0.2589486359	identification task
0.2588917697	speech extraction
0.2585190080	an input
0.2582264992	an ablation study
0.2581218437	low computational
0.2580917028	based method
0.2576796001	audio and video
0.2572184416	audio to score alignment
0.2571049244	tts system
0.2570532786	achieves state of
0.2569348788	caused by
0.2569187657	acoustic parameters
0.2565599558	proposed techniques
0.2559112271	supervised approach
0.2558471748	supervised methods
0.2557327752	sound signal
0.2556797203	non local
0.2555894257	non causal
0.2555701792	a simple
0.2550807457	single neural
0.2550726646	recent progress in
0.2550648701	baseline method
0.2545813075	spatial audio
0.2545056105	recent advancements in
0.2543471758	enhancement performance
0.2540059704	wide range of
0.2539495779	visual speech enhancement
0.2539323162	a user study
0.2533881048	paper addresses
0.2533782973	while keeping
0.2533736003	paper shows
0.2532106613	compared to
0.2531460443	minimum mean
0.2531307249	relations between
0.2531139242	the wild
0.2528013455	music domain
0.2526014002	results show
0.2523413360	enhancement algorithms
0.2522367718	very small
0.2522278457	proposed method improves
0.2522098572	evaluation data
0.2516796001	audio and visual
0.2514880294	based techniques
0.2513198428	asr system
0.2511615554	large dataset
0.2510094748	one dimensional
0.2509690634	deep speaker
0.2508534758	greater than
0.2504189159	neural models
0.2503746944	so called
0.2501411951	sub optimal
0.2501179538	dealing with
0.2500067300	discriminate between
0.2495493736	neural sequence to sequence
0.2495195766	models achieve
0.2494695745	an online
0.2491117102	learning techniques
0.2489372462	speaker independent speech
0.2486578885	without parallel data
0.2479909266	learning algorithms
0.2475792316	improve performance
0.2474661362	detection cost
0.2472417272	experiments performed on
0.2472201500	trained models
0.2469848551	song identification
0.2467961525	focuses on
0.2462549118	for sound event detection
0.2458369174	audio samples
0.2457930592	range of applications
0.2455787350	embedding methods
0.2455724121	enhancement methods
0.2455424864	training corpus
0.2447602822	suffering from
0.2440891153	extensive experiments on
0.2436691772	universal sound
0.2435797993	embedding models
0.2434116451	user study
0.2433753438	treated as
0.2431128487	hybrid dnn
0.2430594939	faster than
0.2429397474	model performs
0.2424162853	training algorithm
0.2422338068	learning setting
0.2417678715	data corpus
0.2417107297	network parameters
0.2413826928	vector analysis
0.2408575878	any to many
0.2408209566	proposed scheme
0.2399249499	test other
0.2397421455	an end to end
0.2396645680	consists of
0.2394326280	this article
0.2393675894	achieve high
0.2393137798	shown promising
0.2391831464	an utterance
0.2390742608	relation between
0.2387122108	based model
0.2383992693	neural text
0.2380096844	evaluation results
0.2379874168	labeled training
0.2377265116	a data driven
0.2375254509	representations of audio
0.2374780976	depends on
0.2373594038	invariant signal
0.2373246457	recognition problem
0.2370586226	embeddings extracted
0.2368915720	rely on
0.2363878356	outperforms previous
0.2363566957	detection task
0.2362338520	wer improvement
0.2362022557	cross lingual voice
0.2360638575	case study
0.2355455827	suffers from
0.2354353894	different speakers
0.2353850728	focus on
0.2352294330	relies on
0.2347129537	regarded as
0.2346929805	learning approaches
0.2346267888	multiple speaker
0.2346058927	audio files
0.2344558690	maximization algorithm
0.2342634802	inspired by
0.2338444602	time domain audio separation
0.2337847643	depending on
0.2335972432	based features
0.2332057175	detection performance
0.2331054057	fully convolutional neural
0.2330461643	significant improvements in
0.2329768135	sequence model
0.2328015216	audio quality
0.2327774538	test results
0.2325292653	single model
0.2323157542	much larger
0.2321119407	robustness against
0.2319609077	detection methods
0.2318245281	the proposed method improves
0.2316977363	u net architecture
0.2315970171	sound analysis
0.2314408643	audio representations
0.2314389252	time frequency representation
0.2312898436	human auditory
0.2312417736	with skip connections
0.2310886304	the art results
0.2307196101	results demonstrated
0.2306249572	enhancement approach
0.2303927997	model training
0.2302503700	insights into
0.2302347375	models trained
0.2301164671	important role in
0.2301156461	conversion challenge
0.2300852379	loss functions for
0.2296919382	while preserving
0.2295033118	outperforms existing
0.2294462472	a small
0.2292200834	based se
0.2286878854	vector embeddings
0.2285715468	current state
0.2284784896	processing step
0.2280783037	commands dataset
0.2272405431	an effective
0.2265532901	many to many voice conversion
0.2265192019	speech source
0.2263135844	on device
0.2257568676	in many speech
0.2255196716	non standard
0.2248990226	information about
0.2245172821	without requiring
0.2242447898	three dimensional
0.2241574200	different domains
0.2240313009	mean squared
0.2240198475	speech enhancement method
0.2240031050	does not
0.2238659694	past few
0.2238208548	a strong baseline
0.2237856160	end to end fashion
0.2235734318	vocabulary speech
0.2235400347	parametric speech
0.2234710261	on test clean
0.2229447212	a challenging problem
0.2228532574	synthesis models
0.2228131986	baseline models
0.2225492152	music signal
0.2222611910	depend on
0.2220835114	self attention based
0.2220638038	prior work
0.2220186599	challenge 2018
0.2217025975	recent deep learning
0.2216347176	hybrid ctc
0.2213182386	relationships between
0.2212584964	few years
0.2211455978	audio visual dataset
0.2209541253	higher than
0.2205060518	end to end speaker verification
0.2205058516	markov models
0.2204880889	well defined
0.2204085629	distances between
0.2199084084	artificial neural
0.2198959184	achieve competitive
0.2196941780	human like
0.2192756994	a unified
0.2192235595	capture long
0.2190425473	proposed architecture
0.2189614929	achieved state of
0.2188295062	speech database
0.2188211428	the proposed scheme
0.2184183549	an iterative
0.2180925026	the target speaker
0.2175678923	during inference
0.2175277858	machine learning based
0.2172972020	audio visual speech
0.2172341135	neural sequence
0.2170265681	learning models
0.2168225415	audio event
0.2165028794	to distortion ratio
0.2158152857	more discriminative
0.2154836986	number of
0.2154021832	the proposed method outperforms
0.2150693479	ranging from
0.2148226310	these models
0.2143847810	representations learned
0.2142692786	great success in
0.2142584239	for speaker recognition
0.2140370556	recognition models
0.2138212520	consisting of
0.2137315853	diarization system
0.2135127394	to retrieve
0.2133956466	a deep learning based
0.2133466525	this study
0.2132697301	an important role
0.2131858177	the short time fourier transform
0.2130204769	this problem
0.2130191864	available at https
0.2129256861	significant improvement in
0.2128294722	an alternative
0.2125533213	separation method
0.2124731478	perform experiments
0.2121929461	speech synthesis system
0.2121855258	~ \
0.2121853924	difference between
0.2121711822	relative word
0.2121593019	for audio classification
0.2119112534	extracted from
0.2118379175	model trained
0.2116727562	phone error
0.2115939284	time steps
0.2112630126	google speech
0.2111358290	new speakers
0.2110265661	goodness of
0.2110137383	speech spectrum
0.2106880425	an interactive
0.2106841254	previous work
0.2103304918	proposed approaches
0.2103131816	applied to
0.2102835675	network transducer
0.2101004909	art results on
0.2098657343	relative reduction in
0.2097156902	learning architectures
0.2095854854	leads to
0.2095463204	processing algorithms
0.2095344195	model shows
0.2094618441	word error rate by
0.2094033486	trained end to end
0.2093445597	speech enhancement task
0.2093312356	human performance
0.2091162017	while retaining
0.2090333862	network to learn
0.2088763450	do not
0.2084552429	world scenarios
0.2084479693	low resource speech
0.2084210788	learning methods
0.2081261795	use cases
0.2081054374	from multiple speakers
0.2079393898	non parallel training
0.2079375607	acoustic source
0.2079363902	based speech separation
0.2077968757	transfer learning from
0.2074583037	an adaptive
0.2073355772	amounts of data
0.2072922068	model trained on
0.2064244114	$ m
0.2063542451	asr training
0.2060176113	acoustic information
0.2060106188	more realistic
0.2060087918	in addition
0.2059817028	best performing
0.2058143311	specific data
0.2052231289	relationship between
0.2051883259	2020 challenge
0.2049551559	aims at
0.2048964067	\ sim
0.2047603487	time invariant
0.2047083262	recognition model
0.2046183952	the proposed architecture
0.2045251231	attention layers
0.2043607995	to train
0.2043159283	deep learning model
0.2039342656	models trained on
0.2037979505	compared with
0.2037306811	well studied
0.2032266372	dnn based speech
0.2030544442	to learn
0.2029295678	as input
0.2027257719	generated by
0.2027141081	speech input
0.2026886227	end to end spoken
0.2025173612	for acoustic scene classification
0.2025015717	recent deep
0.2020741862	recent progress
0.2017098499	for speech emotion recognition
0.2016302017	to improve
0.2014688281	deep learning based speech
0.2009374815	art accuracy
0.2007998827	2mix dataset
0.2007302796	the first step
0.2005746156	the proposed algorithm
0.2004909344	focused on
0.2003348273	based acoustic model
0.2003145641	a multi modal
0.1999326690	two stage training
0.1999032259	large number
0.1998567215	comparison between
0.1997609528	shared across
0.1995315268	opposed to
0.1994584425	training framework
0.1994167251	large set of
0.1991820268	conditioned on
0.1991610979	asr model
0.1991372913	the ground truth
0.1989589835	an important task
0.1989204762	correlation between
0.1985841261	large number of
0.1984308500	the art result
0.1982562736	the final
0.1980403381	evaluation of speech quality
0.1977884298	more than
0.1976939354	each source
0.1973455938	to image translation
0.1969191041	many to many
0.1967124628	results indicate
0.1961701313	the proposed
0.1961511563	based architecture
0.1961456555	end to end systems
0.1960535171	a semi supervised
0.1960201240	multi speaker speech
0.1960169716	recent advances in deep
0.1959429579	characterized by
0.1959014724	for robust speech recognition
0.1958889788	during training
0.1958822592	end to end approach
0.1958423819	speech recognition task
0.1957669327	mismatch between
0.1955384382	speech enhancement systems
0.1953810835	a deep neural
0.1953625285	evaluation results show
0.1952586738	well known
0.1952406601	a lot
0.1950712041	recent success
0.1949981241	the proposed technique
0.1949416843	faster than real
0.1946015671	viewed as
0.1945012349	perform better
0.1944016786	a novel
0.1934856675	information into
0.1934330750	multichannel audio
0.1930382649	learning approach
0.1930064497	domain features
0.1928982886	as opposed
0.1927788258	extract features
0.1927580927	for environmental sound
0.1922245547	the end of
0.1917286308	to detect
0.1916605118	relative improvement in
0.1914226019	conjunction with
0.1914075215	attention based neural
0.1913302447	speaker classification
0.1913073311	take advantage of
0.1912488730	automatic recognition of
0.1912038900	speech enhancement performance
0.1911173041	independent speaker
0.1907339922	over smoothing
0.1907283116	not clear
0.1906518755	connection between
0.1901852653	the domain of
0.1900616586	data recorded
0.1900146928	art results in
0.1899372323	recent work
0.1898955000	speech synthesis systems
0.1898941684	knowledge about
0.1897807430	monaural speech
0.1896629160	voice conversion using
0.1895481515	from scratch
0.1895300404	classification models
0.1894203489	to extract
0.1893498895	this thesis
0.1892685987	the prediction of
0.1891852653	the voice of
0.1891852653	the level of
0.1891844408	a brief
0.1890746736	head attention
0.1890424082	the network to
0.1889662660	speech transformer
0.1886010294	speech enhancement using
0.1885185987	the purpose of
0.1885185987	the result of
0.1885185987	the style of
0.1883255992	experiment results
0.1883254169	due to
0.1882685987	the outputs of
0.1882514924	acts as
0.1882445279	competitive performance on
0.1881876710	performs better than
0.1881852653	the parameters of
0.1881852653	the length of
0.1881852653	the recognition of
0.1880424082	the knowledge of
0.1879208667	to end automatic speech recognition
0.1879203489	to predict
0.1878317261	improvement over
0.1875185987	a corpus of
0.1875185987	the objective of
0.1874825748	second step
0.1872247664	method based on
0.1872198161	based acoustic models
0.1872073021	found data
0.1870209799	supervised data
0.1869932575	serves as
0.1867911023	to address
0.1866894670	a function of
0.1866496865	both audio and
0.1866324550	obtained from
0.1866188934	to separate
0.1865197174	as well as
0.1862685987	the study of
0.1859996319	an acoustic model
0.1858792020	extract speaker
0.1857849512	good performance
0.1857728004	the generation of
0.1855462305	to generate
0.1854968947	attention based model
0.1853767922	based framework
0.1853060203	trained on
0.1852433395	similarity between
0.1851915703	integrated into
0.1850538278	this work
0.1850228004	the issue of
0.1849384646	single neural network
0.1848830899	a deep learning model
0.1848171316	experiments show
0.1846894670	the identification of
0.1843621752	owing to
0.1843421752	corresponds to
0.1840889547	end to end spoken language
0.1838898127	dominated by
0.1837531348	the help of
0.1836997826	improvements over
0.1836894670	the phase of
0.1836572326	produced by
0.1834244581	previous state of
0.1833710205	in conjunction
0.1833430416	lead to
0.1833035641	significantly better than
0.1832838232	benefit from
0.1831651504	the process of
0.1830222933	the extraction of
0.1830193071	$ n
0.1828887550	end to end learning
0.1826693521	a study of
0.1825623085	prior knowledge of
0.1825500026	conducted on
0.1825203030	distinguish between
0.1824457496	a neural network
0.1824419414	better performance
0.1823037014	more and more
0.1822834371	embedding model
0.1822110585	audio embeddings
0.1821540301	a single model
0.1820804023	the system to
0.1820358615	to perform
0.1820112319	sequence to sequence neural
0.1818869196	the original
0.1816044158	based end to end
0.1815969529	time frames
0.1815500107	this task
0.1815037673	to map
0.1810384150	capable of
0.1807947783	notion of
0.1807934833	to obtain
0.1807718330	followed by
0.1807482856	motivated by
0.1805853138	event recognition
0.1803881535	did not
0.1803718571	formulated as
0.1801994430	input data
0.1800222933	the performances of
0.1798609454	clean audio
0.1798211666	two stages
0.1793629061	relatively small
0.1792245547	the learning of
0.1791651504	the cost of
0.1790139609	in reverberant environments
0.1787623554	an important
0.1787572489	time step
0.1786463731	learning systems
0.1786338763	derived from
0.1783356029	to estimate
0.1782138762	2d cnn
0.1782110547	speech recognition model
0.1781143180	amount of training data
0.1780114342	framework for
0.1779472864	this issue
0.1779214342	such as
0.1777791995	for classification of
0.1777716388	close to
0.1776693521	the framework of
0.1774315937	in order to
0.1773213241	a generative adversarial network
0.1772460543	speech utterance
0.1771269475	the art approaches
0.1770961161	audio waveforms
0.1770914448	three different
0.1768885127	unsupervised speech
0.1768740267	mismatch between training
0.1767423925	investigate different
0.1764412920	performance comparable to
0.1764261335	an investigation
0.1763712736	directly from
0.1763155977	further improve
0.1759702151	n best
0.1759345041	the present study
0.1757511766	sequence based
0.1755011459	converted into
0.1752108992	pitch dependent
0.1750937817	an additional
0.1750225390	proposed approach outperforms
0.1750222774	in terms of
0.1745980946	level feature
0.1745680516	compared to previous
0.1743689269	by introducing
0.1742686016	1 d
0.1742572843	serve as
0.1742044398	the spectrogram of
0.1741781237	speech recognition using
0.1741765825	affected by
0.1739678108	trained with
0.1737771380	loss based
0.1735589868	level representations
0.1732228460	enhancement models
0.1731843961	non target
0.1731696014	proposed models
0.1728853355	neural network model
0.1727755101	target data
0.1726094885	investigate whether
0.1725890274	lower than
0.1725875078	obtained by
0.1725102418	based singing voice
0.1724491509	these methods
0.1723476797	two main
0.1722625329	between speech and
0.1720390105	for automatic speech recognition
0.1718947817	to recognize
0.1718615822	noise model
0.1716864101	two step
0.1714603876	recognition system
0.1714060672	by exploiting
0.1712696328	inference time
0.1711706545	to reduce
0.1709790265	a new dataset
0.1709550734	propose to use
0.1709171658	this approach
0.1708919883	hours of
0.1706693521	the tasks of
0.1706628882	quality than
0.1706613820	compared to conventional
0.1706170364	based data augmentation
0.1705397546	relying on
0.1703774188	data from multiple
0.1703551386	step towards
0.1703080062	$ \
0.1702919446	the encoder and
0.1699186409	experiments on
0.1698730667	jointly trained with
0.1698446640	a pre trained
0.1697865135	an average
0.1697482251	based systems
0.1696584343	best result
0.1690311807	based on deep learning
0.1689850321	and classification of acoustic scenes
0.1689256788	features learned
0.1688675805	the main
0.1687332742	yields better
0.1687147942	speaker recognition challenge
0.1686700073	an ensemble
0.1685468962	for single channel
0.1683548900	efficient way
0.1682601804	achieved by
0.1682029856	experiments indicate
0.1679833715	able to
0.1678912427	to produce
0.1678862998	features from
0.1678355307	performs well
0.1674715430	sound event localization and
0.1672263269	and classification of
0.1672078471	mapping between
0.1671089185	long time
0.1670356865	use case
0.1669288462	evaluated on
0.1669263240	more robust
0.1669203316	to achieve
0.1667934610	combined with
0.1667551660	to solve
0.1664920407	end to end training
0.1664765508	the notion
0.1664255030	visual scene
0.1663306375	approach provides
0.1663115456	between training and
0.1663067057	each utterance
0.1662320541	knowledge into
0.1661642871	a large
0.1661529630	$ k
0.1660142068	corpus show
0.1657841321	lot of
0.1657245760	to build
0.1652044398	the code and
0.1650950802	significantly better
0.1648584728	based loss
0.1648048768	student learning
0.1646704623	these techniques
0.1645779814	a neural network based
0.1641586896	accuracy than
0.1640223092	for use in
0.1638877714	art performance on
0.1637524450	entropy loss
0.1636020533	three tasks
0.1633618939	perceptual evaluation of speech
0.1632140955	gains over
0.1632042462	to convert
0.1631518179	interactions between
0.1630498164	frame by
0.1630299518	explore various
0.1629095113	compared against
0.1626159622	methods like
0.1623666010	verification system
0.1623227823	the training set
0.1621740111	training method
0.1617996191	control over
0.1617815833	the model
0.1616855990	a multi task learning
0.1616706545	to identify
0.1615603655	to evaluate
0.1615332655	system achieves
0.1614047886	focusing on
0.1611484210	time warping
0.1608101966	presence or absence of
0.1605563531	dependencies between
0.1603907089	provided by
0.1599571078	represented by
0.1599544398	a technique to
0.1598080556	very low
0.1597432730	on wsj0 2mix
0.1596157611	drawn from
0.1590763344	model performance
0.1587816903	dependent speaker
0.1586641242	different languages
0.1585053219	smaller than
0.1583916548	learned from
0.1583915159	belonging to
0.1582808466	asr based
0.1580579334	end to end neural network
0.1580458973	simple yet
0.1578812142	by applying
0.1578548984	an auxiliary
0.1577719481	performs better
0.1577291843	for end to end speech
0.1577029281	results than
0.1575587299	generalize well
0.1574597584	neural text to
0.1573848659	two fold
0.1573711367	to noise ratio
0.1573584375	deep speech
0.1573261266	in real world
0.1572670335	audio analysis
0.1572232112	pre trained on
0.1569672187	synthesis system
0.1568664744	better than
0.1567930187	extracted by
0.1567588870	audio generation
0.1567060198	interpreted as
0.1566492157	performance on
0.1565532154	the fly
0.1564933447	leading to
0.1564519888	significantly more
0.1563261257	audio clip
0.1561940472	to create
0.1561639793	datasets show
0.1561609772	improves over
0.1560693326	source separation using
0.1558374953	best performance
0.1558061565	the binaural cues
0.1557484572	deep network
0.1556899901	a preliminary
0.1556028020	of speakers in
0.1555977976	carried out using
0.1555443147	the other hand
0.1555167405	for text independent
0.1554772064	existing state of
0.1552887671	in real life
0.1552009548	end to end automatic
0.1551264044	based speaker recognition
0.1548813662	unknown number of
0.1547346854	distance between
0.1546110705	accuracy over
0.1539657270	much faster
0.1537717457	suitable for
0.1536872564	set of
0.1536168537	of sound events
0.1536030917	audio content
0.1535872043	end to end speaker
0.1535765133	not only
0.1533688934	a strong
0.1533197807	relative transfer
0.1532251357	recognition dataset
0.1531724768	reverberation time
0.1530842204	an audio clip
0.1530776775	time delay neural
0.1528011459	to enhance
0.1527242017	for text independent speaker verification
0.1526980644	to leverage
0.1519903016	belong to
0.1519419866	information from
0.1519027795	much higher
0.1518996758	good quality
0.1517901144	a fully connected
0.1516263405	relative improvements of
0.1515459309	the training phase
0.1515044656	an essential
0.1512078630	correspond to
0.1511458333	kws system
0.1510447933	at least
0.1509172372	speech recognition tasks
0.1509009766	visual speech recognition
0.1508558411	audio waveform
0.1503328990	to develop
0.1501916328	the target
0.1498468295	knowledge from
0.1498013378	method does not
0.1495786837	modeled by
0.1493665932	to classify
0.1491601864	replaced by
0.1491519208	seen during training
0.1489981854	publicly available at
0.1487675581	more efficient
0.1486734793	audio scene
0.1485328639	detection system
0.1485149263	training approach
0.1483145677	stage approach
0.1480258020	network acoustic models
0.1479455828	using eeg
0.1479160762	the same
0.1479090567	point to
0.1476370563	paper provides
0.1474573310	small amount of
0.1471478154	better performance than
0.1468644531	limited number of
0.1468597767	end to end framework
0.1468155735	voices from
0.1468087539	the last few
0.1467756610	used to train
0.1467076852	the research community
0.1466980644	to enable
0.1465498454	shown to
0.1463019384	considered as
0.1462992188	the proposed methods
0.1458893530	more complex
0.1458785627	learning method
0.1458741357	less than
0.1458497158	these features
0.1457386405	to capture
0.1457089974	access to
0.1452581814	each frame
0.1452044937	asv system
0.1448350392	human perception of
0.1448287093	produce high
0.1448073477	outperforms state of
0.1447006590	according to
0.1445995682	tasks like
0.1445886964	generating speech
0.1444817234	perform well
0.1444401004	model achieved
0.1443636711	enhancement system
0.1443535459	the target domain
0.1442042871	second stage
0.1440152954	the student model
0.1438786159	using convolutional neural networks
0.1437915288	word error rate on
0.1431501815	a multi task
0.1426237130	audio source
0.1426208227	consist of
0.1425748124	emotional voice
0.1422506766	the time frequency domain
0.1421758851	amounts of
0.1421487605	for low resource
0.1418947228	\ pm
0.1418075058	the conventional
0.1414768317	a significant improvement
0.1413209560	of arrival estimation
0.1412416925	available online
0.1411115376	achieves competitive
0.1410995981	the entire
0.1409658175	these systems
0.1409252621	attempt to
0.1407326412	system outperforms
0.1406906993	aim at
0.1406479258	a low dimensional
0.1405666957	achieve better
0.1404437726	with respect to
0.1402953595	not seen during training
0.1400880715	related to
0.1400329521	to provide
0.1398506155	variety of
0.1395629701	composed of
0.1394194421	first layer
0.1391865632	connections between
0.1390754677	achieves better
0.1386212927	aims to
0.1386146153	far field speech
0.1385199942	by combining
0.1384496580	able to produce
0.1381504349	for text independent speaker
0.1379087574	interested in
0.1375677075	for end to end speech recognition
0.1375428376	results on
0.1374787724	an arbitrary
0.1373756105	an automatic
0.1372885544	resulted in
0.1368649576	parts of
0.1368124173	speaker verification using
0.1366077456	art end to end
0.1364666136	performance under
0.1364008314	for on device
0.1363985861	this work proposes
0.1362805607	classification based
0.1362608288	detection using
0.1361533267	art approach
0.1361443668	kinds of
0.1360405180	a single neural network
0.1356468838	text to speech system
0.1354795868	other words
0.1353591414	the development set
0.1351601094	the target speaker's
0.1351240667	speech enhancement based on
0.1351003888	successfully used
0.1348888137	applications like
0.1348268531	emerged as
0.1347510647	to understand
0.1347392693	easy to
0.1344250055	data from
0.1341462741	an end to end neural
0.1339562294	coupled with
0.1339511749	more flexible
0.1339436240	to minimize
0.1336004467	simple but
0.1335618758	a common
0.1335083850	end to end system
0.1332457931	the training data
0.1331439253	limited amount of
0.1331051171	different types of
0.1328660678	model trained with
0.1328179376	speaker recognition system
0.1328025151	based on deep neural
0.1327052973	two steps
0.1326938487	all pass
0.1325485309	tend to
0.1324422377	submitted system
0.1324257272	a wide range
0.1323595899	version of
0.1322316328	by adding
0.1321862712	the training process
0.1321832115	as well
0.1320796848	used to generate
0.1318231476	the performance of
0.1317266774	audio stream
0.1317216675	impact on
0.1317175258	look at
0.1316727297	supported by
0.1314826921	to tackle
0.1314444762	mismatch between training and
0.1313962031	online speech
0.1313162136	interaction between
0.1313087692	indistinguishable from
0.1312967348	the baseline
0.1312436103	developed by
0.1311612186	better results than
0.1311529845	to overcome
0.1311367379	the first pass
0.1311306456	types of
0.1309857152	based on convolutional
0.1309710547	a comprehensive
0.1308198569	training phase
0.1306944248	well trained
0.1306701665	these approaches
0.1305850741	for automatic speaker verification
0.1305681392	stage training
0.1303965123	non parallel voice
0.1302034998	other methods
0.1301857259	sub network
0.1301618902	audio representation
0.1300718290	very large
0.1297699085	a feed forward
0.1297074619	type of
0.1296978996	the t f
0.1296778544	robust speech
0.1296145088	for end to end
0.1295546376	the speech signal
0.1294774045	the shelf
0.1294256472	apart from
0.1293482293	produce more
0.1292692472	in most cases
0.1292547433	first stage
0.1291539596	for music source
0.1291247458	all neural
0.1291132911	the fact
0.1290657027	each time frequency
0.1289596454	more specifically
0.1289226504	compared to state of
0.1288487870	built on
0.1286869276	relied on
0.1285212820	thousands of
0.1284955672	large amount of
0.1284251094	refers to
0.1283764404	led to
0.1283647311	to combine
0.1279023723	propose to train
0.1277572946	network to predict
0.1273198037	a recently proposed
0.1273194592	based on deep neural networks
0.1271852465	aim to
0.1269525699	method uses
0.1267916443	represented as
0.1267340672	an image
0.1267080102	music emotion
0.1265297066	sub networks
0.1264389089	a new method
0.1264315049	to handle
0.1261895996	on librispeech
0.1260529442	large amount of data
0.1260168090	without parallel
0.1257701000	able to generate
0.1256893495	based sequence to sequence models
0.1256585554	yet effective
0.1256507482	learned by
0.1255907040	recognition evaluation
0.1254206061	audio effects
0.1253178318	a shared
0.1253084553	range of
0.1249694859	based architectures
0.1246184723	to extract features
0.1245313978	to optimize
0.1245290300	the input
0.1245139262	time aligned
0.1244605638	role in
0.1244223917	to text translation
0.1241653540	deal with
0.1241011996	contribute to
0.1240690256	various conditions
0.1240547418	perceptual quality of
0.1239785632	generalize well to
0.1239590266	outperforms other
0.1238841758	integrated with
0.1238133301	collected from
0.1237824400	speech sounds
0.1237269615	based multi
0.1237071808	representation of speech
0.1237070132	methods in terms
0.1235351240	against adversarial
0.1231615921	these issues
0.1230654313	identification using
0.1229537400	the state of
0.1228877958	for code switching
0.1228777850	applications such as
0.1227656365	the visual modality
0.1227593977	the art systems
0.1226434721	perceptual evaluation of
0.1224896830	detection systems
0.1223307771	able to achieve
0.1223162109	different ways
0.1222201874	compare different
0.1220518520	scores from
0.1219711242	by leveraging
0.1219533865	event detection using
0.1218100856	test clean and
0.1217484778	the proposed network
0.1217144367	by minimizing
0.1216424600	small number of
0.1214756795	part of
0.1214331353	signal into
0.1214273097	emotion recognition from
0.1213690337	for monaural
0.1213413739	an equal error
0.1212416146	refer to
0.1209901916	correlated with
0.1207943548	still challenging
0.1207625533	millions of
0.1206660431	vulnerable to
0.1206650764	approach to
0.1206015133	a fully
0.1205159654	a novel architecture
0.1204352125	neural speech
0.1204056458	to unseen speakers
0.1203703087	based music
0.1202599074	make use of
0.1202034893	audio adversarial
0.1201282808	other domains
0.1199295010	adoption of
0.1199116859	a convolutional
0.1198578808	performance comparable
0.1197927894	emergence of
0.1196119735	taken into
0.1194362529	very effective
0.1194203320	equipped with
0.1192821670	models trained with
0.1192189970	computer interaction
0.1191584650	dataset contains
0.1190390412	comprised of
0.1189674973	for text dependent
0.1187323117	model for
0.1186993269	ability to
0.1186689616	comparable to
0.1186300861	classification system
0.1184537987	error rate by
0.1184438846	two speakers
0.1183122655	loss function for
0.1181237780	speech recognition performance
0.1181139083	factors such as
0.1179824842	these tasks
0.1179693522	very limited
0.1178927556	difficult to
0.1177898861	a convolutional recurrent neural
0.1177894730	used for training
0.1177871585	this work explores
0.1177426218	multichannel speech
0.1177421869	a convolutional recurrent
0.1176878667	based sequence to sequence
0.1173196624	kind of
0.1172534795	art performance
0.1172016394	to facilitate
0.1171757739	audio sources
0.1171297604	separation model
0.1170772768	for robust speech
0.1169655015	a significant
0.1167911364	x vector system
0.1167247840	to determine
0.1167133806	with limited
0.1166978971	a priori
0.1166410170	the use of
0.1165880624	not seen during
0.1163734395	techniques such as
0.1163670065	unseen during
0.1162896623	at https
0.1162827484	very little
0.1162328557	representation of
0.1160251989	overall accuracy
0.1160046413	model accuracy
0.1158663916	a wide
0.1158543957	se system
0.1157708386	alignment between
0.1156209548	more difficult
0.1154934734	a wide range of
0.1154409101	an external language
0.1151523362	for distant speech
0.1150569147	speaker modeling
0.1150347266	model to generate
0.1148460516	performed by
0.1147871985	ease of
0.1147042037	parallel speech
0.1146521035	dataset show
0.1146174594	but also
0.1145940377	the input signal
0.1145072673	applicable to
0.1145027758	experiment on
0.1144799649	performance over
0.1143206152	these limitations
0.1141937837	the art models
0.1141520409	learning model
0.1140257571	the relationship between
0.1139581428	the raw waveform
0.1138014226	the effectiveness of
0.1137198179	an audio signal
0.1137094378	two different
0.1135951927	generated from
0.1135562765	signal based
0.1134378654	tested on
0.1134266371	first step
0.1134192449	similar to
0.1133580108	a new
0.1133195003	set of experiments
0.1132038594	two parts
0.1131471624	on top of
0.1129205747	condition training
0.1127402887	the voxceleb speaker recognition
0.1127086921	to avoid
0.1126606124	characteristics of
0.1126458948	classification using
0.1125783162	prone to
0.1125527229	challenging due to
0.1124362990	by proposing
0.1122749505	analysis of
0.1122122291	embeddings from
0.1118819283	to date
0.1118698319	each language
0.1115501196	able to reduce
0.1114941513	demonstrated on
0.1114815617	vector based
0.1114574345	of interest
0.1114312888	based speech recognition
0.1114056915	not always
0.1113967380	not yet
0.1111872542	taking into
0.1110670306	both objective and subjective
0.1108207380	for real time applications
0.1106689750	the case of
0.1106284854	challenge task
0.1105177519	because of
0.1104888464	even if
0.1103715981	possible to
0.1102024494	effectiveness of
0.1101654873	the converted speech
0.1100283906	in order to improve
0.1099157937	model uses
0.1096227759	to further improve
0.1096063222	the teacher student
0.1095035553	performance compared to
0.1094791653	these two
0.1093524886	so as to
0.1092466723	these new
0.1091734944	the time domain
0.1091425809	input speech
0.1091198919	corrupted by
0.1090571710	challenging due
0.1090411492	along with
0.1090033119	small amount
0.1088565525	the efficacy of
0.1088445749	based end to end speech
0.1088276717	the target language
0.1086456968	a series of
0.1086362787	such as wavenet
0.1086158979	speech spectrogram
0.1085829281	the most important
0.1084743970	training time
0.1084309482	performance than
0.1083535505	leads to better
0.1081558114	for text dependent speaker
0.1080729654	art model
0.1079698042	able to learn
0.1079590496	starting from
0.1079491651	and long short term
0.1078831566	the art neural
0.1078438678	these problems
0.1078206360	improvement compared to
0.1077265609	submission to
0.1075954374	known as
0.1074468979	a transformer based
0.1073317884	current speech
0.1073280413	order to improve
0.1071492698	a key
0.1070775871	first present
0.1070310107	scarcity of
0.1070205747	mixture models
0.1070175078	combination of
0.1069596967	model consists of
0.1069337423	learning method for
0.1069231149	more effective than
0.1067702186	the proposed system
0.1067200940	levels of
0.1066081583	model for music
0.1065235253	algorithm based on
0.1064920156	previous work in
0.1062355806	a variety of
0.1062331649	audio feature
0.1060329114	much more
0.1059702838	sequence to sequence speech
0.1059193790	learn from
0.1058802062	on device speech
0.1058668616	the timit dataset
0.1058093710	improvement on
0.1056264407	multiple sound
0.1056116055	the art performance on
0.1054484506	work proposes
0.1054153600	each layer
0.1052471110	performance of
0.1050850315	to do so
0.1050412308	properties of
0.1049990496	captured by
0.1049727373	mapping from
0.1049596976	and time consuming
0.1048900539	for single channel speech
0.1048823696	on average
0.1048621605	the lack of
0.1048162479	to ensure
0.1048142482	the primary
0.1047946847	subset of
0.1046714314	an unknown number of
0.1045067873	tasks such as
0.1044687465	other hand
0.1044635682	time frame
0.1044332046	in many cases
0.1044086287	classification model
0.1043545311	method for
0.1043211327	a comparative
0.1043046765	solutions for
0.1040865393	by means of
0.1040650294	to make
0.1038710631	designed to
0.1038621605	the field of
0.1037393490	both tasks
0.1036927412	a two step
0.1035702425	model to learn
0.1034275167	very challenging
0.1034182634	over time
0.1033580108	used to
0.1032265677	used to compute
0.1032201040	resulting in
0.1031810512	to assess
0.1031564810	the problem of
0.1031019059	corresponding to
0.1030734635	time frequency representation of
0.1030256604	available at
0.1030148419	an adversarial
0.1028492989	the most popular
0.1026944260	used to evaluate
0.1026803584	best knowledge
0.1026799254	to deal with
0.1026267055	under noisy
0.1025406886	improvement in
0.1022506379	approach based on
0.1022176659	sensitive to
0.1021564810	a set of
0.1020727991	to adapt
0.1019488983	a recurrent neural network
0.1018688818	feasibility of
0.1018294159	this purpose
0.1018148966	well as
0.1017836732	achieve good
0.1016931056	contained in
0.1016877525	nature of
0.1016370335	to represent
0.1015008436	the vocal tract
0.1015004424	streaming speech
0.1014437731	two channel
0.1013956967	a deep
0.1013287583	to compute
0.1012492167	comparison with
0.1012326814	the baseline model
0.1011799263	through experiments
0.1011647649	mapped to
0.1011564810	the task of
0.1011001755	tests show
0.1010972582	measured by
0.1009827706	2019 challenge
0.1009576180	speech recorded
0.1009544835	the current
0.1008012085	recordings from
0.1007814110	model based on
0.1007208963	used for
0.1007095290	aspects of
0.1006593696	a small number of
0.1006516977	architecture for speech
0.1005167315	the equal error rate
0.1003647331	than others
0.1003635776	embedding network
0.1002865853	by employing
0.1002856378	amount of
0.1001074746	a deep convolutional
0.1000660068	any additional
0.1000440024	detection models
0.0999555274	fed to
0.0999518188	the desired
0.0999002446	the art results on
0.0998855807	this work investigates
0.0998621605	the goal of
0.0998470835	the latent space
0.0998301030	driven approach
0.0998165703	speech recognition with
0.0995679770	often used
0.0993963535	a sequence of
0.0992969574	single network
0.0992178475	bag of
0.0991855614	the teacher model
0.0991396126	based on multi
0.0991289196	a fixed
0.0991259007	most existing
0.0990829346	to analyze
0.0989007380	classification accuracy of
0.0988599740	a novel technique
0.0987956966	framework based on
0.0986905541	more challenging
0.0986301454	tool for
0.0985215999	at test time
0.0984852138	up to
0.0984424238	the output of
0.0984231827	able to perform
0.0984216213	a cross modal
0.0983550582	the cocktail party
0.0983445996	previous work on
0.0982788214	this limitation
0.0981849254	an open
0.0981848480	the design of
0.0981238794	utterances from
0.0981201695	to guide
0.0979590638	limited amount
0.0979292114	evaluations show
0.0978979700	the fundamental frequency
0.0978886763	for example
0.0977580791	sound event detection with
0.0976978271	relevance of
0.0976685916	reference audio
0.0976674198	interact with
0.0976219007	a multi
0.0975745395	these embeddings
0.0975436044	a critical
0.0975140887	to reconstruct
0.0974549215	also present
0.0973904828	adapted to
0.0973621605	the robustness of
0.0972749289	used as
0.0971564810	the quality of
0.0971228301	availability of
0.0970081710	a low resource
0.0970081614	applied as
0.0968366656	operating in
0.0967490588	majority of
0.0967041713	consists of three
0.0966986719	two approaches
0.0966800106	a microphone array
0.0966673066	unlabeled audio
0.0965655569	an empirical
0.0964378743	unable to
0.0963743817	real time on
0.0963235241	scale datasets
0.0962004604	a real time
0.0961756675	the synthesized speech
0.0961434542	very deep
0.0960505445	a given
0.0959891943	terms of
0.0959838268	much attention
0.0959311645	two sets of
0.0958883384	the two modalities
0.0958151908	simultaneous speech
0.0957901491	each other
0.0957180158	of arrival
0.0957024289	a stack of
0.0956709943	limited by
0.0955874826	the latent variables
0.0955247136	different approaches
0.0955127213	the frame level
0.0954627496	based speech synthesis
0.0954499307	experiment with
0.0954238190	dataset containing
0.0953699342	across multiple
0.0953513342	spoken by
0.0951461134	audio clips
0.0951143571	an extensive
0.0950358865	neural source
0.0949962673	without increasing
0.0949765678	implementation of
0.0949245210	a complete
0.0949035867	decoder based
0.0948221830	large amount
0.0948014226	the effect of
0.0947408800	the second one
0.0947067269	time consuming and
0.0945490494	used to extract
0.0944185522	also investigate
0.0943888276	many applications
0.0943621605	the presence of
0.0941408556	performance in terms
0.0940451627	generative model for
0.0939352184	all other
0.0937834720	the importance of
0.0937360861	approach for
0.0936921105	further improvements
0.0936388957	a fast
0.0936210895	the context of
0.0935378670	the number of
0.0935233962	from eeg
0.0935093752	instead of
0.0935012694	\ relative
0.0934289366	collection of
0.0932096190	an accuracy of
0.0931150950	unsupervised acoustic
0.0930900793	a novel framework
0.0930282712	an acoustic
0.0930248444	more general
0.0930077361	evaluation of
0.0929616880	to exploit
0.0928839524	widely used in
0.0928297186	a variational autoencoder
0.0927974894	the model performance
0.0927284648	more effective
0.0925702091	auditory system
0.0925586001	used to build
0.0924505349	for deep speaker
0.0924326218	recorded from
0.0924231964	definition of
0.0923892049	optimized by
0.0922419100	the far field
0.0922233816	further improvement
0.0919159422	for monaural speech
0.0919011106	lies in
0.0918757817	many to many voice
0.0918743213	to localize
0.0918467819	based speech
0.0918080894	art models
0.0917766623	recognition using
0.0917414349	side information
0.0917370906	to mimic
0.0916692439	in contrast
0.0915568883	together with
0.0914400774	to increase
0.0913811337	a novel approach
0.0913119141	a well trained
0.0912790857	submitted to
0.0912296893	a combination of
0.0911564810	the impact of
0.0911477046	unsupervised speaker
0.0911039000	amount of data
0.0910663452	the experiment results
0.0910483374	the number of speakers
0.0910466966	the need for
0.0909546027	on par with
0.0908348841	the development of
0.0907174393	across different
0.0906978271	diversity of
0.0906689750	the effects of
0.0905694149	fraction of
0.0904538021	outputs from
0.0903268698	in particular
0.0903197704	a study on
0.0902736376	methods based on
0.0901875496	combinations of
0.0900309695	proven to
0.0899705834	determined by
0.0898686998	coherence of
0.0896882269	a few
0.0895696025	development of
0.0895614914	query by
0.0895509102	proposed multi
0.0894968363	the performance
0.0893843311	original speech
0.0893600528	other languages
0.0890013282	for acoustic scene
0.0889648059	to listen
0.0889178537	to infer
0.0888776674	the art accuracy
0.0888384996	learning framework for
0.0887575033	the first
0.0886323529	and sum
0.0886254958	run on
0.0884011584	a word error rate
0.0883895986	in word error rate
0.0882604385	reduction over
0.0882593747	features such as
0.0881891603	robust to
0.0880898217	an audio
0.0880658212	employed for
0.0879027438	eer on
0.0878902027	an optimal
0.0876071865	the microphone array
0.0876021486	also outperforms
0.0874925987	the audio signal
0.0874114775	the art baseline
0.0873830586	possibility to
0.0873826083	better results
0.0873376202	need to
0.0872357601	a real world
0.0871718017	real time speech
0.0871178725	or not
0.0870986213	used to predict
0.0870858598	to mitigate
0.0870790944	on par
0.0870731490	the edge
0.0870667143	sound sources in
0.0870621605	the potential of
0.0867538627	in contrast to
0.0866971797	a large number
0.0866579487	relative improvement of
0.0866198533	mixed with
0.0864582182	a latent space
0.0863685935	several different
0.0863491844	transcription system
0.0862341135	other approaches
0.0860894855	to alleviate
0.0859620825	for automatic speaker
0.0858923250	recorded by
0.0858683972	methods such as
0.0858160027	for multi modal
0.0857551464	overview of
0.0857518683	the utterance level
0.0857374777	\ beta
0.0856877414	baselines on
0.0856329775	different modalities
0.0855891206	approaches based on
0.0853867527	error rate of
0.0853218970	improved by
0.0853189262	the output
0.0852744043	an important role in
0.0851962050	novel approach
0.0851805831	baseline by
0.0851349663	generative models for
0.0851231340	the art deep
0.0851224299	named as
0.0849657008	a well known
0.0848386025	a non autoregressive
0.0848103792	the standard
0.0847990556	competitive with
0.0846629663	most important
0.0846611704	able to improve
0.0844385256	performed on
0.0844378976	based speaker verification
0.0844284136	the aim of
0.0844081336	the computational cost
0.0843895031	based approach for
0.0843630114	voice conversion with
0.0843318666	the proposed method significantly
0.0843229861	music source
0.0843183646	associated with
0.0841750986	song dataset
0.0840900723	two types of
0.0837877336	the same time
0.0837522308	applicability of
0.0836407484	a wide variety of
0.0835744818	the generated
0.0835743861	adopted as
0.0833582654	attributed to
0.0832301255	for robust
0.0832037544	the current state
0.0831978811	this work presents
0.0831910824	in comparison to
0.0831900094	each task
0.0829577993	at different
0.0829013350	listen to
0.0828901718	each individual
0.0828707337	tested with
0.0828458541	model with
0.0827321063	a number of
0.0826754186	audio only
0.0826049358	musical audio
0.0825038150	the baseline system
0.0825014576	elements of
0.0824771195	better quality
0.0824549379	the most common
0.0824151988	three datasets
0.0823927691	performance on various
0.0823885446	the art techniques
0.0823536318	a state of
0.0822322658	samples from
0.0822166469	a single channel
0.0821491930	fail to
0.0820736831	benefits from
0.0820048127	to maximize
0.0818728486	the word error rate
0.0818543308	various types of
0.0818368990	supervised speech
0.0818368990	speech conversion
0.0817991011	the whole
0.0817838233	representations from
0.0817103235	made available
0.0816752616	models for automatic
0.0816024286	attention mechanism in
0.0815893644	a survey
0.0815259139	tasks show
0.0814829821	the aforementioned
0.0813420723	the experimental results
0.0812067810	both modalities
0.0811856052	frequency representation of
0.0810177951	created by
0.0809960010	a single network
0.0809655903	emphasis on
0.0809385766	techniques based on
0.0809007966	a machine learning
0.0808822651	processed by
0.0808632818	a large number of
0.0807945077	the input data
0.0806873159	between speakers
0.0805624119	as much as
0.0804073866	further show
0.0803871050	various tasks
0.0802777732	words from
0.0802605921	compared to other
0.0802289468	as opposed to
0.0801060156	further propose
0.0800614067	addressed by
0.0800425185	the last
0.0798872107	same language
0.0798342133	a deep convolutional neural
0.0798300540	a detailed
0.0797602295	ways of
0.0796846354	obtained using
0.0796487961	than conventional
0.0796166246	a new algorithm
0.0796108664	a limited number of
0.0795362700	an entire
0.0795313762	output speech
0.0795139954	a frame level
0.0795099632	par with
0.0793431950	consists of two
0.0791923100	used in
0.0791712941	for predicting
0.0791676624	each component
0.0790647000	useful for
0.0790376962	vc system
0.0789964500	two datasets
0.0789778955	than real time on
0.0789337944	wer on
0.0787416272	group of
0.0787083586	in conjunction with
0.0786415132	an output
0.0784671076	theory of
0.0782437863	addressed in
0.0781777666	in fact
0.0781023822	an active
0.0781023062	by comparing
0.0780749315	evaluated by
0.0779840407	for far field
0.0779787156	possibility of
0.0778981714	other state of
0.0778613561	information between
0.0778130883	novel deep learning
0.0778052616	original audio
0.0777880050	in practice
0.0777836924	the long term
0.0777757741	the influence of
0.0776398080	obtained with
0.0775421087	the latent representation
0.0774520592	music using
0.0774176768	than other
0.0773301407	to encode
0.0773284235	respect to
0.0772593461	models for
0.0772405051	systems without
0.0771860220	the above
0.0770389711	computed from
0.0769383626	attention mechanism for
0.0768956329	at hand
0.0768789829	an audio visual
0.0767234638	degree of
0.0766896797	assessment of
0.0765751171	the voxceleb speaker
0.0764163525	audio segments
0.0764075211	sampling from
0.0763767054	an end to end model
0.0763435628	geometry of
0.0763428457	occur in
0.0763225276	impact of
0.0763193080	the learned representations
0.0761844564	the short time fourier
0.0760632732	the basis of
0.0760008275	fails to
0.0759723700	while achieving
0.0759327292	a hierarchical
0.0759244845	attention mechanism to
0.0758536159	improved speech
0.0758291845	estimated by
0.0757298919	accuracy of
0.0756639109	by more than
0.0756520281	allowing for
0.0756395434	detection based on
0.0754997832	to integrate
0.0754190013	at interspeech
0.0754108322	without relying on
0.0753964818	end to end deep
0.0753285455	systems use
0.0753254777	speech datasets
0.0753092666	locations of
0.0751538609	learning for audio
0.0750484487	in adverse
0.0750410998	to synthesize
0.0748964818	neural end to end
0.0748652454	for speech emotion
0.0746091841	the input text
0.0745147257	to preserve
0.0744265501	two major
0.0743907027	attempted to
0.0743833975	more natural
0.0743519918	and speaker similarity
0.0742548027	all three
0.0742288377	per speaker
0.0741636627	generated using
0.0740965800	the speaker characteristics
0.0740480029	the experimental results demonstrate
0.0739989014	the source code
0.0739617867	single system
0.0738688342	also allows
0.0736925174	the input speech
0.0736604044	this way
0.0735468227	the trade off between
0.0734699474	the test set
0.0734573312	most effective
0.0733822793	an encoder
0.0733207823	a relative improvement of
0.0728516899	much better
0.0728025275	a dual
0.0726793667	aspect of
0.0725235418	optimized for
0.0724738430	by adopting
0.0724057318	cope with
0.0721911587	the role of
0.0721371567	in domestic
0.0721202979	adapt to
0.0720280201	text to
0.0719953333	a large amount of
0.0719621414	neural networks for
0.0719315556	to emphasize
0.0719232968	then uses
0.0718976956	the possibility
0.0718318994	models based on
0.0718304019	the success of
0.0717283176	most state of
0.0717255224	different aspects of
0.0716805800	efficacy of
0.0716618099	by utilizing
0.0716137943	the generated speech
0.0715740525	for emotion recognition
0.0715728131	degrees of
0.0715599286	sounds from
0.0715579511	estimated from
0.0715477810	a trade off
0.0714722857	to support
0.0714598447	new state of
0.0713736583	novel framework
0.0713142994	an appropriate
0.0713110677	the equal error
0.0712970920	the evaluation set
0.0711865283	error rate on
0.0711340340	methodology for
0.0711058507	augmented with
0.0710854961	proposed system
0.0710516063	neural networks for speech
0.0710316908	a high quality
0.0710144508	a result
0.0710120937	learns from
0.0709068574	label classification
0.0708565885	this area
0.0708326921	to deploy
0.0707152634	review of
0.0707080634	the ability to
0.0706968252	a sound event detection
0.0705642924	word error rate of
0.0704215381	presence of
0.0704032203	this paper focuses on
0.0703550286	the conventional methods
0.0702276783	different layers
0.0702183269	this work aims
0.0701892180	robustness to
0.0701747968	designed for
0.0700576941	an intuitive
0.0699389245	two separate
0.0699186396	to reach
0.0698340599	in addition to
0.0698205884	two aspects
0.0695775300	a lot of
0.0695581871	features into
0.0694437904	interest in
0.0693268703	defined as
0.0693089370	a hybrid
0.0692812420	the input features
0.0692518920	better understanding of
0.0692504787	the first time
0.0691916977	experiments on two
0.0691839688	hundreds of
0.0691796258	a consequence
0.0691691853	heavily on
0.0691074403	conditioning on
0.0690305590	adversarial training for
0.0690248673	at inference time
0.0689875664	a novel method
0.0687701996	on two datasets
0.0686161377	deep neural networks for
0.0686077664	to implement
0.0685822972	research on
0.0685627545	both visual
0.0685265830	description of
0.0684467743	while still
0.0684250997	the presence or absence
0.0684202802	build on
0.0683326094	adversarial audio
0.0683247480	the low frequency
0.0682127762	to align
0.0681586815	in terms
0.0680986204	implemented by
0.0680043563	utilization of
0.0679211439	then applied
0.0678358026	most common
0.0678130386	extracted using
0.0678022297	not require
0.0677584333	the most
0.0677218961	performance in terms of
0.0677039196	the literature
0.0676926033	several state of
0.0676503389	by incorporating
0.0676221469	the notion of
0.0675436958	by using
0.0674559242	dependent on
0.0672528219	dictionary of
0.0671833171	in combination with
0.0671749361	recorded in
0.0670897777	methods in terms of
0.0670009925	under various
0.0669912643	generation using
0.0669385053	\ times
0.0669283547	to construct
0.0669006952	concept of
0.0667553430	advances in
0.0666376551	regardless of
0.0665503600	the art speech recognition
0.0665191314	to establish
0.0665069495	to assist
0.0665025388	then used
0.0664851690	a model trained
0.0663934211	defined by
0.0663659946	the art deep learning
0.0662620128	novel deep
0.0661970830	the direction of arrival
0.0661562738	even better
0.0661471958	absence of
0.0661366880	to collect
0.0661120427	toolkit for
0.0660718064	neural network based speech
0.0659074322	intended to
0.0658358721	first work
0.0657578293	a speech signal
0.0657204723	2017 challenge
0.0656320284	to apply
0.0655956515	an approach
0.0655591217	a factor of
0.0655058358	helps to
0.0654628915	model to
0.0654575962	introduced by
0.0654550028	the word error
0.0652152500	change in
0.0652055356	this report
0.0651771118	improvement of
0.0651665876	2018 challenge
0.0651419501	predicted by
0.0651142339	sound event detection in
0.0650344961	end training
0.0650344961	sequence speech recognition
0.0649512180	implemented on
0.0649497432	the number of sources
0.0649351336	this goal
0.0649146972	the front end
0.0648195205	to get
0.0647651018	to use
0.0647360625	a general
0.0647352388	studies on
0.0645762562	a simple yet
0.0645648656	a self supervised
0.0644638248	as good
0.0644447255	to incorporate
0.0643477956	from raw
0.0642913882	audio frames
0.0642474656	different tasks
0.0642427039	the training of
0.0642419830	signals from
0.0642315928	the difference between
0.0642016210	an external
0.0641502926	a target speaker
0.0640038150	the previous state of
0.0639026434	the x vector
0.0638396680	suited for
0.0637558358	consistent with
0.0636916813	an attention
0.0636309062	method for speech
0.0635659572	convolutional neural networks for
0.0635511089	weights for
0.0634133776	a song
0.0633894913	an initial
0.0633165721	trained from
0.0632845903	a review of
0.0632416647	a popular
0.0631818021	both approaches
0.0630866091	place of
0.0630702354	paradigm for
0.0630701363	in music information
0.0630618861	an unseen
0.0630235625	each sound
0.0629397544	result in
0.0628852945	by analyzing
0.0628243708	also introduce
0.0627644978	in response to
0.0627443572	the degree of
0.0627394601	fidelity audio
0.0625867926	training data from
0.0625091852	the overall
0.0624959976	for model training
0.0624648206	proposed end to end
0.0624236363	boundaries of
0.0621823357	2018 task
0.0621182797	an existing
0.0620704135	at inference
0.0619872550	an interesting
0.0619760375	four different
0.0619627345	information at
0.0619290029	system uses
0.0619023237	many different
0.0618953505	diagnosis of
0.0617839015	an autoregressive
0.0617552326	contribution of
0.0617404367	the art results for
0.0617260363	different levels of
0.0617128653	operates on
0.0616921295	under different
0.0616898391	best system
0.0616678977	family of
0.0616190803	the objective function
0.0616138379	performance in
0.0615663837	lack of
0.0615338563	the latter
0.0614842545	evaluations on
0.0614647465	the feature extraction
0.0614419390	an enhanced
0.0614148506	or absence of
0.0614091982	a loss function
0.0613595323	and slot
0.0612885083	not available
0.0612786839	variants of
0.0612088281	with varying
0.0611990003	present study
0.0611935539	the possibility of
0.0611272003	many other
0.0611095323	to speak
0.0610111065	sources from
0.0609599723	learning task
0.0609385053	many cases
0.0608712738	an automatic speech recognition
0.0607845706	come from
0.0607463138	encoded in
0.0605897845	area of
0.0605280857	models for speech
0.0604924450	not seen
0.0604329803	most relevant
0.0604080469	two tasks
0.0602707079	people with
0.0601667903	detection and classification of
0.0599759344	a primary
0.0599249539	added to
0.0598960077	minutes of
0.0598667773	music audio
0.0598481370	metric learning for
0.0597564169	novel technique
0.0597563134	conversion system
0.0597274914	occurrence of
0.0597092753	effect on
0.0597017347	in comparison with
0.0597007724	search for
0.0596177822	tens of
0.0595898859	the same speaker
0.0595830192	as inputs
0.0595042102	even more
0.0594924156	system consists of
0.0594281855	the learned
0.0594158182	encountered in
0.0594012180	gap by
0.0592471492	separation system
0.0592302684	to compress
0.0591674572	a typical
0.0591661790	used to learn
0.0590648753	two key
0.0590462349	gap in
0.0590250951	but not
0.0589686755	contains only
0.0589232350	a special
0.0588342160	for music generation
0.0588221332	to investigate
0.0588046889	also includes
0.0586485833	scenario with
0.0585853345	collections of
0.0585745063	a new approach to
0.0585179130	three types of
0.0585032758	measurement of
0.0584419390	also improves
0.0584413805	approach by
0.0584164094	stages of
0.0583986821	an asr system
0.0583432157	comes from
0.0583419650	recorded with
0.0582924374	models without
0.0582875472	available datasets
0.0582392405	automatic recognition
0.0581962906	a total of
0.0581552691	different methods
0.0581523975	to generalize to
0.0581474941	from raw audio
0.0581408938	the current state of
0.0581237306	novel end to end
0.0580427375	reduction on
0.0580424599	utilized in
0.0579841570	proposed network
0.0578149293	advantage of
0.0577288983	the air
0.0576013234	an ensemble of
0.0574374255	variation in
0.0574329803	better understanding
0.0574164094	topic of
0.0574094087	and then
0.0573941096	portion of
0.0573363355	an autoencoder
0.0572567015	five different
0.0571992347	this technique
0.0571342678	this strategy
0.0571311261	quality speech
0.0570762051	the three tasks
0.0570520911	the well known
0.0570412691	or even
0.0570199173	the former
0.0569370446	success in
0.0568538804	an objective
0.0568514147	seen during
0.0568489585	the art performance in
0.0568455234	the accuracy of
0.0568278445	acoustic models for
0.0568276598	influence of
0.0567999128	the absence of
0.0567966649	for end to end asr
0.0567897124	a mixture of
0.0567721350	an analysis of
0.0567597058	propose to
0.0567522736	acoustic model for
0.0567378957	architecture for
0.0566638699	the baseline systems
0.0565026385	the back end
0.0564064056	in many applications
0.0563999418	the art end to end
0.0563372124	a generalized
0.0563197255	a crucial
0.0562431326	a robust
0.0561856940	utility of
0.0561049219	data augmentation for
0.0560199275	important for
0.0560176519	to follow
0.0559917268	most popular
0.0559900745	the separation performance
0.0559852597	the converted
0.0559667903	networks for audio
0.0559660225	by replacing
0.0559389643	solely on
0.0559338645	acoustic word
0.0558877992	for audio event
0.0558291854	also explore
0.0557889165	source separation with
0.0557046698	task learning
0.0556998721	both objective
0.0556930997	applied on
0.0556657385	dataset of
0.0556463268	contributes to
0.0556247240	differences in
0.0556170271	aiming to
0.0555561863	directly on
0.0554821723	role of
0.0554697263	extension of
0.0554608071	the distance between
0.0554580634	a method for
0.0553961801	by integrating
0.0553635357	a unique
0.0553382557	to attend
0.0552308177	fields of
0.0551747050	to speech
0.0551650765	pairs of
0.0551455421	generalize to
0.0551008349	to match
0.0550642372	content from
0.0550309265	structure of
0.0549635083	based approach to
0.0548204338	required for
0.0547936511	the number of parameters
0.0546667438	aligned with
0.0545488656	system achieved
0.0545032758	seconds of
0.0544879440	novel architecture
0.0544844459	a multi label
0.0544247136	an adaptation
0.0544191111	a joint
0.0543961214	time domain speech
0.0543750839	adopted in
0.0542833442	a target
0.0541110239	a pair of
0.0540483574	a speaker embedding
0.0540254769	evolution of
0.0540105281	needs to
0.0539931326	a high
0.0539168036	limitation of
0.0539051071	the enhanced speech
0.0538680854	an estimate of
0.0537697418	the receiver
0.0537032758	details of
0.0536641613	an increase
0.0536335988	idea of
0.0536135555	a mixture
0.0535892090	by performing
0.0535530501	conventional speech
0.0534954820	methods for
0.0534823431	different architectures
0.0534772307	the teacher
0.0534348536	gains in
0.0533880672	tools for
0.0533152753	level speech
0.0532557756	networks for
0.0532512582	approximation of
0.0531967160	the target speech
0.0531291201	the asvspoof 2019
0.0531138285	supervised learning for
0.0529821281	unsupervised learning of
0.0529452680	a comparison of
0.0528463142	a reference
0.0528236458	self attention for
0.0527570060	at scale
0.0526281974	in depth
0.0525674572	a major
0.0525291229	do so
0.0524137909	an absolute
0.0524137909	by humans
0.0523595323	of western
0.0523229581	an alternative to
0.0521574254	to cope with
0.0520969836	form of
0.0520547501	from youtube
0.0520518697	the predicted
0.0520333482	employed to
0.0520063593	generalizes to
0.0520054169	an issue
0.0519112301	done by
0.0519055844	the proposed loss
0.0518858411	a range of
0.0518827375	available data
0.0518629488	of acoustic scenes and events
0.0518539008	novel method
0.0518506692	by explicitly
0.0518330531	the training
0.0517981807	trained language
0.0517603620	deployed in
0.0517548954	choice for
0.0517280588	for detecting
0.0517260704	usefulness of
0.0516733172	the model parameters
0.0516344072	contributions of
0.0515966449	an accurate
0.0515657344	the applicability of
0.0515286218	utilized to
0.0515265214	operate in
0.0514548275	results compared to
0.0514307373	a new approach
0.0513305301	referred to
0.0513206378	validated on
0.0513167360	introduction of
0.0512218690	to utilize
0.0511873276	by optimizing
0.0511758870	progress in
0.0511147109	new architecture
0.0510509897	more suitable for
0.0509451804	a static
0.0508992483	the rise
0.0508692627	series of
0.0508496657	the art results in
0.0506994602	the trade off
0.0506040983	performance of speaker
0.0505859067	demand for
0.0505706434	to carry
0.0504989564	input audio
0.0504833857	to form
0.0504741006	activities of
0.0504363922	a powerful
0.0504103606	system without
0.0503891124	a sequence to sequence
0.0502613804	needed to
0.0502496983	constraints on
0.0502396159	task of
0.0502248198	the corresponding
0.0501213519	exploration of
0.0501185094	capability to
0.0500463506	the art speaker
0.0500431881	requirement for
0.0500289365	method to
0.0500223201	for generating
0.0500132821	locations in
0.0500114027	operating on
0.0499597262	reduced by
0.0498507528	algorithms for
0.0498388998	a neural
0.0498259568	this gap
0.0498240886	the idea of
0.0498240886	the area of
0.0498197255	to distinguish
0.0498160130	a time domain
0.0497962349	implemented in
0.0497605833	an experiment
0.0497562687	principles of
0.0496994085	combination with
0.0496749316	number of training
0.0496242434	then used to
0.0495956845	learning architecture
0.0495937908	a time frequency
0.0495291431	a good
0.0495248053	dependencies in
0.0495131782	for instance
0.0494905565	library for
0.0494326317	a straightforward
0.0493772008	the second
0.0493417762	investigation of
0.0493417762	capability of
0.0492601635	superiority of
0.0492489480	in isolation
0.0492270273	each time
0.0491333116	by providing
0.0490585518	over state of
0.0490544658	for audio source
0.0489538150	the superiority of
0.0489201984	application to
0.0489100955	out of
0.0488862215	by generating
0.0488725210	compensate for
0.0486922277	new method
0.0486794658	the amount of training
0.0486693220	a small amount of
0.0486343186	on timit
0.0485508279	after training
0.0485305644	also found
0.0484995637	cer of
0.0484683093	point of
0.0484585233	core of
0.0484314450	to generalize
0.0483738922	an overview of
0.0483093705	the benefits of
0.0482825732	the other
0.0482092356	assumed to
0.0481191855	discussed in
0.0480960602	study on
0.0480406086	estimation using
0.0480278712	made by
0.0480269074	detection of
0.0479794178	a single neural
0.0479125543	tends to
0.0478898713	the task
0.0478857210	to allow
0.0478448623	extended to
0.0477892913	component of
0.0477698101	probabilities of
0.0477044671	end to end audio
0.0476829255	for developing
0.0476665846	sequence to
0.0476617482	the shape of
0.0476602587	as part of
0.0476301364	developments in
0.0476237238	contrast to
0.0475961816	to validate
0.0475575588	numbers of
0.0475327134	sense of
0.0475247872	pair of
0.0474911424	a new method for
0.0474818878	an extension of
0.0474675611	a novel multi
0.0474654363	a continuous
0.0474630401	basis for
0.0474023388	question of
0.0473094418	signal to
0.0472954888	more suitable
0.0472845903	the existence of
0.0472263373	the art on
0.0471894744	sets of
0.0471394817	same speaker
0.0470993779	the proposed multi
0.0470093705	a subset of
0.0470093705	the validity of
0.0469790629	to discriminate
0.0469351785	a transfer learning
0.0469214850	a large amount
0.0468742741	expected to
0.0468685230	the perceptual quality
0.0468341097	models such as
0.0466289786	increase in
0.0465827452	interpretability of
0.0465755616	strategy for
0.0465539382	the two tasks
0.0465472091	a newly
0.0465259681	a user
0.0464727081	aimed to
0.0464444418	different levels
0.0464120927	dedicated to
0.0463941163	to examine
0.0463166757	different types
0.0462647751	orders of
0.0462473397	developed for
0.0461386097	by taking
0.0460327134	generalizability of
0.0459637479	run in
0.0458139665	to compensate for
0.0457703477	superior to
0.0457571778	an algorithm
0.0457562687	flexibility of
0.0457541126	to help
0.0457284112	audio recording
0.0456341613	by reducing
0.0456129515	trained by
0.0456087662	the usefulness of
0.0455713346	a novel deep
0.0455356660	the art method
0.0455306105	various types
0.0455195194	reported in
0.0454763834	a novel method for
0.0454742741	crucial for
0.0454164277	first time
0.0453682517	location of
0.0453549954	dependency of
0.0453103220	network architecture for
0.0453093705	a new state of
0.0452821966	the time frequency
0.0449114465	input to
0.0448994086	interpretation of
0.0448881779	an improvement of
0.0448195043	view of
0.0447928914	by conditioning
0.0447448373	to update
0.0447306820	the utility of
0.0446367323	to boost
0.0446354451	reduction of
0.0445756994	converted to
0.0445687025	by estimating
0.0445673068	a lightweight
0.0445503938	different from
0.0445039352	validity of
0.0444635352	advantages of
0.0443426947	order to
0.0443212542	channel audio
0.0443203737	an emotional
0.0441644989	restricted to
0.0441224416	deployment of
0.0441048760	to fine
0.0440750056	application of
0.0439920352	an integrated
0.0439435538	a method
0.0439360992	an approach to
0.0439039068	comparing with
0.0438454374	system for
0.0437770990	to reproduce
0.0436836494	attempts to
0.0435468332	property of
0.0435093705	the feasibility of
0.0434993645	reduction in
0.0434863922	to derive
0.0434538150	a database of
0.0434008349	to verify
0.0433952391	same time
0.0433594185	improvements in
0.0433550168	proved to
0.0433203737	an e2e
0.0433149638	variant of
0.0433105196	mixtures with
0.0432784664	an f1
0.0431700201	to speech synthesis
0.0430311584	choice of
0.0429771476	needed for
0.0429338363	a recurrent neural
0.0429333142	new dataset
0.0428803169	comparison of
0.0427599930	for audio source separation
0.0427183064	to quantify
0.0426887987	an english
0.0424694438	a variant of
0.0424159490	characteristic of
0.0424116013	capacity of
0.0424019799	to remove
0.0423803306	the presence or
0.0422475499	extractor for
0.0421939759	to compensate
0.0421110239	the difference in
0.0420924957	problem of
0.0420712218	a word error
0.0420689733	a broad
0.0420524755	use of
0.0419762550	direction of
0.0419738899	involved in
0.0419724400	two novel
0.0418889697	integration of
0.0418566354	for improving
0.0418514616	decoding with
0.0418276159	trained to
0.0417884808	equal to
0.0417665320	decomposition of
0.0417561867	novel method to
0.0417243179	pipeline for
0.0416983589	a machine
0.0416511155	the need of
0.0416212418	a person
0.0415999687	work aims
0.0415625847	categories of
0.0415593705	the ability of
0.0415231222	speaker voice
0.0414818486	algorithm for
0.0414396279	a multi speaker
0.0413978404	by up to
0.0413568332	critical for
0.0413567698	the actual
0.0413454269	the potential
0.0412986739	estimation of
0.0412673023	existence of
0.0412271197	two types
0.0411879049	system based on
0.0411236840	advancements in
0.0410577866	step for
0.0410017890	account for
0.0409944135	end to end multi
0.0408841420	reliability of
0.0408738061	an approach for
0.0408634365	different acoustic
0.0408317719	a generic
0.0407726299	the latent
0.0407175328	to explore
0.0406779952	to run
0.0405231222	based feature
0.0404974743	solution for
0.0404137909	an explicit
0.0404074617	the art speech
0.0404032506	hard to
0.0403701284	and thus
0.0403099519	contribution to
0.0402774399	separation using
0.0402254169	this project
0.0402059829	required to
0.0401876513	conducted with
0.0399870404	an individual
0.0399388090	score of
0.0399271221	importance of
0.0399219182	the creation of
0.0399202971	synthesis using
0.0398698869	a novel approach for
0.0397941163	to approximate
0.0397207026	work presents
0.0397095963	to control
0.0396739221	popularity of
0.0396738640	difficulty of
0.0396386432	benefit of
0.0396228021	issue in
0.0395989029	strategies for
0.0395969358	a clear
0.0395093705	the choice of
0.0394638678	take into
0.0394230963	a modified
0.0394198562	preservation of
0.0394074617	the art model
0.0393748488	different aspects
0.0392841130	a new speaker
0.0392614745	perception of
0.0391457390	variation of
0.0391211325	a self attention
0.0391203120	room for
0.0390997176	the amount of
0.0390101275	the past
0.0389834459	helpful for
0.0389404367	a technique for
0.0389074617	an end to end speech
0.0388699649	a novel method to
0.0388427039	a dataset of
0.0387527002	stack of
0.0386863545	the core
0.0386092669	behavior of
0.0385118625	a closed
0.0385033320	information in
0.0384923049	a considerable
0.0384724020	two modalities
0.0384538150	the difficulty of
0.0384395743	techniques for
0.0382574654	comparing to
0.0382070845	represented in
0.0381916652	variability in
0.0381683641	to highlight
0.0381532614	employed in
0.0379839319	an automatic speech
0.0379761126	capabilities of
0.0379225265	shape of
0.0379219358	a substantial
0.0379013865	to account for
0.0378034032	studied in
0.0376761213	creation of
0.0374828969	to yield
0.0374825355	from unlabeled
0.0374410511	learning approach to
0.0373600214	the task of speech
0.0373494786	to transcribe
0.0373173068	the learnt
0.0372512912	the same model
0.0371952680	the contribution of
0.0371334030	the commonly used
0.0369955439	forms of
0.0368924017	effective in
0.0368791720	a convolutional neural
0.0368786601	to encourage
0.0368752824	equivalent to
0.0368673532	probability of
0.0368653531	the base
0.0368256721	modification of
0.0368176969	for evaluating
0.0367768871	a dataset
0.0366675667	by maximizing
0.0366403647	the application of
0.0366312309	included in
0.0365594564	the superiority
0.0365529021	an example
0.0365120516	the art in
0.0363862674	investigation on
0.0363545918	without using
0.0363213426	understanding of
0.0362487501	observed in
0.0362455650	a quantitative
0.0360956028	mixture of
0.0360926727	means to
0.0360429915	the first work
0.0360289645	functions for
0.0359618356	adopted to
0.0359272045	value of
0.0358841420	construction of
0.0358793909	a stacked
0.0358743179	baselines for
0.0357385849	the introduction of
0.0356248488	most cases
0.0355897235	with respect
0.0355606243	as much
0.0355055244	sum of
0.0354691038	the subsequent
0.0354249294	self attention to
0.0353961761	especially in
0.0352400120	the receptive
0.0352372342	to intent
0.0352319623	the perceived
0.0351169374	the potential to
0.0350935009	effect in
0.0350486458	the correlation between
0.0350157745	provided for
0.0349900699	position of
0.0349321955	the best performing
0.0348862111	usage of
0.0348854694	for creating
0.0348627133	collected in
0.0348158688	from background
0.0347315928	a method of
0.0346774395	solution to
0.0346753607	essential for
0.0346671946	work investigates
0.0346562633	an extension
0.0345976456	a group of
0.0345594564	to add
0.0345454294	the dcase 2019
0.0344394623	to aid
0.0344360469	system using
0.0344317410	likelihood of
0.0343381160	the detection of
0.0343227032	list of
0.0342692057	the strength of
0.0342547431	distribution of
0.0342323381	spectrogram as
0.0341600622	the official
0.0341535195	trends in
0.0341009865	limitations of
0.0340441404	of thousands of
0.0335336040	a collection of
0.0334880139	to extend
0.0334651905	spectra of
0.0334074200	an overall
0.0333192067	presented in
0.0333141358	technique for
0.0332162909	essential to
0.0331952680	the concept of
0.0331158308	phonemes in
0.0330761624	a black
0.0330683093	evaluated with
0.0330664066	introduced to
0.0330384145	method does
0.0330232661	try to
0.0329910322	dataset for
0.0329440528	to simulate
0.0329328395	the problem
0.0327313864	degradation in
0.0327275769	alternative to
0.0326023266	aid in
0.0325112464	for depression
0.0324669859	effective than
0.0324415961	both objective and
0.0324056731	to recover
0.0322998853	a flexible
0.0322237269	research in
0.0321806485	monitoring of
0.0321574584	conversion using
0.0321431401	the first one
0.0320876716	learns to
0.0320325605	the distribution of
0.0319262368	trained using
0.0319041597	the sequence to
0.0319015635	enough to
0.0318819703	an analysis
0.0318433486	the presence
0.0318119859	necessary to
0.0317419100	a pretrained
0.0316559596	difficult for
0.0316446383	the art audio
0.0315431751	amount of training
0.0315360513	network for
0.0315008244	to measure
0.0314963580	to state of
0.0313913626	the input audio
0.0313912746	versions of
0.0313901632	the detection and classification of
0.0313096879	beneficial for
0.0313032215	to disentangle
0.0312627537	eer of
0.0312038803	different time
0.0311282318	voices with
0.0311127133	metric for
0.0310903125	the model on
0.0310903125	the model to
0.0310246125	a novel approach to
0.0310196059	with minimal
0.0309723087	the internet
0.0308728287	potential for
0.0308478732	the gap between
0.0308398853	a compact
0.0308381818	way to
0.0308034492	an ablation
0.0307282408	topic in
0.0307100602	issues in
0.0306388983	to gain
0.0305343181	to fuse
0.0304930902	the nature of
0.0304790690	new training
0.0302676542	a text to
0.0301855164	to resolve
0.0301796107	with skip
0.0300740686	an unknown
0.0299702528	a statistical
0.0299675789	quality of
0.0297706428	need for
0.0297632853	to augment
0.0296325958	a siamese
0.0295605196	benchmark for
0.0295486458	the integration of
0.0295239593	a variant
0.0294552565	explored in
0.0294459838	the best
0.0294400757	achieved with
0.0294200290	introduced in
0.0294187490	a semi
0.0294098485	a system for
0.0293930924	the same data
0.0293548268	measure for
0.0292941404	the reliability of
0.0292412742	to bring
0.0292377964	robustness of
0.0292198099	a result of
0.0291090650	degradation of
0.0291017932	similarly to
0.0290939687	a variety
0.0290482594	the form of
0.0290342463	the analysis of
0.0290297409	the naturalness of
0.0289615633	size of
0.0289519630	make use
0.0288991509	goal of
0.0288919804	top of
0.0288718462	the size of
0.0288703106	to design
0.0288409239	by considering
0.0288041040	to fool
0.0287522811	success of
0.0286709803	crucial to
0.0286487529	labels from
0.0286419905	the dominant
0.0285585156	a representative
0.0285464628	power of
0.0285220435	to replace
0.0285220435	to characterize
0.0284877540	done in
0.0284406658	the number
0.0283726765	ability of
0.0282761056	far from
0.0282741937	a rich
0.0282300602	tested in
0.0281164460	seen as
0.0280903125	the input of
0.0280054570	obtained on
0.0277774992	addition to
0.0277593705	the benefit of
0.0275621413	to discover
0.0275584722	strategy to
0.0275522839	a low
0.0275444945	for measuring
0.0274926317	a custom
0.0273877244	to bridge
0.0273668445	for estimating
0.0273285871	used by
0.0272435110	a great
0.0271019460	effect of
0.0270902292	new approach
0.0270288475	classification based on
0.0269613923	fusion of
0.0269296107	this type
0.0267996059	to select
0.0266902400	framework with
0.0265422241	evaluated for
0.0264930902	the availability of
0.0264703476	to keep
0.0263834383	embeddings for
0.0263593095	for extracting
0.0263439700	dynamic time
0.0263258137	a big
0.0261000497	to find
0.0260888945	the system's
0.0260472011	off between
0.0260336040	a fusion of
0.0259511445	a certain
0.0259482097	the short time
0.0258307654	to translate
0.0257340503	changes in
0.0257184038	a cycle
0.0256278121	fusion with
0.0255621413	a theoretical
0.0255545526	allows for
0.0254930902	the raw audio
0.0254780485	the diversity of
0.0254729315	this problem by
0.0254552565	scenarios with
0.0254135255	result on
0.0253844539	a probabilistic
0.0253369055	of things
0.0252593705	the majority of
0.0252154136	effects of
0.0251549923	potential of
0.0249927355	means of
0.0249911562	the respective
0.0249876947	effective for
0.0249623013	to suppress
0.0249109130	the combination of
0.0248958839	the mismatch between
0.0248359090	trained for
0.0248201211	a method to
0.0248181411	architectures for
0.0248069990	considered in
0.0247710781	augmentation with
0.0247494740	the interaural
0.0247424750	the impact
0.0245646576	the effectiveness
0.0245546107	2019 task
0.0245081709	new method for
0.0244930902	the estimation of
0.0244766131	in capturing
0.0244703476	want to
0.0244389938	database of
0.0244341733	architecture with
0.0244275990	to illustrate
0.0243312247	a proxy
0.0242439900	a 2d
0.0242227591	to prevent
0.0241785805	the feasibility
0.0241604475	to give
0.0241575605	the advantages of
0.0241430668	applied for
0.0240480305	the synthesized
0.0240431751	the use of deep
0.0238237096	ensemble of
0.0237318231	a systematic
0.0236317321	to text
0.0235602072	to interference
0.0235486458	and tested on
0.0235348485	the first to
0.0235208680	a type of
0.0235187908	a generative
0.0234494740	in turn
0.0234389938	field of
0.0233831352	a framework for
0.0233735953	a reliable
0.0233618049	code for
0.0233504541	supervision for
0.0233147833	the performance of speaker
0.0232632853	to maintain
0.0232599080	for building
0.0232451999	to take
0.0232129824	evaluated in
0.0231859085	benefits of
0.0231839387	to calculate
0.0231839387	to answer
0.0230904136	case of
0.0230594564	a piece
0.0230501939	new approach to
0.0230061153	efficiency of
0.0229705058	models do
0.0228979910	the goal
0.0228678729	module to
0.0227835464	the usage of
0.0227475877	to focus on
0.0227178862	tries to
0.0226418373	scheme for
0.0224780485	a lack of
0.0224024914	a universal
0.0222732503	naturalness of
0.0222058857	a challenging
0.0220486458	the power of
0.0220040119	work on
0.0218377992	the ground
0.0218044728	especially for
0.0217756692	a simplified
0.0215743463	the development
0.0215405299	a sequence
0.0215087334	autoencoders for
0.0214567905	an auto
0.0212769120	with up to
0.0211951253	performed in
0.0210810846	to learn from
0.0209473928	precision of
0.0208684712	the direction of
0.0208201211	the fusion of
0.0207587334	strength of
0.0205071238	of up to
0.0204808680	the complexity of
0.0204274017	the behavior of
0.0204266880	the degree
0.0203449105	in detail
0.0200104665	an average of
0.0200031120	measure of
0.0199104475	allow for
0.0198377992	to focus
0.0197184038	a connectionist
0.0196524914	a public
0.0196442890	appropriate for
0.0195449000	a naive
0.0195335223	generation by
0.0194930902	the structure of
0.0194078576	an improvement
0.0192933547	to clean
0.0192808724	sufficient to
0.0192385849	the probability of
0.0192130512	a dictionary
0.0191789681	found to
0.0191527936	issue by
0.0191257093	a baseline system
0.0190150654	a regression
0.0190134301	popular in
0.0189877992	to sequence
0.0189838093	the input to
0.0189631160	the advantage of
0.0188711325	the importance
0.0188588093	the assessment of
0.0188165867	vector system
0.0187819511	problem by
0.0186992211	framework by
0.0186101159	factor of
0.0185187908	a real
0.0185094539	a weighted
0.0182675617	design of
0.0182073595	to noise
0.0181614236	the efficiency of
0.0180976103	a way to
0.0180565588	complexity of
0.0179987110	a gated
0.0178625265	distillation for
0.0178372342	a group
0.0176413277	a non
0.0175665706	of research in
0.0175594564	the flexibility
0.0174421665	useful in
0.0168365859	to outperform
0.0167894847	function based on
0.0167253776	the user's
0.0165433522	a realistic
0.0164505812	demonstrated to
0.0164049975	aim of
0.0163909530	optimized to
0.0163398764	a sequence to
0.0159275541	a tool
0.0158541132	likely to
0.0158528403	a form of
0.0157895597	recognition via
0.0153842989	piece of
0.0153398764	the signal to
0.0153372342	the efficacy
0.0153270223	developed in
0.0152887377	this technical
0.0152739593	the applicability
0.0151565689	seen in
0.0151447922	procedure to
0.0150021665	to describe
0.0148861148	enhancement based on
0.0148240650	expensive to
0.0147203336	a subset
0.0146594125	the need to
0.0144924750	a multilingual
0.0144114236	the limitations of
0.0142188351	mechanism for
0.0140296761	and hence
0.0140068981	mechanism to
0.0139987110	to include
0.0138979910	the idea
0.0138962350	to speed
0.0128927897	an estimate
0.0128927897	to employ
0.0128670793	internet of
0.0125100188	a variational
0.0121685373	even with
0.0120037544	found in
0.0118927897	a bi
0.0114526326	the capability of
0.0113355261	a recurrent
0.0111578267	to conduct
0.0104240415	a full
0.0104103336	a total
0.0101197112	response to
0.0100315783	made in
0.0098670793	left to
0.0094550214	to do
0.0088629017	a set
0.0080845523	help to
0.0078185207	key to
0.0072815783	to consider
