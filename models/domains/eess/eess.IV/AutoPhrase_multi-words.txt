0.9790340563	discrete cosine transform
0.9778242941	positron emission tomography
0.9754915128	augmented reality
0.9737467151	unmanned aerial vehicles
0.9725823189	virtual reality
0.9715987163	alzheimer's disease
0.9714257693	hausdorff distance
0.9712461680	remote sensing
0.9705079351	colorectal cancer
0.9699203612	optical coherence tomography
0.9698994232	left ventricle
0.9697406147	electron microscopy
0.9697228251	parkinson's disease
0.9687426234	synthetic aperture radar
0.9685170772	fluorescence microscopy
0.9684806972	diabetic retinopathy
0.9683051255	random forest
0.9680829314	compressive sensing
0.9676433053	autonomous driving
0.9675269880	kalman filter
0.9656915574	artificial intelligence
0.9656132778	computed tomography
0.9653279941	compressed sensing
0.9651684665	optic disc
0.9651643035	breast cancer
0.9651091234	euclidean distance
0.9649640948	polymerase chain reaction
0.9645505273	jaccard index
0.9635436839	question answering
0.9633122484	sheet music
0.9631077201	logistic regression
0.9629753964	mobile phones
0.9626249273	anomaly detection
0.9625251149	coronary artery
0.9623842204	myocardial infarction
0.9618306548	chest radiographs
0.9617708811	social media
0.9614748135	left atrium
0.9611342774	optic nerve
0.9609222204	traumatic brain injury
0.9607803894	maximum likelihood
0.9607613676	point cloud
0.9606870070	quantitative susceptibility mapping
0.9604209501	nuclear norm
0.9596157224	capsule endoscopy
0.9595795347	support vector machine
0.9593028941	satellite imagery
0.9592211159	land cover
0.9590473583	ischemic stroke
0.9589094275	histogram equalization
0.9588383186	mild cognitive impairment
0.9579765217	gradient descent
0.9579208238	densely connected
0.9579089478	coronary arteries
0.9572369598	chest radiography
0.9568488416	bit allocation
0.9565654801	receptive field
0.9564591990	autonomous vehicles
0.9564107763	gray matter
0.9562564299	adipose tissue
0.9559630442	correlation coefficient
0.9558029152	earth observation
0.9555327858	acoustic impedance
0.9554178991	viral pneumonia
0.9552252918	magnetic resonance fingerprinting
0.9551104579	domain adaptation
0.9546483847	facial expression
0.9543248860	monte carlo
0.9543157652	metal artifact reduction
0.9542879877	point clouds
0.9541297641	total variation
0.9539052705	digital breast tomosynthesis
0.9538667281	radiation therapy
0.9536688811	spinal cord
0.9536602374	pulmonary nodules
0.9535553076	multiple sclerosis
0.9535406598	hepatocellular carcinoma
0.9528431478	white matter
0.9526075790	wavelet transform
0.9525061694	red blood cells
0.9519574130	variational autoencoders
0.9519245010	bundle adjustment
0.9518883432	variational autoencoder
0.9518411779	machine learning
0.9514779409	social distancing
0.9514310482	receiver operating characteristic
0.9509817359	dice similarity coefficient
0.9508496440	electrical impedance
0.9508189015	world health organization
0.9508065596	cohen's kappa
0.9507617215	lung cancer
0.9506939192	prostate cancer
0.9506451448	optical flow
0.9500691276	dipole inversion
0.9496085979	hough transform
0.9493368081	skin lesion
0.9492640757	gene expression
0.9491656627	minimally invasive
0.9485719114	climate change
0.9485562951	principal component analysis
0.9481673649	motion compensation
0.9481560181	persistence diagrams
0.9481310639	ductal adenocarcinoma
0.9481188436	lung nodules
0.9480217915	magnetic resonance
0.9476560053	raspberry pi
0.9475300304	dimensionality reduction
0.9472785349	autism spectrum disorder
0.9472688792	remotely sensed
0.9472474401	knowledge distillation
0.9469118404	coded aperture
0.9464208347	point spread function
0.9459062438	acute ischemic stroke
0.9458695163	magnetic resonance imaging
0.9454386260	project page
0.9449121088	fourier ptychographic microscopy
0.9448102308	false alarm
0.9447878252	sparsifying transform
0.9447346036	active contour
0.9444471284	support vector machines
0.9439992015	phase retrieval
0.9438767448	arterial spin labeling
0.9435763792	rain removal
0.9430850771	ejection fraction
0.9430482580	eye gaze
0.9426467649	pascal voc
0.9422847093	style transfer
0.9417112303	tone mapping
0.9416464427	reinforcement learning
0.9416364035	heart rate
0.9414350428	nutrient intake
0.9412166142	lower bound
0.9411737516	monocular depth estimation
0.9410171828	skip connection
0.9409987164	attenuated inversion recovery
0.9405395271	mitral valve
0.9405309417	receptive fields
0.9401333124	dice coefficient
0.9389843099	gastric cancer
0.9389488438	open access
0.9386068361	fourier transform
0.9380754345	breast masses
0.9377615690	persistent homology
0.9376301112	augmented lagrangian
0.9371671718	dilated convolutions
0.9370702127	fourier ptychography
0.9369883734	feature extraction
0.9368888449	radiology reports
0.9366104679	disease neuroimaging initiative
0.9354994490	neural networks
0.9354291858	radio frequency
0.9352553506	ultrasound elastography
0.9351612075	nonnegative matrix factorization
0.9351509035	knee osteoarthritis
0.9348535662	blood vessels
0.9347830014	proximal femur
0.9345080205	unmanned aerial vehicle
0.9344381156	light field
0.9340934813	neural network
0.9337475020	mode collapse
0.9337382041	densely sampled
0.9336204798	conditional random field
0.9335310582	radon transform
0.9334960777	google earth
0.9334688882	lossless compression
0.9333598452	ct scans
0.9333180502	electric field
0.9328139262	ductal carcinoma
0.9326223835	cranial implant
0.9324236215	emotion recognition
0.9321179049	genome atlas
0.9318265954	celiac disease
0.9316094243	markov random field
0.9313385462	batch normalization
0.9312928233	inverse problems
0.9312171648	skip connections
0.9310597035	blood flow
0.9310487401	ghost imaging
0.9310181409	adversarial examples
0.9309934992	macular edema
0.9307041273	radiation dose
0.9306143084	structural similarity index
0.9302545378	simulated annealing
0.9301607111	floating point
0.9300839927	steady state
0.9299550881	community acquired pneumonia
0.9297092667	depthwise separable
0.9296162149	plant disease
0.9294869086	matrix factorization
0.9294221312	multimode fiber
0.9293562768	optic nerve head
0.9293188085	principal component
0.9289244739	optimal transport
0.9288454372	consensus equilibrium
0.9285562997	lge cmr
0.9284348271	dice score
0.9284165754	mobile devices
0.9283947865	porous media
0.9282834263	adversarial attacks
0.9281578062	mixed reality
0.9280589596	shear wave
0.9277889869	progressive growing
0.9275465929	bounding box
0.9274663481	diabetic macular edema
0.9274504044	thalamic nuclei
0.9273423848	bokeh effect
0.9264551390	attention mechanism
0.9262662642	weather conditions
0.9262086420	chest radiograph
0.9261127477	decision tree
0.9260692945	energy compaction
0.9257345187	f1 score
0.9255200312	skin cancer
0.9254115566	nearest neighbors
0.9253772514	weakly supervised
0.9252809327	digital micromirror
0.9252380162	refractive index
0.9250484919	background subtraction
0.9248833310	ionizing radiation
0.9247116320	material decomposition
0.9232968498	coronary angiography
0.9231162104	rain streaks
0.9230890780	false positive
0.9225885549	spatial light modulators
0.9223814388	rain streak
0.9222622483	lung nodule
0.9219060896	cardiac cine
0.9218094741	quality control
0.9217872222	transfer learning
0.9213848691	false negatives
0.9213270763	cone beam
0.9213217697	class imbalance
0.9212967405	free breathing
0.9202130874	stain normalization
0.9201827252	action units
0.9201390069	deep learning
0.9200655596	versatile video coding
0.9198107795	latent space
0.9195226995	nearest neighbor
0.9194808610	blood vessel
0.9191212408	inception v3
0.9187452189	synthetic aperture
0.9186730028	generative adversarial networks
0.9186021439	spatial pyramid pooling
0.9184469343	conditional random fields
0.9182181397	change detection
0.9174127761	frame rate
0.9170473247	lossy compression
0.9166520193	age related macular degeneration
0.9163276152	reversible data hiding
0.9162494317	gold standard
0.9160036900	ad hoc
0.9159440215	genetic algorithm
0.9158738321	memory footprint
0.9158523839	primal dual
0.9156105787	facial expressions
0.9154726757	edge detection
0.9152144835	complex valued
0.9151693543	functional connectivity
0.9146844329	feature extractor
0.9145416575	fully connected
0.9142218092	privacy preserving
0.9141182956	particle swarm
0.9141081123	retinal vessels
0.9141073019	latent variable
0.9140683728	cortical bone
0.9139743849	visually pleasing
0.9136327629	late gadolinium
0.9135423947	frequency domain
0.9132084961	spatially varying
0.9128943940	high fidelity
0.9127932362	neural architecture search
0.9126663392	structural similarity
0.9124188107	sparsely sampled
0.9112931174	vessel wall
0.9108218800	confocal laser
0.9108200346	uncertainty quantification
0.9105819202	medical imaging
0.9105625806	lossy image compression
0.9105290943	laplacian pyramid
0.9103207988	neurological disorders
0.9100810683	lip reading
0.9100780854	rt pcr
0.9096612988	diffusion mri
0.9094232047	deformable image registration
0.9091226559	presentation attack
0.9089160301	urban areas
0.9088958219	action recognition
0.9087065707	data augmentation
0.9086081026	pearson correlation
0.9085203886	spectral unmixing
0.9082018134	` `
0.9079660438	breast mass
0.9079538102	ct scan
0.9079104253	user interface
0.9076580404	pose estimation
0.9076192392	metal artifact
0.9070481722	cerebrospinal fluid
0.9066416996	spin echo
0.9063151285	channel attention
0.9062992598	sliding window
0.9061427577	fundus photography
0.9060963823	dynamic range
0.9059196434	colorectal polyps
0.9054567612	magnetic susceptibility
0.9052273810	differential equations
0.9049729198	single shot
0.9048714405	support vector
0.9048056732	gesture recognition
0.9047483291	deformable registration
0.9035733711	irregularly sampled
0.9034165314	phase unwrapping
0.9031916518	image processing
0.9029343397	motion correction
0.9027749457	fine grained
0.9024209135	projection profilometry
0.9023904094	cryo em
0.9023457054	loss function
0.9021333054	brain tumor
0.9019789397	high dynamic range
0.9015739347	long short term memory
0.9014556824	instance segmentation
0.9013273715	latent variables
0.9011637409	night vision
0.9010332939	uk biobank
0.9009853613	cycle consistency
0.9008979275	left ventricular
0.9008305500	disease progression
0.9008160831	loss functions
0.9003412107	adversarial attack
0.9001874368	rotation invariant
0.8997872552	convolutional neural networks
0.8990699304	tongue contour
0.8990211150	object detection
0.8990203251	forward pass
0.8988860878	opinion scores
0.8988727553	airborne lidar
0.8988352592	iterative reconstruction
0.8987731392	unsupervised learning
0.8984847251	crowd counting
0.8984250272	pulmonary nodule
0.8983936129	numerical aperture
0.8983283570	domain shift
0.8983283284	latent codes
0.8983206305	l1 norm
0.8982451183	led array
0.8981676384	gerchberg saxton
0.8980096235	generative models
0.8979849524	contrast agent
0.8978005986	positron emission
0.8977072153	reverse transcription
0.8976732656	optical coherence tomography angiography
0.8976154419	iris recognition
0.8975989321	digital elevation
0.8975012440	bounding boxes
0.8972697101	skin lesions
0.8968612686	face recognition
0.8967634666	cross modal
0.8964725740	public health
0.8962674633	generative adversarial network
0.8961923695	depth estimation
0.8961757863	bit rate
0.8957849282	single molecule
0.8957682914	fetal brain
0.8953501730	triplet loss
0.8945412969	digital pathology
0.8944635708	false positives
0.8937875040	focal plane
0.8933980677	scattering media
0.8930771754	chest ct scans
0.8929663452	bd rate
0.8929588898	quality assessment
0.8929568112	open source
0.8929349386	myocardial perfusion
0.8925834904	ct volumes
0.8924503511	knowledge transfer
0.8924257234	cervical cancer
0.8923991232	semantic segmentation
0.8921600036	lidc idri
0.8921499296	point spread functions
0.8920930026	feed forward
0.8919378968	pattern recognition
0.8917035493	machine vision
0.8916196988	mri scans
0.8915285158	residual blocks
0.8914179617	grand challenge
0.8914086407	generative adversarial
0.8913191492	cycle consistent
0.8912618712	motion estimation
0.8912321270	closed loop
0.8910361524	fractional anisotropy
0.8909326111	artificial neural networks
0.8906931927	deep neural networks
0.8906589131	computerized tomography
0.8904883718	federated learning
0.8903824950	black box
0.8899576146	fully automatic
0.8896511746	photon counting
0.8894448626	photoacoustic microscopy
0.8892673521	soft tissue
0.8891863643	scanning transmission electron microscopy
0.8890929056	auto encoder
0.8890134939	visual odometry
0.8889185199	unsupervised domain adaptation
0.8882934515	filter array
0.8882880159	resting state
0.8882294954	principle component
0.8879063787	convolutional sparse coding
0.8877480825	t2 weighted
0.8875004837	feature selection
0.8872321467	low rank
0.8867754520	memory consumption
0.8867463221	fringe patterns
0.8865859479	ablation studies
0.8865797740	root mean square error
0.8864965625	cross entropy
0.8864938473	skin disease
0.8864755589	bi directional
0.8863494563	convolutional neural network
0.8862631910	high efficiency video coding
0.8859737403	perceptual quality
0.8856645242	grad cam
0.8855056147	confocal microscopy
0.8853124025	object detectors
0.8852938172	artifact reduction
0.8847025043	post operative
0.8845110982	view synthesis
0.8840652112	attention module
0.8839915958	fast fourier transform
0.8837680198	single image deraining
0.8837405977	sar despeckling
0.8837294878	blood pool
0.8836273361	super resolution
0.8835723323	post hoc
0.8831788401	video compression
0.8831200972	gaussian mixture
0.8826709095	auto encoders
0.8825209692	magnetic particle imaging
0.8821996192	perceptual loss
0.8820481717	neuro oncology
0.8818248237	photoacoustic tomography
0.8815164281	contrast enhancement
0.8814692287	feature space
0.8811534746	bright field
0.8810187910	image denoising
0.8807253642	video coding
0.8805634590	t1 weighted
0.8804985570	physics informed
0.8803863826	hyperparameter tuning
0.8803813897	sparsity promoting
0.8803509969	dice loss
0.8801862979	encoder decoder
0.8800357938	error rate
0.8798538528	discrete wavelet transform
0.8793529956	fully automated
0.8793211279	late gadolinium enhancement
0.8792735447	high definition
0.8791967414	mr fingerprinting
0.8789552009	feature fusion
0.8789045559	anti spoofing
0.8786258338	region proposal
0.8781322409	brain tumors
0.8780942320	image compression
0.8780820449	human connectome project
0.8780680723	body parts
0.8780508701	computationally expensive
0.8776579842	inter observer
0.8774631578	ray tracing
0.8771437296	previously published
0.8769986968	content based image retrieval
0.8768353539	false negative
0.8764175000	sparse coding
0.8761887479	cancer types
0.8760931179	teacher student
0.8758885164	intra operative
0.8758688502	average precision
0.8758084607	photo realistic
0.8757504848	robot assisted
0.8755108184	power consumption
0.8752697190	gaussian noise
0.8751480569	multi modal
0.8750829587	upper bound
0.8750502534	residual connections
0.8749719333	noise removal
0.8748730384	diffraction limit
0.8747948437	higher order
0.8746343335	bi rads
0.8744393394	prostate biopsies
0.8743126314	unmanned aerial
0.8742024958	variable rate
0.8741541278	daily life
0.8739800997	low dose ct
0.8739306833	signal processing
0.8738735164	visual perception
0.8737383222	light fields
0.8736756308	diffraction tomography
0.8735104955	shearlet transform
0.8734794342	cross sectional
0.8732803328	scanning transmission electron
0.8727613335	labor intensive
0.8727357854	aerial imagery
0.8726006316	vessel segmentation
0.8725885024	depth map
0.8725445056	super resolve
0.8723876025	life threatening
0.8723444958	filter banks
0.8722689047	dictionary learning
0.8722525504	compares favorably
0.8721607601	fixed point
0.8720128125	resonance imaging
0.8719408407	closed form
0.8718412672	fine tuning
0.8718023344	future directions
0.8717594331	single photon
0.8717134708	lighting conditions
0.8716681731	true positive
0.8715053254	high resolution
0.8714935191	limited angle
0.8714627344	computationally efficient
0.8711939131	diffusion weighted
0.8711214490	double compressed
0.8709820299	wasserstein distance
0.8708954974	traffic sign
0.8708131177	parameter tuning
0.8705801760	regularization term
0.8705665079	class activation
0.8705606193	fully sampled
0.8704933977	sparse representation
0.8704810559	image registration
0.8704558348	chest ct
0.8704061233	depthwise convolution
0.8703625910	brain metastases
0.8703618611	object recognition
0.8702583933	artificial neural network
0.8702438899	retinal vessel
0.8702147881	spatio temporal
0.8700788122	supervised learning
0.8700440027	cardiac magnetic resonance
0.8694332462	fully connected layers
0.8693775213	adversarial training
0.8692908981	echet inception distance
0.8692608568	contrast enhanced
0.8691713705	wgan gp
0.8690519016	clinically relevant
0.8686727467	fold cross validation
0.8686646766	saliency maps
0.8685285036	semi supervised
0.8683824984	medical image analysis
0.8683590870	gauss newton
0.8681745391	transmission matrix
0.8680514482	convolutional layer
0.8679512213	convolutional neural
0.8678846455	hyperspectral unmixing
0.8674462391	hand crafted
0.8672159681	dynamic scenes
0.8670720831	mask rcnn
0.8670150880	frame rates
0.8669040268	phase shifting
0.8668844107	structured illumination microscopy
0.8667138302	spatially variant
0.8666099723	multiple instance learning
0.8665167855	scattering medium
0.8665066556	latent vectors
0.8664920250	random forests
0.8662807603	natural language
0.8661438393	ms ssim
0.8660224179	medical image segmentation
0.8660052742	convex optimization
0.8658255761	single image super resolution
0.8657879576	digital holography
0.8657474600	cardiac mri
0.8656967542	hand gesture recognition
0.8656302221	speech recognition
0.8655981524	cross modality
0.8655164771	cell phone
0.8649128086	cortical plate
0.8648577287	variational auto encoder
0.8647654966	inverse tone mapping
0.8647604058	anatomically plausible
0.8642983747	early stage
0.8641075741	statistically significant
0.8627209922	additive white gaussian noise
0.8624665770	pre operative
0.8624512304	adversarial loss
0.8624321117	ct images
0.8624062058	mr images
0.8623965229	decision making
0.8623850498	medical images
0.8620984397	vfa fleet
0.8620162274	driver assistance
0.8619446570	ground truth
0.8619244530	edge computing
0.8617893502	landmark detection
0.8616450572	inter rater
0.8614601101	shift exchange
0.8613224334	jpeg compression
0.8612277263	coronavirus disease
0.8611746248	computationally intensive
0.8611046447	long standing
0.8610279353	large scale
0.8605681524	photo response non uniformity
0.8600781395	alternating direction
0.8600610753	underwater image enhancement
0.8595396894	partially annotated
0.8593725068	multiple sclerosis lesion
0.8591418000	image restoration
0.8590479584	semi automatic
0.8589867814	active learning
0.8588227766	low light image enhancement
0.8588006302	natural language processing
0.8586715948	high throughput
0.8580011709	real world
0.8579036122	ms coco
0.8572184820	transcription polymerase chain reaction
0.8570329349	pan sharpening
0.8569579934	1p 19q
0.8567961195	light source
0.8564881922	street view
0.8564880993	cardiac mr
0.8563685266	low bitrates
0.8561437194	adversarial perturbations
0.8559433709	weak supervision
0.8558425216	contrast agents
0.8558423012	extensively studied
0.8553506423	brain tumour
0.8551000185	fetal head
0.8548736346	activity recognition
0.8547063050	mimic cxr
0.8542286768	low power
0.8541377707	metal artifacts
0.8540125659	cryo electron microscopy
0.8532960422	low rank tensor
0.8531804788	image quality assessment
0.8528419492	swarm optimization
0.8527212389	lesion segmentation
0.8523650562	deep neural network
0.8522168575	synthetically generated
0.8521455273	feature representation
0.8520295288	treatment planning
0.8519764366	structured light
0.8519350137	long term
0.8518713662	regularization terms
0.8518503738	variable density
0.8517372358	vice versa
0.8516971444	computational complexity
0.8516325172	lv myocardium
0.8513418447	artery stenosis
0.8512825878	healthy volunteers
0.8512734239	fine tune
0.8509076426	sampling pattern
0.8508706925	specially designed
0.8508450908	transfer function
0.8506783620	multi scale
0.8506683070	fully convolutional
0.8506090844	log likelihood
0.8504637626	perfusion roi
0.8504390653	dark field
0.8504193850	max pooling
0.8501399257	real valued
0.8501361209	optical coherence
0.8500736359	project website
0.8500358134	traffic sign detection
0.8496734457	short axis
0.8496020068	image generation
0.8494997230	resonance fingerprinting
0.8493323798	high dimensional
0.8490488879	board certified
0.8488094245	synthetic data
0.8486284248	performs favorably
0.8483032617	magnetic field
0.8479944086	tight frame
0.8476719663	radiomic features
0.8474971928	natural scenes
0.8472718948	fluid attenuated inversion
0.8472074466	case study
0.8469835649	vq vae
0.8469658436	short term
0.8469228898	low dose
0.8467667064	linear regression
0.8466447700	post processing
0.8464956216	motion blur
0.8463001767	speckle noise
0.8460268840	polyp detection
0.8459709863	healthy controls
0.8459055137	weakly labeled
0.8459045178	ground glass
0.8457676450	nr iqa
0.8457305411	convolutional layers
0.8456771731	health care
0.8456685590	tomographic reconstruction
0.8455730245	clinical trials
0.8455333808	survival prediction
0.8454122805	false positive rate
0.8452131837	hyperspectral imagery
0.8451288140	fiber orientation
0.8447181511	artifact removal
0.8446287721	dice similarity
0.8445711238	computational ghost imaging
0.8442400910	inception score
0.8440659232	frame prediction
0.8440396737	parameter estimation
0.8439043619	solving inverse problems
0.8438030035	uncertainty estimation
0.8437674903	blind deblurring
0.8437643642	image analysis
0.8437265027	imaging modalities
0.8435689978	rain snow
0.8434259121	sperm cells
0.8433686402	great progress
0.8433305339	deep image prior
0.8432842812	wide field
0.8432472725	multi site
0.8432403756	dynamic contrast enhanced magnetic resonance
0.8432123605	widely adopted
0.8431060940	feature maps
0.8430023336	video streaming
0.8428029416	noise reduction
0.8426682644	image quality
0.8424341328	big data
0.8423321858	mutual information
0.8419959058	fine tuned
0.8417448639	similarity index
0.8416338078	cross domain
0.8414532249	brain tumor segmentation
0.8411047597	chest computed tomography
0.8410623472	biomedical imaging
0.8410598546	color fundus
0.8408764767	recurrent neural network
0.8408604560	decision support
0.8408590588	tensor decomposition
0.8408543756	retinal vessel segmentation
0.8398358707	compression ratio
0.8398107286	neurodegenerative diseases
0.8398088192	human perception
0.8397464180	edge devices
0.8397013246	shadow removal
0.8395384519	paramount importance
0.8394700120	previous works
0.8392710378	memory usage
0.8390715833	sickle cell
0.8390420099	opinion score
0.8388006505	data set
0.8387737255	neural style transfer
0.8386996711	bit rates
0.8386304585	feature extractors
0.8382811366	hand gesture
0.8380123028	conditional generative adversarial networks
0.8378684892	fully convolutional networks
0.8378140365	standard deviation
0.8374773897	test set
0.8374201731	intersection over union
0.8371770969	impulse noise
0.8371581384	energy efficiency
0.8369790783	cone beam ct
0.8369617123	cross validation
0.8368267430	edge preserving
0.8367861268	energy consumption
0.8367599632	evaluation metrics
0.8367409782	virtual staining
0.8365227136	image reconstruction
0.8363358951	cost function
0.8362227762	search space
0.8361521269	radiation exposure
0.8360565923	fast spin echo
0.8360435208	proximal gradient
0.8360356929	open set
0.8358766642	fashion mnist
0.8357488578	computational resources
0.8355623020	context aware
0.8352394597	universal style transfer
0.8350895498	attention guided
0.8348198324	bit depth
0.8344989698	mini batch
0.8343013337	template matching
0.8342233041	residual block
0.8341047101	breast density
0.8338391579	deep belief
0.8335919811	rate distortion
0.8334142234	error prone
0.8334011476	bayesian inference
0.8333650108	predictive uncertainty
0.8332141470	brain mri
0.8325521291	alternating minimization
0.8324295040	low cost
0.8323394589	defect detection
0.8322196432	high frequency
0.8317652776	computational pathology
0.8314482010	squared error
0.8313640267	main contributions
0.8312805840	channel pruning
0.8312474267	cross correlation
0.8308256789	multi view
0.8307941804	dce mri
0.8307360556	dense blocks
0.8306638630	severity assessment
0.8304217772	motion artifacts
0.8301771278	instance normalization
0.8300773619	latent representation
0.8297516616	quality metric
0.8296633770	remote sensing imagery
0.8285343780	adversarial learning
0.8285230717	light detection and ranging
0.8284625405	differential equation
0.8284491358	recurrent neural networks
0.8281198754	surgical instruments
0.8280617237	clinical practice
0.8280464577	field programmable gate
0.8278235041	high speed
0.8275533674	inverse problem
0.8272964314	low dimensional
0.8270331865	diffusion tensor imaging
0.8268875860	evaluation metric
0.8268108102	future research directions
0.8266323544	bcd net
0.8265273675	single particle
0.8264443889	blur kernel
0.8263739610	tumor growth
0.8262035115	optimization problem
0.8260524386	video frames
0.8259544963	source code
0.8258899368	frame interpolation
0.8258238771	diffraction patterns
0.8257710440	pancreatic ductal
0.8256948460	blind spot
0.8254871924	coronary artery disease
0.8252871322	ego motion
0.8250280530	parallel imaging
0.8249875630	audio visual
0.8249189038	quality metrics
0.8248567917	radiological findings
0.8246792963	remarkable success
0.8245284710	capsule network
0.8244893771	surface distance
0.8243746406	deep learning models
0.8242704229	acute ischemic
0.8240427916	filter pruning
0.8237598269	great promise
0.8237144285	line segment
0.8234251422	fundus images
0.8231548643	arterial spin
0.8230812006	posterior distribution
0.8230755724	data sets
0.8230114964	hyperspectral image
0.8228091757	visible light
0.8227491533	data collection
0.8224690377	image segmentation
0.8224484812	predictive coding
0.8224309116	ocular diseases
0.8224138916	computational cost
0.8223907032	manually annotated
0.8222274342	physics guided
0.8221387394	global context
0.8221378782	low rankness
0.8215331174	depth completion
0.8214192582	user friendly
0.8211623496	capsule networks
0.8211158871	domain knowledge
0.8210967293	attack success rate
0.8210801291	contextual information
0.8210696759	vector field
0.8210478612	learned image compression
0.8204398798	satellite images
0.8203290608	root mean square
0.8203115378	ultrasound imaging
0.8202209069	pixel wise
0.8200357088	previously unseen
0.8199812939	embedded devices
0.8198623327	poisson gaussian
0.8196879564	fine details
0.8194910913	data driven
0.8192159586	dual path
0.8191933214	cross entropy loss
0.8191931580	lip movements
0.8189795277	cross sections
0.8188973738	receiver operating
0.8188251529	ultrasound tongue
0.8187448966	extreme low light
0.8185623813	feature vector
0.8185563927	computational burden
0.8185134096	higher resolution
0.8184977891	inter observer variability
0.8183429898	voxel wise
0.8182651494	phase contrast
0.8181356611	adversarial networks
0.8179726258	manually labeled
0.8179038312	transmission electron microscopy
0.8177579949	convolution neural network
0.8176763678	treatment response
0.8174026910	failure modes
0.8173917457	salient object detection
0.8173687784	multi slice
0.8173554814	malignant tumors
0.8171137436	facial landmark
0.8169506760	generalization ability
0.8166810781	aerial vehicles
0.8165855539	video streams
0.8165460439	low resolution
0.8163426465	long axis
0.8162355467	speckle patterns
0.8160563083	risk assessment
0.8155334916	conditional gan
0.8155084766	f1 scores
0.8153273029	anatomical landmarks
0.8149760366	multi atlas
0.8149403219	cryo electron
0.8148461362	variational auto encoders
0.8148445639	attention modules
0.8148284354	speckle tracking
0.8147874890	respiratory syndrome
0.8147564598	photoacoustic imaging
0.8146751291	image enhancement
0.8145320593	random access
0.8143214641	tumor segmentation
0.8142758627	numerical simulations
0.8142277940	histopathology images
0.8140691438	root mean squared error
0.8138357871	video sequence
0.8136041282	fringe pattern
0.8135920591	automatic differentiation
0.8135680982	low light
0.8135117518	head mounted
0.8132194819	multi label
0.8131134470	spatial frequency
0.8130706260	recent advances
0.8130639337	training samples
0.8127035200	medical image
0.8126723989	lung infection
0.8126056960	dense connections
0.8125479800	connectome project
0.8120256651	roc auc
0.8117716468	subjective quality
0.8117667758	early stages
0.8117582648	blind deconvolution
0.8117048841	singular value decomposition
0.8116597296	poor generalization
0.8114082564	lesion detection
0.8112739304	fully connected layer
0.8112386587	poisson noise
0.8109386978	recent works
0.8106754246	deformable convolution
0.8101410279	conjugate gradient
0.8100661421	audio visual speech enhancement
0.8099439767	high performance computing
0.8096396784	multi modality
0.8093936325	low contrast
0.8093918862	proton density
0.8092212754	partial volume effects
0.8091668861	lower dimensional
0.8089923418	electron microscope
0.8089637021	histopathological images
0.8088317247	operating characteristic curve
0.8087662449	image synthesis
0.8084217512	ct angiography
0.8083623588	multi coil
0.8083361413	discrete cosine
0.8082544106	super resolved
0.8082355765	hierarchical clustering
0.8081540833	multi channel
0.8081034081	fourier domain
0.8080565301	success rate
0.8076758985	electron tomography
0.8076360564	single particle cryo
0.8076182922	deep neural
0.8074834023	computational imaging
0.8074735548	fuzzy c means
0.8073979327	anatomical structures
0.8073921457	liver tumor segmentation
0.8073416511	deep learning based
0.8072810352	latent representations
0.8072306644	activation functions
0.8071556530	dark channel
0.8070437742	health monitoring
0.8068678471	validation set
0.8066567206	person re identification
0.8065503165	filtered back projection
0.8064352130	multi sequence
0.8061080375	enhancing tumor
0.8060213958	real life
0.8059486473	discrete wavelet
0.8057509493	recent years
0.8057039488	boundary conditions
0.8055872927	wide area
0.8053852818	conditional generative
0.8053299520	shot epi
0.8053262289	trade offs
0.8052566175	lossless image compression
0.8051715643	emission tomography
0.8046518471	cortical surface
0.8045549015	brain anatomy
0.8045102169	blind denoising
0.8043632446	matrix completion
0.8042021698	fully supervised
0.8041261874	long range
0.8039604553	convolution layers
0.8038801673	retinal fluid
0.8038747364	computational costs
0.8036714296	short term memory
0.8035906056	disentangled representations
0.8035878968	video sequences
0.8035318676	deep convolutional neural networks
0.8033182769	speech separation
0.8028587490	face verification
0.8027953830	path planning
0.8026734626	cardiac cine mri
0.8026424126	mode decomposition
0.8026302378	visual tracking
0.8026221534	multi path
0.8025311821	acceleration factors
0.8025060013	respiratory motion
0.8024515713	coherence tomography
0.8022888089	spatio spectral
0.8022187796	eye tracking
0.8020934044	transparent objects
0.8019561364	manual annotations
0.8019558926	network architecture
0.8019175165	adaptive optics
0.8012588477	low dynamic range
0.8011584469	skeleton based action recognition
0.8010843094	video quality assessment
0.8009920114	cost effective
0.8009350411	gradient penalty
0.8008032728	median filter
0.8007942425	multi exposure image fusion
0.8007437584	mr image reconstruction
0.8005373670	safety critical
0.8004978201	hyper parameter
0.8004770735	cardiovascular diseases
0.8003884890	convolutional autoencoder
0.8002204756	crack detection
0.8001988480	adaptive weighting
0.7999285714	organs at risk
0.7998908067	masked face
0.7996398196	total variation regularization
0.7994595952	scene flow
0.7993588547	color difference
0.7991985345	mouse brain
0.7990409603	image inpainting
0.7989570400	gradient echo
0.7988281365	video quality
0.7987817499	convolutional networks
0.7985915603	significantly improves
0.7985011424	pre processing
0.7984094007	superior performance
0.7982989563	high spatial resolution
0.7982910265	video codec
0.7982002702	entropy coding
0.7980154990	spatial resolution
0.7979979212	human body
0.7979633611	perceived quality
0.7978120989	action unit
0.7977645566	screening mammography
0.7974220590	blood cells
0.7973755827	retinal fundus images
0.7973238963	single frame
0.7970230341	rate control
0.7969184397	basis functions
0.7968680866	starting point
0.7967282049	pixel level
0.7966170415	pet ct
0.7965399156	high accuracy
0.7962604876	surgical planning
0.7960743543	graph signal processing
0.7959717097	landmark localization
0.7959024800	multi centre
0.7957609166	patch based
0.7957187134	eeg signals
0.7956241958	coherent diffraction imaging
0.7955025148	generalization capability
0.7954390119	blood cell
0.7953626453	sampling rate
0.7950690883	biomedical image analysis
0.7946443385	cell nuclei
0.7945794924	noise levels
0.7943548793	discriminant analysis
0.7941920025	extensive experiments
0.7941398009	block matching
0.7940768501	tensor train
0.7938646796	focal loss
0.7938419362	deformation fields
0.7938138212	forgery detection
0.7937863608	medical diagnosis
0.7935201775	fully convolutional network
0.7933858134	holographic microscopy
0.7933271897	artifact free
0.7933225402	structural information
0.7933015230	objective functions
0.7931036606	building blocks
0.7930590023	binary classification
0.7929530241	visual inspection
0.7927405858	autism spectrum
0.7926513716	multi institutional
0.7925680583	source domain
0.7925575294	event driven
0.7925469208	moving objects
0.7925308948	micro ct
0.7925256662	label free
0.7924760607	severe acute respiratory
0.7923274817	conditional generative adversarial network
0.7922224733	image translation
0.7920587999	medical image registration
0.7918948956	faster convergence
0.7917875073	reverse transcription polymerase
0.7916388481	convolution neural networks
0.7915408567	speckle pattern
0.7914671623	receiver operating characteristic curve
0.7910987510	motion corrected
0.7910097864	randomly selected
0.7909901083	image classification
0.7907799286	intra observer
0.7905816201	breast cancer screening
0.7905031653	locally linear
0.7904331525	stereo vision
0.7903210801	multi dimensional
0.7899393222	patient specific
0.7898224802	level co occurrence matrix
0.7898032060	training set
0.7897333393	high quality
0.7897124580	intra class
0.7896380431	inception distance
0.7896250003	general purpose
0.7895866631	benchmark datasets
0.7895557009	gaussian distribution
0.7894896478	past decade
0.7894006560	gait recognition
0.7893520902	saliency prediction
0.7889633797	multi class
0.7888800968	mask r cnn
0.7888740777	quantitative phase
0.7885271707	piece wise
0.7882892377	similarity measure
0.7882806847	recent studies
0.7878907286	similarity coefficient
0.7876801854	unlabeled data
0.7876413919	contrastive learning
0.7873592505	image formation
0.7872244326	functional magnetic resonance imaging
0.7871633448	resource constrained
0.7869072967	pre trained
0.7864544024	spectral variability
0.7864273585	saliency map
0.7864162513	feature pyramid
0.7863386526	models genesis
0.7862448093	incremental learning
0.7862347334	biomedical image segmentation
0.7858383589	patch wise
0.7857941699	high level semantics
0.7855900139	smartphone camera
0.7853840855	conditional generative adversarial
0.7853205195	power spectrum
0.7853016129	lidar point cloud
0.7850014097	fake face
0.7849747051	experimental results confirm
0.7849632530	memory efficient
0.7848994068	video frame
0.7846726204	cycle consistency loss
0.7841809466	parameter selection
0.7841273033	semi automated
0.7839235184	evolutionary algorithm
0.7839200390	infected regions
0.7838624179	post processed
0.7836776993	microscopic images
0.7833943658	inter class
0.7831765679	cerebral blood
0.7831524975	prior art
0.7829723510	picture quality
0.7821536832	newly developed
0.7820453547	high sensitivity
0.7815827142	illumination pattern
0.7815087331	biological samples
0.7814352821	breast tomosynthesis
0.7811386082	physics based
0.7810223817	bias field
0.7809951578	multitask learning
0.7809632936	recent developments
0.7808479600	outcome prediction
0.7807926936	hardware friendly
0.7803427724	element wise
0.7802961459	cancer genome
0.7802717051	low complexity
0.7800420593	ultrasound images
0.7799976064	achieved great success
0.7799612431	meta learning
0.7797477940	low dose computed tomography
0.7796922234	objective quality metrics
0.7795050952	single pixel
0.7793806143	disparity map
0.7790374225	gaze estimation
0.7788301949	high frame rate
0.7788178093	computational efficiency
0.7786677045	optical microscopy
0.7785080823	deep generative models
0.7785040556	prior knowledge
0.7783553470	numerical experiments
0.7782596421	graph laplacian
0.7782460776	environmental conditions
0.7782381157	spectral bands
0.7780743624	dual energy ct
0.7779543448	phase shift
0.7779506068	quantitative phase imaging
0.7777509412	carefully designed
0.7777324114	coherent diffraction
0.7776950403	space exploration
0.7776810972	channel wise
0.7775147024	human pose estimation
0.7774695705	pls net
0.7771741242	visual object tracking
0.7769406536	deep unfolding
0.7769075931	memory requirements
0.7768256800	residual dense
0.7767567222	retinal fundus
0.7767331813	low rank property
0.7766604497	image dehazing
0.7763063404	anchor free
0.7760604172	kidney tumor segmentation
0.7760026366	markov random
0.7757744686	multimodal brain tumor
0.7756730774	pair wise
0.7756649164	correlation coefficients
0.7755662234	multi source
0.7755597904	discrete fourier
0.7754750507	false negative rate
0.7751118139	precision recall
0.7749727602	gray level
0.7749585614	end users
0.7748654628	dynamic contrast enhanced
0.7748227458	computed tomography angiography
0.7744966883	cine mri
0.7743610955	key points
0.7739939990	spectral domain
0.7738217704	wasserstein generative adversarial
0.7733586247	coding efficiency
0.7732882418	empirical study
0.7731618815	early diagnosis
0.7728759188	dilated convolution
0.7728630632	cycle gan
0.7724417072	experimental results
0.7723672317	skin lesion segmentation
0.7720874352	diffractive optical
0.7716454709	spatial transformer
0.7715628648	lr hr
0.7712613014	target domain
0.7710148083	handcrafted features
0.7710126456	fully annotated
0.7708450998	panoptic segmentation
0.7706902031	aperture radar
0.7702079143	natural images
0.7700739413	manual annotation
0.7699842689	minimization problem
0.7698289995	shown promise
0.7696492543	high precision
0.7695408893	cutting edge
0.7692894562	programmable gate
0.7692311133	object detector
0.7692166188	significantly outperforms
0.7691162372	hyper parameters
0.7690337439	lidar point clouds
0.7689195775	data science
0.7686376667	differential diagnosis
0.7685695785	multi level
0.7685381009	activation maps
0.7684721351	dual energy
0.7683852546	dice scores
0.7682601585	similarity metric
0.7682401485	brain atlas
0.7681923770	domain specific
0.7681517326	video codecs
0.7680275698	graph convolutional networks
0.7680145105	deepfake detection
0.7676933429	localization microscopy
0.7676124514	human activity recognition
0.7675130431	wasserstein gan
0.7674282142	kidney tumor
0.7673037765	missing modalities
0.7670892335	limited view
0.7670382148	scene text
0.7669347946	future research
0.7666689019	retinal oct
0.7664810334	diagnostic tool
0.7664017323	multi task
0.7663693304	jointly optimize
0.7663335633	denoising autoencoder
0.7661604561	convolutional autoencoders
0.7660628613	temporal resolution
0.7656139600	delay and sum
0.7655919982	multi task learning
0.7655809349	comparative analysis
0.7655458988	higher quality
0.7654892389	layer wise
0.7654180109	illumination patterns
0.7650359163	conduct extensive experiments
0.7647994209	chest x ray
0.7647296924	vehicle detection
0.7646224373	signal recovery
0.7646165849	spiking neural network
0.7645901508	rate distortion optimization
0.7644473106	image pairs
0.7643383205	single stage
0.7643191515	isotropic resolution
0.7642543413	chain reaction
0.7640843645	embedded systems
0.7639518214	inverse scattering
0.7636292096	motion artefacts
0.7634781090	graph convolution
0.7634601322	hand pose
0.7631016293	step size
0.7630242305	hyperspectral imaging
0.7629279572	subject specific
0.7627085692	inter frame
0.7624744946	dimension reduction
0.7620010963	tumor core
0.7619667468	wasserstein gans
0.7618324383	clinical routine
0.7615485217	intra coding
0.7614779392	space variant
0.7610888967	high order
0.7607196010	brain mris
0.7607019209	visual slam
0.7606202451	remains challenging
0.7603345049	region growing
0.7596715559	high performance
0.7596462402	fewer parameters
0.7596138572	video clip
0.7595923783	gaussian mixture model
0.7595591158	aerial images
0.7594701593	multi contrast
0.7594143065	low delay
0.7593283665	significantly improve
0.7591482250	highly accurate
0.7591185546	unrolled network
0.7590785653	cnn based
0.7590665081	imbalanced data
0.7590599653	probability distribution
0.7589732011	faster r cnn
0.7587792555	comparative study
0.7586327402	age estimation
0.7585343239	late fusion
0.7583831788	body pose
0.7583683454	mixed precision
0.7582058347	soft tissues
0.7579832501	deep convolutional neural network
0.7579467077	mnist dataset
0.7576549556	chest x rays
0.7573425969	additive white
0.7569519181	gpu accelerated
0.7568901864	sars cov
0.7567679826	convolution layer
0.7567318343	machine learning techniques
0.7566014957	disease diagnosis
0.7565936660	covid net
0.7565670122	generative model
0.7561361748	memory effect
0.7558242511	complex scenes
0.7557138405	computational overhead
0.7556249604	significantly improved
0.7554977013	saliency detection
0.7554247704	results suggest
0.7552542523	compact representation
0.7550523870	visual quality
0.7547461350	spatial spectral
0.7545331019	force estimation
0.7543343441	abdominal ct
0.7539342055	long range dependencies
0.7535635409	lung nodule detection
0.7534515049	motion capture
0.7533807666	healthy subjects
0.7533469813	cross view
0.7531968534	multi stage
0.7531625006	acquisition times
0.7530935582	preliminary results
0.7529520566	rate adaptation
0.7529139092	structured low rank
0.7527794827	multi layer
0.7525445500	audio visual speech
0.7521196139	video content
0.7515032115	edge detector
0.7515020777	cognitive impairment
0.7514934225	quantitative susceptibility
0.7513640061	iterative phase retrieval
0.7512180897	free electron
0.7510258943	valuable tool
0.7509980000	shot noise
0.7504880964	imaging modality
0.7503724907	closed form solution
0.7503684935	video surveillance
0.7503500757	computing resources
0.7502768882	feature map
0.7502303078	pedestrian detection
0.7501717510	brain activity
0.7501305043	knee mri
0.7500752417	dice overlap
0.7498697537	brain tissues
0.7497632008	require large amounts
0.7494271607	survival rate
0.7490625160	perform poorly
0.7489714249	pre defined
0.7488004204	prostate segmentation
0.7487486221	high dimensionality
0.7486174470	significant differences
0.7485574426	low latency
0.7481594964	vector machine
0.7480402261	peak signal to noise ratio
0.7477903640	siamese network
0.7477297206	scale invariant
0.7475167176	objective function
0.7472598012	gaussian process
0.7471066880	major challenges
0.7469955487	intra prediction
0.7469045104	data mining
0.7466063544	visual inertial
0.7465382266	attention based
0.7465312191	noisy labels
0.7458950999	significantly reduces
0.7458335106	auto encoding
0.7457351283	feature aggregation
0.7457192301	dce mr
0.7456412365	voxel level
0.7456082604	age related macular
0.7455227610	histology images
0.7455089139	spread function
0.7454617619	spectral ct
0.7452457503	expression recognition
0.7452192090	graph reasoning
0.7451863366	promising results
0.7451657831	processing units
0.7451604417	significant progress
0.7450384229	learned video compression
0.7449540829	plane wave
0.7449407060	t1 mapping
0.7449196395	image encryption
0.7448798687	highly accelerated
0.7446662552	brain tissue
0.7446366965	alternating direction method of multipliers
0.7446169624	ego motion estimation
0.7444695255	object tracking
0.7442929798	highly correlated
0.7439714606	radiomics features
0.7438299513	compressed videos
0.7432665753	report generation
0.7432505418	stroke lesion segmentation
0.7432086979	graph convolutional network
0.7428790980	macular degeneration
0.7428360348	feature vectors
0.7427310731	deep convolutional
0.7427202146	high contrast
0.7425893854	stroke lesion
0.7424304420	cloud detection
0.7422288393	representation learning
0.7421818029	image pair
0.7420331364	single photon lidar
0.7419890292	lung diseases
0.7418686583	consecutive frames
0.7418261742	visual cues
0.7417202500	multi resolution
0.7414669322	lower resolution
0.7413323933	transmission map
0.7412853208	imaging systems
0.7411467435	times faster
0.7409191368	image matting
0.7408417383	brain mri scans
0.7407389679	characteristic curve
0.7406952750	adversarial network
0.7406733438	probability maps
0.7405363211	scene understanding
0.7401054868	computation cost
0.7397816041	physical properties
0.7397483788	iterative algorithms
0.7395352408	glaucoma detection
0.7394741693	complementary information
0.7394258769	terrasar x
0.7392030398	component analysis
0.7390109217	previously proposed
0.7390036884	parameter sharing
0.7387076034	noise ratio
0.7386783221	transmission electron
0.7385169936	simultaneous localization and mapping
0.7385018037	architecture search
0.7384754611	gland segmentation
0.7381813323	shape priors
0.7381433858	retrospective study
0.7381166647	activation function
0.7378916084	lensless imaging
0.7377797868	feature refinement
0.7376349383	competitive performance
0.7374963723	output layer
0.7374566146	tomographic imaging
0.7374109729	pyramid pooling
0.7373811839	multimodal fusion
0.7373679270	spatial information
0.7369477139	image stitching
0.7369095654	residual learning
0.7366883378	image deblurring
0.7365749302	recently published
0.7363575268	level set
0.7361030827	chemical shift
0.7354196496	local binary
0.7351770593	gpu memory
0.7348495916	intra observer variability
0.7347394763	abnormality detection
0.7344729490	invasive surgery
0.7343211413	significantly reduce
0.7343073069	domain gap
0.7341413802	data acquisition
0.7340745019	sparse view
0.7340479967	training strategy
0.7340371508	nuclei segmentation
0.7336532235	task specific
0.7334606770	comprehensive survey
0.7332938699	vision sensor
0.7332903390	labeled data
0.7332320604	roc curve
0.7331387370	spectral resolution
0.7330993052	spatially adaptive
0.7329331392	diffraction limited
0.7327914425	compressive imaging
0.7326290077	light weight
0.7325797120	count pet
0.7325480441	angular resolution
0.7323006281	context encoding
0.7322116402	independent test set
0.7321631072	model based iterative reconstruction
0.7321582174	significant improvements
0.7320702011	ablation study
0.7320186634	cone beam computed
0.7319370921	low quality
0.7318126271	fully convolutional neural networks
0.7316572628	signal to noise ratio
0.7315657635	previous studies
0.7315336533	blood vessel segmentation
0.7309141714	gadolinium enhanced
0.7306016573	acceleration factor
0.7299770794	gan generated
0.7299100444	artificial neural
0.7298099868	depth maps
0.7297811746	encoder decoder architecture
0.7297186096	pooling layer
0.7296158443	reference frame
0.7295724341	intensive care
0.7294508520	land cover classification
0.7292666432	unsupervised anomaly
0.7287821306	statistical shape
0.7283659560	pre training
0.7283420349	multi organ segmentation
0.7283213552	endoscopic images
0.7281443370	semi supervised learning
0.7278137883	monocular camera
0.7277675993	low rank matrix
0.7276220712	spectral sensitivity
0.7275836148	source camera
0.7270713160	image completion
0.7270693046	test sets
0.7270097547	probability map
0.7269918322	sars cov 2
0.7269768410	multi frame
0.7268856221	molecule localization
0.7262749603	gaussian filter
0.7258672601	motion vector
0.7258658479	breast ultrasound
0.7257896009	high level
0.7257817141	high density
0.7255047237	spatial frequency domain
0.7251771159	high frequency components
0.7251044773	spatial location
0.7250790812	cs mri
0.7249451782	momentum net
0.7248678453	optical imagery
0.7248634292	image stacks
0.7247242969	density estimation
0.7244534422	observer variability
0.7243255341	mri sequences
0.7240990775	low dimensional manifold
0.7240614580	principal components
0.7237447785	vector machines
0.7236310808	fetal mri
0.7234907688	widely applied
0.7234331835	multi spectral
0.7230467933	mean square error
0.7227806066	mixture model
0.7227129051	partial volume
0.7226013224	lung segmentation
0.7224185333	image watermarking
0.7222043909	cycle consistent adversarial
0.7221122314	point cloud compression
0.7221065453	human action
0.7219406301	human experts
0.7216587679	point wise
0.7215915537	existing works
0.7215657564	dual domain
0.7213815598	mechanical properties
0.7213544098	breast tumor
0.7211706018	low level
0.7211043547	l2 loss
0.7207939694	parallel mri
0.7207552290	surveillance videos
0.7207072837	single pixel imaging
0.7206510700	desirable properties
0.7205629004	generating realistic
0.7205194692	activation mapping
0.7205169197	performance drop
0.7204686774	real world scenarios
0.7202749683	sparse view ct
0.7199326047	multi domain
0.7199315059	stereo matching
0.7195739506	monocular depth
0.7195165451	mr imaging
0.7195089000	single modality
0.7194083878	digital image
0.7193478329	accelerated mri
0.7192405302	prnu based
0.7191202247	diffractive imaging
0.7191037820	increasingly popular
0.7190921390	low snr
0.7189842157	unpaired training
0.7187158538	road surface
0.7184468341	human pose
0.7182634030	pixel size
0.7181646723	training data
0.7181328695	statistical model
0.7179603969	multiple scales
0.7177013435	head motion
0.7175398237	multi planar
0.7175039939	liver segmentation
0.7174797234	multi sensor
0.7172940222	deep learning techniques
0.7169048160	mean squared error
0.7166965918	multi organ
0.7166865712	spatial attention
0.7166442741	experimental evaluations
0.7164210728	highly flexible
0.7164209872	image editing
0.7163577667	rate distortion performance
0.7162196787	squeeze and excitation
0.7159142064	diffusion weighted magnetic
0.7157135608	digital cameras
0.7156621852	attention mechanisms
0.7153395728	inversion recovery
0.7151694102	web based
0.7146997161	coronavirus disease 2019
0.7146013949	cardiac magnetic resonance imaging
0.7145622277	single image
0.7142944962	gpr data
0.7139951269	hyperspectral image classification
0.7139268128	practical applications
0.7138458518	degrees of freedom
0.7138084679	pixel level annotations
0.7136520048	multi stream
0.7136370000	aggregation network
0.7136274317	salt and pepper
0.7135689014	human connectome
0.7134126061	camera calibration
0.7134125065	power lines
0.7133469465	spatial context
0.7132102098	color space
0.7130535767	pixel values
0.7129949311	vision based
0.7129068539	great potential
0.7126484903	polyp segmentation
0.7122946825	cell tracking
0.7122743621	wavelet domain
0.7122232569	paper presents
0.7121724573	convergence rate
0.7121315924	imaging biomarkers
0.7118585746	chest computed
0.7107533774	domain generalization
0.7106749012	flow field
0.7104206318	seismic data
0.7103636086	biological tissues
0.7103314583	multispectral images
0.7103146228	learning based
0.7102901992	multi echo
0.7100661427	thresholding algorithm
0.7095565690	fidelity term
0.7094818227	hyperspectral data
0.7094456207	experimental evaluation
0.7093289993	trainable parameters
0.7093266123	dynamic mr
0.7090925999	impressive performance
0.7089685409	artifacts removal
0.7089412073	metric learning
0.7087237845	attention maps
0.7084708232	low frequency
0.7084362168	user generated
0.7084326871	omnidirectional images
0.7081793488	tomography angiography
0.7080283507	mr scans
0.7078930776	sparse representations
0.7078505047	lge mri
0.7077104000	tissue types
0.7072559663	diffusion weighted magnetic resonance
0.7072508870	public datasets
0.7071825457	fully unsupervised
0.7067339782	neural nets
0.7066534213	classification tasks
0.7065935819	hand crafted features
0.7064785314	manually segmented
0.7063053872	target tracking
0.7061508755	organ segmentation
0.7060141181	extensive experiments demonstrate
0.7060106490	iterative optimization
0.7059208473	recurrent neural
0.7057371943	mri scan
0.7056948841	jpeg 2000
0.7056111383	optical properties
0.7053766619	magnetic resonance images
0.7051977575	stroke patients
0.7051961575	acquisition scheme
0.7051569233	brain imaging data
0.7045871691	deep convolutional neural
0.7045746073	clinical utility
0.7045418498	jointly trained
0.7043064903	generative modeling
0.7042937597	activity detection
0.7042611563	c arm
0.7042405884	intensity values
0.7041929330	high frequency details
0.7041086842	additive white gaussian
0.7039513359	compressed video
0.7033479273	multi center
0.7031318863	clinical translation
0.7028617567	nodule detection
0.7028578963	speckle reduction
0.7028213024	multi object tracking
0.7025390879	research topic
0.7025335817	quantitative metrics
0.7023196803	previously reported
0.7022156140	processing pipeline
0.7019390199	compression rates
0.7014564742	hardware implementation
0.7012591154	multi parametric
0.7012568370	depth range
0.7011428420	vulnerable to adversarial attacks
0.7010056734	isic 2017
0.7009536521	multiple instance
0.7008786197	transform coding
0.7006007040	fluid attenuated
0.7005856615	valuable information
0.7005399218	image quality metrics
0.7004399116	patient level
0.7001400741	testing set
0.7001163443	impressive results
0.6999913998	dynamic contrast enhanced magnetic
0.6998115750	texture details
0.6996714266	hematoxylin and eosin
0.6996412756	b spline
0.6995542151	performance gain
0.6994946026	edge aware
0.6991682070	face detection
0.6987812507	graph based
0.6986720202	echet inception
0.6985466687	taking into account
0.6982736836	pre processing step
0.6981202252	pet reconstruction
0.6980918690	energy efficient
0.6979395625	spatial relationship
0.6979192546	prediction error
0.6977796199	diffusion weighted mri
0.6977446221	reverse transcription polymerase chain
0.6974100547	recently introduced
0.6967883079	computational photography
0.6966629642	dw mri
0.6964554590	inter prediction
0.6963809871	annotated dataset
0.6962831418	extensive experimental results
0.6962106741	widely studied
0.6957262867	ntire 2020 challenge
0.6956779006	tandem x
0.6952896382	multi instance
0.6949650401	image retrieval
0.6948824324	human action recognition
0.6947860485	deep residual
0.6945716205	digital image processing
0.6941407890	ground truth labels
0.6941304661	outstanding performance
0.6940836951	performance improvement
0.6939669985	data augmentation techniques
0.6938586831	kitti dataset
0.6937537241	small size
0.6935343451	functional brain
0.6934969858	spectral clustering
0.6933392826	slide images
0.6932674935	ptychographic microscopy
0.6932189361	dermoscopic images
0.6931934759	nonnegative matrix
0.6931707905	cardiac image segmentation
0.6930886073	existing methods
0.6928537441	multi exposure
0.6928462759	image colorization
0.6928296408	lidar data
0.6925715522	inter subject
0.6922763893	comparable performance
0.6918587220	highly realistic
0.6917642341	excellent performance
0.6915719771	shown great potential
0.6915466522	recently emerged
0.6915255934	radiologist level
0.6912984714	theoretical analysis
0.6912745090	augmentation techniques
0.6912417532	experimental results demonstrate
0.6910780465	regression model
0.6910356320	cross validated
0.6909531593	graphical user
0.6909455670	class specific
0.6905603728	performance evaluation
0.6904177007	head pose
0.6898229764	predictive models
0.6897973027	single cell
0.6897518084	significant improvement
0.6895343323	case studies
0.6892823216	underwater images
0.6891630344	temporal correlation
0.6891035383	root mean squared
0.6888280566	manifold learning
0.6887982812	high risk
0.6886452406	cell carcinoma
0.6885407328	reconstruction quality
0.6884437586	infected patients
0.6883881489	segmentation mask
0.6883781551	image sensor
0.6883663732	search algorithm
0.6881336962	feature representations
0.6875372400	classification accuracy
0.6874760927	vision tasks
0.6872998500	brain volume
0.6870780089	low light conditions
0.6865648766	similar patches
0.6863831593	light intensity
0.6863793020	smartphone based
0.6863712624	ldr images
0.6860721869	multi temporal
0.6860670174	spatial alignment
0.6859336713	experimentally demonstrate
0.6859284307	event based
0.6858177049	landsat 8
0.6855899645	great success
0.6853156460	early detection
0.6849787292	sampling patterns
0.6847393303	feature pyramid network
0.6846371069	linear model
0.6842567695	retinal disease
0.6841384723	fast iterative
0.6840796776	downstream tasks
0.6838626115	quantitative evaluation
0.6837307133	challenging conditions
0.6836553680	matched filter
0.6835520833	perceptual similarity
0.6834193564	encrypted images
0.6832160250	significantly higher
0.6831106651	image deraining
0.6829037865	temporal dynamics
0.6826966606	cancer diagnosis
0.6826414891	attention map
0.6825394696	small scale
0.6825302082	deep networks
0.6824961940	hybrid approach
0.6823895563	content adaptive
0.6822157647	light sources
0.6821876650	multiple levels
0.6815774535	brain tumor patients
0.6815246404	extensive experimental
0.6812622436	image manipulation
0.6811499544	decision making process
0.6809291190	sar images
0.6804655841	prior works
0.6801829395	motion fields
0.6799156844	high efficiency
0.6798258585	grayscale images
0.6798228211	signal to noise ratios
0.6797535883	mean absolute error
0.6795892360	quality of experience
0.6793986217	low bit
0.6793472409	temporal information
0.6793142752	facial images
0.6792985779	rgb camera
0.6792788323	previous approaches
0.6792593545	cell types
0.6790082470	gan based
0.6789755467	resolution enhancement
0.6786846967	dense block
0.6785991334	human head
0.6784017292	least squares
0.6781073118	horizontal and vertical
0.6779366456	recent advancements
0.6778417028	learning strategy
0.6778193872	segmentation masks
0.6775225862	risk factors
0.6773295181	co occurrence
0.6771394150	deep reinforcement learning
0.6770730130	challenging problem
0.6766157483	entropy model
0.6765935175	research area
0.6765789270	cifar 100
0.6762876812	single view
0.6760422678	comparable results
0.6759894304	parameter maps
0.6756109211	panoramic images
0.6755605815	biomedical image
0.6753678277	road segmentation
0.6753470259	machine learning based
0.6750051675	important role
0.6749998931	vgg 16
0.6747279527	patient cohort
0.6745689461	dense depth
0.6739399150	generalized gaussian
0.6738313854	de raining
0.6737571982	pneumonia detection
0.6734045995	accurately estimate
0.6730829335	research directions
0.6730795623	retinal layer
0.6730235320	takes into account
0.6728649916	sample sizes
0.6726570893	multiple modalities
0.6726396453	data fusion
0.6724126984	clinical setting
0.6719514562	high level semantic
0.6718073411	input output
0.6713693617	breast cancer diagnosis
0.6712903687	hidden layers
0.6710423695	unique challenges
0.6709701965	plug and play
0.6705155070	zero shot
0.6703299322	right ventricle
0.6701691604	limited memory
0.6699384594	acquisition protocols
0.6699086468	contrast to noise ratio
0.6697688547	significantly outperform
0.6694902971	pre processed
0.6689307015	hyperspectral images
0.6685469615	higher level
0.6684618518	human visual system
0.6683216230	brain lesions
0.6681807364	embedding space
0.6681134730	computer aided diagnosis
0.6680330470	facial recognition
0.6680084346	neuroimaging data
0.6677492751	neuroimaging initiative
0.6677482115	isic 2018
0.6677373466	medical image fusion
0.6675953645	application specific
0.6675403232	dice coefficients
0.6672857543	diagnostic accuracy
0.6670070720	model agnostic
0.6668815290	coherent imaging
0.6668060779	unsupervised manner
0.6667671644	breast mri
0.6666593066	results confirm
0.6666270367	temporal bone
0.6665049453	image fusion
0.6664244199	ensemble learning
0.6663046432	deep learning approaches
0.6661662879	tissue sections
0.6658684264	increasingly important
0.6655363040	image to image translation
0.6653901798	competitive results
0.6652540448	typically require
0.6652368030	expert radiologists
0.6652213761	forward problem
0.6650848873	labeled training data
0.6650218659	cifar 10
0.6649726574	total number
0.6644458766	increasing attention
0.6643302778	experimental results showed
0.6642548143	diffraction pattern
0.6641178404	slice wise
0.6636878696	diabetic macular
0.6634702589	content aware
0.6633910266	dl based
0.6632177021	speed of sound
0.6631200826	residual attention
0.6630155733	data regime
0.6628178122	large size
0.6625318786	global pandemic
0.6625057771	experiments confirm
0.6622771274	lesion size
0.6622256555	spatial angular
0.6622090519	multi branch
0.6617861401	domain translation
0.6617176488	image priors
0.6616542063	statistical properties
0.6616130311	neural image compression
0.6615993218	motion tracking
0.6615656439	supervised training
0.6615212544	rgb d
0.6613331489	convolutional dictionary
0.6613073826	object interaction
0.6612320029	inference speed
0.6611701961	proposed method
0.6611030420	brain magnetic resonance
0.6604703294	re id
0.6604437539	discriminative features
0.6600424740	vision systems
0.6597345491	deep learning frameworks
0.6596842602	experiments demonstrate
0.6595972452	parallel mr
0.6595025649	clinical decision making
0.6593665840	remote sensing images
0.6593332833	divide and conquer
0.6592383480	processing steps
0.6589912991	nerve head
0.6585935679	result shows
0.6585206042	orders of magnitude
0.6584461673	multi focus
0.6583926038	average pooling
0.6581309224	acquisition protocol
0.6580811577	u nets
0.6579756584	retinal image
0.6578651994	compressed images
0.6578360827	low level vision
0.6578304229	high computational cost
0.6578242421	pre treatment
0.6577334794	image super resolution
0.6577326971	significant margin
0.6573251759	successfully applied
0.6573124901	fixed size
0.6570691041	susceptibility mapping
0.6570164279	disease detection
0.6567811689	building block
0.6564408890	spectral spatial
0.6563684805	satellite image
0.6562427235	clinical application
0.6561616824	near lossless
0.6558731196	deep generative
0.6557115319	registration error
0.6556781387	type classification
0.6555640950	point of care
0.6555460188	practical application
0.6554460625	dynamic mri
0.6553013068	noise level
0.6541800441	supervised manner
0.6538454589	significantly lower
0.6534444063	paper proposes
0.6533387701	generalization performance
0.6531763529	head and neck
0.6530211902	structured illumination
0.6527559984	quality enhancement
0.6527358248	ultra low
0.6527332543	competing methods
0.6524358351	human eye
0.6518682844	unlike existing
0.6517117658	labeled samples
0.6516478226	conduct extensive
0.6515811689	conduct experiments
0.6514380751	distance map
0.6513820060	artery disease
0.6513597892	imaging protocols
0.6512855965	excellent results
0.6512293742	axial resolution
0.6510984587	highly efficient
0.6510287546	chest ct images
0.6505317381	manual labeling
0.6504125733	compression rate
0.6504045326	ct scanners
0.6504005816	accurately classify
0.6503745704	quantitative measures
0.6502702714	chest x ray images
0.6502195289	extremely challenging
0.6499669695	detection and recognition
0.6491947504	subjective scores
0.6490770946	existing approaches
0.6486526308	high quality reconstructions
0.6484985889	manual segmentations
0.6484581803	network pruning
0.6483442621	mathematical models
0.6479528561	k nearest
0.6478058246	research community
0.6477245139	medical applications
0.6476089890	uncertainty measures
0.6474286769	qualitative evaluations
0.6473393668	medical experts
0.6473152287	higher spatial resolution
0.6470653273	clinical applications
0.6470577370	sentinel 2
0.6469917492	convergence speed
0.6469161694	regularization by denoising
0.6469069313	convolutional network
0.6468986929	conditional adversarial
0.6468909529	point spread
0.6468741181	raw images
0.6465226289	comprehensive experiments
0.6464571366	disease severity
0.6462994073	quantitatively and qualitatively
0.6461783234	least square
0.6458432288	self driving cars
0.6458251210	depth sensing
0.6454421963	sampling strategy
0.6452369471	routine clinical
0.6452175263	feature descriptors
0.6450908137	oct images
0.6449915944	single step
0.6448496613	guided attention
0.6447390428	motion vectors
0.6447052580	main challenge
0.6446427064	conducted experiments
0.6445395370	application scenarios
0.6444506731	temporal context
0.6444336751	rgb image
0.6441600396	multispectral image
0.6441576976	class labels
0.6435964296	pet images
0.6435739715	segmentation tasks
0.6434418453	shape prior
0.6432948445	recognition tasks
0.6432407280	forward model
0.6429477991	satellite data
0.6428147989	instance level
0.6427892972	near infrared
0.6427361598	imagenet dataset
0.6427311732	brats 2019
0.6426858534	no reference image quality assessment
0.6426138149	statistical analysis
0.6425388523	singular value
0.6425234691	numerical simulation
0.6424049210	digital images
0.6423080590	displacement field
0.6421969733	coherent illumination
0.6421677862	significantly reduced
0.6420229322	sentinel 1
0.6420204337	performance improvements
0.6419117371	annotation free
0.6417398648	point sets
0.6416878272	medical image synthesis
0.6415623311	benign and malignant
0.6414563113	adversarial samples
0.6413504398	functional mri
0.6413158751	training strategies
0.6412371680	diagnostic quality
0.6410629495	limited data
0.6410278829	user study
0.6409232503	atlas based
0.6406986971	important factor
0.6406815079	target recognition
0.6405435549	unlabeled target
0.6403968816	tissue properties
0.6402216447	edge information
0.6402055385	retinal diseases
0.6398848611	promising performance
0.6394381450	ultrasound image
0.6393557400	biomedical research
0.6391577865	highly desired
0.6390567013	achieves competitive
0.6389248474	optical tomography
0.6389085316	automatic segmentation
0.6389036021	recently gained
0.6387910052	standard convolution
0.6386898418	compression artifacts
0.6386421941	microscopy images
0.6385585381	main idea
0.6384537573	qualitatively and quantitatively
0.6381916836	wireless capsule
0.6381658533	sampling method
0.6381480472	visual attention
0.6380157507	human observers
0.6376597105	dnn based
0.6376211322	fundus image
0.6375917013	noisy data
0.6375265056	energy ct
0.6374586542	label noise
0.6372237042	frequency components
0.6370605445	low field
0.6366143630	breast ct
0.6363486978	spatial domain
0.6362080108	slice by slice
0.6361645574	jpeg images
0.6361415478	breast tumors
0.6359219988	uncertainty estimates
0.6359189878	results showed
0.6357371703	imaging through scattering media
0.6357166382	deep learning enabled
0.6354919381	age related
0.6351744352	newly proposed
0.6350409220	blood volume
0.6350400548	significant reduction
0.6349365536	higher accuracy
0.6346359629	retinal images
0.6346068122	modality specific
0.6345178763	image patches
0.6344349952	back propagation
0.6341231235	weak labels
0.6340108217	fast inference
0.6338924046	texture analysis
0.6336537012	deep image compression
0.6336013825	accurately detect
0.6333916265	linear inverse problem
0.6329975218	fmri data
0.6329403475	cnn architectures
0.6328172367	cosine transform
0.6326709668	experimental setup
0.6325077512	deep learning methods
0.6323346650	psnr and ms ssim
0.6323342050	computational power
0.6322545023	relative error
0.6322296604	spectral reconstruction
0.6319825473	numerical examples
0.6317414907	source codes
0.6316737266	sampling density
0.6316466787	encoder decoder structure
0.6316375985	blurred image
0.6315878466	computation complexity
0.6314192005	annotated data
0.6313729908	human subjects
0.6312152699	proof of principle
0.6310633167	reference standard
0.6309381735	gradient based
0.6308647151	proof of concept
0.6307862433	x ray
0.6307571721	natural image
0.6307343435	brain regions
0.6306488410	target detection
0.6305165227	deep residual network
0.6302261088	off axis
0.6301212419	real world applications
0.6301113124	image recovery
0.6299171211	cellular structures
0.6297510950	color distortion
0.6296855977	challenging task
0.6293497199	visual recognition
0.6292420236	extensively evaluated
0.6289956039	fusion network
0.6288394812	generate high quality
0.6287359022	convolution neural
0.6285112679	biomedical images
0.6284993321	performance gains
0.6284326071	unlike conventional
0.6283511823	objective lens
0.6282326273	considerable attention
0.6281759228	backbone network
0.6280510093	image prior
0.6279249412	data analysis
0.6278654766	unlike previous
0.6278158914	labelled data
0.6276258562	hsi classification
0.6276137876	easy to implement
0.6275719589	visual features
0.6275477785	bottom up
0.6274794374	single source
0.6274043172	sample size
0.6273776914	uncertainty aware
0.6273449187	high dynamic range imaging
0.6271358309	computer aided
0.6268314269	block based
0.6266252449	rgb images
0.6265313241	achieves superior
0.6261723130	generate realistic
0.6261419837	quantitative comparison
0.6261307270	compression efficiency
0.6258652531	remarkable performance
0.6258177256	latent code
0.6257406975	automated diagnosis
0.6256422121	object classification
0.6253937810	laplacian of gaussian
0.6252804341	machine learning algorithms
0.6250303501	fake images
0.6249613437	sar data
0.6249315924	intermediate features
0.6248382139	human effort
0.6246101010	body mri
0.6245038007	subjective evaluation
0.6244883506	head ct
0.6243598561	crucial role
0.6243448137	quality estimator
0.6242595982	back projection
0.6242516147	effectively reduce
0.6240112852	sar image
0.6239088249	denoising algorithms
0.6238822476	brain mr
0.6237520536	visual information
0.6237184565	selection strategy
0.6233797626	spatial relations
0.6233638382	iterative algorithm
0.6232988506	image processing techniques
0.6232160892	interferometric synthetic
0.6231388463	coarse to fine
0.6229631403	line of sight
0.6229417992	quantitative and qualitative evaluations
0.6228442811	data imbalance
0.6228296766	human eyes
0.6224276064	feature spaces
0.6224068696	achieved impressive
0.6223919871	results demonstrate
0.6221334580	memory limitations
0.6221176090	end to end
0.6220743353	image representation
0.6220376542	source domains
0.6219995652	brain structures
0.6218519954	key components
0.6218194513	improved performance
0.6217650641	query image
0.6217300498	partial differential
0.6216262069	cardiac segmentation
0.6213612936	ground truth annotations
0.6213499775	depth information
0.6211300647	representation ability
0.6210693196	main challenges
0.6210078759	k means
0.6209080897	extensive evaluation
0.6207844628	synthetic ct
0.6206883516	performed manually
0.6206472116	data compression
0.6206213560	consistently outperforms
0.6205941649	lidar sensors
0.6205619921	leave one out
0.6202881172	encouraging results
0.6201978401	traditional machine learning
0.6200104652	white gaussian noise
0.6194915943	mri reconstruction
0.6193948172	deep learning architectures
0.6191838101	color channels
0.6191275492	image resolution
0.6189883470	unlabelled data
0.6188617968	sampling rates
0.6187383321	research purposes
0.6187062405	image sequences
0.6186488920	image transformation
0.6185931521	longitudinal data
0.6185167274	positive cases
0.6185027828	rule based
0.6183871260	preprocessing step
0.6182622138	multi objective
0.6182149836	data generation
0.6181979699	kernel size
0.6180966173	spectral imaging
0.6179542712	breast cancer detection
0.6178795006	recently proposed
0.6177855091	field of view
0.6177800414	brain extraction
0.6176757042	public dataset
0.6176499967	image guided
0.6174577643	disease classification
0.6174389412	l spect
0.6172663024	shows promising results
0.6171672861	level annotations
0.6170335672	training examples
0.6167773236	redundant information
0.6167290531	coherence tomography angiography
0.6165368937	mr image
0.6163739177	powerful tool
0.6160155746	image database
0.6159399695	significantly outperformed
0.6158715051	few shot
0.6158021284	prediction accuracy
0.6157915000	feature learning
0.6156391852	multiple sources
0.6156063006	deep learning approach
0.6149528019	structure aware
0.6146861448	human activity
0.6145403917	fusion strategy
0.6144254888	simple yet effective
0.6143593807	structural details
0.6143524079	long short term
0.6143112946	u net
0.6141967993	graph neural network
0.6140276418	small datasets
0.6139721184	residual network
0.6139279167	simulated data
0.6139125405	ct imaging
0.6139080026	cancer patients
0.6137655219	brats 2018
0.6135596152	low resource
0.6133744166	high level features
0.6133614133	prior information
0.6132984894	joint optimization
0.6132597051	scanning electron
0.6131632304	effectively capture
0.6128634756	small dataset
0.6128387606	parameter space
0.6124783123	convolutional blocks
0.6124582755	deep autoencoder
0.6122348439	color images
0.6121883198	single slice
0.6121223322	phase imaging
0.6119829954	parametric maps
0.6119686308	data sources
0.6114882536	image transmission
0.6114041433	dataset comprising
0.6113145590	recent research
0.6108006460	ct slices
0.6107647870	clinical applicability
0.6105092344	class conditional
0.6103487043	research efforts
0.6101491471	five fold cross validation
0.6101370614	object segmentation
0.6100744716	global scale
0.6099830696	segmentation network
0.6098506671	qualitative analysis
0.6097862540	recognition accuracy
0.6096523329	object pose
0.6096267856	fully connected neural network
0.6095001926	fields of view
0.6094533037	computer assisted
0.6093164268	optical flow estimation
0.6092842386	t1 and t2
0.6090391731	order of magnitude
0.6087898321	end to end trainable
0.6087792353	target modality
0.6087625191	deep features
0.6087592886	multi frequency
0.6085586379	thermal images
0.6085558745	greatly improved
0.6085411237	feature engineering
0.6084263602	undersampled data
0.6083697615	positive and negative
0.6082389906	attention network
0.6082293404	qualitative and quantitative
0.6081589959	significant challenges
0.6080531183	generative networks
0.6080296300	optimal solution
0.6079069915	raw data
0.6076608573	current approaches
0.6076247588	patch level
0.6075396543	multiple views
0.6073870171	initial results
0.6072713475	lung disease
0.6070446969	main contribution
0.6069676067	acquisition speed
0.6068359756	color image
0.6068355209	model based
0.6067882975	whole slide
0.6066902633	non destructive
0.6066408367	missing data
0.6066295915	representation capability
0.6065853759	quantitative imaging
0.6065097369	resnet 50
0.6064938637	training sets
0.6064365760	qualitative results
0.6063340361	parameter free
0.6058358040	manual labels
0.6057340760	attention block
0.6054186077	compressed sensing mri
0.6053378958	lateral resolution
0.6051054885	ct reconstruction
0.6050402700	relevant information
0.6048707462	liver tumor
0.6047173332	image signal processing
0.6045006946	topological features
0.6044219445	manual segmentation
0.6041595101	existing solutions
0.6041593109	super resolution microscopy
0.6040820097	iterative process
0.6039042827	whole heart
0.6037900156	image recognition
0.6037127748	unsupervised fashion
0.6036939933	ct dataset
0.6036338086	region segmentation
0.6035748482	clinical ct
0.6034622256	achieved remarkable
0.6034425407	recently developed
0.6034264662	ill posedness
0.6034072057	spatial temporal
0.6033610965	vital role
0.6032227293	feature level
0.6031498110	shows superior
0.6029850085	open source tool
0.6028888972	satisfactory results
0.6028122436	5 fold cross validation
0.6026912203	disease specific
0.6025291687	cnn architecture
0.6024464812	visual question
0.6022029593	deep learning algorithms
0.6021572215	image augmentation
0.6021481219	reconstruction network
0.6019643285	red blood
0.6019206529	ai models
0.6018084756	biological tissue
0.6017427755	transform domain
0.6015499586	low grade
0.6014102014	additive noise
0.6010110433	imaging technique
0.6007774732	depth of field
0.6001633862	pretrained models
0.6001514336	ill posed
0.6001199503	level labels
0.6000345657	cnn model
0.5997128036	unpaired image to image translation
0.5996886951	step by step
0.5996642995	unet architecture
0.5996248345	accurately segment
0.5995037427	time lapse
0.5994729661	information contained
0.5993318961	joint learning
0.5992688845	regularization parameter
0.5988464894	undersampled k space data
0.5987622500	resonance images
0.5986433090	fully convolutional neural network
0.5985954472	foreground and background
0.5982909213	psnr and ssim
0.5982338063	follow up
0.5980911579	image sensors
0.5980351929	wide field of view
0.5977703551	cloud based
0.5974199891	volumetric data
0.5973284785	achieved excellent
0.5973262524	vision applications
0.5972955749	structure similarity
0.5971456216	subspace learning
0.5970575907	original input
0.5969370556	brain mr images
0.5965235162	covid 19 infection
0.5963774911	shown remarkable
0.5963169454	joint reconstruction
0.5962885661	hidden objects
0.5962205590	poor performance
0.5959410552	angular information
0.5958870840	distance metric
0.5957410715	learning techniques
0.5954608389	neural architecture
0.5953178042	hyperspectral super resolution
0.5951880077	wide angle
0.5951853748	cxr images
0.5951329781	long acquisition
0.5949940485	segmentation challenge
0.5949655486	high probability
0.5949296591	compression methods
0.5946153166	image content
0.5944134667	highly undersampled
0.5941737971	large datasets
0.5941414982	human intervention
0.5938459226	noise free
0.5937839361	image space
0.5936223079	validation dataset
0.5934620676	cardiac ct
0.5932720640	scene classification
0.5932558843	camera model
0.5931801874	temporal consistency
0.5930170849	multiple scattering
0.5929714943	impedance tomography
0.5929588742	video data
0.5929262006	informative features
0.5928127997	high resolution images
0.5927894182	enhancement technique
0.5927067246	pre train
0.5925844978	fully sampled data
0.5924993840	labeled source
0.5923869443	normalization layers
0.5923075030	propagation based
0.5921612331	expert level
0.5919448900	frame level
0.5919340015	global local
0.5918749580	shown promising results
0.5918619547	comprehensive evaluation
0.5917655927	top hat
0.5915360037	multi scale features
0.5913387227	brain volumes
0.5912795082	context information
0.5911429785	effectively improve
0.5910926489	x rays
0.5910265625	inpainting network
0.5907660413	detailed analysis
0.5907518550	hyperspectral image analysis
0.5903880678	imbalance problem
0.5903201645	reconstruction methods
0.5900363677	projection data
0.5899519742	object localization
0.5896476406	cardiac structures
0.5895698619	medical data
0.5895408969	descent algorithm
0.5893626272	past few years
0.5893491347	ct image
0.5891554625	optimization method
0.5889836109	processing unit
0.5888476929	low count
0.5887766512	hierarchical feature
0.5887655059	sensitivity and specificity
0.5887056200	numerous applications
0.5886391728	based trackers
0.5884056282	automated methods
0.5882004514	optimization techniques
0.5875243095	significant difference
0.5873518578	imaging problems
0.5873234901	imu data
0.5871703787	t2 *
0.5867729090	brain imaging
0.5864290421	multi shot
0.5863710011	generated images
0.5863424306	ct scan images
0.5863217004	data fidelity
0.5863115862	deep generative model
0.5863033571	multi target
0.5862674886	ct image reconstruction
0.5862660740	reconstructed image
0.5862113152	benchmark dataset
0.5861468107	gan generated images
0.5861131232	optimization problems
0.5859676517	image features
0.5856062006	sensor fusion
0.5855608766	probability model
0.5854735376	learning based methods
0.5851655963	deep network
0.5850549874	methods fail
0.5850144898	video super resolution
0.5849764363	image preprocessing
0.5847002887	domain invariant
0.5845573649	resource limited
0.5845114978	performance degradation
0.5844892547	trade off
0.5843155525	segmentation networks
0.5843007255	learned features
0.5842544030	attribution methods
0.5842369368	cell detection
0.5841305283	heart disease
0.5840955345	video generation
0.5839884615	data driven approaches
0.5838734013	lung ct
0.5835137515	anatomical regions
0.5831501461	single scale
0.5829863316	top down
0.5827318678	sampled k space data
0.5826058234	high temporal resolution
0.5825465007	generated samples
0.5822256518	skeleton based action
0.5821608358	weighted loss
0.5819030153	r2 *
0.5813108879	network architectures
0.5810665281	w net
0.5809329793	automatically detecting
0.5809304627	active research
0.5809284917	images captured
0.5807895058	weights and activations
0.5807175644	achieves higher
0.5801125754	label maps
0.5800062820	crucial step
0.5799400162	overall survival
0.5793411759	detection rate
0.5792871044	residual networks
0.5792071809	encoding and decoding
0.5791869871	camera motion
0.5791123125	recurrent convolutional
0.5790632045	learning framework
0.5789000424	direction method of multipliers
0.5788764420	video dataset
0.5787213896	self supervised
0.5784219046	difficult task
0.5783138803	volumetric imaging
0.5781081656	applications of deep learning
0.5780258199	encoder network
0.5777799837	post processing step
0.5772625203	encoder decoder network
0.5765792898	position and orientation
0.5764802463	imaging device
0.5761784483	fully exploit
0.5759671519	based methods
0.5758721019	camera sensors
0.5758703054	final classification
0.5758010248	significantly increase
0.5757574372	post training
0.5756495972	group based
0.5755786506	previous methods
0.5754102053	absolute error
0.5752434337	object categories
0.5751928084	adversarial images
0.5748143772	pixel space
0.5747971442	thin film
0.5746976661	image dataset
0.5746753622	noisy images
0.5744884878	\ url https
0.5742982887	input data
0.5742894178	iterative method
0.5742492518	model predictions
0.5741495781	constrained optimization
0.5738805617	motion artifact
0.5738785358	cell imaging
0.5738144816	unseen test
0.5734734759	hierarchical structure
0.5732516152	task driven
0.5732091876	information flow
0.5731243464	multiple domains
0.5729021816	reference frames
0.5726385088	uncertainty analysis
0.5723506307	optimization algorithms
0.5723096876	fusion module
0.5721729659	neural network based
0.5718713641	aerial vehicle
0.5714841836	point of view
0.5714039690	achieve comparable
0.5708199773	spatial resolutions
0.5708189046	automated detection
0.5707359283	achieves superior performance
0.5701452750	lr image
0.5700687815	automatic detection
0.5699360918	ntire 2020
0.5698618409	sensor data
0.5698254387	image acquisition
0.5697711880	flow based
0.5697655059	generator and discriminator
0.5696134454	significant attention
0.5695302780	entire network
0.5694252962	volumetric medical
0.5693116883	challenge dataset
0.5692601526	peak signal to noise
0.5692328386	real data
0.5689202198	f score
0.5686700714	unsupervised deep learning
0.5685570855	graph convolutional
0.5684713967	cycle consistent generative
0.5684319680	wavelet based
0.5683525605	large variations
0.5683497399	tissue segmentation
0.5683282659	class label
0.5674445185	motion information
0.5667285798	superior results
0.5662735187	fpga based
0.5661422669	whole slide images
0.5660508404	ct data
0.5659426241	highly effective
0.5656812309	inter and intra
0.5655868329	speech enhancement
0.5654768341	covid 19
0.5652737892	cross resolution
0.5652381433	b1 +
0.5650985201	p 0.05
0.5648750136	difficult problem
0.5648142056	pathology segmentation
0.5647084589	deep cnns
0.5646130351	low signal to noise ratio
0.5644834828	target domains
0.5644453963	image matching
0.5644316334	deep reinforcement
0.5642622484	probabilistic model
0.5642016676	fusion method
0.5641545506	efficiency video coding
0.5640882829	cancer risk
0.5640870877	clean images
0.5638812152	graph attention
0.5637165194	decide whether
0.5636038023	class classification
0.5634825497	geometric features
0.5634797028	simulation study
0.5634674705	optimization algorithm
0.5633313568	ground truth images
0.5630674392	cardiac motion
0.5629086032	guided deep
0.5627977992	corrupted images
0.5627867119	resolution limit
0.5624582990	significant role
0.5623176635	optical imaging
0.5622679981	infrared images
0.5621671873	medical image classification
0.5618150633	diffusion tensor
0.5617552206	data collected
0.5614604697	deep learning framework
0.5614309383	based adaptive
0.5612916561	texture features
0.5611170155	fast algorithm
0.5610802404	regularization parameters
0.5607042626	multi phase
0.5606401321	data consistency
0.5606119723	times faster than
0.5602179917	robust segmentation
0.5600163775	reconstruction algorithms
0.5593523958	image noise
0.5591475576	training labels
0.5588865022	ground truth data
0.5588142836	paper introduces
0.5586836169	shape analysis
0.5586149146	sequence to sequence
0.5585380631	noise sources
0.5584784802	computational methods
0.5584004691	cmr images
0.5583575012	non uniform
0.5582307627	context based
0.5579535488	amplitude and phase
0.5578269610	b mode
0.5577903442	p 0.001
0.5574902335	random noise
0.5574889251	deep convolutional networks
0.5574802302	cell classification
0.5574734481	ultrasound data
0.5573921553	image compression framework
0.5573513223	traditional methods
0.5572331656	face images
0.5571725046	illumination microscopy
0.5570287638	achieves comparable
0.5566424840	enhancement network
0.5566236258	global features
0.5566210437	formation model
0.5563777505	image restoration tasks
0.5562868354	extracted features
0.5556478356	learning approaches
0.5555751608	image based
0.5555533784	texture information
0.5555214572	multi band
0.5554484258	phase recovery
0.5553054138	morphological features
0.5552600438	encoder and decoder
0.5551403143	oct scans
0.5551142724	dataset includes
0.5550889592	real world settings
0.5550254468	quantitative features
0.5547124151	fetal motion
0.5546141354	node features
0.5546134858	fourier ptychographic
0.5544185297	cancer related
0.5542911985	feature based
0.5542554154	model uncertainty
0.5541998553	self supervision
0.5540634069	anatomical information
0.5538759048	cnn models
0.5534653409	clinical diagnosis
0.5534392197	segmentation methods
0.5533141765	automatically detect
0.5531617445	image and video
0.5531593573	application areas
0.5531482646	dynamic vision
0.5531275433	annotated training data
0.5531196291	semantic features
0.5527832745	stereo images
0.5527729013	performance metrics
0.5526871454	baseline model
0.5523114570	non rigid
0.5518507814	arbitrary style
0.5517466599	simulation and experimental results
0.5514910448	tensor based
0.5513326816	cancer cells
0.5512872289	shape information
0.5512483246	weakly supervised learning
0.5510980163	video prediction
0.5510714039	square error
0.5509961524	re identification
0.5507186130	cell segmentation
0.5506246045	video segmentation
0.5503963960	image interpolation
0.5503188578	dmri data
0.5502595453	image level
0.5501413521	mild cognitive
0.5500656784	structural mri
0.5499660208	image set
0.5498856743	projection algorithm
0.5497884220	energy based
0.5492771283	computer vision
0.5491105016	domain transfer
0.5486938994	360 degree
0.5485956720	registration method
0.5485444048	anatomical structure
0.5483906282	important features
0.5482454574	video enhancement
0.5481532313	non convex
0.5479650723	deep learning based methods
0.5478844338	real and fake
0.5478460892	diagnosis and treatment
0.5475223237	training procedure
0.5475111603	until now
0.5473985858	method named
0.5473439040	evaluations demonstrate
0.5472162483	architecture called
0.5471893951	histology image
0.5470059519	gan training
0.5466425559	machine learning models
0.5463701728	annotated images
0.5461927542	r2 =
0.5460109659	quantitative assessment
0.5456702412	mammography images
0.5456531049	b mode ultrasound
0.5455412451	test cases
0.5454396804	network design
0.5453651841	cross dataset
0.5453548093	stochastic gradient
0.5452865497	spatial correlation
0.5451077413	significant potential
0.5449641097	sparse sampling
0.5449244134	deep neural network architectures
0.5447833904	similarity metrics
0.5446499045	fast fourier
0.5446162960	imaging scenarios
0.5443444455	camera and lidar
0.5442384939	dose ct
0.5442246171	learning based image compression
0.5440586627	unified framework
0.5439113455	image degradation
0.5436257854	recognition systems
0.5436253851	rigid registration
0.5434352262	degraded images
0.5433734703	^ *
0.5431059800	aided diagnosis
0.5430323823	transform learning
0.5430047756	cancer detection
0.5429503442	performance comparison
0.5425077206	ex vivo
0.5422149894	regularization methods
0.5421746658	video based
0.5418825563	image processing tasks
0.5417252866	variational auto
0.5416282149	imaging studies
0.5414407762	neural network architecture
0.5408610650	local features
0.5408164468	super resolution network
0.5408134979	proposed method achieves
0.5407294530	kidney and tumor
0.5406986926	quantitative and qualitative
0.5406782901	images acquired
0.5405072820	likelihood estimation
0.5404120869	dynamic objects
0.5403541113	aperture imaging
0.5402676387	interpolation network
0.5400441136	co registered
0.5400007346	learning methods
0.5398360663	information loss
0.5398241139	image domain
0.5397595647	noisy image
0.5395693285	deep learning model
0.5395297356	pre trained models
0.5393826250	accurate predictions
0.5392563680	low resolution images
0.5387668563	brain images
0.5386308262	refinement network
0.5385638973	subjective and objective
0.5384607825	baseline method
0.5382993960	image data
0.5382509487	phase step
0.5381410942	clean image
0.5380991219	pneumonia cases
0.5380978516	hr images
0.5378457320	vision models
0.5377915334	sign detection
0.5375612251	cbct images
0.5375209636	background noise
0.5374830992	region based
0.5371700430	noise models
0.5371266000	fmri datasets
0.5368344224	texture classification
0.5366999280	quantitative analysis
0.5366456169	down sampled
0.5366101988	ultrasound image segmentation
0.5365842327	non invasive
0.5363303598	visual localization
0.5363183930	medical image reconstruction
0.5362366167	man made
0.5362340259	low light imaging
0.5361516448	training and testing
0.5360521234	mri data
0.5360482530	t1 weighted mri
0.5359855972	image domains
0.5359746349	open dataset
0.5358963095	image alignment
0.5357666466	method outperforms
0.5353995777	visual analysis
0.5352395535	deep models
0.5352007071	paired data
0.5351708228	human motion
0.5351277569	learning approach
0.5350240419	human brain
0.5344634065	achieve superior
0.5343507391	level features
0.5342813654	inference accuracy
0.5340921112	lr images
0.5340857626	efficient solution
0.5340214831	p value
0.5340162095	long short
0.5338471376	consistency loss
0.5334970378	attention networks
0.5334777747	diagnosis and prognosis
0.5334519008	brain segmentation
0.5333995814	imaging through scattering
0.5333761716	reconstruction error
0.5333173644	method achieves
0.5333173175	magnetic resonance image
0.5332925299	low and high
0.5331463884	hybrid loss
0.5330953774	spatial pyramid
0.5329319039	sub aperture
0.5328875769	significantly faster
0.5326850243	automated segmentation
0.5326258746	octa images
0.5324926619	context adaptive
0.5320208370	three dimensional
0.5317488016	audio and video
0.5316168587	structural and functional
0.5315957610	features extracted
0.5314971643	deep architectures
0.5314962415	human machine
0.5313183114	fast and accurate
0.5308410980	extremely low
0.5305159742	histological images
0.5302767733	detection method
0.5301953896	localization and mapping
0.5301255415	annotated datasets
0.5300564033	microscopy data
0.5300532548	map estimation
0.5299321298	prior distribution
0.5296997440	imaging data
0.5296716183	self similarity
0.5295284444	target images
0.5294820341	covid 19 pneumonia
0.5293315811	clinical settings
0.5292861789	brain structure
0.5292433125	rapid development
0.5291348965	light microscopy
0.5291299376	high grade
0.5289518222	hybrid deep
0.5288969952	patient data
0.5287023554	single energy
0.5286802144	breast tissue
0.5286516448	spatial and temporal
0.5284548079	data sharing
0.5283706520	y net
0.5283433556	random sampling
0.5281771540	benchmark datasets demonstrate
0.5279427014	shape and texture
0.5278566546	no reference
0.5275371977	small objects
0.5275183651	potential applications
0.5274908036	phase retrieval algorithm
0.5271824524	high and low
0.5270881548	265 hevc
0.5269838758	multispectral filter
0.5269525575	sensor noise
0.5268663806	8 bit
0.5268483445	shown promising
0.5268324128	predictive model
0.5268198383	slide level
0.5268101637	et al
0.5268035262	fusion techniques
0.5266822937	pyramid network
0.5266101708	mr reconstruction
0.5264940913	functional magnetic resonance
0.5264143462	improved reconstruction
0.5263789175	local and global
0.5263130351	super resolution imaging
0.5262988019	transfer learning approach
0.5262228643	results highlight
0.5261054893	ill posed inverse problem
0.5260804185	machine learning approaches
0.5259462234	calibration method
0.5259262251	labeled images
0.5259118309	non invasively
0.5258474364	detection accuracy
0.5255134776	data hiding
0.5253017995	264 avc
0.5252295494	ultra high
0.5250844357	computer aided detection
0.5247909033	positive rate
0.5245315634	applying deep learning
0.5244869828	hybrid method
0.5244334651	hyperspectral image super resolution
0.5242602864	processing algorithms
0.5241132354	reconstruction techniques
0.5240140914	surface reconstruction
0.5240125739	processing speed
0.5239344022	inpainting based
0.5238203125	reconstruction technique
0.5236516448	spatial and spectral
0.5236147279	deep learning based approaches
0.5236093681	theory and methods
0.5234408959	rate reduction
0.5234178938	high acceleration
0.5233157979	labeled datasets
0.5232326744	video processing
0.5231713601	generator network
0.5230442256	image search
0.5227935641	weighted imaging
0.5227045585	test data
0.5226448432	95 ci
0.5226338232	medical imaging applications
0.5224968551	sr methods
0.5224353811	temporal features
0.5223365023	gan models
0.5223215904	detection methods
0.5222267639	regression based
0.5217860620	flow estimation
0.5216385114	tumour segmentation
0.5214864887	open challenge
0.5212376663	supervised and unsupervised
0.5211454926	limited training data
0.5210949421	functional magnetic
0.5209927139	low resolution image
0.5207777131	simple and efficient
0.5205774116	motion prediction
0.5202799546	segmentation algorithm
0.5201641671	lesion regions
0.5200126001	key idea
0.5200021208	multiple images
0.5199893492	numerical results
0.5197456151	detection model
0.5196403047	synthetic images
0.5196369556	machine learning methods
0.5195536861	achieve competitive
0.5195239507	non uniformity
0.5194898778	signal detection
0.5194626375	specifically designed
0.5192993893	approach yields
0.5191712435	down sampling
0.5191299162	spatial transformation
0.5190568371	pixel based
0.5190311773	phase information
0.5189087656	disease related
0.5187349781	training and validation
0.5185778294	image transformations
0.5182392449	generating high quality
0.5180840939	learning scheme
0.5180837327	convolution based
0.5180036488	simulated and real
0.5178076629	detection and tracking
0.5177214393	pet data
0.5174925451	covid 19 cases
0.5172785944	spectral analysis
0.5172750031	results illustrate
0.5171910152	generated data
0.5171199080	detect and classify
0.5169918053	data driven approach
0.5169592826	simulation results
0.5168065922	train and test
0.5166064962	semantic information
0.5164503322	images and videos
0.5164191585	object based
0.5160660345	difficult to obtain
0.5154492638	detection and segmentation
0.5149996031	generating synthetic
0.5147667124	image to image
0.5145975702	quality evaluation
0.5143696579	hdr images
0.5141380258	training dataset
0.5140228569	global and local
0.5136091242	non line of sight
0.5135049859	fast and robust
0.5134264972	noise robust
0.5132224423	approach outperforms
0.5128330334	time series
0.5127184559	mask based
0.5125282629	registration network
0.5124798067	human expert
0.5123706123	deep transfer learning
0.5123657977	automatically segment
0.5123188560	digital breast
0.5122750615	optical systems
0.5120018392	camera parameters
0.5119503551	360 \ deg
0.5119207633	mri images
0.5119019929	segmentation results
0.5117405733	data synthesis
0.5116773866	unpaired image to image
0.5116463670	distorted images
0.5113334629	source and target
0.5110051387	tumor detection
0.5108158470	test dataset
0.5104571565	clinical research
0.5103897840	imaging speed
0.5103112029	brain networks
0.5102153697	multi shell
0.5102018026	histopathology image
0.5099811974	output images
0.5097992810	sparse signal
0.5096764440	accurate classification
0.5096258476	becoming increasingly
0.5094826656	intensity based
0.5089421634	network structure
0.5089225687	similar performance
0.5088480227	diagnostic imaging
0.5088370168	bayesian deep learning
0.5088230271	cloud shadow
0.5088091731	sr network
0.5087859534	classical methods
0.5087670646	ct and mri
0.5086970680	temporal and spatial
0.5085490254	spiking neural
0.5085049859	trained and validated
0.5084821751	orientation distribution
0.5084492638	segmentation and tracking
0.5083521122	real images
0.5078526017	local structure
0.5077896398	approach achieves
0.5077831776	training and inference
0.5077587984	local information
0.5075673148	tedious and time consuming
0.5075617836	low light images
0.5075242448	\ mathbf x _1
0.5073727437	image formation model
0.5073171467	speed and accuracy
0.5072322649	autoencoder based
0.5071380616	materials and methods
0.5070123342	unsupervised deep
0.5070086792	information processing
0.5069006973	tumor regions
0.5068789247	existing techniques
0.5067833817	diagnostic performance
0.5064890694	real space
0.5060180028	optimization framework
0.5054070814	high efficiency video
0.5053892256	k space
0.5053055420	neural network training
0.5051475778	rgb + d
0.5050906262	labeled dataset
0.5049975184	trained end to end
0.5049637803	model based reconstruction
0.5048672422	segmentation algorithms
0.5048660833	joint spatial
0.5047860008	number of iterations
0.5047200379	captured images
0.5044613144	segmentation accuracy
0.5044067889	independent test
0.5042427804	self attention
0.5040396386	signal reconstruction
0.5040280059	baseline models
0.5038614473	ill posed inverse
0.5037151372	mr image segmentation
0.5036716801	full reference
0.5036699742	develop and evaluate
0.5036677774	image coding
0.5036240809	high frequency information
0.5035810856	spatial relationships
0.5034691218	dimensional light
0.5032502617	classification network
0.5031135125	lidar based
0.5030016588	spectral information
0.5029260435	estimation method
0.5028962379	data processing
0.5028793663	testing phase
0.5027196424	taking advantage of
0.5027122508	accurate and robust
0.5025921309	c + +
0.5025795366	few shot learning
0.5023992659	single shell
0.5023209920	feature points
0.5021339827	video denoising
0.5020601379	fold cross
0.5020001296	trained and evaluated
0.5018176206	style image
0.5018017905	current methods
0.5014749485	optimization based
0.5014333476	based approaches
0.5012706924	synthetic and real
0.5012445110	patch based methods
0.5011261980	effectiveness and efficiency
0.5010648343	quality prediction
0.5009438837	5 fold cross
0.5008810247	image decomposition
0.5008253322	training and test
0.5007315814	data points
0.5005942647	recent approaches
0.5004622508	accuracy and robustness
0.5004137491	video action
0.5003453073	clinical information
0.5003251987	segmentation task
0.5003205097	segmentation maps
0.5002886151	optical coherence tomography images
0.5002791694	research field
0.5001749213	deep feature
0.5001027146	proposed method outperforms
0.4996409255	segmentation framework
0.4994956680	unseen datasets
0.4994734314	biopsy images
0.4994492307	gadolinium enhancement
0.4994382287	modal retrieval
0.4994044091	iterative methods
0.4993978914	depth prediction
0.4992561524	input images
0.4991811313	experiments conducted
0.4991538943	latent features
0.4990679924	open problem
0.4990242706	depth images
0.4988391848	dl models
0.4987772597	scale estimation
0.4985360149	hardware based
0.4984759607	input image
0.4984443798	spectral and spatial
0.4983255264	detection and classification
0.4982872813	image level labels
0.4982805560	increasing demand
0.4981443698	^ \ circ
0.4979824495	self attention mechanism
0.4977735981	real time
0.4977601577	key features
0.4976436945	image details
0.4973821260	unsupervised domain
0.4973594935	grayscale image
0.4972977651	unseen data
0.4971716525	acquisition and reconstruction
0.4967479651	detection and localization
0.4966895235	input and output
0.4965947874	high resolution image
0.4965049859	efficient and effective
0.4958769773	shown great
0.4956161540	learning based approaches
0.4953941460	essential role
0.4953299586	results reveal
0.4952192716	segmentation and classification
0.4948761980	simple and effective
0.4947308017	single pixel detector
0.4944071574	image normalization
0.4943199096	video analysis
0.4942490470	average dice
0.4941636928	model outperforms
0.4941542926	synthetic and real data
0.4939103760	classification task
0.4938945419	low level features
0.4938379048	frame based
0.4937243580	underwater image
0.4933838437	imaging applications
0.4933596779	registration accuracy
0.4931038975	physical model
0.4929036539	3 d
0.4928355620	supervised methods
0.4922893454	target dataset
0.4922043579	experiments performed
0.4922006294	low dose computed
0.4921008353	regularization techniques
0.4920733164	sampling scheme
0.4920641174	global information
0.4917421912	conventional methods
0.4915941883	model size
0.4914757974	restoration tasks
0.4914398269	code and trained
0.4909093574	multiple datasets
0.4907785304	unet + +
0.4907705373	recent methods
0.4904750353	similarity based
0.4903914832	realistic images
0.4901868593	inverse tone
0.4900411863	spatial and channel
0.4899741198	real world datasets
0.4899519166	sr models
0.4899318749	medical image datasets
0.4896344967	models trained
0.4895636625	moir \
0.4893407000	method shows
0.4888686516	ct volume
0.4887122508	accurate and efficient
0.4886896035	reconstruction algorithm
0.4885912630	datasets demonstrate
0.4878209627	method performs
0.4874690781	imaging techniques
0.4874214867	imaging technology
0.4873468010	skeleton based
0.4872274858	training scheme
0.4872144153	classification and segmentation
0.4869916517	color information
0.4869798337	hierarchical features
0.4869336846	connected layers
0.4869094942	brats 2020
0.4868988564	aerial image
0.4867927765	$ \ ell_ \ infty
0.4867758180	high field
0.4867081570	original image
0.4866206355	neural network architectures
0.4860690462	undersampled k space
0.4860547282	trained and tested
0.4859378771	recent success
0.4859295419	real world data
0.4858386043	dimensional space
0.4857478259	coherence tomography images
0.4856285921	residual u net
0.4854605841	time resolved
0.4853392074	test images
0.4852707250	high noise
0.4851862611	accuracy and efficiency
0.4849845020	convolutional neural network based
0.4849343819	experimental design
0.4847154174	unsupervised methods
0.4845277190	highly challenging
0.4843694171	area under
0.4843583065	medical imaging data
0.4843527161	jpeg image
0.4843215351	land use
0.4841695474	contrast enhanced magnetic resonance imaging
0.4840095685	image size
0.4839520062	network structures
0.4837368223	resnet based
0.4836859970	important step
0.4836377880	global spatial
0.4834299532	detailed information
0.4833229520	image series
0.4832209112	h & e
0.4832105703	detection performance
0.4831178886	segmentation predictions
0.4828354284	pulmonary disease
0.4828322203	reconstruction loss
0.4827440954	conventional method
0.4825899867	covid 19 patients
0.4824443355	spread functions
0.4823615879	fr \
0.4815094556	paper addresses
0.4813462946	prediction model
0.4812911863	efficient and accurate
0.4809869229	learning algorithms
0.4809237824	leading causes
0.4808053021	spectrum disorder
0.4807168074	empirical results
0.4805428647	robust and accurate
0.4805428647	code and models
0.4804353828	cancer screening
0.4804285028	method called
0.4801621227	baseline methods
0.4801168342	reconstructed images
0.4797674939	large number
0.4796817895	non contact
0.4795087789	p =
0.4793139135	effective and efficient
0.4792243232	non trivial
0.4790746974	reconstruction process
0.4789298437	model selection
0.4785690810	moving object
0.4781067998	self supervised learning
0.4780136567	achieved promising
0.4775940122	freely available
0.4775535039	built upon
0.4775387662	data acquired
0.4774021475	registration methods
0.4773772505	real noise
0.4773636770	negative rate
0.4772817477	neural net
0.4770569035	achieved great
0.4768302570	limited availability
0.4767405143	mri acquisition
0.4767072520	ai based
0.4765662890	expensive and time consuming
0.4765389272	salient object
0.4763727809	trained models
0.4757577226	image similarity
0.4757031385	experiment results
0.4756108829	clinical studies
0.4755742589	extremely high
0.4755428647	accuracy and speed
0.4753458001	synthetic and real world
0.4750045309	large volume
0.4749694263	clinical data
0.4749479331	measurement noise
0.4747339988	whole body
0.4746630188	phantom data
0.4745488936	embedding based
0.4745460351	content based
0.4744689901	network weights
0.4743504937	feature detection
0.4743410655	forward operator
0.4742814565	n =
0.4742765772	reference data
0.4740721403	fused images
0.4738825868	network parameters
0.4735370349	outperforms existing
0.4733066799	require large
0.4731182075	recent progress
0.4730638980	imaging devices
0.4729052538	learning models
0.4727833070	fusion methods
0.4727606161	takes advantage
0.4727042097	raw image
0.4725683622	noise model
0.4723296433	testing data
0.4723236659	classification performance
0.4723004138	stained images
0.4722596146	paper describes
0.4720111989	invariant feature
0.4719530933	reconstruction accuracy
0.4718315497	phase contrast imaging
0.4718233747	area of research
0.4718133464	training of deep
0.4717825930	tracking performance
0.4717206710	enhancement methods
0.4716901498	compression algorithms
0.4715582150	based classifier
0.4714956280	$ \ ell_2
0.4711702121	camera based
0.4711619352	approach shows
0.4710834051	one shot
0.4710388203	classification models
0.4709237579	accuracy loss
0.4708158020	achieving high
0.4705271932	detection and diagnosis
0.4703048269	segmentation performance
0.4703016455	synthetic datasets
0.4701426053	deep neural network based
0.4697994066	produce high quality
0.4697843978	data distributions
0.4696228616	segmentation models
0.4691217605	multi task network
0.4690202177	base model
0.4689204858	multi object
0.4689080728	proposed approach
0.4688332306	classification problem
0.4688139389	dnn models
0.4686395536	f measure
0.4684114525	model learns
0.4682953681	regression models
0.4682910875	gan model
0.4682482676	medical ultrasound
0.4681755986	clustering algorithms
0.4680066601	synthetic dataset
0.4677171205	unpaired data
0.4675793831	combining multiple
0.4666114127	high end
0.4665162148	achieve higher
0.4664092024	established methods
0.4662772359	graph neural
0.4662606949	test datasets
0.4662576648	methods require
0.4662146282	visual assessment
0.4661260525	great importance
0.4660532067	dense network
0.4659228682	model driven
0.4658157911	spatial features
0.4657819783	segmentation labels
0.4657733604	relevant features
0.4657237852	hdr image
0.4656330272	important information
0.4655668286	reconstruction method
0.4655240512	critical role
0.4654608078	publicly available
0.4654476700	time consuming
0.4648843533	retrieval process
0.4648424718	$ \ ell_1
0.4647769542	model parameters
0.4647763027	whole brain
0.4645023689	deep encoder decoder
0.4644282053	prediction models
0.4641180020	visual speech
0.4639942542	predictive value
0.4639732840	denoising methods
0.4639545887	closely related
0.4639374880	denoising performance
0.4639345764	learned image
0.4637547321	interpolation methods
0.4637236561	segmentation result
0.4636311392	ai systems
0.4635707038	specific features
0.4633724665	generalizes well
0.4633711566	high spatial
0.4633436211	algorithm called
0.4630384880	even though
0.4630205126	few view
0.4629286344	accurate diagnosis
0.4626378128	realistic looking
0.4626268083	two dimensional
0.4625824360	proposed architecture
0.4625786845	validation accuracy
0.4623774413	testing dataset
0.4623311095	similar accuracy
0.4622101121	generated image
0.4621204461	training stage
0.4620304756	enhanced mri
0.4619272975	volumetric video
0.4618054739	trained from scratch
0.4615811188	framework achieves
0.4612748590	human vision
0.4612367308	reference image quality assessment
0.4610824131	related features
0.4608843430	clustering methods
0.4608075308	model achieves
0.4607183315	front end
0.4604170339	closely related to
0.4601407055	detection algorithm
0.4601285840	processing methods
0.4599366408	training process
0.4598965066	results demonstrated
0.4598193329	\ mu m
0.4596622492	10 fold cross validation
0.4595292716	deep model
0.4592453524	\ emph
0.4591064798	deep network based
0.4588228367	traumatic brain
0.4586992974	image pixels
0.4586977809	well established
0.4586432760	method generates
0.4586319993	doing so
0.4585313824	non local
0.4585199485	world health
0.4585055349	study demonstrates
0.4584960715	method produces
0.4584538052	automated algorithm
0.4584309362	deep learning technique
0.4584005454	fully convolutional neural
0.4583878297	convolutional neural network architecture
0.4581999451	clinical decision
0.4581178460	\ cite
0.4577860144	method of multipliers
0.4576682126	large field of view
0.4574875462	compression framework
0.4572361204	generating adversarial
0.4571496459	require manual
0.4571226457	problems in imaging
0.4570137261	^ 2
0.4569583928	challenging datasets
0.4566048049	deep supervision
0.4564697880	dose ct denoising
0.4562569190	gan architecture
0.4561613619	image datasets
0.4559927512	inverse imaging
0.4557594498	require additional
0.4556371830	reconstruction problems
0.4556263251	clustering algorithm
0.4554678840	spectral images
0.4552466007	prediction performance
0.4552299932	disease prediction
0.4551912468	acquisition parameters
0.4551575645	experimental data
0.4551123606	qualitative and quantitative results
0.4546945628	estimation error
0.4545789753	model based iterative
0.4545692185	deep learning based medical image
0.4544332071	brain scans
0.4543786632	remote sensing image
0.4542927125	proposed methodology
0.4539771751	method combines
0.4538844682	training images
0.4535122341	language processing
0.4534865965	\ pm
0.4533518459	reconstruction pipeline
0.4533416514	modified u net
0.4533384021	non parametric
0.4531328416	specific information
0.4531158887	optical diffraction
0.4530341147	public benchmark
0.4530272789	under sampled
0.4530057678	alternative methods
0.4527430499	object level
0.4526755616	decoder network
0.4526343586	model complexity
0.4524110467	based approach
0.4523375433	high cost
0.4523220880	approach produces
0.4518515557	source images
0.4517599363	medical imaging tasks
0.4517081186	takes advantage of
0.4516950869	framework called
0.4513566130	reconstruction problem
0.4513516528	iqa methods
0.4512015547	learning process
0.4511278693	publicly available datasets
0.4510421312	semantic label
0.4506782627	2 d
0.4505592818	transfer learning based
0.4504923258	large number of parameters
0.4504757353	structure information
0.4504360362	technique called
0.4503910825	paper reviews
0.4501538241	model based methods
0.4501056748	statistical models
0.4500674543	v net
0.4499568462	improved image quality
0.4499028746	registration algorithms
0.4498225097	tissue classes
0.4498144061	image regions
0.4496627099	proposed framework
0.4496285764	$ \ beta
0.4494986712	b scans
0.4494264461	\ textbf
0.4493217257	priori knowledge
0.4490140255	data privacy
0.4486167769	target model
0.4484476074	features learned
0.4484271312	data fitting
0.4483513871	retrieval problem
0.4480613296	image contrast
0.4479396051	improves performance
0.4478614551	average accuracy
0.4478584965	capable of producing
0.4477800402	low noise
0.4477434101	training datasets
0.4470828253	motion patterns
0.4469234780	detection and ranging
0.4466160761	mri based
0.4463810289	imaging tasks
0.4460613415	method improves
0.4458921651	imaging sensors
0.4457599115	original data
0.4456752750	target object
0.4454386462	experiments validate
0.4454123865	quality reconstructions
0.4453562825	supervised deep
0.4452608488	vulnerable to adversarial
0.4452059019	generation network
0.4451433191	traditional approaches
0.4451114827	additional information
0.4450692776	network learns
0.4449995054	second order
0.4447901601	data distribution
0.4447495596	geometric information
0.4447468358	level fusion
0.4445096167	per pixel
0.4441107565	supervised models
0.4440668835	method achieved
0.4439264461	\ deg
0.4438319647	light conditions
0.4436428212	high energy
0.4436309488	super resolution methods
0.4434333572	attention u net
0.4431727388	testing accuracy
0.4431169513	wave imaging
0.4430609841	translation tasks
0.4430058889	based metrics
0.4429938223	ensemble model
0.4429891146	large scale video
0.4427305340	datasets including
0.4424409160	nodule classification
0.4421987047	discriminator network
0.4420425074	so far
0.4419714893	head models
0.4419019821	filtered back
0.4415499116	eeg data
0.4415214060	cancer tissue
0.4412094755	simulated and experimental
0.4412029488	existing algorithms
0.4410105631	reconstruction performance
0.4410033262	method yields
0.4409492202	multi camera
0.4407190953	auc =
0.4406891836	alternating direction method
0.4406787040	tissue contrast
0.4406782723	noise patterns
0.4406221081	simulation and experimental
0.4405239342	pre trained model
0.4402869160	essential step
0.4402358236	achieves high
0.4401795449	low light image
0.4397983689	volume segmentation
0.4391402196	determine whether
0.4391250064	dataset consisting
0.4390128257	gaussian model
0.4388130377	method learns
0.4387590323	random fields
0.4387368725	$ t_1
0.4382377897	classification results
0.4381641061	every year
0.4381035392	$ l_2
0.4380843577	significant improvements over
0.4379634381	large annotated
0.4379542509	$ l_
0.4372745815	ir images
0.4371157254	medical image processing
0.4369955904	based metric
0.4364835162	proposed model
0.4362735097	normal brain
0.4361343868	analysis shows
0.4359345748	level information
0.4358892497	non rigid registration
0.4357663002	higher performance
0.4356724454	quality scores
0.4356324571	collected data
0.4355148223	\ textit
0.4354941949	camera pose
0.4353152717	number of training samples
0.4352353183	large margin
0.4351752254	deep denoising
0.4345731223	edge features
0.4339579559	super resolution reconstruction
0.4337602653	speed up
0.4336716006	pathology images
0.4336544770	paper explores
0.4336312027	great significance
0.4334434500	sub band
0.4332762137	denoising algorithm
0.4332241586	attack success
0.4331800094	linear combination
0.4331328781	not necessarily
0.4330855020	domain adversarial
0.4330147103	reconstruction times
0.4327525704	processing step
0.4323327898	large scale datasets
0.4321744107	last years
0.4319907180	data driven methods
0.4319344888	ensemble method
0.4318629564	reconstructing images
0.4318186979	localization accuracy
0.4317789170	domain adaptation methods
0.4314891232	context model
0.4314796952	rank tensor
0.4314257306	x ray images
0.4314234549	algorithm outperforms
0.4310725430	amounts of labeled
0.4309636509	local context
0.4306301435	fast reconstruction
0.4306003256	reconstruction from undersampled
0.4304012309	analysis tasks
0.4303173958	classification and object
0.4302192561	take into account
0.4296191620	$ \ ell_
0.4291651694	retrieval methods
0.4290617672	deep prior
0.4290328743	registration algorithm
0.4287444115	real clinical
0.4287118770	phase microscopy
0.4284193774	extraction and classification
0.4283367449	convolutional sparse
0.4283324694	state of art
0.4280384365	labeled training
0.4278590061	reference image
0.4276797553	high dimensional data
0.4274619707	typically rely
0.4274382217	optical images
0.4274159917	magnetic particle
0.4274038082	single image super
0.4272879184	existing studies
0.4272823857	widely used
0.4271848807	reconstruction module
0.4270598041	reconstruct high quality
0.4267994288	paper demonstrates
0.4267983737	$ \ mu
0.4267173013	real datasets
0.4266902519	wide range
0.4262813721	reference images
0.4261918760	commonly used
0.4261510546	ai applications
0.4260185335	supervised machine
0.4259002367	localization algorithms
0.4258205003	output image
0.4257922529	space sampling
0.4257260132	sub sampled
0.4252300018	sparse data
0.4251666968	depth image
0.4250555195	without losing
0.4249517712	flow imaging
0.4247544349	multi tissue
0.4243905044	image characteristics
0.4243640091	tasks including
0.4243298145	segmentation quality
0.4243159809	additional computational
0.4243118725	\ infty
0.4243039618	image translation tasks
0.4242832671	supervised machine learning
0.4242241942	cardiac magnetic
0.4240593621	augmentation methods
0.4240011266	real image
0.4238739548	image plane
0.4235031298	data augmentation method
0.4234293256	improved accuracy
0.4232191472	last decade
0.4231145851	accurate prediction
0.4230869517	measured data
0.4229971967	human level
0.4229797052	increasing number
0.4228621634	conducted extensive
0.4227478656	video recognition
0.4227398428	prediction task
0.4226539120	deep learning based approach
0.4224814320	spatio temporal data
0.4223480422	generation tasks
0.4223397198	rather than
0.4221638476	based framework
0.4221118203	compression scheme
0.4219491748	computed tomography images
0.4218647350	driven approach
0.4216415580	visual representation
0.4208855173	field programmable
0.4208112165	next generation
0.4206020565	point operations
0.4205093290	learning rate
0.4205049860	based method
0.4204524641	gan based methods
0.4203294471	generative network
0.4202638552	frames per second
0.4201719217	^ p
0.4200650728	field of research
0.4199349031	data preprocessing
0.4194712552	image generator
0.4194269040	optimization approach
0.4190901446	applications of deep
0.4188087197	commercially available
0.4185757289	reconstruction results
0.4183976084	image volumes
0.4183079152	design space
0.4182051747	denoising problem
0.4180314592	autoencoder network
0.4179386435	whole slide imaging
0.4176940409	multi scale feature
0.4176319558	classification model
0.4175981893	classification loss
0.4173562385	$ l_1
0.4169923119	experimental results show
0.4169240694	aware network
0.4167420263	classification problems
0.4165540881	health problem
0.4163784437	a comprehensive review
0.4160159518	test samples
0.4159627356	current clinical
0.4158562385	\ mathbf
0.4158542905	net outperforms
0.4157853226	optimal performance
0.4157852679	imaging quality
0.4156050159	10 fold cross
0.4152456004	$ \ mathcal
0.4150712751	convolutional encoder
0.4150165260	information content
0.4147555993	high quality reconstruction
0.4147367229	very high resolution
0.4146845410	segmentation map
0.4146646011	dataset collected
0.4144547571	proposed scheme
0.4144261473	reconstruct high resolution
0.4144235079	spectral features
0.4143034396	accurate detection
0.4140520708	positive reduction
0.4138904867	hr image
0.4135637136	training phase
0.4133651180	high correlation
0.4131704312	single images
0.4131081184	image sets
0.4130457044	rate estimation
0.4128661966	non cartesian
0.4127656139	segmentation method
0.4126358924	simultaneous localization and
0.4125755879	image sequence
0.4122599621	network depth
0.4121893602	spatial light
0.4120772412	synthesized images
0.4120188050	dl model
0.4117925333	extraction method
0.4113750206	x ray phase contrast
0.4111259060	optimization process
0.4109370902	image modalities
0.4107735306	divided into
0.4107183165	image patch
0.4106959985	far field
0.4102125869	average dice score
0.4102043360	improved results
0.4097317508	data samples
0.4096627075	invariant features
0.4094692048	complex object
0.4094455605	super resolution techniques
0.4088548539	aim 2020
0.4088523344	study shows
0.4086611636	did not
0.4085923373	comprehensive review
0.4085524716	point cloud data
0.4083313769	compression techniques
0.4082773197	learned compression
0.4079064460	$ \ sim
0.4077184543	cloud compression
0.4075935968	specific task
0.4075351895	specific tasks
0.4074093170	fusion approach
0.4070906056	compressed data
0.4069577292	regularization method
0.4066281980	tracking algorithm
0.4066098143	detection task
0.4064829083	single camera
0.4064199990	learning based registration
0.4063924913	trained networks
0.4058197170	requires large
0.4057750139	images of covid 19
0.4056119380	based iterative reconstruction
0.4055153438	projection images
0.4054662489	obtain high
0.4054031604	$ \ ell_0
0.4051609963	detection results
0.4051427775	lesion classification
0.4048949833	proposed algorithm
0.4048350481	conventional approaches
0.4047583413	m ^
0.4046877209	imaging resolution
0.4045833751	medical datasets
0.4044374052	left and right
0.4041023177	image classifiers
0.4040224812	photo realistic images
0.4039266472	non linear
0.4039230164	achieve similar
0.4036901220	hybrid deep learning
0.4036162383	generalize well
0.4035693582	objective quality
0.4034162763	lf image
0.4028482554	high computational
0.4024240926	super resolved images
0.4022391976	results obtained
0.4020836124	deep learning systems
0.4020528952	oct image
0.4019404510	conventional algorithms
0.4015866365	fed into
0.4015353952	accurate results
0.4014408827	demonstrate superior
0.4013497204	per second
0.4009820611	\ approx
0.4006712861	conditional random
0.4002735626	recognition performance
0.4002104678	retinal blood
0.3998788932	computational ghost
0.3997857480	peak signal to
0.3997067138	\ mathcal
0.3994719474	achieved significant
0.3994448223	t1 weighted mr
0.3992614928	acquisition process
0.3992241773	deep learning based segmentation
0.3990772514	model produces
0.3990041058	spatial structure
0.3988881460	quantitative results
0.3988150281	compression performance
0.3987333697	resulting model
0.3987206936	model performance
0.3986868861	an end to end fashion
0.3985147291	non contrast ct
0.3979354842	real world images
0.3979076036	ground based
0.3977859473	prior knowledge about
0.3974842336	estimation algorithm
0.3972817052	this paper proposes
0.3970992298	covid 19 pandemic
0.3970766134	covid 19 diagnosis
0.3969154669	image structure
0.3968960322	learning based approach
0.3968519707	dimensional feature
0.3968103614	framework outperforms
0.3967810578	architecture outperforms
0.3966724408	classification algorithms
0.3964115918	non euclidean
0.3963194523	out of distribution
0.3960337602	$ \ mathbf
0.3959295543	obtained results
0.3958140764	mass segmentation
0.3957598529	perceptual image
0.3955467809	ct denoising
0.3955411291	based post processing
0.3953547914	important task
0.3952555984	model achieved
0.3951804732	generated content
0.3948759467	u net architecture
0.3944125241	c means
0.3943639368	variation regularization
0.3942845932	image reconstruction method
0.3941568816	proposed approach outperforms
0.3941530986	original images
0.3938700157	well known
0.3938223965	unlabeled images
0.3937312701	reduce artifacts
0.3936885070	distance based
0.3936091890	mean square
0.3932698403	clinical dataset
0.3932575536	processing techniques
0.3932111754	computer graphics
0.3931742464	existing datasets
0.3928315472	captured data
0.3925951560	conventional approach
0.3922145749	pixel detector
0.3917136924	3d inavs
0.3915271676	segmentation model
0.3914581926	significant performance
0.3914213987	test accuracy
0.3914041848	maps generated
0.3912192731	this paper presents
0.3911252670	vision task
0.3903575586	more importantly
0.3903202817	pixel wise loss
0.3900183894	high quality images
0.3896656339	major challenge
0.3895603830	registration performance
0.3893334913	image interpretation
0.3890590654	network training
0.3886629809	rgb d images
0.3886156986	based action recognition
0.3883190324	ill posed problem
0.3879833132	in vitro
0.3878608762	while maintaining
0.3878013756	aims to develop
0.3874262441	trained network
0.3872593393	methods rely
0.3866505289	proposed network
0.3866114465	hold out
0.3863191119	real dataset
0.3863146108	accurate reconstruction
0.3862591563	learned video
0.3860338997	computational framework
0.3858440804	deep learning networks
0.3858257179	optimization methods
0.3856265949	world scenarios
0.3855980103	acquired pneumonia
0.3855236040	generate high resolution
0.3853810127	multi spectral images
0.3849662748	without sacrificing
0.3847583381	residual convolutional
0.3847077491	detection network
0.3841033932	measurement data
0.3840957204	dataset size
0.3833931231	large scale dataset
0.3829547481	challenging tasks
0.3825906435	super resolution task
0.3824981559	traditional machine
0.3822434395	ability to learn
0.3820304472	proposed methods
0.3820264418	proposed technique
0.3820037769	dense u net
0.3818790502	tomographic image
0.3813951467	two stream
0.3813747311	approach improves
0.3812830375	signal to noise
0.3809095976	person image
0.3803536601	computer vision tasks
0.3801860924	cause of death
0.3801066002	provide additional
0.3795386033	data volumes
0.3795283240	trade off between
0.3793138647	machine learning model
0.3789839364	super resolution problem
0.3789779210	source model
0.3787765753	number of parameters
0.3785361056	high inter
0.3784815028	leading cause of
0.3784381219	readily available
0.3782423217	medical image data
0.3781253998	data augmentation methods
0.3777735930	small subset
0.3777657703	key step
0.3777308025	an open source
0.3776503834	low spatial
0.3774462230	based optimization
0.3773983742	image reconstructions
0.3772444940	diagnose covid 19
0.3769866664	\ ell_0
0.3762730804	significant improvement over
0.3760118348	connected network
0.3757040785	proposed model outperforms
0.3756342281	\ kappa_
0.3753464475	medical domain
0.3750062448	means clustering
0.3749540209	high frame
0.3747062903	ray ct
0.3746317069	publicly available at https
0.3745761522	important problem
0.3739434133	graph signal
0.3739260062	area under receiver
0.3738685823	diagnosis systems
0.3734978227	$ ^ 2
0.3733624366	machine learning algorithm
0.3730383597	critical step
0.3730320221	dynamic contrast
0.3726442705	z net
0.3725057517	r cnn
0.3724445757	test results
0.3723120264	method outperformed
0.3721646308	high resolution imaging
0.3721180428	clinical datasets
0.3719309723	large amounts of
0.3718889416	image velocimetry
0.3715563522	roi detection
0.3714704692	validation experiments
0.3709386699	region of interest
0.3707368286	evaluation results
0.3703892922	critical applications
0.3703752270	ability to detect
0.3697898393	existing deep learning methods
0.3694668740	test image
0.3692963221	trained cnn
0.3687401804	favorably against
0.3686305876	small sample
0.3685771105	deep transfer
0.3680193085	regarded as
0.3680080929	human visual
0.3678335246	microscope images
0.3677504081	generate synthetic
0.3672670882	retrieval algorithms
0.3672207282	art video
0.3671872624	public data
0.3671200489	image distortion
0.3670468391	simulation data
0.3662529435	diffraction imaging
0.3658417659	improve image quality
0.3653750664	traditional image
0.3652052082	shows promising
0.3651409831	neural network models
0.3647542845	deep learning technology
0.3642879204	$ \ pm
0.3642575641	extract features
0.3640906893	+ +
0.3637349497	an end to end manner
0.3634350057	simulated images
0.3631950502	non stationary
0.3629375838	propose to add
0.3628821228	fully connected neural
0.3627876577	top performing
0.3625029270	learning paradigm
0.3623461028	traditional single
0.3623450247	proposed solution
0.3621369325	deep learning method
0.3616689850	modified version of
0.3615375263	sub optimal
0.3611026570	does not
0.3609367253	caused by
0.3606128816	visual data
0.3597354373	while keeping
0.3594565031	covid 19 ct
0.3589454281	compared to conventional
0.3589024525	field imaging
0.3587999269	rank matrix
0.3587326235	recent advances in
0.3585569767	mean squared
0.3583799263	deep architecture
0.3583127186	outperforms current
0.3582753369	wise attention
0.3579696409	brain magnetic
0.3575381920	relatively small
0.3571635832	angiography images
0.3570786422	significant challenge
0.3570355433	multimodal image
0.3569937947	fused image
0.3553456645	a large margin
0.3546064640	two stage
0.3545796725	\ mu
0.3544805630	covid 19 infected
0.3543788209	self consistent
0.3541903316	t net
0.3540970176	real applications
0.3540788552	model compression
0.3537515161	fire detection
0.3533458004	compression method
0.3532485344	chest x ray image
0.3532277627	real time video
0.3531435458	surface based
0.3528475810	integrated into
0.3525614281	network for single image
0.3523224673	faster than
0.3522479839	achieve better performance
0.3521158507	learning ability
0.3520549009	vision community
0.3513236073	both synthetic and real world
0.3513113206	random field
0.3511776477	existing models
0.3509312039	tumor segmentation challenge
0.3506077757	\ ell_2
0.3503857653	real time processing
0.3503829208	slide imaging
0.3502625017	level feature
0.3501247430	video compression methods
0.3498982076	lossy image
0.3496387601	mri segmentation
0.3495400501	automatic diagnosis
0.3494581460	directly applied
0.3493176108	hampered by
0.3492151333	high degree
0.3490563415	reconstruction networks
0.3488611324	processing tasks
0.3488522327	pre trained network
0.3488084508	greater than
0.3485039300	high signal to noise ratio
0.3482205213	recent deep learning based
0.3477348721	capable of generating
0.3476525099	\ ell_1
0.3476068877	trained model
0.3472022567	learning problem
0.3470618524	low computational
0.3468830855	denoising results
0.3468477323	resulting images
0.3466468648	multi modal data
0.3464813225	much fewer
0.3463614402	dose ct image
0.3458527642	input feature
0.3457181060	low quality images
0.3456569802	deep convolution
0.3456128778	experimental results indicate
0.3452771126	$ \ sigma
0.3452419529	feature information
0.3452361062	cnn based methods
0.3448764232	segmentation plays
0.3448688691	produces high
0.3448674068	automatic methods
0.3447435780	best performing
0.3446916554	computer generated
0.3445856967	segmentation problem
0.3442610855	outperforms previous
0.3441478122	without compromising
0.3439736332	state of
0.3439651600	near field
0.3439294981	based fusion
0.3437139282	gap between
0.3436986149	image classification tasks
0.3436693474	based algorithms
0.3435243463	accurate segmentation of
0.3433702038	out of focus
0.3430105384	trained on imagenet
0.3428621790	cancer imaging
0.3427992667	promising approach
0.3426174673	classification methods
0.3425808229	self driving
0.3425421071	image sr
0.3422403551	the uk biobank
0.3421611216	non local attention
0.3420761843	achieve high
0.3415622148	scanning transmission
0.3413595254	into account
0.3413437359	automatic brain
0.3412278751	sub bands
0.3411395698	detection algorithms
0.3411104635	does not require
0.3408784639	recent advances in deep learning
0.3399978269	categorized into
0.3397375157	learning tasks
0.3392130354	an efficient
0.3389030821	deep learning architecture
0.3386740855	do not
0.3385821970	study proposes
0.3383410209	residual image
0.3383234752	ever increasing
0.3382123723	diagnosis of skin
0.3380864991	relationship between
0.3376231561	detection models
0.3376105741	covid 19 screening
0.3375016658	reliance on
0.3374314181	supervised deep learning
0.3370119635	histopathological image
0.3367614545	diagnosis of covid 19
0.3366841156	low signal to noise
0.3366688073	learning based method
0.3366432663	success of deep learning
0.3366385624	annotated training
0.3365325847	the art methods
0.3360535882	time of flight
0.3358950795	detection approaches
0.3354307647	a fixed number
0.3348023228	2d or 3d
0.3346337254	applying deep
0.3343864207	the existing state
0.3343397391	adversarial example
0.3336251348	versatile video
0.3335335904	sampled k space
0.3334838537	whole tumor
0.3334575554	enhancement method
0.3334088473	3d point cloud
0.3330939057	this paper reviews
0.3327049065	much larger
0.3323778757	nodule segmentation
0.3323058490	alternative approach
0.3320641781	robustness against
0.3320261543	estimation network
0.3320252012	image segmentation tasks
0.3319204377	non circular
0.3317625990	$ ^ 3
0.3316342354	built up
0.3314808834	\ circ
0.3312821273	high level feature
0.3312593361	reconstruction framework
0.3304865977	\ theta
0.3302792314	time varying
0.3300293303	smaller than
0.3299858898	run time
0.3298150805	x ray computed tomography
0.3297566376	art methods
0.3297512247	deep cnn
0.3296141831	residual neural
0.3295659496	learning based medical
0.3294865581	x ray and ct
0.3291202067	robust to noise
0.3290738345	prediction network
0.3288352183	novel coronavirus
0.3288239432	light field image
0.3284570068	segmentation techniques
0.3282597532	image compression methods
0.3282179309	prior based
0.3282107307	improve performance
0.3276127771	medical segmentation
0.3275110704	few seconds
0.3274037537	without requiring
0.3268515606	covid 19 detection
0.3267527256	using deep learning
0.3266240248	dose computed tomography
0.3265958913	sub pixel
0.3263686274	in vivo human
0.3263455997	supervised learning approach
0.3262763541	existing gan
0.3261449971	image super resolution via
0.3261024676	serve as
0.3260229234	community acquired
0.3259390069	an essential step
0.3253896793	dimensional data
0.3253750521	image analysis tasks
0.3253253771	out of plane
0.3250208031	presence of noise
0.3249375294	normal data
0.3247880825	top 1 accuracy
0.3242162845	shed light on
0.3240192313	large dataset
0.3239190385	improve accuracy
0.3236516922	co attention
0.3236144979	source dataset
0.3235057440	encoder decoder based
0.3234893168	noise distribution
0.3233610762	added value
0.3232903179	deep neural network architecture
0.3230215191	a simple but effective
0.3230087648	unsupervised image to image
0.3222630396	super resolution method
0.3219438616	large volumes of
0.3215569292	outperforms state of
0.3214763186	source data
0.3214185874	full field
0.3214180709	rely on
0.3211357743	driven approaches
0.3210925708	proposed method shows
0.3208976987	recent deep
0.3208028170	imaging datasets
0.3207092023	an average dice
0.3207042407	model training
0.3203191916	weighted magnetic resonance imaging
0.3201634293	an ablation study
0.3201103042	available at https
0.3197613833	large number of
0.3194889726	end to end training
0.3192492332	3d point clouds
0.3186486192	a unified framework
0.3184510314	depending on
0.3183168281	higher accuracy than
0.3182331066	facial image
0.3179952742	suffer from
0.3179849132	differences between
0.3177592268	model architecture
0.3175658436	without relying
0.3173353127	no reference image quality
0.3173058839	real ct
0.3172954816	focuses on
0.3170070177	influenced by
0.3169534333	learning method
0.3165976881	this paper develops
0.3164810622	data structure
0.3163906617	insight into
0.3162183158	proposed method performs
0.3161272881	proposed approach achieves
0.3160512001	first order
0.3156952249	exposure image
0.3156439314	vision algorithms
0.3154775164	face image
0.3150765758	learning enabled
0.3150370295	learning algorithm
0.3142879692	classification algorithm
0.3139984944	robustness to noise
0.3137544926	in situ
0.3137435451	growing interest
0.3136093767	aimed at
0.3134533474	method for estimating
0.3132711229	trained deep neural
0.3131100080	small data
0.3129305593	an upper bound
0.3129105771	deep learning applications
0.3127518013	k space data
0.3126602948	level semantics
0.3120348974	inspired by
0.3118672128	learning model
0.3117118534	$ \ times
0.3116835786	co occurrence matrix
0.3116513970	extensive experiments on
0.3116277808	this letter
0.3115177060	network based
0.3113694217	visual object
0.3111869343	networks trained
0.3111019130	\ sigma
0.3109802279	learn features
0.3109175695	$ _
0.3105900151	entire image
0.3105283721	synthetic image
0.3105057837	images of different
0.3102009873	detection systems
0.3101557465	mr data
0.3101069155	content image
0.3099666166	segmentation based
0.3097710970	whole brain segmentation
0.3096512167	\ times
0.3090903988	large amounts of data
0.3089823685	an important tool
0.3087412951	achieves better performance
0.3086389979	acquired images
0.3086042544	high false
0.3085493850	well suited
0.3085278698	echo time
0.3085057837	datasets with different
0.3084607299	volumetric images
0.3084140224	deep learning based image
0.3082685620	$ \ approx
0.3081473570	a single pixel detector
0.3080624549	wide range of
0.3076670678	in recent years
0.3075548925	4d flow
0.3072786392	high compression
0.3072184286	aware loss
0.3070456781	video datasets
0.3069799616	of great importance
0.3069235238	compared with traditional
0.3068797045	sampled data
0.3064328465	an overview
0.3062385304	regions of interest
0.3061975804	suffers from
0.3061707422	a deep learning approach
0.3060129372	image information
0.3059951207	multimodal brain
0.3052749992	recent advancements in
0.3052715387	experiments conducted on
0.3052300175	relying on
0.3050649721	non negative
0.3049765392	end to end fashion
0.3049398750	denoising network
0.3048628740	based model
0.3045814584	brain network
0.3045190319	adaptive histogram
0.3044313239	an in depth
0.3043464315	depend on
0.3042474723	challenge 2020
0.3041105313	image texture
0.3040889708	a comprehensive survey
0.3037164728	in silico
0.3037155612	deep learning based framework
0.3031864840	one or more
0.3031144471	relies on
0.3027768926	effective approach
0.3022700700	spatial distribution
0.3022671365	this paper describes
0.3018386944	advanced deep
0.3018265099	retrieval method
0.3016922176	model called
0.3016633157	sensing imagery
0.3015846309	one to one
0.3014543598	unpaired image
0.3012135741	task based
0.3011803081	covid 19 positive
0.3011229597	intersection over
0.3011209995	models achieve
0.3009972429	among others
0.3009161859	real world image
0.3005836865	mr segmentation
0.3005244171	frequently used
0.3005008173	a deep learning framework
0.3004873044	deals with
0.3000221083	an effective tool
0.2998317972	chest x ray dataset
0.2995099319	real time applications
0.2993402450	execution time
0.2992234525	vision problems
0.2991187880	iterative phase
0.2989687735	increasing number of
0.2989683943	global feature
0.2988031454	outperforms other methods
0.2985926088	a major challenge
0.2985338626	reconstruction approaches
0.2982259929	the art
0.2979674775	step towards
0.2976720293	recognition models
0.2973721155	aims at
0.2972905268	an important step
0.2968053549	2d and 3d
0.2967545034	leads to
0.2967437880	promising technique
0.2967064715	even if
0.2966808751	recent deep learning
0.2966548437	re sampling
0.2964760371	referred to as
0.2964340273	this study proposes
0.2962134189	followed by
0.2960487622	\ lambda
0.2958808431	x ray ct
0.2955719124	the shelf
0.2954198489	art techniques
0.2952457706	significant impact
0.2952060572	a case study
0.2947112625	achieves state of
0.2945855522	target image
0.2945453369	extensive experiments show
0.2944677017	camera images
0.2943926766	incorporated into
0.2942644287	bayesian deep
0.2936050025	of covid 19 from
0.2932875862	an innovative
0.2930351046	dozens of
0.2926745531	dealing with
0.2926499320	this paper
0.2925089326	last few years
0.2921353028	this chapter
0.2920518293	method consists
0.2920466022	based super resolution
0.2918834786	learning based super
0.2916558184	relationships between
0.2916054776	a deep convolutional neural network
0.2915288670	spectral image
0.2915028759	un trained
0.2914752411	supervised approach
0.2914523653	x _1
0.2912364895	from scratch
0.2912292078	optical image
0.2911273660	many to one
0.2909541582	image super resolution using
0.2908666189	trained to predict
0.2908596644	using convolutional neural networks
0.2908024632	as opposed
0.2906058810	semantic segmentation network
0.2903924502	computer tomography
0.2902980138	much faster
0.2902722134	suffering from
0.2902190242	connected layer
0.2900410402	estimation methods
0.2899810634	^ 3
0.2896183342	supervised segmentation
0.2893082847	unavailability of
0.2892098476	higher than
0.2890936636	covid 19 disease
0.2888757900	compressed image
0.2888014674	training approach
0.2887104728	realistic image
0.2886642226	particle image
0.2886503962	resulting image
0.2886326302	without retraining
0.2885181133	opposed to
0.2879149784	in vivo
0.2877912625	current state of
0.2877114050	learning based algorithms
0.2875335792	synthesis methods
0.2873743155	depends on
0.2872960369	previous work
0.2872059111	performs better than
0.2869870576	whole slide image
0.2869256511	the proposed method
0.2868791057	low signal
0.2866836306	frequency information
0.2865194640	network outperforms
0.2865074563	compared to
0.2864713049	based on deep learning
0.2864389841	as few as
0.2864169228	plethora of
0.2863429988	learning based segmentation
0.2857307788	fundamental problem
0.2853024141	imaging based
0.2852887395	image artifacts
0.2851554941	mri methods
0.2851127703	refers to
0.2849130462	learning based framework
0.2848381639	state of art methods
0.2847360700	encoder side
0.2846978368	sub cellular
0.2846257393	performance analysis
0.2841870046	tracking methods
0.2840685476	medical field
0.2838756041	architecture design
0.2838404605	small subset of
0.2837209107	good generalization
0.2831069873	at least
0.2830873246	results indicate
0.2828277951	relationships among
0.2826130750	average dice score of
0.2825599868	current deep learning
0.2825444018	take advantage of
0.2823005240	information about
0.2819935965	proposed method achieved
0.2819685554	image style
0.2818915518	tend to
0.2818217788	up sampling
0.2817635590	two fold
0.2817329167	learning networks
0.2816136988	$ norm
0.2813641598	single model
0.2812512256	insights about
0.2809618594	augmentation method
0.2804929586	existing deep learning based
0.2801445239	modern deep
0.2800999029	proposed approaches
0.2800846687	supervised learning based
0.2799741324	faster and more
0.2798174844	through scattering media
0.2794466517	end to end learning
0.2794422631	cnn training
0.2788401692	x ray ghost
0.2787858004	+ t
0.2787808477	image representations
0.2787551525	mri datasets
0.2785553975	pre trained cnn
0.2785438764	mismatch between
0.2783808978	important tool
0.2783761618	previous method
0.2781255807	three dimensions
0.2780656453	increasing demand for
0.2780039329	learning based algorithm
0.2777830960	less than
0.2777199168	on pascal voc
0.2776435616	method requires
0.2771941867	serves as
0.2771359203	multiple models
0.2765268888	\ mathbf x
0.2765145573	of such models
0.2763189667	of great significance
0.2758735081	spatial feature
0.2754466765	image adaptive
0.2754299479	shown to perform
0.2750663261	tracking method
0.2750167324	techniques provide
0.2748508064	insights into
0.2745448117	owing to
0.2742696941	based on
0.2741792860	dynamic range imaging
0.2741736862	segmentation of retinal
0.2737300897	accurate quantification
0.2737099898	based systems
0.2736606594	this article
0.2736267777	net architecture
0.2733512202	motivated by
0.2729764301	near optimal
0.2726268380	more accurate
0.2725544410	per frame
0.2724743581	susceptible to
0.2722720395	mask r
0.2711362182	accounting for
0.2708405233	using deep neural networks
0.2707158205	ct based
0.2707028543	trained deep
0.2706313729	segmentation process
0.2704481526	normal images
0.2701058588	reconstruction approach
0.2693271252	additional data
0.2691303342	detecting covid 19
0.2686616349	converted into
0.2685904650	compared against
0.2683529221	training algorithm
0.2680916798	approach consists
0.2676480625	art algorithms
0.2675812261	lidar point
0.2673741854	method relies
0.2672634940	similar images
0.2671964891	number of layers
0.2670526806	due to
0.2669533642	advent of
0.2667053558	limited availability of
0.2665867795	the whole network
0.2664496983	using generative adversarial networks
0.2664417897	weighted mr images
0.2660520684	lead to
0.2659950259	very few
0.2658690813	structure segmentation
0.2657167839	analysis techniques
0.2656850544	imaging performance
0.2656724852	test time
0.2655371699	t test
0.2650358124	complex data
0.2649710344	number of classes
0.2649687330	guided image
0.2649344548	large images
0.2648033148	before and after
0.2646872889	detection problem
0.2645969783	neural style
0.2644639451	well defined
0.2644052347	large data
0.2643489552	terms of psnr
0.2642800002	encoding network
0.2642615712	single gpu
0.2642341745	segmentation approaches
0.2640969781	supervised classification
0.2635988235	learning frameworks
0.2635042939	affected by
0.2632671701	while preserving
0.2630637878	deep video
0.2629794127	deep image
0.2628964529	wide variety of
0.2625384304	a fully convolutional network
0.2624088013	side information
0.2621078870	on two datasets
0.2618215621	interactions between
0.2617778535	denoted as
0.2616700536	based features
0.2616159809	two photon
0.2615894299	a priori knowledge
0.2614609723	sr images
0.2610951821	decomposed into
0.2603655011	advances in deep learning
0.2602968814	single shot 3d
0.2595540318	a deep neural network
0.2593907239	discriminate between
0.2593445519	data for training
0.2592986523	deep super resolution
0.2592255685	based solutions
0.2590621056	four dimensional
0.2589806078	based registration
0.2582514120	novel coronavirus disease
0.2581708500	a weakly supervised
0.2575218193	space time
0.2570537547	into consideration
0.2569409019	morphological changes
0.2569304942	root mean
0.2568012943	full resolution
0.2563513053	high bit
0.2561195128	human computer
0.2556487033	achieves significant
0.2556451757	number of samples
0.2555623884	non overlapping
0.2554062958	limited number
0.2551194345	denoising method
0.2550514749	mri dataset
0.2549812899	spectral data
0.2548769905	tradeoff between
0.2548150432	coding method
0.2539730361	efficient network
0.2539062528	well studied
0.2535093995	improvement over
0.2535036598	based image retrieval
0.2534338032	compared with existing
0.2533045597	existing deep
0.2530938657	more than
0.2529105989	investigate whether
0.2529090494	x ray projection
0.2525772394	this paper introduces
0.2524137212	significantly better than
0.2521461290	not always
0.2521417380	translation model
0.2519661692	convolution network
0.2519096189	phase images
0.2517984498	the proposed approach
0.2515813477	experimental results on
0.2515715615	network performance
0.2515033327	computer vision community
0.2514493417	10 ^
0.2514332190	3 dimensional
0.2514032502	compared to standard
0.2509732281	prior information about
0.2508382793	acquired data
0.2507483681	noise ratios
0.2506100727	ranging from
0.2505747494	segmentation pipeline
0.2505583026	mri brain
0.2504828187	successfully applied to
0.2504730750	typically rely on
0.2502150402	sensing images
0.2500251991	imaging process
0.2500191186	wide range of applications
0.2499552141	according to
0.2499473606	non linear mapping
0.2499459303	global image
0.2499424378	segmentation approach
0.2497597290	time dependent
0.2497347532	a convolutional neural network
0.2496440468	and vice versa
0.2495969055	as well as
0.2495478130	commonly known
0.2494775572	and vice
0.2492567667	focus on
0.2490082348	training method
0.2490011098	existing cnn
0.2487676396	trained to perform
0.2487166455	comparison between
0.2486600944	associations between
0.2485006881	easy to use
0.2480424225	elevation models
0.2477855657	instance learning
0.2470681259	the art performance
0.2469160046	learning based reconstruction
0.2463301265	u net based
0.2461719936	try on
0.2460061498	robust image
0.2459565125	based classification
0.2458626292	focus image
0.2455916366	in spite
0.2455765843	segmentation of brain
0.2452631143	two stage framework
0.2449152422	alternating direction method of
0.2449061524	contrast mri
0.2446585550	a broad range
0.2445231090	high degree of
0.2443801124	very close
0.2439873980	roc curve of
0.2439445704	features extracted from
0.2434983668	better than
0.2433332638	shot learning
0.2432618808	become one of
0.2432270846	publicly available dataset
0.2431651499	interaction between
0.2430250811	intensity images
0.2426939936	based models
0.2425568335	the receiver operating characteristic curve
0.2425196619	focused on
0.2423841699	covid 19 lung
0.2415693355	number of false
0.2415639247	clinical images
0.2415165463	attention model
0.2414330581	human object
0.2413435188	results show
0.2412306639	proposed metric
0.2409940406	good agreement
0.2408849419	processing technique
0.2407914623	algorithm based
0.2405012433	automatic classification
0.2403938754	increasing interest
0.2403902697	improved image
0.2399925221	near real time
0.2398390244	better results than
0.2398030270	spatial image
0.2396090799	end to end manner
0.2394421614	focus images
0.2393962449	and tracking of
0.2393438914	the other two
0.2390904118	compared to existing
0.2390211576	deep super
0.2389264263	from highly undersampled
0.2388609027	tomography images
0.2387759121	convolutional long
0.2385342725	based representation
0.2380761018	temporal data
0.2380507649	emerged as
0.2380314704	experiments show
0.2378082864	proposed hybrid
0.2376549577	the art approaches
0.2375912543	so called
0.2375048982	image feature
0.2374834654	efficient video
0.2374781571	based loss
0.2373229621	~ 1
0.2372739137	shortage of
0.2370134550	a deep learning based
0.2369195011	diagnosis of breast
0.2368830023	method to automatically
0.2368314632	as much as possible
0.2367973686	super resolution images
0.2367369127	self adaptive
0.2365823653	automatic segmentation of
0.2360705266	order to reduce
0.2360365470	characterized by
0.2358438914	the time of
0.2355475951	for use in
0.2353693472	machine learning approach
0.2352612133	efficient method
0.2351477373	derived from
0.2349752165	cope with
0.2348659232	of computer vision and
0.2347096286	image distribution
0.2346264512	simulated and real data
0.2345975226	correspond to
0.2341856657	larger than
0.2339917819	number of
0.2339909687	non line of sight imaging
0.2339247863	robust deep
0.2338534024	enhanced images
0.2338252944	method enables
0.2337904753	including image
0.2334714661	these issues
0.2333438362	lower than
0.2332447545	accomplished by
0.2331848881	a fully convolutional neural network
0.2331515448	to date
0.2330858324	this end
0.2330255898	weighted images
0.2328748102	during training
0.2328495557	compared to existing methods
0.2328438914	to work with
0.2326354380	the proposed algorithm outperforms
0.2326308737	automated analysis
0.2326137279	represented by
0.2325021405	and so on
0.2324365877	an open problem
0.2324017474	compared to traditional
0.2322718625	inference time
0.2322359073	quite challenging
0.2318131098	training framework
0.2317637799	capable of
0.2317618808	with or without
0.2315075904	an end to end trainable
0.2311893040	using convolutional neural
0.2311319256	consisting of
0.2309751228	a fully automated
0.2308420955	belonging to
0.2306230051	automated segmentation of
0.2305943535	new opportunities
0.2303142439	detection based
0.2302569565	while retaining
0.2301509503	images obtained
0.2301182564	high dynamic
0.2293187413	existing deep learning
0.2288451831	a wide variety
0.2287686668	of magnitude faster
0.2285290578	inspired by recent
0.2284866590	consist of
0.2284309800	tends to
0.2283520181	great interest
0.2283419548	an input image
0.2280636389	critical role in
0.2280177200	knowledge about
0.2279511454	methods perform
0.2276575616	this paper investigates
0.2276260722	with and without
0.2274698341	the proposed algorithm
0.2273164466	unsupervised approach
0.2272536829	limited number of
0.2272501564	for fast and
0.2272460517	this study
0.2271519137	not only
0.2270465267	more and more
0.2268542870	for low dose ct
0.2267956110	for accurate and
0.2265887330	an alternative
0.2264423780	as long as
0.2260468781	architecture consists
0.2260102669	mean shift
0.2259985339	ct datasets
0.2259761376	an end to end
0.2257437384	five fold
0.2256014901	able to
0.2253807690	based algorithm
0.2253283058	previous state of
0.2252014978	covid 19 chest x ray
0.2251307566	applied to
0.2249301985	f1 score of
0.2242851338	a model to
0.2241248395	does not need
0.2238549391	validation data
0.2236663714	aiming at
0.2235468104	the image to
0.2233364668	a comparative study
0.2232539439	network model
0.2232343673	produced by
0.2231623704	using deep learning techniques
0.2231483838	regardless of
0.2228007701	performs well
0.2226214757	$ \
0.2225618161	joint image
0.2222740632	network layers
0.2220902074	an emerging
0.2220900987	scan time
0.2220677990	computation time
0.2218736293	but also
0.2216749829	unable to
0.2215618077	detection framework
0.2214911139	very challenging
0.2214868172	compared with
0.2214649367	non blind
0.2214585514	images generated
0.2212275013	semantic image
0.2211406573	learning network
0.2211122028	order to provide
0.2211080702	based image
0.2210105581	the movement of
0.2210105581	the identity of
0.2209582987	3d object detection
0.2207636733	network for retinal
0.2203937062	this work
0.2203789717	\ em
0.2202340236	\ rho
0.2201186321	to improve
0.2200105581	the guidance of
0.2200105581	the formation of
0.2200105581	the contrast of
0.2200105581	the phase of
0.2199819860	camera image
0.2197472912	recent work
0.2194719523	sub sampling
0.2193815985	popular deep
0.2190105581	the method of
0.2190105581	the problems of
0.2188329565	to address
0.2188011563	the results of
0.2187015373	balance between
0.2186744966	5 fold
0.2186558078	from different sources
0.2183283058	recent state of
0.2182649678	regularization based
0.2181868652	video image
0.2181283842	single forward
0.2180911526	transformed into
0.2179939129	based techniques
0.2179433829	between real and
0.2177834826	pet image
0.2175394539	an explainable
0.2173256608	based on deep
0.2170537990	learning technique
0.2168868264	additional training
0.2168622817	scale features
0.2168011563	the network to
0.2166979146	early detection of
0.2165084902	sensing applications
0.2164943519	video object
0.2164229270	level classification
0.2163438914	of deep learning in
0.2162069426	based on convolutional neural networks
0.2161584125	imaging methods
0.2160105581	the details of
0.2159249339	arising from
0.2158342239	decoder architecture
0.2158011563	the shape of
0.2158011563	the learning of
0.2157864850	a multi task learning
0.2157721298	kinds of
0.2155991824	with applications in
0.2155524506	for solving inverse
0.2150937909	view image
0.2149255428	formulated as
0.2148011563	the parameters of
0.2148011563	the study of
0.2147810540	a modified version
0.2147629252	order to obtain
0.2147588903	based motion
0.2146149256	two steps
0.2145405738	on chip
0.2143152785	recent advances in deep
0.2143096309	the resolution of
0.2141485587	resolution images
0.2140105581	the encoder and
0.2140105581	the content of
0.2140105581	the assumption of
0.2140105581	the increase of
0.2139555631	imaging features
0.2139420745	a novel
0.2139002165	accounts for
0.2138893692	responsible for
0.2138613339	a generative adversarial network
0.2138075617	analysis algorithms
0.2138011563	the domain of
0.2138011563	the model to
0.2138011563	the end of
0.2137631267	using deep convolutional neural networks
0.2136102252	dataset demonstrate
0.2135142444	lossless image
0.2132695148	accurate quantification of
0.2132305059	adversarial domain
0.2132127968	tumor classification
0.2131289842	widespread use
0.2130105581	the processing of
0.2129407672	at risk
0.2128011563	a measure of
0.2128011563	the loss of
0.2127640856	an interactive
0.2126451331	general framework
0.2126272031	a comprehensive
0.2125105581	the optimization of
0.2125105581	the removal of
0.2124074193	residual u
0.2124046053	based compression
0.2123329583	this issue
0.2122219065	dataset consisting of
0.2120105581	the network on
0.2119871186	a large scale
0.2118011563	the acquisition of
0.2118011563	the objective of
0.2118011563	the fusion of
0.2118011563	the information of
0.2116761858	optic disc and
0.2116462721	benefit from
0.2115105581	the addition of
0.2115105581	the challenges of
0.2115105581	the spatial and
0.2114773816	temporal convolutional
0.2112971533	so as to
0.2112472299	limited training
0.2110002473	an average
0.2108011563	the space of
0.2108011563	a function of
0.2106166595	the proposed model
0.2105869185	a memory efficient
0.2105105581	the predictions of
0.2105105581	the theory of
0.2105105581	the results with
0.2105105581	the uncertainty of
0.2102198133	the past decade
0.2101179337	correspondences between
0.2100400902	prone to
0.2100105581	the aid of
0.2100105581	the user to
0.2098011563	the order of
0.2098011563	the outputs of
0.2098011563	the weights of
0.2095105581	the boundary of
0.2095105581	the similarity of
0.2095105581	the performances of
0.2094216056	one dimensional
0.2091816675	agreement between
0.2090883735	experiments on synthetic
0.2090105581	the measurement of
0.2088773377	shown to improve
0.2085691999	range of applications
0.2085105581	the physics of
0.2085105581	the result of
0.2085105581	the computation of
0.2085105581	the noise in
0.2083938936	application of deep learning
0.2083670909	good performance
0.2080162141	amounts of training
0.2079110892	this paper explores
0.2077392839	end to end deep
0.2077142618	the diagnosis and
0.2075907721	extracted from
0.2075358221	achieve state of
0.2075105581	a method of
0.2075105581	the sparsity of
0.2075105581	the architecture of
0.2074617564	multi exposure image
0.2074283382	such as
0.2073580524	methods in terms
0.2073500751	classification approaches
0.2072157165	captured by
0.2071690792	source image
0.2071624144	synthesis method
0.2070105581	the model on
0.2068011563	the issue of
0.2067229950	experiment results show
0.2066625719	deep learning segmentation
0.2066591827	co segmentation
0.2066074112	field of machine learning
0.2065352708	to solve
0.2065134593	based on convolutional neural
0.2065105581	a class of
0.2065105581	the geometry of
0.2064576782	based on generative adversarial
0.2062732756	model performs
0.2057142178	leave one
0.2055105581	the framework of
0.2054855631	simple method
0.2053184840	perform better
0.2052142618	in relation to
0.2051409692	a unified
0.2050219626	much more
0.2050105581	a database of
0.2048451228	a challenging task
0.2048330201	processing method
0.2048085611	conditional image
0.2047597183	using generative adversarial
0.2046538676	formed by
0.2046168681	net model
0.2044761930	real time performance
0.2044090537	large scale data
0.2042142618	the tasks of
0.2042142618	the challenge of
0.2041075177	this paper addresses
0.2040872690	extensively used
0.2040105581	the key to
0.2038138892	view data
0.2028162884	correlation between
0.2027907032	detection tasks
0.2027765703	much higher
0.2027618877	belong to
0.2026580992	trained on
0.2026344261	each pixel
0.2025105581	the statistics of
0.2023699076	learning features
0.2023456937	data volume
0.2021820857	available at
0.2019834564	diagnosis of lung
0.2018421300	space data
0.2018238200	much smaller
0.2018008659	current deep
0.2017597284	trained to generate
0.2016515116	broad range of
0.2014120581	network trained
0.2012142618	the core of
0.2010144004	available online
0.2009601865	machine learning method
0.2006688072	the code and
0.2005513252	mean absolute
0.2004755111	useful tool
0.2003468134	learning based medical image
0.2003180630	a deep convolutional
0.2002026346	the roc curve
0.1999568457	covid 19 chest
0.1999214795	free image
0.1997310442	numerical experiments show
0.1996004112	act as
0.1995597839	this problem
0.1995534198	deep learning algorithm
0.1992908263	a key role
0.1991712230	the proposed framework
0.1990960825	often fail
0.1987018599	each iteration
0.1986180374	recent progress in
0.1986058765	end to end deep learning
0.1985730920	leading to
0.1984051945	a multi task
0.1981291229	starting from
0.1980964880	detection system
0.1980336224	a simple
0.1979549136	to detect
0.1979268542	close to
0.1977626127	a deep network
0.1973423040	disease 2019
0.1973383664	pre trained on
0.1973197699	and cloud shadow
0.1970605815	difference between
0.1968439899	consists of
0.1968354923	deep learning system
0.1967868652	objects of interest
0.1966677270	automated method
0.1965824706	domain data
0.1965166314	more precisely
0.1965084629	in spite of
0.1964308317	accurate segmentation
0.1963929748	a large scale dataset
0.1961791873	make use of
0.1961358721	reconstruction tasks
0.1960207077	to reduce
0.1958885541	reconstruction model
0.1958356693	very low
0.1957623636	effective method
0.1957131934	as high as
0.1956096098	detection of covid 19
0.1954710290	two step
0.1954252599	to overcome
0.1952364368	quality images
0.1949463474	advantages over
0.1948686041	in addition
0.1948234889	tested on
0.1948060676	an important
0.1948051483	target data
0.1946718896	use case
0.1946503823	segmentation datasets
0.1944493136	linear combination of
0.1943951790	non covid
0.1943188444	automatic detection of
0.1942499081	this thesis
0.1942251886	including data
0.1941299091	method to generate
0.1939172424	become increasingly
0.1938599080	large scale image
0.1938132624	outperforms other state of
0.1937959020	significant impact on
0.1937740236	class segmentation
0.1930274536	art object
0.1929331165	compared to previous
0.1928605633	improved training
0.1928379660	this manuscript
0.1928064639	level accuracy
0.1927575337	some extent
0.1927135162	an encoder decoder
0.1927109582	number of measurements
0.1925791932	the wild
0.1925002537	improved classification
0.1924946635	better performance than
0.1923589892	efficient algorithm
0.1923247723	an unsupervised
0.1922888314	accompanied by
0.1919159865	an encoder decoder architecture
0.1918652892	focusing on
0.1916877280	sub networks
0.1916334000	showed good
0.1915398363	makes use of
0.1914472723	an integral part
0.1914463638	trained on synthetic
0.1913479419	treated as
0.1912685265	in order to
0.1911215419	training time
0.1910670203	model shows
0.1909532642	to generate
0.1906208822	information from multiple
0.1905946138	computer vision applications
0.1903793625	one stage
0.1903080233	similarities between
0.1902458011	proposed pipeline
0.1896663430	important role in
0.1892164376	access to
0.1892076130	time consuming task
0.1891857948	$ \ textit
0.1886198678	to extract
0.1885964138	microscopy image
0.1885134072	real time imaging
0.1883342545	the proposed method achieves
0.1879177524	an important role
0.1875090121	performance compared
0.1874170689	network to learn
0.1873542849	a posteriori
0.1873374090	three classes
0.1869673496	scale feature
0.1869646014	large range of
0.1869009069	determined by
0.1868769468	structured low
0.1867324833	model trained
0.1864113836	in terms of
0.1863342962	trained to learn
0.1861541970	function based
0.1861387252	great potential for
0.1857127509	feature extraction from
0.1854425983	based reconstruction
0.1854223574	challenged by
0.1853086137	conditioned on
0.1852626921	slightly better
0.1851382537	domain adaptation for
0.1850400204	model accuracy
0.1850027668	one class
0.1847663094	a deep learning
0.1847046206	acts as
0.1846686188	detection based on
0.1846439916	neural architecture search for
0.1845069813	discrepancy between
0.1843149191	visible image
0.1841441250	pre trained deep
0.1839191441	real time object
0.1838867222	a single
0.1837814293	b mode images
0.1835171215	end to end deep neural
0.1835059672	a deep learning model
0.1834886080	low dynamic
0.1834837267	a two stage
0.1833305821	to obtain
0.1831916793	one step
0.1830721750	differentiate between
0.1829510077	3d printed
0.1828760830	learning applications
0.1824532642	to learn
0.1824370884	used to train
0.1824262428	x ray image
0.1822885615	information provided
0.1820321605	proposed to solve
0.1820016721	per patient
0.1817286338	very useful
0.1816380293	coding standard
0.1816364692	integral part of
0.1816032677	using deep
0.1814126610	entropy loss
0.1813722543	on par
0.1813682769	proposed techniques
0.1813405701	classification framework
0.1812781951	image signal
0.1812610405	a benchmark dataset
0.1812140271	tracking system
0.1810386866	deal with
0.1808947938	cad system
0.1806198678	to predict
0.1805799771	dominated by
0.1801074312	neural network model
0.1800655949	development of deep learning
0.1799697545	relied on
0.1797110481	detection using
0.1795592354	by introducing
0.1793617598	running time
0.1793444632	both synthetic and real
0.1791066345	limited amount of
0.1790899565	network for image
0.1788042161	$ m
0.1787925151	model to learn
0.1786463687	an edge
0.1785812928	x ray imaging
0.1784070237	images from multiple
0.1780951250	improve upon
0.1780397679	very time consuming
0.1780163894	computer vision algorithms
0.1779824098	composed of
0.1777360630	$ g
0.1777017204	tissue images
0.1776671472	divided into two
0.1775405981	obtained by
0.1775359039	comprehensive review of
0.1775225883	based applications
0.1774490462	covid 19 dataset
0.1773622646	the last layer
0.1772790950	super resolution via
0.1772578897	local attention
0.1771256357	benchmark data
0.1770499269	in most cases
0.1769361324	field of machine
0.1767118226	split into
0.1766534669	mri image
0.1765350417	image as input
0.1764981812	other words
0.1764888027	two public datasets
0.1764097354	to ensure
0.1763863861	powerful tool for
0.1762130980	conventional image
0.1762066657	deep learning network
0.1758814298	three fold
0.1758116295	the other hand
0.1756536285	accurate estimation of
0.1755278087	on demand
0.1754130434	interpreted as
0.1754050044	classified into
0.1753812997	a new
0.1753456250	achieved state of
0.1753120470	benefiting from
0.1753067929	top 1
0.1750542587	a machine learning based
0.1748964462	proposed to detect
0.1748637822	distances between
0.1747768297	generation method
0.1747608402	specifically designed for
0.1746877713	to take advantage
0.1746160924	proposed strategy
0.1744224451	sensing data
0.1743782472	supported by
0.1743463285	existing state of
0.1743407069	visual results
0.1742639514	art performance on
0.1741811797	the art results
0.1741272266	relation between
0.1739722302	with respect to
0.1736810312	an ensemble
0.1736434929	main contribution of
0.1735819710	perform experiments
0.1735246014	3d u net
0.1735102324	resolution features
0.1734908462	at test time
0.1734006912	generative adversarial network for
0.1733725122	an intelligent
0.1731510431	ai system
0.1729980596	very important
0.1729336261	a modified u net
0.1729303110	enables high
0.1727904883	the proposed scheme
0.1727075335	standard methods
0.1726550235	amounts of data
0.1723077746	evaluated on
0.1722017575	small number of
0.1721850080	a key step
0.1721580761	an iterative
0.1720993892	impact on
0.1720490077	respect to
0.1719629514	the proposed method outperforms
0.1716932694	a priori
0.1715482291	using transfer learning
0.1715157463	low data
0.1713735067	essential role in
0.1713578473	rapid development of
0.1712335757	viewed as
0.1711765153	estimation approach
0.1711318322	high resolution 3d
0.1711240020	hundreds of
0.1708958188	3d video quality
0.1708326180	pairs of images
0.1707766036	metric based
0.1707126349	corresponds to
0.1706156175	connection between
0.1705690292	this limitation
0.1705668064	served as
0.1705548006	much less
0.1703379765	task network
0.1703328349	proposed to address
0.1703265150	an essential role in
0.1701626838	second stage
0.1700792587	advantage over
0.1700112306	an initial
0.1699259167	sub regions
0.1698142948	to avoid
0.1696732889	1 d
0.1696266767	methods provide
0.1696259947	freely available at
0.1694346045	a significant challenge
0.1693248428	camera system
0.1693190433	self learning
0.1692779356	this study demonstrates
0.1691888291	weighted mri
0.1691011486	a real world
0.1687132282	over smoothing
0.1686623810	equipped with
0.1686377073	recent developments in
0.1686221938	tumor patients
0.1686059778	results compared
0.1683981019	segmentation using deep learning
0.1683946883	\ url
0.1683870067	u net architectures
0.1683143173	millions of
0.1682515816	a promising technique
0.1682105735	with gradient penalty
0.1681784242	a fundamental problem
0.1681282263	high quality image
0.1680252747	accurate estimation
0.1679500953	a series of
0.1677748062	a survey
0.1676684936	diagnosis system
0.1676514304	for retinal vessel segmentation
0.1676470610	over union
0.1676007377	segmentation of multiple
0.1675450924	analysis methods
0.1674877634	data from multiple
0.1674469875	outperform state of
0.1674300268	robust against
0.1673333231	the proposed network
0.1673211233	propose to learn
0.1671347826	hindered by
0.1670062522	a multi stage
0.1669998542	along with
0.1669521617	sampled images
0.1667080289	existing ones
0.1666820984	\ &
0.1666079466	vision system
0.1665818495	multiple data
0.1664976875	a critical step
0.1664346043	both quantitatively and qualitatively
0.1664162097	the dice similarity coefficient
0.1664067085	based diagnosis
0.1663440124	based image processing
0.1663156888	prediction based
0.1662943938	to retrieve
0.1662831042	led to
0.1662404207	different sizes
0.1660713957	overlap between
0.1660266050	models trained on
0.1658329591	this regard
0.1657961969	based feature
0.1657808807	presented here
0.1651925357	a lightweight
0.1651621007	an order of magnitude
0.1651082896	accurate method
0.1651047693	most importantly
0.1649857889	deep network for
0.1649483490	a synthetic dataset
0.1649095503	to classify
0.1647629197	a semi supervised
0.1646575206	to generate realistic
0.1643359974	based on deep neural networks
0.1642970023	refer to
0.1641268519	early diagnosis of
0.1640169145	an adaptive
0.1640056709	differs from
0.1639546055	based pipeline
0.1638743535	accordance with
0.1637417755	for brain tumor segmentation
0.1636894842	an explicit
0.1634837573	covid 19 from chest
0.1633553645	prior work
0.1633517609	the past few
0.1633309012	a cnn based
0.1631949443	non expert
0.1630485874	very little
0.1628979772	by combining
0.1628954432	learning pipeline
0.1627467311	classification datasets
0.1627253931	3d convolutional neural network
0.1623086021	based technique
0.1622798975	multitude of
0.1622348673	significant role in
0.1621242018	fusion based
0.1618779755	paper shows
0.1618344488	experiments on real
0.1616710315	non contrast
0.1616383288	to enhance
0.1614109992	publicly available at
0.1612811520	extract features from
0.1612187361	architecture based
0.1610687261	image samples
0.1610129198	neural networks trained
0.1609897096	positive rate of
0.1609805545	generated by
0.1608792031	obtained from
0.1607438933	distinguish between
0.1607110747	mainly focus on
0.1606993905	stage approach
0.1605014466	the first stage
0.1604354200	time points
0.1604270367	the same time
0.1602892709	segmentation method based on
0.1602806704	an online
0.1602383844	learning based image
0.1601828375	computer vision techniques
0.1601195372	each year
0.1600770962	three kinds
0.1598780867	on imagenet
0.1598685891	images for training
0.1597585132	acquisition time
0.1596521170	tens of
0.1595466643	methods suffer
0.1594557933	the help of
0.1593055244	identification of covid 19
0.1591351805	emphasis on
0.1591309018	automated detection of
0.1589312367	a novel deep learning based
0.1586258382	contrast imaging
0.1585219712	based sr
0.1585159786	deep learning based method for
0.1584088111	correlations between
0.1583701672	method works
0.1582677297	$ z
0.1582304668	a dual
0.1581514300	based architecture
0.1580763531	unsupervised method
0.1580576800	a generative model
0.1579268279	non invasive imaging
0.1579067566	proposed module
0.1577428837	to identify
0.1577250314	cancer classification
0.1576433199	imaging approach
0.1575882515	over fitting
0.1574636598	applicable to
0.1572899141	compatible with
0.1570914162	imaging method
0.1570240584	segmentation dataset
0.1569457832	conventional 2d
0.1568483362	use cases
0.1568154476	consists of three
0.1567640285	algorithm based on
0.1566535971	deep learning for medical
0.1566520054	art performance
0.1565395754	success in image
0.1565309842	free images
0.1565244017	propose to use
0.1564762698	in house
0.1563973770	a generalized
0.1562771940	relations between
0.1562042388	cardiac image
0.1561006474	generating high
0.1560746245	art networks
0.1559674887	every pixel
0.1559528606	distance between
0.1557959136	the receiver operating characteristic
0.1557014856	retrieval algorithm
0.1556600950	approach based
0.1554295692	based camera
0.1553345581	on top of
0.1552976071	a single gpu
0.1552579904	good agreement with
0.1552089235	proposed models
0.1551009319	made publicly
0.1549361825	great success in
0.1548782287	almost all
0.1548750426	made available
0.1548549047	the same
0.1545912730	a wide range of applications
0.1544008277	the art techniques
0.1543363743	image registration using
0.1542623956	network achieves
0.1541841725	method based on
0.1541836459	end to end framework
0.1539939997	image super
0.1539928837	to reconstruct
0.1539689932	deep learning method for
0.1538552421	in vivo data
0.1538391002	to assist
0.1536045149	the ground truth
0.1535480367	visual system
0.1535250154	feature extraction and
0.1534493957	resolution optical
0.1534364762	data shows
0.1534054302	contribute to
0.1532912971	supervised method
0.1530836741	provided by
0.1529495134	learning architectures
0.1529215512	achieving state of
0.1528909636	the left ventricular
0.1526606341	model for image
0.1526543113	a wide range
0.1524712170	to train
0.1524620001	an important role in
0.1523512025	by minimizing
0.1523328399	need for manual
0.1523043067	small training
0.1522516275	the past few years
0.1518872745	much lower
0.1518422568	network models
0.1516646225	unsuitable for
0.1516462412	to mitigate
0.1516126203	very difficult
0.1515948915	imaging model
0.1515810482	often suffers
0.1515279607	50 \
0.1514750167	apart from
0.1513887469	learning based framework for
0.1513502143	the proposed
0.1513263106	experiments on
0.1512536716	through extensive experiments
0.1512206534	difficult to
0.1510957123	improvements over
0.1510848317	data size
0.1509844424	a general framework
0.1509686919	trained neural
0.1508492965	a multi scale
0.1507911978	a small number
0.1505567016	indistinguishable from
0.1505134601	a systematic
0.1504712170	to achieve
0.1502662437	large scale 3d
0.1501843212	learning solutions
0.1499224205	an active
0.1497702011	by means of
0.1497691155	compete with
0.1496316699	automated deep learning
0.1494437119	attributed to
0.1494340104	comprised of
0.1494330172	a preliminary
0.1494314980	u net model
0.1492087517	method to improve
0.1491109434	optical system
0.1490366041	consists of two
0.1489085538	deep neural network for
0.1486545145	an independent test
0.1485271794	automated approach
0.1484543740	small amount of
0.1484057507	by proposing
0.1480731602	thousands of
0.1480548876	deep learning architecture for
0.1480236569	neural network approach
0.1478787143	model trained on
0.1478737711	proposed multi
0.1475660963	method to extract
0.1470908634	most likely
0.1470005187	amounts of
0.1469610843	a variety of
0.1468306682	this work proposes
0.1467158628	the art algorithms
0.1466085834	layer segmentation
0.1464442757	covid 19 classification
0.1464315460	brain image
0.1462180534	automatic classification of
0.1461719817	multi image
0.1460649828	two main
0.1460526904	conjunction with
0.1459132669	exposure images
0.1458440268	as opposed to
0.1457397438	contrast to noise
0.1457140397	an automatic
0.1456810049	the first time
0.1456049984	a single rgb
0.1455128353	resulted in
0.1454109470	deep learning methods for
0.1453072382	a wide range of
0.1450749897	unsupervised image
0.1449260118	information contained in
0.1449033723	images collected
0.1448753513	in mind
0.1448030830	major challenge for
0.1447964787	using deep convolutional
0.1447329046	help improve
0.1446896384	against adversarial
0.1443772963	plenty of
0.1442028702	method achieves state of
0.1440836249	deep learning framework for
0.1440302643	set of
0.1438213805	information provided by
0.1438076619	made possible
0.1438022963	derivation of
0.1437253088	an internal
0.1436186750	the forward operator
0.1435205951	combined with
0.1434380904	well suited for
0.1431353974	widely available
0.1428702276	for medical image segmentation
0.1426679433	an f1 score
0.1425908650	dependencies between
0.1424690362	contributes to
0.1424637418	while avoiding
0.1423103954	large amount of training
0.1423040269	used to
0.1422027129	dice score of
0.1421701793	significant improvement in
0.1420684422	the original image
0.1420636525	resolved images
0.1420509175	an effective
0.1419975019	compared to other methods
0.1419513761	\ ~
0.1417187343	on par with
0.1417159471	a review
0.1415657480	a deep convolutional neural
0.1415527391	at different scales
0.1413864589	statistical analysis of
0.1413566459	detection method based on
0.1412969965	performance compared to
0.1411807719	to tackle
0.1410677804	very high
0.1410183466	classification method
0.1409567242	to perform
0.1409041204	images using deep learning
0.1408878714	the last few years
0.1408484724	an ill posed problem
0.1408419265	from natural images
0.1407200090	nature of
0.1406828919	by adding
0.1406446295	images acquired by
0.1406286008	accurate detection of
0.1406219668	to automatically segment
0.1405213683	architecture for image
0.1404689763	methods rely on
0.1404571489	a pre trained
0.1403746876	making use of
0.1402760710	approach based on
0.1401491019	extended to other
0.1400888775	deep learning based method
0.1400877497	key role in
0.1400588401	methods based on
0.1400256297	differential diagnosis of
0.1400064015	real time detection
0.1399946307	specific data
0.1399763655	an essential
0.1399456667	network to predict
0.1398948286	time domain
0.1397804804	a fully automatic
0.1397662695	dice scores of
0.1397206415	quantitative analysis of
0.1396109653	in particular
0.1395337299	automatic diagnosis of
0.1394867619	trained end
0.1393961620	model based on
0.1393622158	comparison with state of
0.1393226066	sub network
0.1392700451	much attention
0.1391967806	long time
0.1391250329	a single image
0.1391109653	associated with
0.1390682575	dice coefficient of
0.1389315195	further investigation
0.1387054962	the most common
0.1386668330	the input image
0.1386194640	under sampling
0.1386071010	challenging because
0.1385659211	scale image
0.1383874935	a single shot
0.1383662276	to fully exploit
0.1383136382	two classes
0.1382777665	with high accuracy
0.1382312556	the left ventricle
0.1382277754	to recover
0.1380615885	better understand
0.1380121882	the proposed method performs
0.1379571365	better performance
0.1379362534	adversarial attacks on
0.1377430362	time series data
0.1376655827	the art deep
0.1375866462	framework based on
0.1375818056	$ n
0.1375731716	the key idea
0.1375722592	resolution techniques
0.1373974718	comprehensive evaluation of
0.1373160821	mean intersection
0.1372811135	more robust
0.1372101991	more complex
0.1370581157	deep neural networks for
0.1370361280	by applying
0.1367124802	neural networks for
0.1367020168	segmentation using deep
0.1364537418	more sophisticated
0.1364207096	significantly better
0.1363666022	does not rely on
0.1360407854	$ t
0.1360047224	an adversarial
0.1359559726	to fine tune
0.1357767192	the use of
0.1356734264	dynamic range of
0.1356624148	belongs to
0.1356441178	images generated by
0.1356358846	thus allowing
0.1355810724	to noise ratios
0.1355751833	360 \
0.1355172382	different types of
0.1354135754	different domains
0.1352245035	deformable image
0.1351899107	learning systems
0.1351556715	to further reduce
0.1351335087	trained convolutional
0.1351069577	able to achieve
0.1350260368	a very challenging task
0.1349266226	feature maps from
0.1347772581	the current state
0.1346443257	more likely
0.1346405679	multiple types of
0.1344846328	present paper
0.1343242969	2019 challenge
0.1342452949	deep learning techniques for
0.1342442837	to alleviate
0.1338581149	image reconstruction using
0.1338295983	real time 3d
0.1337608156	to generate high quality
0.1337190107	released at
0.1336628682	automatic analysis
0.1336224600	a lot of
0.1335268991	very deep
0.1334731100	automatic analysis of
0.1331685701	this purpose
0.1331065815	quantitative assessment of
0.1330925295	the art models
0.1328372670	the proposed technique
0.1327818614	test set of
0.1327675248	resonance image
0.1325747357	trained on large
0.1324486371	a high speed
0.1323655848	the latent space
0.1319396018	perform better than
0.1319209597	data demonstrate
0.1319209210	method for automatic
0.1318818319	then processed
0.1317528190	an integrated
0.1317480543	perform well
0.1314736702	based image compression
0.1314609596	a vital role
0.1314586446	a high resolution image
0.1313793108	a low dimensional
0.1312077235	clinical imaging
0.1311544784	with skip connections
0.1311090463	to remove
0.1310844814	prior knowledge of
0.1310660619	an improved
0.1309567242	to estimate
0.1309154088	embedded into
0.1308680253	an image
0.1308343528	classification of covid 19
0.1307776319	instead of
0.1307695396	last few
0.1307689611	little attention
0.1306730290	images captured by
0.1305321232	to create
0.1304540784	fine tuning of
0.1304061747	learning architecture
0.1303536434	intend to
0.1302440010	amount of
0.1301171281	a promising approach
0.1298243404	outperform other
0.1296540869	good quality
0.1295628249	more accurately
0.1294550425	the point spread function
0.1294191817	new loss function
0.1290483055	the proposed approach achieves
0.1289506688	continue to
0.1287585419	adaptation method
0.1286058008	represented as
0.1285156493	coming from
0.1283477818	confined to
0.1283366330	a two stream
0.1283325307	attention network for
0.1282486954	variety of
0.1281359542	an approximate
0.1281098167	art deep learning
0.1280518214	decoder structure
0.1279319270	$ k
0.1279130886	field of computer vision
0.1277885296	a deep neural
0.1277631825	convolutional neural network for
0.1276148099	learning classifiers
0.1274377530	3d medical image segmentation
0.1274327388	an automated
0.1272610725	modified u
0.1268816583	an ill posed
0.1268433345	scan images
0.1268046985	to optimize
0.1267534159	diverse set of
0.1267249286	imposed by
0.1266560462	semantic segmentation of
0.1265074224	inverse problems in
0.1264905620	on mobile devices
0.1264410379	and pre trained models
0.1263040269	used as
0.1261740136	recently deep
0.1261713835	scheme based on
0.1261394485	continues to
0.1261376716	the purpose of
0.1260858215	based x ray
0.1260735015	potential of deep
0.1260027173	a challenging problem
0.1259807784	research topic in
0.1259731862	further boost
0.1258763282	3d convolutional neural networks
0.1258130334	$ ^ \
0.1257583401	looking at
0.1256935123	the proposed model outperforms
0.1256824545	related to
0.1256680229	well designed
0.1256667266	a data driven approach
0.1256303482	replaced by
0.1255837791	based segmentation
0.1255611698	acquired during
0.1254916248	the art method
0.1254476435	standard deviation of
0.1254095345	more specifically
0.1253976995	order to address
0.1253576522	a high resolution
0.1253278301	concerned with
0.1252217093	also known as
0.1252000846	a novel technique
0.1251934625	often require
0.1251375420	= 1
0.1251270746	suitable for
0.1250457649	deep learning models for
0.1249530857	the art accuracy
0.1247844715	all optical
0.1246433687	a brief
0.1244315240	learning framework for
0.1243344001	network trained with
0.1243180938	evaluated against
0.1243019032	a low cost
0.1242968679	vital role in
0.1240595669	specifically designed to
0.1240351053	the entire
0.1240076211	learning based models
0.1239657257	on chest radiographs
0.1239591005	network based methods
0.1239556698	and error prone
0.1238884903	by leveraging
0.1238055448	the proposed methodology
0.1237822421	the diffraction limit
0.1235572382	in contrast to
0.1235502964	each frame
0.1235321232	to handle
0.1234374835	a python
0.1233816605	local network
0.1233669161	more efficient
0.1233597576	the target domain
0.1231261000	an area under
0.1229051969	a conditional generative adversarial network
0.1228011171	complex image
0.1227533787	imaging domain
0.1227072942	image segmentation using
0.1226616699	large volumes
0.1226519580	more reliable
0.1226329591	to super resolve
0.1226199854	devoted to
0.1225884618	generalization ability of
0.1225879367	these methods
0.1225409542	great potential in
0.1224949537	using chest x ray
0.1224652544	works well
0.1224288168	\ beta
0.1223951021	first attempt
0.1223704699	deep learning based model
0.1223605966	different scales
0.1223018079	$ mm
0.1222722532	version of
0.1221309792	many real world
0.1220589622	improvement compared to
0.1220262413	the arts
0.1219433859	the performance of
0.1219194406	refer to as
0.1218966795	to get
0.1218470633	model trained with
0.1218011171	specific image
0.1217860432	made publicly available
0.1215961176	more stable
0.1215684480	to acquire
0.1215315726	by fine tuning
0.1215067398	a novel loss function
0.1213425030	used to generate
0.1213184480	to understand
0.1213121420	the proposed approach outperforms
0.1213044835	inferred from
0.1211916372	connections between
0.1211901293	10 fold
0.1210720667	based architectures
0.1209569676	based on convolutional neural network
0.1207404387	non gaussian
0.1207189396	from chest x ray images
0.1206175994	to enable
0.1205453854	in resource limited
0.1203593138	a fully convolutional neural
0.1203276442	a hybrid
0.1202637012	challenging due to
0.1201195408	a data driven
0.1200713604	aims to
0.1198332038	the original
0.1197512169	notion of
0.1197237718	advantage of
0.1195841498	modeled as
0.1195765377	those obtained
0.1194939758	results obtained by
0.1193201371	over time
0.1193140280	to assess
0.1193023561	accuracy compared
0.1192884710	dataset contains
0.1192794228	multiple image
0.1192025834	more flexible
0.1191644842	by incorporating
0.1190956544	architecture consists of
0.1190588316	field data
0.1189694621	model to predict
0.1188803304	increasingly used
0.1188447267	contrast images
0.1188022198	in many cases
0.1187422941	an advanced
0.1186118211	different resolutions
0.1185267668	the receiver operating
0.1185187292	to diagnose
0.1184971812	a portable
0.1184936471	real ones
0.1184509327	to segment
0.1184433693	a single model
0.1184318267	a recurrent neural network
0.1184257006	contrary to
0.1183564551	outperforms other
0.1181950976	the proposed methods
0.1180755199	more precise
0.1178597080	each class
0.1178396428	2020 challenge
0.1178003700	in conjunction with
0.1176296231	$ p
0.1174038855	simple yet
0.1173273728	while achieving
0.1172419455	to do so
0.1171571582	field images
0.1171119961	and healthy controls
0.1170640866	a light weight
0.1167621555	end to end network
0.1166162710	classified as
0.1165791202	the proposed pipeline
0.1165341966	deep learning approaches for
0.1164898196	perform well on
0.1164348993	interested in
0.1164117192	used for
0.1163952898	full use of
0.1163286586	no additional
0.1162428541	quality assessment of
0.1162419647	a linear combination
0.1162129532	operates on
0.1161387269	although deep learning
0.1161288152	vulnerable to
0.1160682861	to realize
0.1159434689	collected from
0.1158075265	a powerful tool
0.1157110065	interact with
0.1156931736	useful information
0.1155960892	re training
0.1155071732	encoded into
0.1154251381	learning based approach for
0.1154166698	1 mm
0.1154017617	solved by
0.1153584766	non iterative
0.1153106386	an application
0.1152108929	network to perform
0.1151094446	acquired at
0.1150971205	very effective
0.1150741709	the main idea
0.1149668002	technique based on
0.1149442373	cell detection and
0.1148793069	image de
0.1148575145	par with
0.1146002481	performs better
0.1145458040	for hyperspectral image classification
0.1144214670	this paper shows
0.1142701075	attached to
0.1142688234	cause of cancer
0.1142397198	both simulated and real
0.1142227569	significant improvements in
0.1141783366	multiple types
0.1141273613	to facilitate
0.1141242832	two consecutive
0.1141073949	a closed form
0.1140576309	future work
0.1140199694	made great
0.1140179959	an early
0.1139587096	an inverse problem
0.1139539810	an improvement
0.1139111658	at last
0.1138850546	1 2
0.1138505578	re trained
0.1138137135	a proxy
0.1137757955	quantitative evaluation of
0.1137478710	using deep convolutional neural
0.1136512115	the source domain
0.1135697252	a conditional generative
0.1135503700	an auc of
0.1134509327	to produce
0.1134206477	comes at
0.1133134820	induced by
0.1131664458	the last decade
0.1131068620	frames per
0.1131037767	and structural similarity index
0.1129426626	to accurately segment
0.1129142162	two types of
0.1129059907	builds on
0.1127980796	with ground truth
0.1127779046	present experimental
0.1127731571	a low rank
0.1126788476	deep learning model for
0.1125958068	stereo image
0.1125552743	role in
0.1125160086	comparative study of
0.1124782344	images acquired from
0.1124509327	to capture
0.1124377069	using chest
0.1124371622	the proposed solution
0.1124078464	small amount
0.1123617672	based approach for
0.1122130821	a single frame
0.1121874274	the most popular
0.1121814026	level image
0.1119701174	lies in
0.1118917348	phase only
0.1117960414	to automatically detect
0.1116791574	2 3
0.1116539713	linked to
0.1116151001	the proposed architecture
0.1115513501	based method for
0.1114767039	each category
0.1114682389	to guide
0.1114657027	a single camera
0.1113000588	less than 1
0.1112778102	a loss function
0.1112556494	the training process
0.1112046142	covid 19 cases from
0.1111099046	consisted of
0.1110187914	based on deep neural
0.1110051204	by employing
0.1108906073	absence of
0.1108900200	this task
0.1108742772	important part of
0.1105664778	benign or
0.1105537539	on board
0.1104685400	as much as
0.1104464310	terms of accuracy
0.1101987452	avenues for
0.1101273613	to determine
0.1101039633	at different resolutions
0.1100996435	often suffer
0.1100984948	x ray phase
0.1100870017	a low resolution
0.1100765696	3d shapes
0.1100048854	a variational autoencoder
0.1099659445	image diagnosis
0.1098775707	registration based
0.1098405853	deep learning approach for
0.1097942537	images collected from
0.1097768119	a deep learning system
0.1097651515	for lung nodule
0.1096934484	able to generate
0.1096889364	similarity between
0.1096418484	on github
0.1096264187	anomaly detection in
0.1096197849	these problems
0.1095834868	computational time
0.1095525966	analysis based
0.1094851589	in practice
0.1092049512	the experimental results demonstrate
0.1092000440	considered as
0.1089001003	driven methods
0.1088980951	2d slices
0.1088295673	images from low
0.1088069303	vision techniques
0.1087493602	a neural network model
0.1087255751	a single forward
0.1086810602	comparable performance to
0.1086678914	in combination with
0.1086455350	on real data
0.1085318874	learning based methods for
0.1085183503	3d cnns
0.1085101339	a deep learning based method
0.1084771157	best knowledge
0.1084577422	the art sr
0.1083866198	validated on
0.1082431693	diagnosis using
0.1082061904	calls for
0.1081947360	images demonstrate
0.1081933583	a v
0.1081067454	do so
0.1079683573	amenable to
0.1078922748	more compact
0.1077639073	truth data
0.1077226604	while requiring
0.1076713361	three kinds of
0.1075841037	sensing image
0.1075309596	a large
0.1074214089	deep brain
0.1073424662	residual network for
0.1073287338	usually require
0.1071864399	these challenges
0.1071283935	becoming more
0.1070480652	scale datasets
0.1070264826	starts with
0.1070091926	operate on
0.1069907054	the training data
0.1068409622	as possible
0.1068183859	the problem of
0.1068048618	advancements in
0.1067703795	a preprocessing step
0.1067079999	to enforce
0.1065608091	sub tasks
0.1065060523	suited for
0.1064679325	information regarding
0.1064213534	1 3
0.1063977046	a crucial step
0.1063415934	network consists
0.1063318791	a small set of
0.1063123017	networks trained on
0.1062606365	single 2d
0.1062178872	proposed method uses
0.1061416436	approaches based
0.1060654856	by conducting
0.1059449578	thus reducing
0.1059352004	mode images
0.1058471684	to circumvent
0.1056525925	do not require
0.1053995440	standard 3d
0.1050521002	the source code
0.1050065912	a neural network
0.1050010867	together with
0.1049814261	not clear
0.1049745770	bag of
0.1049154867	in medical image analysis
0.1048407027	the proposed strategy
0.1048221536	task learning
0.1048038929	also discussed
0.1047699318	comparable to
0.1047509277	a wide variety of
0.1046909087	visual inspection of
0.1046400224	network to generate
0.1046036880	2d slice
0.1046002919	on cifar 10
0.1044546528	\ sim
0.1042144872	image super resolution with
0.1040954429	performance on
0.1040754059	learning approach to
0.1039561949	based model for
0.1038992166	intensity only
0.1038732137	deep learning models in
0.1038183859	the effectiveness of
0.1037760732	to differentiate
0.1037462119	automated analysis of
0.1036757550	well suited to
0.1034230211	by comparing
0.1033379465	drawn from
0.1033067100	most existing
0.1031969812	ct scans of
0.1031548541	3d semantic segmentation
0.1030894004	aim at
0.1029819023	two decades
0.1028180150	consistency between
0.1028048618	portion of
0.1027782977	more powerful
0.1025403984	tasks like
0.1022887102	the discrete cosine
0.1021082425	a source domain
0.1020933684	in clinical practice
0.1020303611	more complicated
0.1019578467	recent success of
0.1019254289	to maximize
0.1017521724	learning based model
0.1017243188	information from
0.1016759414	a multi
0.1014832562	efficient way
0.1014625544	the potential to improve
0.1014060920	accurate diagnosis of
0.1012789723	any additional
0.1012687604	manual segmentation of
0.1012406484	in turn
0.1012388930	to interpret
0.1011210192	by taking
0.1011176095	based framework for
0.1011064058	for future research
0.1009907906	to further improve
0.1009753654	other tasks
0.1009564171	a small fraction
0.1008488315	method to detect
0.1007834408	order to improve
0.1006832986	model to generate
0.1006790144	to eliminate
0.1006320018	up to 10
0.1006128638	addressed by
0.1005553174	each component
0.1004603057	remarkable performance in
0.1004468238	attempt to
0.1002736481	images using deep
0.1001309019	taking into
0.1001255806	to synthesize
0.1001068622	a single pixel
0.1000843427	to pre train
0.1000595810	based quality
0.1000242382	experimented with
0.0999982525	verified by
0.0999559247	the art performance on
0.0998865734	reconstruction based
0.0997582607	based on u net
0.0997496859	image reconstruction from
0.0997153270	quality metric for
0.0996676543	^ \
0.0996367110	a survey on
0.0996090770	receptive field of
0.0995999115	modeled by
0.0995839739	images from
0.0995156713	a crucial role in
0.0995149553	contrast ct
0.0993861071	proposed method provides
0.0993741995	a human expert
0.0993646499	three main
0.0993535376	many researchers
0.0992112697	more discriminative
0.0991497365	method relies on
0.0990814639	ct scans from
0.0990780742	learn from
0.0990341101	achieve better
0.0987876735	the fly
0.0987729522	large set of
0.0985450725	mainly focus
0.0984665451	each voxel
0.0983055675	a large amount of
0.0982804062	achieve good
0.0982354983	segmentation performance on
0.0982101271	the acdc
0.0980818945	$ net
0.0978396878	by exploiting
0.0977029502	dependence on
0.0976788978	a key role in
0.0976674977	framework consists of
0.0976082854	learning approach for
0.0976007341	classification accuracy of
0.0975941396	performance compared with
0.0974599918	number of images
0.0974399129	method for multi
0.0973627075	models trained with
0.0972743161	potential to improve
0.0972355966	an auxiliary
0.0972095504	various kinds of
0.0971692218	paper focuses on
0.0970795718	very similar
0.0970410314	two orders of magnitude
0.0969130488	learning technologies
0.0968984680	recognition using
0.0968183859	a set of
0.0967767192	a large number of
0.0967704708	assigned to
0.0967647743	while reducing
0.0967619416	compared to state of
0.0966887154	this paper aims
0.0966500466	method consists of
0.0965835949	computational complexity of
0.0965660754	two branches
0.0964459925	an important task
0.0964305321	the training set
0.0962900816	more sensitive
0.0962113078	utilization of
0.0961066633	major challenge in
0.0960800762	particle imaging
0.0960566065	network for 3d
0.0960099605	to evaluate
0.0959916419	quality of images
0.0959800186	trained and tested on
0.0959482558	based denoising
0.0958655928	not feasible
0.0958418229	performance analysis of
0.0957659140	dataset consists of
0.0957352836	contributing to
0.0956941478	used to determine
0.0956861432	applications such as
0.0956612633	no need
0.0956581281	an integral part of
0.0956523590	$ ^
0.0955780572	do not take
0.0955461576	2d cnn
0.0955243317	on three public
0.0954014922	paid to
0.0953826961	a parallel
0.0953104028	single network
0.0952634193	the research community
0.0952328322	different categories
0.0952058988	segmentation based on
0.0951099759	an analysis
0.0950639312	different kinds of
0.0950461474	adapted to
0.0950232798	standard image
0.0948870785	for example
0.0948766638	look at
0.0947574418	single low
0.0946514121	2020 challenge on
0.0946118801	an accuracy of
0.0945454027	few years
0.0945240027	requiring only
0.0944546492	two separate
0.0943713842	spatial distribution of
0.0943587372	the test set
0.0943533373	ground truth for
0.0943387021	an f1 score of
0.0943361474	proven to
0.0943027748	in clinical settings
0.0942801586	not sufficient
0.0941049750	the most widely used
0.0941033357	transfer learning from
0.0939341436	network framework
0.0938843687	fail to
0.0938371888	very promising
0.0937077289	s \
0.0936941950	combination of
0.0936113556	achieves good
0.0935275737	performance of
0.0935041112	from rgb images
0.0934743177	convolutional neural networks for
0.0934714799	the training phase
0.0934613339	crucial for
0.0934485654	by replacing
0.0934276358	to accelerate
0.0933649850	3d cnn
0.0932368274	promising results on
0.0932317100	more effective
0.0931803029	superior performance of
0.0931604092	also discuss
0.0931250823	the state of
0.0930796359	a novel way
0.0930671034	this approach
0.0930248093	change detection in
0.0928981944	the optic disc
0.0928658824	a novel end to end
0.0928199762	proportion of
0.0928033361	of rain streaks
0.0927744989	then apply
0.0927606805	work addresses
0.0927336710	benefits from
0.0926508428	lots of
0.0924994057	replaced with
0.0924446182	up to
0.0923805479	based on convolutional
0.0922555773	the test dataset
0.0922235835	current work
0.0921953729	the gold standard
0.0921117087	an effective way
0.0919064010	promising results in
0.0918773517	well trained
0.0918440797	to minimize
0.0917400198	a significant improvement
0.0917392952	the feature maps
0.0915991420	\ text
0.0915730211	by utilizing
0.0915252783	dataset containing
0.0914682393	accuracy compared to
0.0914620402	evaluation of
0.0914523903	an imaging technique
0.0914318545	an in house
0.0914113952	well annotated
0.0913461658	the expense of
0.0911697372	the task of
0.0911459277	the need for
0.0910787969	the target image
0.0909873270	the art convolutional neural
0.0909787366	chest x
0.0909688712	two stages
0.0909474450	tool for
0.0908932007	classification based on
0.0908774872	deep learning methods in
0.0908696004	effective way
0.0907935598	mean dice
0.0907808616	challenging due
0.0907536147	the validity of
0.0906739645	directly from
0.0905971809	the input images
0.0904950460	effectiveness of
0.0904360382	trying to
0.0904109233	the reconstructed image
0.0902690967	a conditional generative adversarial
0.0902225913	other state of
0.0901793349	extensive experiments on three
0.0901560384	much better
0.0901494087	particularly challenging
0.0901359224	these limitations
0.0901071306	both quantitative and qualitative
0.0899354273	set of images
0.0898914089	to build
0.0898779857	a critical role
0.0898586597	a small fraction of
0.0898195935	two stage deep
0.0898169696	to simulate
0.0897422250	method in terms
0.0897212005	becomes more
0.0897142187	some cases
0.0896714453	neural network architecture for
0.0896678540	learning system
0.0895907791	to infer
0.0895688236	by adopting
0.0894591804	images via
0.0894475118	realization of
0.0893089908	rgb +
0.0892501472	images obtained from
0.0891567653	to converge
0.0891380981	able to reconstruct
0.0891023984	at https
0.0890885758	landmark detection in
0.0890463953	no reference image
0.0890282041	this goal
0.0890123477	to resolve
0.0890073317	and treatment planning
0.0889647189	an end to end deep
0.0889016840	very limited
0.0888983767	further improve
0.0888593342	work proposes
0.0888324789	quality of
0.0887293473	each modality
0.0886894696	automatic method for
0.0886188663	based algorithm for
0.0885586432	cnn architecture for
0.0885061266	implementation of
0.0884954098	stage framework
0.0883207576	various types of
0.0883039611	loss functions for
0.0882862835	foundation for
0.0882503958	from chest x rays
0.0882023435	mean error
0.0881997182	data generated by
0.0881846504	a novel deep
0.0880716333	trained with
0.0880628063	this topic
0.0879900323	most commonly
0.0878930039	different scanners
0.0878026655	compression system
0.0878014745	relatively low
0.0877816857	on two benchmark
0.0876717662	classification system
0.0876557817	better image quality
0.0876125944	the quality of
0.0873125944	the context of
0.0872484532	incorporation of
0.0872140999	images obtained by
0.0871444662	the extracted features
0.0871052799	resolution imaging
0.0870721625	not robust
0.0869788429	takes into
0.0869216592	often requires
0.0868620265	the domain gap
0.0868094964	two ways
0.0867411581	large amount of
0.0867032845	problem in computer
0.0866100885	this phenomenon
0.0865900552	the experimental results
0.0865393290	a small number of
0.0865150349	new insights
0.0865098299	a crucial role
0.0865004362	each other
0.0863734422	this survey
0.0863681017	prerequisite for
0.0863325253	deep learning for
0.0863221391	a point cloud
0.0861827942	by modifying
0.0859797900	breakthroughs in
0.0859484247	to deal with
0.0858527795	p \
0.0857638165	using generative
0.0857111054	source domain to
0.0856288159	segmentation of
0.0856266860	achieving better
0.0855632340	the fused image
0.0855365260	for single image super resolution
0.0855155858	learning model for
0.0854396570	by augmenting
0.0853170358	the art results on
0.0852536153	features from
0.0852473598	2d 3d
0.0852010859	often suffers from
0.0850699399	art results for
0.0849701966	more challenging
0.0849389914	proposed algorithms
0.0849234701	very small
0.0847887645	the second stage
0.0847348800	second step
0.0845904362	final image
0.0845148415	very challenging task
0.0843994129	very large
0.0843946547	a thorough
0.0843560285	yet effective
0.0842736802	to extract features
0.0841215302	fed to
0.0839656937	a reference image
0.0839434098	to protect
0.0838349205	better generalization
0.0838114545	able to produce
0.0837804051	in order to reduce
0.0837599425	able to obtain
0.0837127180	these findings
0.0836532704	constructed by
0.0836435419	an image based
0.0835700082	many works
0.0835511537	compared with state of
0.0835250013	the frequency domain
0.0833224953	the obtained results
0.0833125944	the effect of
0.0833061116	series data
0.0830353233	two kinds of
0.0830123477	to promote
0.0829851506	a model trained
0.0829735796	various fields
0.0829585071	need to
0.0829449771	the medical field
0.0828508009	confirmed by
0.0827912779	applications like
0.0827694213	the domain shift
0.0827626657	cells from
0.0827506385	a total of
0.0826546797	a proof of concept
0.0826525361	different modalities
0.0825920991	representation learning for
0.0825206000	easy to
0.0824136647	3d ct images
0.0823634709	a coarse to fine
0.0823501444	to accurately detect
0.0823269680	the world health
0.0823221481	the most important
0.0823138314	performs well on
0.0822992002	arise from
0.0822923908	the training procedure
0.0822160800	a large dataset
0.0820294437	super resolution with
0.0819944504	performed at
0.0819690866	currently available
0.0819578272	the first work
0.0819502198	2d u net
0.0819140769	a deep learning method
0.0818839269	kind of
0.0817131969	exploitation of
0.0816171509	truth images
0.0815467646	accuracy of
0.0814989139	to fuse
0.0814483334	model consists of
0.0813949657	sensitive to
0.0813222913	an image reconstruction
0.0813186667	results on real
0.0812873160	widely used for
0.0812368633	compared with other
0.0810901391	to provide
0.0810842233	an additional
0.0810574156	pixels into
0.0810275737	analysis of
0.0810111085	based approach to
0.0809805419	learning based method for
0.0809261844	performance in terms of
0.0808043112	to supervise
0.0806686932	or even better
0.0805750479	able to learn
0.0804411340	by simulating
0.0802811968	method for
0.0801878519	without increasing
0.0801769555	learning method for
0.0800664304	and semi supervised learning
0.0800404445	performance of deep
0.0799907106	high spatial and
0.0799749662	corresponding ground
0.0799471901	this work presents
0.0799241383	tensor imaging
0.0799237199	diagnosis from
0.0798912874	high signal
0.0798776365	paper aims to
0.0797767180	latent space of
0.0797412677	a great potential
0.0797126126	deployed on
0.0796801878	an important problem
0.0796015857	an end to end deep learning
0.0795833966	novel algorithm
0.0795822524	to combat
0.0793811037	termed as
0.0793734956	used to assess
0.0792859249	the covid 19 pandemic
0.0792816781	outperforms several
0.0792469364	to detect covid 19
0.0792196412	for few shot
0.0792083220	combinations of
0.0792039899	the low frequency
0.0791843541	a multi modal
0.0791820659	reconstruction using
0.0790729508	possible future
0.0790403374	results compared to
0.0790335295	x ray computed
0.0790098706	estimated by
0.0789991623	to quantify
0.0788972274	two layer
0.0788953772	to develop
0.0787922475	various domains
0.0787761053	compared to other
0.0787677793	3d unet
0.0787124517	inverse problems with
0.0786819130	three popular
0.0786658133	enabled by
0.0786491469	successfully used
0.0786338229	this reason
0.0785994033	a multi layer
0.0784973082	not yet
0.0784970850	method based on deep
0.0784960950	the human brain
0.0784673959	this gap
0.0783840917	a comparative
0.0783645824	a broad range of
0.0783556410	an experienced
0.0782803029	generative model for
0.0781880976	super resolution for
0.0781517975	recognition system
0.0781503297	based on deep convolutional
0.0781291314	converted to
0.0781119023	the noise distribution
0.0779505768	the trained network
0.0778514155	control over
0.0778324667	imaging via
0.0777675799	the white matter
0.0777648567	more difficult
0.0777347001	by fusing
0.0777222550	performed by
0.0776588598	two distinct
0.0775944242	mapping between
0.0774587658	challenging task of
0.0773661049	to refine
0.0773398302	presence of
0.0772339549	the low rank
0.0771952201	the generated images
0.0771115922	on synthetic data
0.0770903706	the generated image
0.0770163145	the main
0.0770115793	network approach
0.0770087696	across different
0.0767910960	a large range
0.0767737110	by enforcing
0.0766571242	significantly more
0.0766081222	suitability of
0.0765609862	the search space
0.0764762853	very fast
0.0764174986	widely used in
0.0763450396	an attention
0.0763272646	data set of
0.0763195158	non covid 19
0.0762964628	3d shape
0.0762394611	majority of
0.0762172603	compensate for
0.0761815666	a recently developed
0.0761119778	bounds on
0.0761102452	relatively high
0.0760735259	$ weighted
0.0760228623	based brain
0.0759926675	lesion segmentation from
0.0759366608	accurate 3d
0.0758591450	comparable or
0.0756843819	the human eye
0.0756838690	network learns to
0.0756632516	a two step
0.0756544738	much as possible
0.0756505815	the trained model
0.0755850279	rise to
0.0755762696	3d volume
0.0755396292	learning architecture for
0.0755368503	an example
0.0754798962	a public dataset
0.0754504187	able to detect
0.0753964969	availability of
0.0753740900	emergence of
0.0753107467	mapping using
0.0752128229	large amount
0.0751955950	an interesting
0.0750999029	more realistic
0.0750895316	during inference
0.0750894242	the deep learning model
0.0750870528	mainly due
0.0749792711	by providing
0.0748783364	knowledge into
0.0748392840	overview of
0.0747994199	by exploring
0.0747918180	a monocular
0.0747893336	used to estimate
0.0747876735	the aforementioned
0.0747622256	in conjunction
0.0747231850	slide image
0.0747141350	three class
0.0745969601	3d volumes
0.0745386437	the art methods in terms
0.0745216372	lack of
0.0744610764	the final
0.0742548916	created by
0.0742140390	image compression with
0.0741940809	the reconstruction error
0.0741851668	extract more
0.0741461588	significant amount of
0.0741261421	the classification performance
0.0740627852	the entire image
0.0740462022	a non convex
0.0740312367	distribution of
0.0740262273	of skin lesions
0.0740207681	measured by
0.0739634749	lot of
0.0738784862	amount of labeled
0.0737781051	the original images
0.0737632033	emerging as
0.0737185855	driven by
0.0737184137	terms of
0.0736940634	a fast
0.0736740512	a person's
0.0736284058	requires only
0.0735460776	sum of
0.0735352653	the forward model
0.0735283587	various scenarios
0.0734868923	this review
0.0734742758	conducted on
0.0733632353	size images
0.0733556435	number of training
0.0733350049	the target object
0.0733281375	helpful for
0.0732046043	adaptation methods
0.0730057142	learn more
0.0729847670	captured using
0.0729805795	to achieve high
0.0729800808	mean average
0.0727563143	often suffer from
0.0724691521	studies show
0.0724466084	approach uses
0.0723948890	u net like
0.0722876735	to augment
0.0722679868	named as
0.0722599435	processing time
0.0722210359	to encourage
0.0721259313	the human body
0.0721223881	value decomposition
0.0720670853	a custom
0.0720488491	able to extract
0.0719585010	high temporal
0.0718191101	supervised training of
0.0717766550	while providing
0.0717501986	the rate distortion
0.0717193807	leads to better
0.0716658039	improvement in
0.0716626735	to compress
0.0716136666	three types of
0.0716051988	score of
0.0715417611	the field of
0.0713400961	resulting in
0.0713299004	a recently proposed
0.0713179384	achieves better
0.0712779515	to make
0.0712440699	a fully convolutional
0.0712394611	superiority of
0.0712141092	each layer
0.0711969359	defined by
0.0711943195	the validation set
0.0711603618	defined as
0.0711384459	on simulated data
0.0711104609	the high frequency
0.0710757258	both local and global
0.0710672595	the acquired data
0.0709632048	to attain
0.0709617283	two domains
0.0709407373	practical use
0.0709202595	scale images
0.0709062487	shown to
0.0709012387	neural network for
0.0707946769	solved using
0.0707865505	identified by
0.0707251805	to image translation
0.0707032869	a post processing
0.0707004131	imaging through
0.0706201102	samples from
0.0705996626	devices such as
0.0705474160	a fully connected
0.0705217743	translation between
0.0704355670	3d facial
0.0704339932	done by
0.0704205714	models based on
0.0703959018	a self supervised
0.0703719882	in order to obtain
0.0703352320	object detection in
0.0702513900	very well
0.0702377590	loss function for
0.0701755017	the field of machine learning
0.0701417476	the input
0.0701417476	the target
0.0701109070	framework for
0.0701041973	compression using
0.0700997935	the last
0.0700738618	two state
0.0700220178	measurements from
0.0700198987	different stages
0.0700153813	the image domain
0.0699291244	estimation using
0.0698829854	converge to
0.0698809852	achieved by
0.0698757885	calculated from
0.0698539015	for detecting covid 19
0.0697541075	the latter
0.0697059170	the art deep learning
0.0696932542	regard to
0.0696872482	scope of
0.0696173716	constrained by
0.0695940593	various levels of
0.0695588115	ease of
0.0695098232	proposed method on
0.0694826577	a large number
0.0694201597	absolute error of
0.0693969961	in comparison to
0.0693497649	parameters than
0.0693391959	lesions from
0.0693155227	an unknown
0.0692871652	x ray data
0.0692334281	still challenging
0.0692274767	to align
0.0692204238	super resolution of
0.0691992974	not limited to
0.0691758148	at once
0.0691524985	the state of art methods
0.0691291314	generalizes to
0.0690927571	reduced by
0.0690886964	a deep
0.0690729340	the past
0.0689525104	good results
0.0689448607	growing interest in
0.0688955932	more efficiently
0.0687661840	solely on
0.0687515373	guided by
0.0687319125	although many
0.0686930789	information across
0.0686725988	also evaluated
0.0686637205	types of
0.0686582217	research work
0.0686255581	stored in
0.0686168809	performance over
0.0685432065	self training
0.0684841133	the ultimate
0.0684348900	time point
0.0684239944	the image quality
0.0684078116	network for
0.0683226488	the output image
0.0683125762	variety of image
0.0683113273	the segmentation accuracy
0.0682906114	approaches do not
0.0681979154	similar to
0.0681309957	to compensate for
0.0680059193	a simple yet
0.0679728945	the brats 2020
0.0679635483	obtained through
0.0679447907	a non linear
0.0679447695	for image classification
0.0679432824	the proposed loss
0.0679126735	to locate
0.0678842997	different classes
0.0677784295	to choose
0.0677321569	the training dataset
0.0677252573	this study aims
0.0677211328	convolutional network for
0.0676714927	also introduce
0.0676544327	a cost function
0.0675603749	most common
0.0675434951	3d scene
0.0675091219	first stage
0.0674821014	an arbitrary
0.0674553226	while simultaneously
0.0674486802	approach does not
0.0674446182	used in
0.0674275892	the usage of
0.0674172801	aided by
0.0674033451	each object
0.0673415805	the fine grained
0.0673310863	the art machine learning
0.0673061017	obtained at
0.0673023403	3d ct
0.0673011637	signal from
0.0672739171	the results obtained
0.0672685961	experiments on three
0.0672525589	to delineate
0.0672507022	purpose of
0.0672501784	enhanced by
0.0672254204	coupled with
0.0672171075	used to detect
0.0671571585	a new paradigm
0.0671468094	to modulate
0.0669963808	challenging task due to
0.0669963757	a wide
0.0669748746	known as
0.0669624862	the spatially varying
0.0669223830	in terms of psnr
0.0669166411	images without
0.0668803461	assessed by
0.0668769794	evaluate several
0.0668437143	neural network with
0.0668253374	imaging system
0.0668225913	a pair of
0.0667983210	any prior
0.0667500721	to judge
0.0667500427	the last few
0.0666708949	union of
0.0665792761	computational cost of
0.0665745672	same patient
0.0665603334	recovered from
0.0665417611	the presence of
0.0665407012	simple but
0.0665281029	a convolutional neural
0.0665122482	abundance of
0.0664347972	a trade off between
0.0664325049	level performance
0.0664143906	the present paper
0.0663981916	corrupted by
0.0663791314	quantity of
0.0663512205	development of
0.0663312973	effective at
0.0663150332	a training dataset
0.0662748766	properties of
0.0662546356	searching for
0.0662520000	extensive experiments on two
0.0662187949	on two public
0.0661650428	a variational
0.0661478212	existing work
0.0660568173	resolution image
0.0660223549	with minimal
0.0659256131	experimental results on two
0.0658915665	an overall accuracy of
0.0658330620	reconstruction time
0.0657995408	as input
0.0657905267	attention mechanism to
0.0657612657	classification using
0.0657500328	limited by
0.0657129374	patients from
0.0656686231	as well
0.0656384469	the receptive field
0.0656256564	introduced by
0.0655140125	performance against
0.0655089830	collected by
0.0654856621	using artificial
0.0654205714	network based on
0.0654048450	method uses
0.0653882028	the data distribution
0.0653523549	possibilities for
0.0653451486	recognition method
0.0652744613	to embed
0.0652132771	image classification with
0.0651994973	become more
0.0651800181	best performance
0.0651790976	sampled from
0.0651390261	segmentation network to
0.0651116109	no need to
0.0650994405	imaging with
0.0650720259	across multiple
0.0650094723	this method
0.0649344442	by integrating
0.0649227456	each patient
0.0648949535	images with
0.0648077141	novel view
0.0647834052	novel self supervised
0.0647568057	to increase
0.0647399629	proposed method compared to
0.0647175219	network with
0.0646369797	each individual
0.0646149776	collected at
0.0646040449	used to create
0.0645966489	an external
0.0645688765	used for training
0.0645603893	imagery using
0.0645597770	learned by
0.0645072504	the measured data
0.0644911049	to recognize
0.0644337898	the art methods in terms of
0.0644311266	application to
0.0644075830	a model based
0.0644009056	two parts
0.0643608694	by analyzing
0.0643599124	four different
0.0643458309	a 3d convolutional neural network
0.0643179299	each block
0.0642653143	a framework
0.0642537391	light detection
0.0642162631	to noise ratio
0.0642121277	data augmentation for
0.0640783482	a novel approach
0.0640602101	3d ultrasound
0.0639564316	a real time
0.0638535537	obtained using
0.0637992462	implemented by
0.0637989390	an autoencoder
0.0637815520	given input
0.0637445575	detection in
0.0636582332	a vital role in
0.0636361920	a convolutional
0.0635787711	image segmentation with
0.0635701469	further research
0.0635572492	hard to
0.0634519296	algorithms based on
0.0634448331	architecture based on
0.0633845966	several challenges
0.0633769794	techniques like
0.0633675483	more general
0.0633609939	likely to
0.0633224998	estimated from
0.0632315938	performance in many
0.0632125034	not only improves
0.0632085910	two major
0.0631509814	of training images
0.0631507618	assumed to
0.0631075986	each sample
0.0631052829	a fully
0.0630977475	work focuses on
0.0630777569	processed by
0.0630407012	obtained via
0.0630189728	deep learning approach to
0.0630172595	a novel deep neural network
0.0630008816	different levels
0.0629944701	methods do not
0.0629120582	to accommodate
0.0628508763	heavily on
0.0628234499	a novel network
0.0627922171	motion during
0.0627511628	the efficacy of
0.0627497410	u net with
0.0627256727	approaches based on
0.0626653337	a lot
0.0626585057	dependent on
0.0626254476	areas such as
0.0626228623	evaluations on
0.0626031970	able to predict
0.0625826847	invariant to
0.0625671458	the current state of
0.0625180245	two modalities
0.0625036625	these metrics
0.0624911049	to collect
0.0624495061	degree of
0.0624449976	approach provides
0.0623309561	the art model
0.0623176472	a case
0.0623172015	problem as
0.0623125740	the first step
0.0622371518	detection from
0.0622341778	range of
0.0621698987	overall accuracy
0.0621616313	acquired from
0.0621315959	various types
0.0621115916	maps from
0.0620880485	or even
0.0620801961	the model performance
0.0620770560	synthesis using
0.0620724998	tested using
0.0620578921	the point spread
0.0620188418	controlled by
0.0619443670	also evaluate
0.0619263418	x ray dataset
0.0618561981	3d pose
0.0618524208	different layers
0.0618124655	all cases
0.0618107281	stacks of
0.0617974402	annotations from
0.0617855015	still limited
0.0617465405	all tested
0.0617306030	the feature level
0.0617104016	proves to
0.0617014337	validated using
0.0616939743	this idea
0.0616841031	each level
0.0616815404	dataset of
0.0616577425	more natural
0.0615981745	each cell
0.0615798695	the imaging process
0.0615785759	fps on
0.0615018869	dataset for
0.0614939982	to mimic
0.0614307618	subsets of
0.0614163264	ct scans with
0.0614126735	to suppress
0.0613856485	seeks to
0.0613751407	validated by
0.0613520425	to meet
0.0613455571	modalities such as
0.0613441787	representations from
0.0613039811	method provides
0.0612729848	proved to
0.0612065305	pairs of
0.0611573571	tasks such as
0.0611181932	classification of
0.0611014943	based machine
0.0610517856	the image space
0.0610172371	these models
0.0609849647	used to improve
0.0609684277	a few
0.0609471939	amount of training data
0.0609219415	a simple yet effective
0.0609021821	an experiment
0.0608834167	the most suitable
0.0608601165	extracted by
0.0608427098	each stage
0.0607812406	the peak signal to noise ratio
0.0607338125	the art methods on
0.0606691364	the main contribution
0.0606673184	to accomplish
0.0606626735	to maintain
0.0606568678	image into
0.0606350944	part of
0.0606264349	approach to
0.0605764349	data from
0.0605478517	family of
0.0605298563	two different
0.0605048260	also provide
0.0604835778	results on
0.0604808061	better understanding
0.0604722923	diagnosis based on
0.0604650460	an object
0.0604588380	demonstrated by
0.0604196745	dimensional images
0.0604130958	non medical
0.0604076637	an end to end learning
0.0603733276	performed using
0.0603233503	to exploit
0.0603179299	several studies
0.0603068504	an embedded
0.0602967994	implications for
0.0602625973	a common
0.0602610503	detection of
0.0602411049	to establish
0.0602395391	arrangement of
0.0602157438	the application of deep learning
0.0601423196	an independent
0.0601373310	without loss
0.0601019926	most relevant
0.0600713871	an open
0.0600148748	vision tasks such as
0.0600086777	the training
0.0599918491	to answer
0.0599421201	the low level
0.0599308565	recovered by
0.0598811113	train models
0.0598624641	first step
0.0598548119	a small set
0.0598446836	to adjust
0.0597985431	also show
0.0597977286	the noise level
0.0597652306	taken at
0.0597482716	with high spatial
0.0597153180	commonly used in
0.0596181932	estimation of
0.0596163362	method for image
0.0595834054	to stabilize
0.0595664840	assumptions on
0.0595594244	methods in terms of
0.0595193281	the one hand
0.0594932600	application of deep
0.0594474979	the perceptual quality
0.0594468941	suited to
0.0594416980	10 \
0.0594194807	patients with
0.0593934499	the medical domain
0.0593394101	essential for
0.0593261879	annotated by
0.0593147798	composed of two
0.0593120793	an array of
0.0593112784	an intermediate
0.0593094059	used to construct
0.0593086449	approach for image
0.0592954958	applied on
0.0592913591	over conventional
0.0592780333	same object
0.0592444541	similar image
0.0592356593	networks for image
0.0591697798	perceptual quality of
0.0591454154	the art networks
0.0591252056	ability to
0.0591182902	assessment of
0.0591156504	designed to
0.0591041870	same class
0.0590843615	image denoising with
0.0590634716	achieved through
0.0590578543	an area
0.0590455729	information within
0.0590454185	datasets show
0.0590362692	impact of
0.0590112029	rate of
0.0589902324	net for
0.0589838125	time consuming and
0.0589801068	for clinical diagnosis
0.0589727315	feature maps of
0.0589316838	light detection and
0.0588841162	a large amount
0.0588010796	development of new
0.0587831214	other hand
0.0587115168	reconstruction methods for
0.0586933436	adoption of
0.0586928718	for model training
0.0586700315	the art video
0.0586677892	this scenario
0.0586133846	taken from
0.0585988574	used to predict
0.0585941146	increased by
0.0585645007	used to perform
0.0585584303	many fields
0.0585120878	take into
0.0585051327	room for
0.0584910263	the segmentation network
0.0584808264	high level of
0.0584711693	an optimized
0.0584698143	information into
0.0584656945	visual quality of
0.0584432302	used to evaluate
0.0584145675	not explicitly
0.0583967573	experiments on two
0.0583915189	efficient than
0.0583702833	report on
0.0583014782	supervised image
0.0582969582	spatial resolution of
0.0582749515	proof of
0.0582740712	manually by
0.0582610475	clinical use
0.0581810884	platform for
0.0581227312	method on
0.0581181932	diagnosis of
0.0581131896	the potential of
0.0580497724	constructed from
0.0580303333	the present work
0.0580058393	a small
0.0579644247	as inputs
0.0579279052	the physical model
0.0578808473	drop in
0.0578756326	different scenarios
0.0578749061	regions of
0.0578339944	different types
0.0577831214	further development
0.0577713037	signals from
0.0577660062	comes from
0.0577602789	reconstructed from
0.0577562157	to denoise
0.0577551310	by up to
0.0577453915	to decode
0.0577402146	segmentation of medical
0.0577344150	account for
0.0577256824	an encoder
0.0577095636	many applications
0.0576691720	generated from
0.0576554304	two key
0.0576117059	various applications
0.0576082102	after training
0.0575744382	to localize
0.0575427797	across datasets
0.0575393883	conditions such as
0.0575298563	these two
0.0574727390	any given
0.0574673887	approach allows
0.0574191322	for real time applications
0.0573801038	by calculating
0.0573309561	the art image
0.0573300514	3d space
0.0573188433	internet of
0.0573062120	deep convolutional neural network for
0.0572825771	the reconstructed images
0.0572706145	knowledge from
0.0572562157	to regularize
0.0571972028	an entire
0.0571939325	more important
0.0571794547	versions of
0.0571723826	more suitable
0.0571459418	by solving
0.0571356359	normal or
0.0571305806	tools for
0.0571157316	for training deep
0.0570358317	created from
0.0570110503	approach for
0.0570012194	run on
0.0569982710	mr images from
0.0569710058	specificity of
0.0569694663	reconstructed using
0.0569584303	three public
0.0569252267	directly on
0.0568915662	\ accuracy
0.0568865437	frame by
0.0568812351	reconstruction from
0.0568668509	a public
0.0567979806	scarcity of
0.0567142936	the proposed models
0.0567025607	contrast between
0.0566760498	implemented using
0.0566144641	further improvement
0.0565869626	acquired by
0.0565797736	the best possible
0.0565792745	the art deep learning based
0.0565275033	a non invasive
0.0565216285	performance than
0.0564888532	various levels
0.0564739129	a multi level
0.0564554575	to assign
0.0564080261	numbers of
0.0563600351	the model trained
0.0563274967	2d ultrasound
0.0563156902	an interpretable
0.0562955047	by extracting
0.0562873319	this project
0.0562453736	result in
0.0562116600	constraint on
0.0561362084	evaluated on three
0.0561318670	both cases
0.0561294630	problems such as
0.0561289517	trained using
0.0561128146	a test set
0.0561070758	dimension of
0.0560527820	source code of
0.0560526952	work demonstrates
0.0560310050	subset of
0.0559958300	the development of deep learning
0.0559719840	two strategies
0.0559359293	the same scene
0.0559257226	a strong
0.0558665680	accurate than
0.0557965963	inability to
0.0557882766	in many fields
0.0557791305	percentage of
0.0557600847	evaluated on two
0.0557294782	models for image
0.0557156622	using machine
0.0557060630	factors such as
0.0556180190	3d lidar
0.0555694297	the pixel level
0.0555671214	the light source
0.0555603808	most current
0.0555438545	provide more
0.0555381170	only limited
0.0555372819	list of
0.0555191668	proliferation of
0.0554732893	these approaches
0.0554554806	semantic segmentation using
0.0554500266	reconstruction method for
0.0554414619	three different
0.0554107966	the number of
0.0554007769	a fixed
0.0553777952	this report
0.0553327080	mapping from
0.0553269602	each point
0.0552929249	an extra
0.0552851642	implemented on
0.0552656006	performed on
0.0552464419	important for
0.0552165901	error of
0.0552159714	problem with
0.0552109554	studies on
0.0552075120	a sequence of
0.0551998528	also provided
0.0551905429	to modify
0.0551424221	used to segment
0.0551125102	the former
0.0550768691	used as input
0.0550765563	equivalent to
0.0550550770	resulting from
0.0550526125	parameters such as
0.0550333508	an inverse
0.0550240338	added to
0.0550206633	less parameters
0.0549572242	a computationally efficient
0.0549521021	the high level
0.0549428010	error between
0.0549242464	reduction of
0.0548644408	models for
0.0548608387	evaluated using
0.0548220051	to analyse
0.0548074992	level of
0.0546689556	robustness to
0.0546071862	to distinguish between
0.0545899708	amount of data
0.0545690872	usefulness of
0.0545675921	signs of
0.0545461508	a pixel wise
0.0545128154	a graph
0.0545115852	model with
0.0545074213	the art segmentation
0.0544929690	no significant
0.0544662825	determination of
0.0544483503	a major
0.0544369141	an individual
0.0544205791	acquired using
0.0543526652	representation of
0.0543503597	full 3d
0.0543439985	diseases such as
0.0543174172	an attempt
0.0542908453	as part of
0.0542879820	all three
0.0542297491	a machine learning
0.0542238229	both qualitatively
0.0541737894	detected by
0.0541613943	variations in
0.0541476203	detection via
0.0541107966	by using
0.0540916069	the choroid
0.0540466232	the trade off between
0.0540190999	scans from
0.0540133207	to automate
0.0539957425	reconstructed by
0.0539753453	2d images
0.0539731761	the current
0.0539729638	cnn trained
0.0539466949	applied to other
0.0539171408	improved by
0.0539159437	able to capture
0.0538943325	acquired with
0.0538863196	to map
0.0538756800	reduction in
0.0538743021	to convert
0.0537612202	ideas from
0.0537528863	by reducing
0.0537360335	the most challenging
0.0537077993	do not perform
0.0536737894	outputs from
0.0536678212	measured using
0.0536560931	employed as
0.0536497582	computer vision tasks such as
0.0535766380	an excellent
0.0535648369	great potential to
0.0535509056	very efficient
0.0535327518	different architectures
0.0534892233	an optical
0.0534817168	an expert
0.0534770991	a 3d
0.0533462140	large dataset of
0.0533242007	also present
0.0533092486	also investigate
0.0532822316	two components
0.0532680827	with negligible
0.0532671142	an extension
0.0532604261	also presented
0.0532084142	the generator
0.0532004560	estimated using
0.0531991710	dedicated to
0.0531782720	techniques for
0.0531731539	generated using
0.0531078880	by changing
0.0530948186	the inverse problem
0.0530917610	while still
0.0530628327	a challenging task due to
0.0529839007	all previous
0.0529828279	the most effective
0.0529667993	to reach
0.0529522382	3d objects
0.0529425318	method to
0.0528982221	restricted to
0.0528368667	five different
0.0528352416	time frequency
0.0528236114	3d localization
0.0528233503	to explore
0.0527777862	properties such as
0.0527771027	not suitable
0.0527612417	learned from
0.0527441299	the first
0.0527411049	to boost
0.0527410466	generate new
0.0527328997	a high quality
0.0527299517	some recent
0.0527015646	convolutional neural network with
0.0526968232	encountered in
0.0526955409	an optimal
0.0526952270	classified by
0.0526810884	allowing for
0.0526740779	an extended
0.0526720673	propose to
0.0526600117	to differentiate between
0.0526257582	influence on
0.0526133148	on two publicly available
0.0525822359	subject to
0.0525587355	performance under
0.0525408453	the need of
0.0525124581	also demonstrated
0.0525117774	often limited
0.0525092017	the field of medical
0.0524928705	robust to
0.0524914340	presence or
0.0524889175	the feature space
0.0524714503	system achieved
0.0524667516	the spatial domain
0.0524335426	utilized as
0.0523935994	uncertainty in
0.0523926298	on various datasets
0.0523830877	performances on
0.0523463690	time required
0.0523308986	none of
0.0522592607	both simulation and
0.0522546451	by simply
0.0522309354	overall performance
0.0522217703	gap by
0.0522195588	methods for
0.0521754496	solution to
0.0521627551	model for
0.0521430274	also tested
0.0521373218	aspects of
0.0521151339	on two publicly
0.0520809741	many clinical
0.0520545913	at hand
0.0520375171	assessed using
0.0520267315	to form
0.0520199388	3d point
0.0519924696	an objective
0.0519392064	different areas
0.0519344021	used to classify
0.0519103682	comparison of
0.0518249867	identification of
0.0517647337	proposed system
0.0516658227	a method based
0.0516622076	best results
0.0516606055	research on
0.0516327342	an extensive
0.0516228336	accuracy than
0.0516168505	techniques such as
0.0516151339	on two challenging
0.0516080261	delineation of
0.0515680466	2d +
0.0515064552	field of
0.0515054436	learns to
0.0514950333	not available
0.0514848598	reconstruction of
0.0514669000	calculated by
0.0514470478	adapt to
0.0514358031	further introduce
0.0514231670	not exist
0.0513916292	the final image
0.0513223119	involved in
0.0513148369	architecture search for
0.0513040768	such systems
0.0512822316	also shows
0.0512772044	challenges such as
0.0512439556	superior to
0.0512430648	the mutual information
0.0512091312	an event
0.0512062544	the art denoising
0.0512038211	more effectively
0.0511896879	the state of art
0.0511860963	evaluation on
0.0511525127	the same class
0.0511262479	amount of time
0.0511153876	in painting
0.0511131896	the robustness of
0.0510854752	several deep learning
0.0510812323	3d bounding
0.0510749516	type of
0.0510720923	3d imaging
0.0510523666	recorded by
0.0510086777	the problem
0.0510039885	characterization of
0.0509930721	used to extract
0.0509897492	to visualize
0.0509839754	different tasks
0.0509801984	make full
0.0509779647	this question
0.0509088634	except for
0.0508951952	a comparison
0.0508789389	performance across
0.0508688799	transferred to
0.0508610982	these artifacts
0.0507856154	feasibility of using
0.0507741141	treatment of
0.0507617496	tries to
0.0507352702	the test data
0.0507037694	top performance
0.0506727434	relative to
0.0506249867	application of
0.0506246122	for covid 19 diagnosis
0.0506225913	the superiority of
0.0505897161	computed from
0.0505389601	a multimode
0.0505277317	the input data
0.0504997658	inclusion of
0.0504848598	structure of
0.0504706206	the pre trained
0.0504353346	approach consists of
0.0504219019	the raw data
0.0503979354	a consequence
0.0503953708	to integrate
0.0503877706	trained to
0.0503596280	an integral
0.0503264443	on three publicly available
0.0503074992	localization of
0.0502344707	most effective
0.0501939456	devised to
0.0501851904	the resultant
0.0501717774	3d geometric
0.0501485246	three key
0.0501463663	this strategy
0.0501424249	assessed on
0.0501384654	comparison with
0.0501378720	an accurate
0.0501232868	these studies
0.0501024425	functionality of
0.0500978601	those obtained by
0.0500609640	while improving
0.0500403292	challenging because of
0.0500359505	position of
0.0500274937	squeeze and
0.0500202524	effective way to
0.0499942786	trained by
0.0499818579	good accuracy
0.0499553974	demonstrated on
0.0499370870	experiments over
0.0499314967	the segmentation model
0.0499284199	both tasks
0.0499242464	extraction of
0.0499160669	existence of
0.0499089501	cohort of
0.0499037465	difficult due to
0.0498979875	obtained with
0.0498925749	opportunity to
0.0498894895	developed for
0.0498825771	the classification task
0.0498461487	a 2d
0.0498387767	the whole image
0.0498325120	the basis of
0.0498225913	the goal of
0.0498181188	conducted using
0.0498040452	segmentation in
0.0497938170	optimized by
0.0497804761	also compare
0.0497670358	a subset of
0.0497512862	the usual
0.0497467534	a 3d convolutional
0.0497373324	by optimizing
0.0497229806	beneficial for
0.0497122037	the natural image
0.0496484800	two public
0.0496433008	this case
0.0496339578	levels of
0.0496231062	introduced as
0.0496139347	needed to
0.0496080261	aspect of
0.0496038676	information from different
0.0495797736	this gap by
0.0495601852	advancement of
0.0495497737	by selecting
0.0495294768	scenarios such as
0.0495131045	these tasks
0.0495093766	only requires
0.0495046568	efforts on
0.0494941285	the output
0.0494707722	consistent with
0.0494288023	problem of
0.0493935994	appearance of
0.0493770644	objects in
0.0493662219	both modalities
0.0493503922	the network parameters
0.0493385399	ensemble of
0.0493233503	to measure
0.0493205575	new approaches
0.0492939023	the retina
0.0492767406	the segmentation results
0.0492374459	aimed to
0.0492315115	modification of
0.0492313587	novel approach
0.0492224625	amplitude of
0.0491334660	to verify
0.0491080261	forms of
0.0490885056	a mean absolute
0.0490880485	by considering
0.0490578005	proposed method over
0.0490575221	segmentation from
0.0490436371	constraints on
0.0490423097	than previous
0.0490192039	the field of computer
0.0490139932	patterns from
0.0490017556	evaluated by
0.0489797649	the well known
0.0489585156	to preserve
0.0489465704	possibility of
0.0489141281	to simplify
0.0488823849	work presents
0.0488756666	to connect
0.0488682728	based method to
0.0488514201	superior performance in
0.0488307735	subjects from
0.0488253496	two sets
0.0487908575	against state of
0.0487380437	a significant
0.0487338125	expected to
0.0487302394	3d convolutional neural
0.0487250507	occurrence of
0.0487205427	sensitivity of
0.0487047491	top of
0.0487008287	various tasks
0.0486901127	helpful to
0.0486688526	existing methods on
0.0486471561	effect on
0.0486237917	organization of
0.0486134567	fraction of
0.0485783186	many medical
0.0485738058	tumors from
0.0485521351	a graph based
0.0485219464	several applications
0.0484930721	used to obtain
0.0484868273	further demonstrate
0.0484796053	behavior of
0.0484729806	developments in
0.0484717742	by creating
0.0484716463	artifacts such as
0.0484714278	from different domains
0.0484558900	this challenge
0.0484440238	this kind of
0.0484419233	generalize to
0.0484156296	to store
0.0484154119	than traditional
0.0483666788	to implement
0.0483604491	similarities in
0.0483305868	generative models for
0.0483122222	system achieves
0.0483037219	different strategies
0.0482778457	definition of
0.0482597545	no prior
0.0482416464	volumes from
0.0482190844	a single network
0.0481673815	and up sampling
0.0481668921	to guarantee
0.0481586129	to fill
0.0481547753	characteristics of
0.0481542461	in chest x rays
0.0481527002	the most significant
0.0481443944	second approach
0.0481435802	two important
0.0481334660	to encode
0.0481183000	used to measure
0.0480951716	the development of
0.0480834404	far from
0.0480657490	a result
0.0480356782	recorded from
0.0479914002	this way
0.0478972763	the reconstruction quality
0.0478805852	an extremely
0.0478749061	prediction of
0.0478007334	the art reconstruction
0.0477950405	bottleneck for
0.0477734495	to combine
0.0477681012	in order to achieve
0.0477667320	a patch based
0.0477635810	a multi view
0.0477551282	of chest x rays
0.0477470616	review of
0.0477229806	tailored for
0.0477213647	an autonomous
0.0477212053	3d reconstruction
0.0477053863	form of
0.0477031232	recovery of
0.0476994797	this area
0.0476702001	then uses
0.0476596153	synthesized from
0.0476309508	also called
0.0476253500	the literature
0.0476171598	with varying
0.0475978277	needed for
0.0475965320	to apply
0.0475766131	a series of experiments
0.0475628723	the art performance in
0.0475440247	an energy
0.0475149585	many computer vision
0.0475041870	two orders
0.0475029219	an input
0.0474893578	a holistic
0.0474678948	available for training
0.0474638922	structure from
0.0474502295	ensembles of
0.0474442612	performance in
0.0474390392	an increased
0.0474205261	variants of
0.0474019430	out of
0.0473912896	these works
0.0473838674	based approaches for
0.0473838185	a time consuming
0.0473645113	captured from
0.0473472671	but not
0.0473250973	a key
0.0472932219	data from different
0.0472839971	architecture for
0.0472838311	allowed to
0.0472737014	a high dimensional
0.0472705766	task of
0.0472681932	technique for
0.0472628404	an image to image
0.0472262107	advances in
0.0472104768	these properties
0.0472056873	in order to address
0.0471890392	by performing
0.0471736063	of interest
0.0471559247	the influence of
0.0471544270	role in many
0.0471500953	use of deep learning
0.0471486624	2d cnns
0.0470507455	expense of
0.0470311561	limited to
0.0470074112	an unseen
0.0469883207	to monitor
0.0469609096	context of
0.0469241786	different levels of
0.0468978231	diagnosed with
0.0468937122	opportunities for
0.0468669512	a pixel level
0.0468484871	these results
0.0468214232	mri scans of
0.0467950378	by modeling
0.0467885130	promising results for
0.0467759940	each task
0.0467613071	correlated with
0.0466455555	between frames
0.0466443574	augmented with
0.0466443574	exist for
0.0466413574	under various
0.0466412714	required by
0.0466294494	method on two
0.0466257582	operating on
0.0466252315	not contain
0.0466230082	explanations for
0.0465892409	dataset with
0.0465825120	the advent of
0.0465668921	a viable
0.0465333146	a plug and play
0.0465197233	a full
0.0465116474	methods such as
0.0465058394	limited due to
0.0464881271	mapped to
0.0464861844	growth of
0.0464581167	prevalent in
0.0464399116	a student
0.0464288115	new approach
0.0464242464	improvement of
0.0464171753	a powerful
0.0463552046	an eye
0.0463465738	metrics such as
0.0463271379	3d object
0.0463168859	discussion of
0.0462886988	novel learning based
0.0462762879	fails to
0.0462441299	a given
0.0462274258	an enhanced
0.0462268551	threat to
0.0462232079	an overall
0.0461835839	us images
0.0461694842	the ground
0.0461613943	increase in
0.0461204463	or not
0.0460621110	contained in
0.0460444323	three publicly
0.0460204815	to construct
0.0460046440	pipeline for
0.0459780535	distribution from
0.0459293444	tumor segmentation in
0.0459157453	a u net architecture
0.0459107966	the accuracy of
0.0458945128	the performance
0.0458367283	the receiver
0.0458344578	beginning of
0.0458225913	the impact of
0.0458203692	images taken
0.0458053641	screening of
0.0457993271	different parts
0.0457395967	designed for
0.0457363568	to save
0.0456740923	an action
0.0456521562	a publicly available dataset
0.0456351824	guaranteed to
0.0455489533	lesion segmentation in
0.0455425228	potential for
0.0455164934	aim to
0.0455053301	also demonstrate
0.0454955976	such cases
0.0454798218	tested with
0.0454749290	also achieves
0.0453896472	degrees of
0.0453886561	a principled
0.0453803142	a novel method
0.0453546003	to analyze
0.0453292925	each sub
0.0453231281	building on
0.0453174710	often rely on
0.0453155159	a multilayer
0.0452894111	the most relevant
0.0452891175	employed for
0.0452815659	to ease
0.0452790134	drawback of
0.0452439402	to search for
0.0451930145	proposed to
0.0451740133	proposed for
0.0451532946	the art convolutional
0.0450831808	place in
0.0450645955	to compute
0.0450613895	not require
0.0450610718	essential to
0.0450571251	a median
0.0450028649	network architecture for
0.0449958990	integrated with
0.0449897492	to calculate
0.0449812059	an experimental
0.0449617207	the evolution
0.0449534994	extraction from
0.0449468898	a deep learning approach for
0.0449450542	response to
0.0449418589	most recent
0.0449343143	visualization of
0.0449335214	relevant to
0.0448869871	to use
0.0448691220	this technique
0.0448418189	to distinguish
0.0448303020	an improvement of
0.0448225913	a combination of
0.0448188598	these measurements
0.0448074992	knowledge of
0.0448046995	to support
0.0448025955	quality than
0.0447675687	demonstrated using
0.0447531543	reconstructions from
0.0447524205	rise of
0.0447422745	to leverage
0.0447409992	to restore
0.0447368534	interpretation of
0.0447029061	3d videos
0.0446961754	automation of
0.0446747750	works on
0.0446712440	simplicity of
0.0446711755	described by
0.0446696506	imaging provides
0.0446571163	performed with
0.0446543788	to adapt
0.0446451298	the ill posed
0.0446304367	equal to
0.0446200873	optimized for
0.0446157453	the so called
0.0446121409	risk of
0.0445973664	this context
0.0445299544	picture of
0.0445235192	also propose
0.0445128154	the latent
0.0444866400	the feature map
0.0444585339	description of
0.0444523169	learning models for
0.0444452202	abilities of
0.0444303037	deep learning model to
0.0444251844	contribution of
0.0444072095	to carry
0.0444070951	drawbacks of
0.0444033634	problem by
0.0443972763	the segmentation performance
0.0443770650	selection of
0.0443659550	a novel adaptive
0.0443628421	period of
0.0443580651	results than
0.0443368667	across various
0.0443191975	to represent
0.0443137137	spatial information of
0.0443073530	an original
0.0442366400	the image sensor
0.0442366397	requirement for
0.0442036228	operate in
0.0441849470	scheme for
0.0441756513	paired with
0.0441361493	by achieving
0.0441298726	this setting
0.0441175076	differences in
0.0441140804	an algorithm
0.0441101624	era of
0.0440225787	to incorporate
0.0440019702	solutions for
0.0439614003	performance with
0.0438949651	a coarse
0.0438864293	1 \
0.0438777170	algorithm for
0.0438359335	the fact
0.0438307908	this research
0.0437797342	well as
0.0437231224	based segmentation of
0.0436786747	various machine
0.0436618869	vulnerability of
0.0436558529	critical for
0.0436275763	potential of
0.0436190227	to select
0.0436183000	used to identify
0.0435708957	deviation of
0.0435698926	discovery of
0.0435616786	the coronavirus disease
0.0435525850	used to reconstruct
0.0435063139	consideration of
0.0434849440	solution for
0.0434631453	a promising
0.0434245042	first introduce
0.0434239610	tissues from
0.0434158453	to account for
0.0434006810	seen as
0.0433997345	diagnosis and treatment of
0.0433977654	based methods for
0.0433709053	same dataset
0.0433268040	to translate
0.0433191252	proposed method with
0.0433154935	little as
0.0433150248	novel end to end
0.0433070938	any other
0.0433028229	goals of
0.0432879031	for 3d medical image
0.0432849336	from undersampled
0.0432831717	to validate
0.0432811904	procedure for
0.0432407662	a compact
0.0432135536	helpful in
0.0431865525	approach on
0.0431804545	by radiologists
0.0431615892	rest of
0.0431504871	than existing
0.0431470616	observed in
0.0431466862	computed by
0.0431437660	an observation
0.0431103788	burden on
0.0431101624	demonstration of
0.0431052302	needs to
0.0430801721	to adversarial attacks
0.0430671458	the amount of
0.0430579106	method against
0.0430535389	to replace
0.0430478570	sequence of
0.0430130419	adapted for
0.0430108458	required to
0.0429990495	a modified
0.0429953589	than conventional
0.0429836713	this kind
0.0429780535	cases from
0.0429252733	an approximation
0.0429060162	a group of
0.0428976515	approaches use
0.0428761130	other approaches
0.0428731124	presented in
0.0428719267	a pre processing
0.0428697899	limitations of
0.0428511391	implemented in
0.0428383644	a 3d u net
0.0428142660	a large set of
0.0428110718	a new approach
0.0427788076	importance of
0.0427689225	goal of
0.0427572112	feasibility of
0.0427554367	tailored to
0.0427372615	three tasks
0.0427286569	challenge due to
0.0427043833	from different modalities
0.0426746618	this assumption
0.0426702001	if not
0.0426513391	new algorithm
0.0426496758	a differentiable
0.0426478300	a fundamental
0.0426472049	proposed framework on
0.0426395050	aiming to
0.0426361563	placement of
0.0426224625	array of
0.0426052689	novel architecture
0.0425949881	to compare
0.0425850376	the generated
0.0425780310	comparison to
0.0425691932	attempts to
0.0425666270	the human visual
0.0425469170	course of
0.0425128154	a reference
0.0425112029	generation of
0.0425088964	built on
0.0424839469	by evaluating
0.0424753453	this process
0.0424737233	to design
0.0424344216	by developing
0.0424339736	region of
0.0424269491	impractical to
0.0424168859	impossible to
0.0423539023	a shared
0.0423451716	in addition to
0.0423450405	fused with
0.0423378799	the most
0.0422948559	perform well in
0.0422887648	to investigate
0.0422025553	unsupervised learning of
0.0422001458	required for
0.0421905773	a modular
0.0421656296	to emphasize
0.0421592942	by humans
0.0421444781	ability of
0.0421070177	success of
0.0420940795	transmitted to
0.0420897492	to explain
0.0420853301	transferability of
0.0420772577	these algorithms
0.0420772254	stability of
0.0420724665	lost in
0.0420622184	network architectures for
0.0420370419	a stack of
0.0419804898	accuracy over
0.0419556181	with respect
0.0419343143	improvements in
0.0419262194	agreement with
0.0419093035	achieved by using
0.0418909329	different datasets
0.0418680252	extended to
0.0417899868	not possible to
0.0417892580	useful for
0.0417741475	scenes with
0.0417012530	learning framework to
0.0416895007	in order to improve
0.0416836229	the mean absolute error
0.0416815822	these techniques
0.0416746618	this observation
0.0416659416	to compensate
0.0416478601	more accurate than
0.0416467121	the curve
0.0415946057	all existing
0.0415918511	formulation of
0.0415275220	status of
0.0414851904	to satisfy
0.0414826064	understanding of
0.0414551908	to decide
0.0414484850	makes use
0.0414342619	new loss
0.0414288391	field of view of
0.0414246122	the signal to noise ratio
0.0414197151	the existing methods
0.0414072095	to decompose
0.0413664685	more recently
0.0413622133	choice for
0.0413614075	better quality
0.0413489272	by estimating
0.0413478570	convergence of
0.0413454068	reproducibility of
0.0413450405	comparisons with
0.0413328218	this methodology
0.0413299544	valuable for
0.0413080679	images with different
0.0413074992	applied in
0.0413034158	decrease in
0.0412990443	all other
0.0412835628	strengths of
0.0412651965	employed to
0.0412633882	comparing with
0.0412547938	solved with
0.0412535149	trends in
0.0412513944	detection and classification of
0.0412336077	an increasingly
0.0412309217	various medical
0.0412075120	the aim of
0.0412034397	three sub
0.0411954344	by computing
0.0411919857	the actual
0.0411764139	various noise
0.0411640530	produced from
0.0411513830	defined on
0.0411344216	by showing
0.0411302693	flexibility in
0.0411201231	a 3d convolutional neural
0.0411186269	a better understanding of
0.0411004013	available datasets
0.0410945476	competitive with
0.0410863727	insights on
0.0410701416	the true
0.0410658588	more suitable for
0.0410652111	able to improve
0.0409865620	the segmentation task
0.0409811217	examination of
0.0409787770	correlation with
0.0409724782	metric for
0.0408854738	to add
0.0408540804	an approach
0.0407998475	redundancy in
0.0407809815	success in
0.0407708467	introduced to
0.0407619666	a trade off
0.0407519881	among different
0.0407474950	to prevent
0.0407445575	process of
0.0407282773	strategy for
0.0407024243	to cope with
0.0406965763	to tune
0.0406702104	the output of
0.0406616815	a new dataset
0.0406571250	introduction of
0.0406503758	image reconstruction with
0.0406488618	to reflect
0.0406029146	other types of
0.0405932426	a 1d
0.0405739911	by generating
0.0405583609	the source
0.0405307678	pool of
0.0405075534	a huge
0.0404806301	progress in
0.0404806301	capabilities of
0.0404518040	to expand
0.0404279362	registration using
0.0404172839	several state of
0.0404096149	by at least
0.0403626591	mean absolute error of
0.0403587815	network trained on
0.0403007948	to retain
0.0402918162	novelty of
0.0402757231	new architecture
0.0402715660	this technology
0.0402677609	to reproduce
0.0402659554	the art on
0.0402552254	study on
0.0402274743	incorporated in
0.0402171947	a challenging
0.0402045848	variant of
0.0401798982	considered to
0.0401739834	a widely used
0.0401500958	to bring
0.0401475140	quantities of
0.0401468686	imaged with
0.0401183603	subjects with
0.0400798563	because of
0.0400719034	detection and segmentation of
0.0400294580	two orders of
0.0400075120	the features of
0.0399378206	requirements for
0.0399202938	modeling of
0.0398459298	the interpolated
0.0398233105	by experts
0.0397841742	capability of
0.0397823856	deep networks for
0.0397271920	evaluated with
0.0397271284	the existing state of
0.0397194405	also provides
0.0397073010	a unique
0.0396999398	a recursive
0.0396962243	smoothness of
0.0396244481	merits of
0.0395954665	working on
0.0395883432	laborious and
0.0395726678	information between
0.0395723430	the rapid development
0.0395170720	area of
0.0395092379	among other
0.0394875007	this direction
0.0394871668	artifacts such
0.0394865361	first study
0.0394692833	information through
0.0394545823	an fpga
0.0394484375	deployed in
0.0394431139	included in
0.0394324072	the segmentation quality
0.0394162287	a mean
0.0393972049	important task in
0.0393770650	independent of
0.0393742660	different parts of
0.0393551586	order to
0.0393429700	a generic
0.0393320013	a recurrent neural
0.0392227134	to find
0.0392108987	benefits of
0.0391658681	the recovered
0.0391645715	an empirical
0.0390890642	from multiple
0.0390752255	on device
0.0390361637	choice of
0.0390114546	the patient's
0.0390092249	for diagnosing
0.0390081346	other methods
0.0389979316	a baseline
0.0389722408	agnostic to
0.0389636624	able to provide
0.0388848058	many recent
0.0388636110	tuned to
0.0388580409	an upper
0.0388251004	to deploy
0.0387930389	represented in
0.0387689225	benefit of
0.0387617072	demand for
0.0387611228	one way
0.0387365593	history of
0.0387271011	a global
0.0387244020	the model's
0.0387223155	the initial
0.0387027576	information than
0.0386637073	a prominent
0.0386366132	to characterize
0.0386136025	series of
0.0386121254	this leads
0.0386025967	an appropriate
0.0385951716	a number of
0.0385823109	utilized to
0.0385671458	alternative to
0.0385590578	potential to
0.0385357010	the attacker
0.0384830540	to annotate
0.0384654698	still not
0.0384531543	learns from
0.0384495877	a way to
0.0384061797	the 3d structure
0.0384037797	between two
0.0383677054	a convex
0.0383425712	against other
0.0383320559	an emphasis
0.0383141011	ranges of
0.0383019151	accuracy on
0.0382962243	run in
0.0382670358	the concept of
0.0382393578	to aggregate
0.0382369561	recordings of
0.0382369561	trend of
0.0381952408	3d us
0.0381655608	more than one
0.0381559247	the success of
0.0381559247	the importance of
0.0381423638	the predicted
0.0381168962	extensions of
0.0380975058	under different
0.0380165678	from chest ct
0.0380159546	an output
0.0380101378	by quantifying
0.0379662559	a robust
0.0379189425	selected from
0.0379169713	robustness of
0.0379011391	spectrum of
0.0378697899	effects of
0.0378476704	popularity in
0.0378462227	non line
0.0378442708	investigation of
0.0378403510	capacity of
0.0378095845	the art results in
0.0378041436	to demonstrate
0.0377765504	the art object
0.0377292399	conducted to
0.0377254260	methodology for
0.0376809447	to propagate
0.0376754195	important problem in
0.0376263722	creation of
0.0376237044	width of
0.0376090325	a well known
0.0375950631	for detecting
0.0375948405	not possible
0.0375914583	likelihood of
0.0375835021	heterogeneity of
0.0375820543	by removing
0.0375704118	quality compared to
0.0375669410	an easy
0.0375654698	found at
0.0375432553	applicable for
0.0375047938	beneficial to
0.0374808473	registered to
0.0374673812	this fact
0.0374528366	in loop
0.0374460090	the previous methods
0.0374214604	similarly to
0.0373963801	better than other
0.0373770650	component of
0.0373657385	function based on
0.0373588860	to utilize
0.0373285834	probabilities of
0.0373157683	a given image
0.0373141011	concatenation of
0.0373078439	3d structure of
0.0372829210	the way for
0.0372686353	new state of
0.0372685777	variability of
0.0372346370	preservation of
0.0372269018	the network's
0.0372127457	parts of
0.0372075723	used to compare
0.0371999398	a reasonable
0.0371999398	for determining
0.0371747786	taken as
0.0371745458	idea of
0.0371706507	in order
0.0371686898	a customized
0.0371559247	the absence of
0.0371153232	to generalize
0.0370955845	evolution of
0.0370914851	for classifying
0.0370683330	to discriminate
0.0370623048	a typical
0.0370272026	significantly different
0.0370084941	often not
0.0369996488	methods do
0.0369837753	vulnerability to
0.0369780847	methods like
0.0369695951	assist in
0.0369589947	a study
0.0369410687	the outer
0.0369340846	the default
0.0369340846	the necessity
0.0368919039	accessible to
0.0368871053	integration of
0.0368695177	utilized for
0.0368601904	to check
0.0368585177	means of
0.0368388822	design of
0.0368268295	$ x
0.0368163425	a special
0.0368028229	integrity of
0.0367770016	approaches such as
0.0367768290	prevalence of
0.0366869671	implementations of
0.0366849725	a joint
0.0366829765	high accuracy in
0.0366685777	utilized in
0.0366522762	training set of
0.0366164743	a clear
0.0365798563	changes in
0.0365646174	autoencoder with
0.0365495669	for 3d object detection
0.0364891291	extent of
0.0364771964	execution of
0.0364732701	novel way
0.0364697824	this retrospective
0.0364683991	guidance for
0.0364673109	a novel architecture
0.0364609096	cost of
0.0364179380	variability in
0.0364137282	tool in
0.0364070951	hours of
0.0363978570	studied in
0.0363938630	impacts of
0.0363801538	a user
0.0363719281	histogram of
0.0363692414	done using
0.0363644578	a popular
0.0363581670	the k space
0.0363517399	diversity of
0.0363353694	the first approach
0.0362965718	due to lack of
0.0362863026	mm for
0.0362531490	all possible
0.0362514918	an uncertainty
0.0362014774	a sequential
0.0361933470	and eosin
0.0361844194	the image quality of
0.0361796530	the effectiveness
0.0361559247	the potential to
0.0361500400	size of
0.0361366132	a broad
0.0361278559	a deep neural network for
0.0361162344	features such as
0.0361106851	most important
0.0361101814	desirable to
0.0361088397	to operate
0.0360982269	by measuring
0.0360930086	3d convolution
0.0360701684	different approaches
0.0360590614	ratio of
0.0360566534	as measured by
0.0360539800	into four
0.0360439943	located in
0.0360168099	the perceptual quality of
0.0360109729	different cameras
0.0359943325	critical to
0.0359932553	progress on
0.0359860129	a single 2d
0.0359736165	a new method
0.0359036228	shortcomings of
0.0359026481	referred to
0.0358937951	time series of
0.0358932800	a predefined
0.0358819741	to deliver
0.0358732220	an artificial
0.0358665141	bottleneck in
0.0358606559	a two dimensional
0.0358572390	effect of
0.0358403510	inspection of
0.0358399071	all types of
0.0358278159	cascade of
0.0358258882	discussed in
0.0358002792	algorithms such as
0.0357882867	outbreak of
0.0357538720	to advance
0.0357233105	a deterministic
0.0356931777	adopted for
0.0356571745	the new method
0.0356526171	employed in
0.0356379621	for generating
0.0356134345	features across
0.0355951716	the lack of
0.0355627394	an algorithm based
0.0355503151	3d medical image
0.0355415098	an instance
0.0354696148	a very small
0.0354365343	same time
0.0354309473	to manage
0.0353425712	even better
0.0353332078	the primary
0.0353272441	the denoised
0.0353095845	the art performance for
0.0352996389	with very high
0.0352563320	the same dataset
0.0352477243	a dedicated
0.0352026171	technology for
0.0351986481	coefficient of
0.0351691848	this study aims to
0.0351651928	signal to
0.0351559247	the cost of
0.0351559247	the case of
0.0350870458	spread of
0.0350772254	half of
0.0350629059	the same patient
0.0350331808	exist in
0.0350251844	interpretability of
0.0350033569	several existing
0.0349970938	even more
0.0349813587	a novel unsupervised
0.0349733105	a valid
0.0349649868	more difficult to
0.0349366397	composition of
0.0349297581	recovery from
0.0348874658	a discussion
0.0348370146	a stochastic
0.0348368355	important to
0.0348048193	not suitable for
0.0347985650	achieved with
0.0347713730	popularity of
0.0347111385	comes with
0.0346958450	a new type of
0.0346955530	three types
0.0346559247	the advantages of
0.0346534390	image quality for
0.0346269728	a finite
0.0345749962	corrupted with
0.0345379181	applicability of
0.0345340996	to extend
0.0345178038	tests on
0.0345071630	or comparable
0.0345032551	back to
0.0344417961	variance in
0.0344339402	a diverse set of
0.0343565560	advantages of
0.0343524417	over state of
0.0343446671	learning method to
0.0343121844	especially at
0.0343045773	realism of
0.0343023931	build on
0.0343009809	quantification of
0.0342798218	a novel framework
0.0342535686	taken by
0.0342535246	a shallow
0.0342527329	pair of
0.0342335710	work aims
0.0342149871	case of
0.0342026171	embedded in
0.0341976829	an extension of
0.0341918511	basis for
0.0341687551	achieved using
0.0340828954	the second
0.0340546471	problem into
0.0340536538	the detection of
0.0340449118	challenge on
0.0340370083	amount of information
0.0340319814	deployment of
0.0340166317	a critical
0.0340162606	the analysis of
0.0340101760	for implementing
0.0339945871	two new
0.0339539277	to render
0.0339499398	a straightforward
0.0339397410	the value of
0.0339349262	the reconstruction of
0.0339123136	the previous state of
0.0338556686	problem in
0.0338462102	the field of computer vision
0.0338369556	3d human
0.0338321862	made by
0.0337888315	mixture of
0.0337852936	the burden of
0.0337795576	aid in
0.0337778193	to decrease
0.0337718784	results on three
0.0337679102	other conventional
0.0337609225	the efficiency of
0.0337502042	than other
0.0337396097	a set
0.0337391225	known to
0.0337292564	for assessing
0.0337246351	for 3d medical
0.0336986481	exploration of
0.0336918511	accuracies of
0.0336913904	a tool
0.0336727014	and then
0.0336333295	the proposed multi
0.0336275763	efficiency of
0.0336011372	difficulties in
0.0335537982	crucial to
0.0335532531	to screen
0.0335414808	models without
0.0334788334	both in terms
0.0334646641	a bidirectional
0.0334595456	other techniques
0.0334428688	uses only
0.0334338125	the diagnosis of
0.0334253124	on synthetic and
0.0334131269	the overall performance
0.0333958857	a learnable
0.0333686096	the corresponding
0.0333072515	the first study
0.0333045733	the process of
0.0333045733	the ability of
0.0332968937	collection of
0.0332654178	the number of parameters
0.0332581518	a cascaded
0.0332555956	complexity of
0.0332290683	two publicly
0.0332278873	the emergence
0.0332036657	the rest
0.0331891291	requirement of
0.0331559247	a range of
0.0331359327	of great interest
0.0330623048	a complete
0.0330524655	in terms of accuracy
0.0330509082	images contain
0.0330379181	usage of
0.0330303016	promise for
0.0329996425	the fovea
0.0329841726	leveraged to
0.0329701220	to discover
0.0329609096	generalization of
0.0329397410	of up to
0.0328971464	a general
0.0328845663	in terms
0.0328676455	and eventually
0.0328402901	the signal to noise
0.0328359327	the visibility of
0.0328343622	other existing
0.0328319814	property of
0.0327892580	to help
0.0327606610	dependency of
0.0327125827	a pre
0.0327104084	a great
0.0327075120	the utility of
0.0327037740	for predicting
0.0326559247	the effects of
0.0326254003	reliability of
0.0326164743	to match
0.0326086898	a prototype
0.0325820543	a convenient
0.0325755569	done on
0.0325663827	the most commonly
0.0325609225	the complexity of
0.0325536538	the training of
0.0325359484	over existing
0.0325171458	the relationship between
0.0324908332	a dataset of
0.0324843339	volume of
0.0324458450	a significant impact on
0.0324169713	power of
0.0323880688	then propose
0.0323670590	for solving
0.0323521405	to run
0.0323446287	for improving
0.0323332078	a reliable
0.0323071541	the loop
0.0322847746	a proper
0.0322829210	the role of
0.0322790155	to process
0.0322713264	an augmented
0.0322581518	a substantial
0.0322295177	adopted in
0.0322237073	a versatile
0.0322225657	dependencies in
0.0322028366	a moderate
0.0322018072	outcome of
0.0321953174	used to reduce
0.0321889517	the next
0.0321683510	presented as
0.0321637210	to control
0.0321376339	a novel generative
0.0321237239	regularization for
0.0320617598	a crucial
0.0320162606	the segmentation of
0.0320063754	studied for
0.0320035423	not fully
0.0319777527	the way to
0.0319746084	other than
0.0319653346	a simplified
0.0319379067	allows for
0.0319349262	the classification of
0.0319059022	in many applications
0.0318684021	the field of view
0.0318671458	the size of
0.0318596382	slice by
0.0318469358	efficacy of
0.0318178316	often leads to
0.0318080973	while most
0.0318052966	the region of interest
0.0317973202	to reverse
0.0317778193	to fit
0.0317725006	the alternating direction method of
0.0317693574	exploited to
0.0317456196	the potential
0.0316993496	the existence of
0.0316918457	addition to
0.0316788076	severity of
0.0316403511	many other
0.0315669110	product of
0.0315612198	a training set
0.0315370458	concept of
0.0315118173	a new deep learning
0.0315029210	the difficulty of
0.0315010039	a novel deep learning
0.0314949848	the possibility
0.0314925549	helps in
0.0314519285	a universal
0.0314463752	biases in
0.0314338125	the distribution of
0.0314338125	the generation of
0.0314045848	extension of
0.0313472763	the existing approaches
0.0313130982	the need to
0.0313045733	the ability to
0.0312573037	on different datasets
0.0312493281	the second step
0.0312350657	workflow for
0.0311981036	reconstruction via
0.0311543905	a big
0.0311467121	a practical
0.0311360834	the reliability of
0.0311252518	the presence
0.0311136076	a deep learning approach to
0.0311007533	a double
0.0311006069	both visually and
0.0310746618	a tedious
0.0310601687	topic in
0.0310590813	in summary
0.0310393680	a method for
0.0309853817	the inherent
0.0309781016	strategies for
0.0309379427	experiment with
0.0309164077	the advantage of
0.0308321566	to reveal
0.0308126254	reconstructed with
0.0307751167	auc of
0.0307678728	able to accurately
0.0307514106	a maximum
0.0307075120	the possibility of
0.0306734832	step by
0.0306649868	the topology of
0.0306259536	with other state of
0.0306083296	anomalies in
0.0306067121	a direct
0.0306062903	then used
0.0305545733	as compared to
0.0305179874	to yield
0.0305128893	new paradigm
0.0305000893	influence of
0.0303965372	a very low
0.0303734832	adaptation for
0.0303440109	most previous
0.0303360899	calculation of
0.0303359327	a training set of
0.0302926202	the art results for
0.0302852301	both synthetic
0.0302790444	construction of
0.0302462243	strength of
0.0302372175	also allows
0.0302122318	very different
0.0301871232	to allow for
0.0300935548	a detailed
0.0300746618	to balance
0.0299808475	a mean dice
0.0299481746	for estimating
0.0298768481	tedious and
0.0298677601	the gap between
0.0298419358	a new framework
0.0298246618	to bridge
0.0298090693	to update
0.0298082906	the network to learn
0.0297816189	this work aims
0.0297793326	to aid
0.0297700103	to share
0.0297395025	further reduce
0.0297364680	to derive
0.0296842113	a publicly available
0.0296559247	the feasibility of
0.0296386299	the level of
0.0295549824	an overall accuracy
0.0295473688	results in
0.0295393680	use of
0.0295022143	strategy to
0.0294825060	research in
0.0294625827	the number
0.0294019430	a very
0.0293664781	especially for
0.0293492088	then use
0.0293362543	this type of
0.0293035779	the recent success
0.0293025994	the first attempt to
0.0292750652	by identifying
0.0292551340	at improving
0.0292311904	presented for
0.0292210532	than state of
0.0292032536	reduced from
0.0291794110	generalizability of
0.0291475994	both simulated and
0.0291467121	a hierarchical
0.0291386299	the sensitivity of
0.0290768807	work well
0.0290566534	a critical role in
0.0289575120	in comparison with
0.0289575120	an ensemble of
0.0289566534	a linear combination of
0.0289557537	mean accuracy
0.0289295560	most suitable
0.0289086898	a balanced
0.0289008393	stack of
0.0288715663	in order to provide
0.0288512894	value of
0.0288412845	orders of
0.0288392659	a group
0.0288388286	to prove
0.0287778193	to gain
0.0287402512	a fast and accurate
0.0287266977	a very challenging
0.0287233201	a significant amount of
0.0287205948	the semantic information
0.0287194386	scalable to
0.0287124618	a scalable
0.0286843183	to conduct
0.0286553134	for many applications
0.0286532925	based system
0.0286386299	the degree of
0.0286275892	the majority of
0.0286184452	struggle to
0.0286064077	an average of
0.0286064077	an image with
0.0286060145	to keep
0.0285956009	a long
0.0285863815	compression with
0.0285674199	the abundance
0.0285393680	the application of
0.0285165101	adopted to
0.0284612897	by predicting
0.0284360834	a novel method for
0.0284338125	the design of
0.0283820367	cells with
0.0283343454	in order to make
0.0282709339	developed by
0.0282416629	frameworks for
0.0282362607	the art 3d
0.0281518329	a good
0.0281230982	the experimental results show
0.0281122739	the benefits of
0.0280426913	a progressive
0.0280169348	the novelty of
0.0280051673	step in
0.0280025994	a comparative study of
0.0279877658	for analyzing
0.0279770305	and time consuming task
0.0279655835	a dice score of
0.0279499757	3d u
0.0279366012	the desired
0.0279139834	the difference between
0.0278937951	as input and
0.0278283264	features into
0.0278243545	the non local
0.0278130982	the type of
0.0278081822	to allow
0.0277892580	corresponding to
0.0277609225	a lack of
0.0277497410	the extraction of
0.0277452751	the gap
0.0277091133	to confirm
0.0277051432	the center of
0.0276479066	architecture with
0.0276021903	method does not
0.0275806501	a method to
0.0275772244	annotated with
0.0275354594	on synthetic and real
0.0275207328	module to
0.0274891602	a patch
0.0274691052	the spatial resolution of
0.0274475629	significance for
0.0274338125	the combination of
0.0274316534	the second one
0.0274115294	the likelihood of
0.0273958857	for evaluating
0.0273419397	to end
0.0273115998	utility of
0.0272945877	an overview of
0.0272933363	at different
0.0272898492	the perceived
0.0272816809	deployed to
0.0272795835	a new algorithm
0.0272682595	the prediction of
0.0271344336	3d convolutional
0.0270627689	microscopy with
0.0270442660	the spatial distribution of
0.0270438951	paradigm for
0.0270218377	the use of deep
0.0270117720	to examine
0.0270020231	to illustrate
0.0269483476	promise in
0.0269195457	built in
0.0268925186	center of
0.0268622739	this problem by
0.0268559479	a separate
0.0268168682	the automatic detection
0.0268130982	the assessment of
0.0268001978	as much
0.0267771038	real time on
0.0267616877	information at
0.0267591115	a novel approach for
0.0267109327	a specificity of
0.0266785852	a new state of
0.0266770527	necessary for
0.0266719764	for segmenting
0.0266396486	a patient's
0.0266122739	the proposed system
0.0265804044	a particular
0.0265707647	also outperforms
0.0265597303	on three different
0.0265566534	an important problem in
0.0265393680	the form of
0.0264947552	system for
0.0264318626	work provides
0.0263698396	the 3d cnn
0.0263416331	to highlight
0.0263164453	taken into
0.0263084126	to train and test
0.0263081822	to take
0.0262655585	different from
0.0262300156	commonly used for
0.0261753633	to follow
0.0261081822	need for
0.0261064077	a fast and
0.0260958359	and easy to
0.0260739413	a central
0.0260686096	and thus
0.0260674199	to define
0.0260533651	the inner
0.0260407137	a non local
0.0260356009	a conditional
0.0260035923	features from different
0.0259655092	visibility of
0.0258628190	the idea
0.0258559911	different noise
0.0258318827	to cover
0.0257864165	19 patients
0.0257609225	the availability of
0.0257603803	and hence
0.0257273924	both local
0.0257144265	developed to
0.0256813337	the first one
0.0256645436	supervision for
0.0256632905	a generative
0.0256362056	a sensitivity of
0.0256332264	a considerable
0.0256279204	the superiority
0.0256233851	the severity of
0.0256230982	the structure of
0.0256017418	a novel attention
0.0255284041	to approximate
0.0255232822	a variety
0.0255167653	enough for
0.0254692069	a desired
0.0254578684	u net for
0.0254577180	a valuable
0.0253922086	calculated for
0.0253871315	limitation of
0.0253648013	all available
0.0253580799	structures from
0.0253378799	especially in
0.0252802205	the most widely
0.0252728975	the spatial distribution
0.0252421859	an array
0.0252209787	to track
0.0251828483	a hardware
0.0251544621	factors such
0.0251464316	the estimation of
0.0251450815	management of
0.0251443705	progression of
0.0251386299	the correlation between
0.0251359327	a crucial step in
0.0250808723	try to
0.0250765684	a hot
0.0250635181	novel method
0.0250493858	the case
0.0250362277	the development
0.0249453367	described in
0.0249265415	novel framework
0.0249139834	the contribution of
0.0248930086	a cascade
0.0248922086	addressed in
0.0248721450	available for
0.0248663436	not rely on
0.0248636031	an accuracy
0.0248434567	network without
0.0248045405	best possible
0.0247798608	the above
0.0247561837	novel graph
0.0247497410	without using
0.0247416629	transforms for
0.0247233201	two sets of
0.0246033345	and reproducibility of
0.0245630915	distance from
0.0245520408	a serious
0.0245502681	an image into
0.0245373355	a novel multi
0.0245326202	not available in
0.0245119871	the evaluation of
0.0244966490	the location of
0.0244916629	published in
0.0244877658	for reconstructing
0.0244015518	by pathologists
0.0243844194	the latent space of
0.0243775994	this paper focuses on
0.0243637214	making use
0.0243224121	causes of
0.0242972328	role of
0.0242743261	to deal
0.0242630915	embedded with
0.0242510028	a benchmark
0.0242407443	overall accuracy of
0.0242180854	the 3d shape
0.0242133915	novel deep
0.0242005488	images from different
0.0241783994	a certain
0.0241738555	the novel coronavirus
0.0241531095	fields from
0.0241502443	formulation for
0.0241268481	difficulty in
0.0241064077	to focus on
0.0240718963	necessary to
0.0240591083	the first two
0.0240429179	the convergence of
0.0240297208	found to
0.0240264191	directions for
0.0239979755	to offer
0.0239683428	the art methods for
0.0239303614	detection through
0.0239123136	the selection of
0.0238950096	\ \
0.0238704407	a nonlinear
0.0238467864	scans of
0.0238185704	allow for
0.0238095845	a need for
0.0237924448	an alternative to
0.0237764191	exploited in
0.0237741108	the user's
0.0237555305	a result of
0.0237540844	a machine
0.0237507706	the possibility to
0.0236955530	an ablation
0.0236745301	perspective of
0.0236539226	results on two
0.0236338063	scenes from
0.0236254230	variance of
0.0236152460	by increasing
0.0236046879	designed by
0.0235995213	the notion
0.0235760259	the detection of covid 19
0.0235638827	possibility to
0.0235254230	probability of
0.0235174373	an effective way to
0.0235174373	better understanding of
0.0235059631	the fastmri
0.0234581787	tuned on
0.0234457335	appear in
0.0233900104	investigated for
0.0233770516	for measuring
0.0233343666	a new data
0.0233187204	for studying
0.0232877527	the appearance of
0.0232829210	a review of
0.0232429440	a new model
0.0232170006	to perform well
0.0232169348	a fraction of
0.0231860556	many computer
0.0231766679	in response to
0.0231699217	to give
0.0231618458	first time
0.0231342416	details from
0.0231310748	different acquisition
0.0231126765	designed as
0.0230806501	the applicability of
0.0230429179	the calculation of
0.0229892709	both psnr and
0.0229829210	the idea of
0.0229740001	consumption of
0.0229495877	an algorithm for
0.0229320845	the amount of data
0.0229311806	on in vivo
0.0228840344	mainly due to
0.0228775994	the inclusion of
0.0228693578	interface for
0.0228655835	the incorporation of
0.0228655835	the notion of
0.0228579002	for training and testing
0.0228512894	interest in
0.0228299824	the spread of
0.0227943478	to see
0.0227764191	technologies for
0.0227724217	correct for
0.0227717829	burden of
0.0227338125	the power of
0.0227261570	applications due to
0.0226989168	the dimensionality of
0.0226692660	this results in
0.0226322501	this issue by
0.0225813306	most state of
0.0225671458	possible to
0.0225544728	capability to
0.0225510860	the requirement of
0.0225384766	then applied to
0.0225232065	free from
0.0225204565	to take advantage of
0.0225051432	to adapt to
0.0224983201	the unavailability of
0.0224966490	the benefit of
0.0224349262	the characteristics of
0.0224344800	synthesis from
0.0224316534	the gold standard for
0.0224316534	then used as
0.0224008393	validity of
0.0223775994	this paper aims to
0.0223671723	an end
0.0223610860	the sum of
0.0223074442	the retrieved
0.0223028723	the area of
0.0222957036	the difficulty
0.0222950611	of interests
0.0222448627	the creation of
0.0222448627	the extent of
0.0222177527	instead of using
0.0221837387	function for
0.0221795560	new perspective
0.0221788129	both quantitatively and
0.0221750361	various datasets
0.0221271508	the abdomen
0.0221064077	the integration of
0.0220983201	an application to
0.0220429179	the evolution of
0.0220429179	the diversity of
0.0220354145	seek to
0.0219989888	accuracy while
0.0219719632	a framework for
0.0219601814	discrimination of
0.0219416629	explanation of
0.0218937951	the position of
0.0218925981	used by
0.0218811467	other applications
0.0218800662	the mean absolute
0.0218762199	by using only
0.0218710726	the encoder and decoder
0.0218575454	the goal
0.0218168697	information such as
0.0217909385	an algorithm based on
0.0217908693	sample from
0.0217221808	to separate
0.0217005512	even with
0.0216871232	the progression of
0.0216860556	various computer
0.0216722036	the merits of
0.0216502681	to generate new
0.0216251915	considered in
0.0216168295	to noise
0.0216123159	available on
0.0215169348	an application of
0.0215077671	testing on
0.0215017779	relevance of
0.0214937951	the human visual system
0.0214646050	the need
0.0214596886	better visual
0.0214176657	both local and
0.0213717571	3d medical
0.0213458016	a vital
0.0213236774	an optimization
0.0213226328	using data from
0.0213107222	the feasibility
0.0213052966	the potential for
0.0212101814	prognosis of
0.0211867297	work aims to
0.0211789802	the area under
0.0211750361	better accuracy
0.0211656811	first attempt to
0.0211495877	the introduction of
0.0211419951	the superior performance of
0.0211271508	for recovering
0.0211249262	the capability of
0.0211101373	outside of
0.0210983201	an increase in
0.0210881331	often used
0.0209875994	a new method for
0.0209634766	the usefulness of
0.0209634766	the construction of
0.0209299858	an order of
0.0209242183	a part of
0.0209227735	cause of
0.0209161890	a predictive
0.0208811467	better reconstruction
0.0208718099	a factor of
0.0208671386	to correct
0.0208348821	each time
0.0208247743	contrast to
0.0207996216	commonly used to
0.0206718068	the definition of
0.0206281674	to account
0.0205890870	a closed
0.0205428866	a novel framework for
0.0205095845	the depth of
0.0205051401	the variance of
0.0204903305	a novel deep neural
0.0204876058	for future work
0.0204640181	dimensionality of
0.0204581787	limitation by
0.0204365336	the capability to
0.0203734693	taken with
0.0203490777	used in many
0.0203057871	a novel self
0.0203026209	combination with
0.0202101814	effort in
0.0201953291	not well
0.0201916629	flexibility of
0.0201692660	a mixture of
0.0201557455	done in
0.0201271508	a massive
0.0201268481	encoded in
0.0201155023	many different
0.0201064077	in patients with
0.0200983201	the rise of
0.0200384766	an approach to
0.0200319878	the best
0.0200051401	the robustness and
0.0200022273	patches from
0.0200021508	for obtaining
0.0199064943	implemented with
0.0198718099	then used to
0.0198718099	the recent success of
0.0198579002	the use of deep learning
0.0198252432	constructed to
0.0198063023	whole image
0.0197636372	validated with
0.0197237521	captured with
0.0197095845	and robustness of
0.0197036527	a series
0.0196860556	not consider
0.0196813337	3d reconstruction of
0.0196699217	to do
0.0196405352	used in clinical
0.0196165593	to state of
0.0196130915	jointly with
0.0196040616	not able
0.0196033345	and specificity of
0.0196033345	the generator to
0.0195666746	to outperform
0.0195442660	the outbreak of
0.0194892852	problem due to
0.0194830808	of interest in
0.0194797649	the implementation of
0.0194517779	factor for
0.0194316534	the visual quality of
0.0194124928	online at
0.0193727223	not take
0.0193726185	task due to
0.0193626140	both quantitative and
0.0192790444	feasible to
0.0192661145	both spatial and
0.0192339203	a better understanding
0.0192169348	the principle of
0.0190983201	a new approach for
0.0190930656	method allows
0.0190852444	factor of
0.0190829179	the management of
0.0190338846	found in
0.0190209327	a limited number of
0.0190112627	a simple but
0.0189970845	this paper provides
0.0189344316	widely used to
0.0189113846	to consider
0.0188655835	the relevance of
0.0188655835	the flexibility of
0.0188542660	more robust to
0.0188233396	location of
0.0188231493	way to
0.0188128539	a plug
0.0188028723	the risk of
0.0188025994	to correct for
0.0187983201	a new framework for
0.0187956122	the role
0.0187473168	the limitations of
0.0187000080	a semi
0.0186923480	then applied
0.0186923480	several recent
0.0186718099	a new approach to
0.0186149420	the first attempt
0.0185889563	a base
0.0185688219	output of
0.0185604617	the recovery of
0.0185095845	the behavior of
0.0184388702	a novel method to
0.0184388702	a generalization of
0.0184316534	an important task in
0.0184115294	the strength of
0.0183651401	a solution to
0.0183604617	the volume of
0.0183548692	a significant improvement in
0.0183280635	an analysis of
0.0183130982	the identification of
0.0183012993	a stack
0.0183012937	off between
0.0182790570	the input of
0.0182020110	the majority
0.0181869562	speed of
0.0181709685	group of
0.0181576213	a parametric
0.0181123904	the properties of
0.0180983201	a new algorithm for
0.0180678356	the absence
0.0180640181	difficulty of
0.0180602435	content from
0.0180442660	to achieve better
0.0180384766	the emergence of
0.0180215358	a mapping between
0.0179661212	explored in
0.0179517779	helps to
0.0179451094	reported in
0.0179295560	most popular
0.0179053021	results in terms of
0.0178868176	method over
0.0178693578	literature on
0.0178537898	the difficulty in
0.0178078439	a test set of
0.0177575517	the improvement of
0.0177169768	distillation for
0.0176956099	the ability
0.0176415593	an image to
0.0176271508	to formulate
0.0176062205	a study of
0.0175885523	not able to
0.0175146670	a novel approach to
0.0174666372	both synthetic and
0.0174417037	found on
0.0173480004	a collection of
0.0173480004	the probability of
0.0172930219	each video
0.0172109327	a large dataset of
0.0171250732	the field of machine
0.0171184549	but also to
0.0171150680	new method
0.0170853473	a novel algorithm
0.0170604617	the deployment of
0.0170534653	investigated in
0.0170442660	a family of
0.0170346224	and sometimes
0.0170338846	useful in
0.0170204565	a variant of
0.0170204565	a study on
0.0169966490	the perspective of
0.0169917075	a refined
0.0169894713	the novelty
0.0169635182	sufficient to
0.0169562247	a pretrained
0.0169322501	to aid in
0.0169095560	both visually
0.0168940815	and up to
0.0168765756	the left
0.0168627753	computer vision tasks such
0.0168542660	to generalize to
0.0168488426	topology of
0.0168331340	an error
0.0168100217	basis of
0.0168052966	a comparison of
0.0167627899	to employ
0.0166797823	a smartphone
0.0166745301	defined in
0.0166346326	the rise
0.0165823023	non imaging
0.0165152463	and testing on
0.0165024385	the generalizability of
0.0164706509	for in vivo
0.0164106082	aim of
0.0162924860	first step in
0.0162718068	the source code of
0.0162385629	to describe
0.0162373517	from chest
0.0162287620	expensive to
0.0162197018	the development of deep
0.0162121114	allows to
0.0161977357	search for
0.0161912017	the nature of
0.0161877997	the concept
0.0161789802	a robust and
0.0159994498	the problem as
0.0159291692	the first to
0.0159095560	both quantitatively
0.0158144651	a novel 3d
0.0158077594	to include
0.0158028723	the choice of
0.0157974927	principle of
0.0157047542	a recurrent
0.0156937951	the generalization of
0.0156322501	the popularity of
0.0155781961	an approach for
0.0155534386	many real
0.0155257978	approach does
0.0153728070	the quantification of
0.0152479063	a novel convolutional neural
0.0152168308	as input to
0.0151604617	the interpretation of
0.0151474182	all over
0.0151123904	the proposed method on
0.0150566061	a statistical
0.0150442660	a challenge for
0.0150442660	a cohort of
0.0150170245	on images from
0.0149384734	to serve as
0.0149242183	the uncertainty in
0.0148429014	new framework
0.0146983260	for guiding
0.0146944194	the limitation of
0.0146858577	estimation from
0.0146836912	of research in
0.0146633157	in case of
0.0146474434	novel method for
0.0146415856	available dataset
0.0145497474	and efficiency of
0.0144762512	a form of
0.0144762512	to assist in
0.0143536229	presented to
0.0143288525	a region of
0.0142988233	both in terms of
0.0142370130	a novel neural network
0.0141678768	enough to
0.0141322381	both qualitatively and
0.0141247937	a relatively
0.0141184549	for reconstruction of
0.0141060510	appropriate for
0.0140766883	works in
0.0140747556	for diagnosis of
0.0140636473	the same model
0.0140503340	a new method to
0.0140221240	the region of
0.0139994498	the capacity of
0.0139291692	the understanding of
0.0139291692	the representation of
0.0138271532	the existence
0.0138239183	this type
0.0137258193	to learn more
0.0137155871	the inference time
0.0137039909	an order
0.0137036912	a coarse to
0.0136841542	3d brain
0.0136819129	of view of
0.0136624679	also able to
0.0136436673	on data from
0.0135810711	many cases
0.0135787570	the outbreak
0.0135662864	a neural network to
0.0135610711	both simulated
0.0135442050	a novel convolutional
0.0135135040	a tool for
0.0134944019	costly to
0.0134734622	of objects in
0.0133856069	the rate of
0.0133584442	the input to
0.0133503579	using deep learning for
0.0133422298	an image from
0.0132551340	for calculating
0.0129994498	the speed of
0.0129620928	a challenging and
0.0129384734	the demand for
0.0129384734	in terms of accuracy and
0.0129291692	a model for
0.0128929769	a framework to
0.0128271532	to serve
0.0127804758	a resource
0.0127344757	both visual
0.0126063102	for identification of
0.0125974233	the 3d u
0.0124620928	the imaging system
0.0124484622	the reduction of
0.0124180094	a new deep
0.0123623511	the modeling of
0.0123422298	the variability of
0.0122679063	for 3d point
0.0122479063	for 3d object
0.0121954664	a combination
0.0121887355	and treatment of
0.0121474182	use only
0.0120443390	a bottleneck
0.0118265772	for dealing
0.0118183676	the efficacy
0.0117989943	a voxel
0.0116363309	the european
0.0115995897	both psnr
0.0115830808	in computer vision and
0.0113422298	a dataset with
0.0112891074	the knowledge of
0.0112891074	the model with
0.0112072540	the distance between
0.0111310748	the isic
0.0110729530	novel approach for
0.0106418395	the overall accuracy
0.0106104988	an increase
0.0105810711	new type
0.0105589048	novel approach to
0.0105539842	the diagnosis and treatment
0.0105477299	for classification of
0.0105315156	better performance in
0.0103093156	the application of deep
0.0103048728	for detection of
0.0102864248	to other methods
0.0102183419	novel technique
0.0101623248	the inclusion
0.0101310748	to observe
0.0101302587	used to model
0.0098541393	an ill
0.0098329530	an algorithm to
0.0097551144	system based on
0.0095405874	a method based on
0.0095135040	and quantification of
0.0092063102	an accurate and
0.0091623248	to contribute
0.0091214088	possible by
0.0088627241	a technique to
0.0088572652	new approach for
0.0085506310	novel framework for
0.0085315156	the similarity between
0.0083833136	to contain
0.0082479063	the number of training
0.0082199182	in applications such as
0.0082072540	the capabilities of
0.0082063102	this method to
0.0081117122	seen in
0.0078329530	with application to
0.0075997381	novel method to
0.0075974233	a proof of
0.0074413231	help to
0.0071928912	for patients with
0.0068751241	a new state
0.0068644067	the signal to
0.0062003645	the highest
0.0061545779	to focus
0.0060679063	the success of deep
0.0056681145	interest from
